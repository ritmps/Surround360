	.text
	.file	"sharpi"
	.section	.text._ZN6Halide7Runtime8Internal14default_mallocEPvm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal14default_mallocEPvm,@function
_ZN6Halide7Runtime8Internal14default_mallocEPvm: # @_ZN6Halide7Runtime8Internal14default_mallocEPvm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$-128, %rsi
	movq	%rsi, %rdi
	callq	malloc@PLT
	movq	%rax, %rcx
	xorl	%eax, %eax
	testq	%rcx, %rcx
	je	.LBB0_2
# BB#1:                                 # %if.end
	movq	%rcx, %rax
	addq	$135, %rax
	andq	$-128, %rax
	movq	%rcx, -8(%rax)
.LBB0_2:                                # %cleanup
	popq	%rbp
	retq
.Lfunc_end0:
	.size	_ZN6Halide7Runtime8Internal14default_mallocEPvm, .Lfunc_end0-_ZN6Halide7Runtime8Internal14default_mallocEPvm

	.section	.text._ZN6Halide7Runtime8Internal12default_freeEPvS2_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12default_freeEPvS2_,@function
_ZN6Halide7Runtime8Internal12default_freeEPvS2_: # @_ZN6Halide7Runtime8Internal12default_freeEPvS2_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	-8(%rsi), %rdi
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end1:
	.size	_ZN6Halide7Runtime8Internal12default_freeEPvS2_, .Lfunc_end1-_ZN6Halide7Runtime8Internal12default_freeEPvS2_

	.section	.text.halide_set_custom_malloc,"ax",@progbits
	.weak	halide_set_custom_malloc
	.align	16, 0x90
	.type	halide_set_custom_malloc,@function
halide_set_custom_malloc:               # @halide_set_custom_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end2:
	.size	halide_set_custom_malloc, .Lfunc_end2-halide_set_custom_malloc

	.section	.text.halide_set_custom_free,"ax",@progbits
	.weak	halide_set_custom_free
	.align	16, 0x90
	.type	halide_set_custom_free,@function
halide_set_custom_free:                 # @halide_set_custom_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end3:
	.size	halide_set_custom_free, .Lfunc_end3-halide_set_custom_free

	.section	.text.halide_malloc,"ax",@progbits
	.weak	halide_malloc
	.align	16, 0x90
	.type	halide_malloc,@function
halide_malloc:                          # @halide_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end4:
	.size	halide_malloc, .Lfunc_end4-halide_malloc

	.section	.text.halide_free,"ax",@progbits
	.weak	halide_free
	.align	16, 0x90
	.type	halide_free,@function
halide_free:                            # @halide_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end5:
	.size	halide_free, .Lfunc_end5-halide_free

	.section	.text._ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,@function
_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc: # @_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$4096, %rsp             # imm = 0x1000
	movq	%rsi, %rbx
	movq	%rdi, %r15
	leaq	-34(%rbp), %r12
	leaq	.L.str(%rip), %rdx
	leaq	-4128(%rbp), %r14
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movzbl	-1(%rax), %ecx
	cmpl	$10, %ecx
	je	.LBB6_2
# BB#1:                                 # %if.then
	movw	$10, (%rax)
	addq	$1, %rax
.LBB6_2:                                # %if.end
	movl	$1, %edx
	subq	%r14, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	addq	$4096, %rsp             # imm = 0x1000
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end6:
	.size	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc, .Lfunc_end6-_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc

	.section	.text.halide_error,"ax",@progbits
	.weak	halide_error
	.align	16, 0x90
	.type	halide_error,@function
halide_error:                           # @halide_error
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end7:
	.size	halide_error, .Lfunc_end7-halide_error

	.section	.text.halide_set_error_handler,"ax",@progbits
	.weak	halide_set_error_handler
	.align	16, 0x90
	.type	halide_set_error_handler,@function
halide_set_error_handler:               # @halide_set_error_handler
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end8:
	.size	halide_set_error_handler, .Lfunc_end8-halide_set_error_handler

	.section	.text.halide_print,"ax",@progbits
	.weak	halide_print
	.align	16, 0x90
	.type	halide_print,@function
halide_print:                           # @halide_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end9:
	.size	halide_print, .Lfunc_end9-halide_print

	.section	.text.halide_set_custom_print,"ax",@progbits
	.weak	halide_set_custom_print
	.align	16, 0x90
	.type	halide_set_custom_print,@function
halide_set_custom_print:                # @halide_set_custom_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end10:
	.size	halide_set_custom_print, .Lfunc_end10-halide_set_custom_print

	.section	.text.halide_start_clock,"ax",@progbits
	.weak	halide_start_clock
	.align	16, 0x90
	.type	halide_start_clock,@function
halide_start_clock:                     # @halide_start_clock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	halide_reference_clock_inited@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	jne	.LBB11_2
# BB#1:                                 # %if.then
	movq	halide_reference_clock@GOTPCREL(%rip), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movb	$1, (%rbx)
.LBB11_2:                               # %if.end
	xorl	%eax, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end11:
	.size	halide_start_clock, .Lfunc_end11-halide_start_clock

	.section	.text.halide_current_time_ns,"ax",@progbits
	.weak	halide_current_time_ns
	.align	16, 0x90
	.type	halide_current_time_ns,@function
halide_current_time_ns:                 # @halide_current_time_ns
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	leaq	-16(%rbp), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movq	halide_reference_clock@GOTPCREL(%rip), %rcx
	movq	-16(%rbp), %rdx
	movq	-8(%rbp), %rax
	subq	(%rcx), %rdx
	imulq	$1000000000, %rdx, %rdx # imm = 0x3B9ACA00
	subq	8(%rcx), %rax
	addq	%rdx, %rax
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end12:
	.size	halide_current_time_ns, .Lfunc_end12-halide_current_time_ns

	.section	.text.halide_sleep_ms,"ax",@progbits
	.weak	halide_sleep_ms
	.align	16, 0x90
	.type	halide_sleep_ms,@function
halide_sleep_ms:                        # @halide_sleep_ms
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	imull	$1000, %esi, %edi       # imm = 0x3E8
	popq	%rbp
	jmp	usleep@PLT              # TAILCALL
.Lfunc_end13:
	.size	halide_sleep_ms, .Lfunc_end13-halide_sleep_ms

	.section	.text._ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,@function
_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc: # @_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rbx, %rdi
	callq	strlen@PLT
	movl	$2, %edi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	write@PLT               # TAILCALL
.Lfunc_end14:
	.size	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc, .Lfunc_end14-_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc

	.section	.text.halide_create_temp_file,"ax",@progbits
	.weak	halide_create_temp_file
	.align	16, 0x90
	.type	halide_create_temp_file,@function
halide_create_temp_file:                # @halide_create_temp_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%r8, %r13
	movq	%rdx, %r12
	movq	%rsi, %rbx
	movl	$-22, %edx
	testq	%rbx, %rbx
	je	.LBB15_7
# BB#1:                                 # %entry
	testq	%r12, %r12
	je	.LBB15_7
# BB#2:                                 # %entry
	testq	%rcx, %rcx
	je	.LBB15_7
# BB#3:                                 # %if.end
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	leaq	.L.str.7(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	%rbx, %rdi
	callq	strlen@PLT
	movq	%rax, %r14
	leaq	.L.str.1(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, %r15
	movq	%r12, %rdi
	callq	strlen@PLT
	addq	-48(%rbp), %r14         # 8-byte Folded Reload
	addq	%r15, %r14
	leaq	1(%rax,%r14), %rax
	cmpq	%r13, %rax
	jbe	.LBB15_5
# BB#4:
	movl	$-22, %edx
	jmp	.LBB15_7
.LBB15_5:                               # %if.end.11
	movq	-56(%rbp), %r15         # 8-byte Reload
	leaq	-1(%r15,%r13), %r14
	leaq	.L.str.7(%rip), %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movb	$0, (%rax)
	movq	%r12, %rdi
	callq	strlen@PLT
	movq	%r15, %rdi
	movl	%eax, %esi
	callq	mkstemps@PLT
	cmpl	$-1, %eax
	movl	$-22, %edx
	je	.LBB15_7
# BB#6:                                 # %if.end.21
	movl	%eax, %edi
	callq	close@PLT
	xorl	%edx, %edx
.LBB15_7:                               # %return
	movl	%edx, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end15:
	.size	halide_create_temp_file, .Lfunc_end15-halide_create_temp_file

	.section	.text.halide_host_cpu_count,"ax",@progbits
	.weak	halide_host_cpu_count
	.align	16, 0x90
	.type	halide_host_cpu_count,@function
halide_host_cpu_count:                  # @halide_host_cpu_count
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$84, %edi
	popq	%rbp
	jmp	sysconf@PLT             # TAILCALL
.Lfunc_end16:
	.size	halide_host_cpu_count, .Lfunc_end16-halide_host_cpu_count

	.section	.text._ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,@function
_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv: # @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	movq	8(%rax), %rdi
	callq	*(%rax)
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end17:
	.size	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv, .Lfunc_end17-_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv

	.section	.text.halide_spawn_thread,"ax",@progbits
	.weak	halide_spawn_thread
	.align	16, 0x90
	.type	halide_spawn_thread,@function
halide_spawn_thread:                    # @halide_spawn_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %r15
	movl	$24, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	movq	%r15, (%rbx)
	movq	%r14, 8(%rbx)
	leaq	16(%rbx), %rdi
	movq	$0, 16(%rbx)
	movq	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv@GOTPCREL(%rip), %rdx
	xorl	%esi, %esi
	movq	%rbx, %rcx
	callq	pthread_create@PLT
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end18:
	.size	halide_spawn_thread, .Lfunc_end18-halide_spawn_thread

	.section	.text.halide_join_thread,"ax",@progbits
	.weak	halide_join_thread
	.align	16, 0x90
	.type	halide_join_thread,@function
halide_join_thread:                     # @halide_join_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	$0, -16(%rbp)
	movq	16(%rbx), %rdi
	leaq	-16(%rbp), %rsi
	callq	pthread_join@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end19:
	.size	halide_join_thread, .Lfunc_end19-halide_join_thread

	.section	.text.halide_mutex_lock,"ax",@progbits
	.weak	halide_mutex_lock
	.align	16, 0x90
	.type	halide_mutex_lock,@function
halide_mutex_lock:                      # @halide_mutex_lock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_lock@PLT  # TAILCALL
.Lfunc_end20:
	.size	halide_mutex_lock, .Lfunc_end20-halide_mutex_lock

	.section	.text.halide_mutex_unlock,"ax",@progbits
	.weak	halide_mutex_unlock
	.align	16, 0x90
	.type	halide_mutex_unlock,@function
halide_mutex_unlock:                    # @halide_mutex_unlock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_unlock@PLT # TAILCALL
.Lfunc_end21:
	.size	halide_mutex_unlock, .Lfunc_end21-halide_mutex_unlock

	.section	.text.halide_mutex_destroy,"ax",@progbits
	.weak	halide_mutex_destroy
	.align	16, 0x90
	.type	halide_mutex_destroy,@function
halide_mutex_destroy:                   # @halide_mutex_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	pthread_mutex_destroy@PLT
	xorl	%esi, %esi
	movl	$64, %edx
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	memset@PLT              # TAILCALL
.Lfunc_end22:
	.size	halide_mutex_destroy, .Lfunc_end22-halide_mutex_destroy

	.section	.text.halide_cond_init,"ax",@progbits
	.weak	halide_cond_init
	.align	16, 0x90
	.type	halide_cond_init,@function
halide_cond_init:                       # @halide_cond_init
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%esi, %esi
	popq	%rbp
	jmp	pthread_cond_init@PLT   # TAILCALL
.Lfunc_end23:
	.size	halide_cond_init, .Lfunc_end23-halide_cond_init

	.section	.text.halide_cond_destroy,"ax",@progbits
	.weak	halide_cond_destroy
	.align	16, 0x90
	.type	halide_cond_destroy,@function
halide_cond_destroy:                    # @halide_cond_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_destroy@PLT # TAILCALL
.Lfunc_end24:
	.size	halide_cond_destroy, .Lfunc_end24-halide_cond_destroy

	.section	.text.halide_cond_broadcast,"ax",@progbits
	.weak	halide_cond_broadcast
	.align	16, 0x90
	.type	halide_cond_broadcast,@function
halide_cond_broadcast:                  # @halide_cond_broadcast
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_broadcast@PLT # TAILCALL
.Lfunc_end25:
	.size	halide_cond_broadcast, .Lfunc_end25-halide_cond_broadcast

	.section	.text.halide_cond_wait,"ax",@progbits
	.weak	halide_cond_wait
	.align	16, 0x90
	.type	halide_cond_wait,@function
halide_cond_wait:                       # @halide_cond_wait
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_wait@PLT   # TAILCALL
.Lfunc_end26:
	.size	halide_cond_wait, .Lfunc_end26-halide_cond_wait

	.section	.text._ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,@function
_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_: # @_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rsi, %rax
	movl	%edx, %esi
	movq	%rcx, %rdx
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end27:
	.size	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_, .Lfunc_end27-_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_

	.section	.text._ZN6Halide7Runtime8Internal17clamp_num_threadsEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi,@function
_ZN6Halide7Runtime8Internal17clamp_num_threadsEi: # @_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpl	$64, %edi
	jle	.LBB28_1
# BB#2:                                 # %if.end.3
	movl	$64, %eax
	popq	%rbp
	retq
.LBB28_1:                               # %if.else
	testl	%edi, %edi
	movl	$1, %eax
	cmovgl	%edi, %eax
	popq	%rbp
	retq
.Lfunc_end28:
	.size	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi, .Lfunc_end28-_ZN6Halide7Runtime8Internal17clamp_num_threadsEi

	.section	.text._ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,@function
_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv: # @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.8(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	jne	.LBB29_2
# BB#1:                                 # %if.end
	leaq	.L.str.1.9(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB29_3
.LBB29_2:                               # %if.then.3
	movq	%rax, %rdi
	popq	%rbp
	jmp	atoi@PLT                # TAILCALL
.LBB29_3:                               # %if.else
	popq	%rbp
	jmp	halide_host_cpu_count@PLT # TAILCALL
.Lfunc_end29:
	.size	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv, .Lfunc_end29-_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv

	.section	.text._ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,@function
_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE: # @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdi, %rbx
	movq	%rbx, -56(%rbp)         # 8-byte Spill
	testq	%rbx, %rbx
	je	.LBB30_13
# BB#1:
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	leaq	80(%r13), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	jmp	.LBB30_2
	.align	16, 0x90
.LBB30_28:                              # %if.then.3.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
.LBB30_2:                               # %cond.true.us
                                        # =>This Inner Loop Header: Depth=1
	movl	24(%rbx), %eax
	cmpl	28(%rbx), %eax
	jl	.LBB30_4
# BB#3:                                 # %cond.end.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$0, 40(%rbx)
	jle	.LBB30_20
.LBB30_4:                               # %while.body.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	64(%r13), %r12
	testq	%r12, %r12
	je	.LBB30_28
# BB#5:                                 # %if.else.8.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	8(%r12), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r12), %r15
	movq	24(%r12), %rbx
	movq	32(%r12), %r14
	leal	1(%rbx), %eax
	movl	%eax, 24(%r12)
	movq	%rbx, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_7
# BB#6:                                 # %if.then.12.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	(%r12), %rax
	movq	%rax, 64(%r13)
.LBB30_7:                               # %if.end.13.us
                                        #   in Loop: Header=BB30_2 Depth=1
	incl	40(%r12)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%ebx, %edx
	movq	%r14, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %ebx
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	je	.LBB30_9
# BB#8:                                 # %if.then.18.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	%ebx, 44(%r12)
.LBB30_9:                               # %if.end.19.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	40(%r12), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r12)
	movl	24(%r12), %ecx
	cmpl	28(%r12), %ecx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	jl	.LBB30_2
# BB#10:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpq	%rbx, %r12
	je	.LBB30_2
# BB#11:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$1, %eax
	jg	.LBB30_2
# BB#12:                                # %if.then.24.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_2
.LBB30_13:                              # %cond.false.preheader
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	cmpb	$0, 792(%rbx)
	jne	.LBB30_20
# BB#14:
	leaq	208(%rbx), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	leaq	144(%rbx), %rax
	movq	%rax, -72(%rbp)         # 8-byte Spill
	leaq	80(%rbx), %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB30_15:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	64(%rbx), %r13
	testq	%r13, %r13
	je	.LBB30_16
# BB#21:                                # %if.else.8
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	8(%r13), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r13), %r15
	movq	24(%r13), %r14
	movq	32(%r13), %r12
	leal	1(%r14), %eax
	movl	%eax, 24(%r13)
	movq	%r14, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_23
# BB#22:                                # %if.then.12
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	(%r13), %rax
	movq	%rax, 64(%rbx)
.LBB30_23:                              # %if.end.13
                                        #   in Loop: Header=BB30_15 Depth=1
	incl	40(%r13)
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%r14d, %edx
	movq	%r12, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %r14d
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	testl	%r14d, %r14d
	je	.LBB30_25
# BB#24:                                # %if.then.18
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	%r14d, 44(%r13)
.LBB30_25:                              # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	40(%r13), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r13)
	cmpl	$1, %eax
	jg	.LBB30_19
# BB#26:                                # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	28(%r13), %eax
	cmpl	%eax, 24(%r13)
	jl	.LBB30_19
# BB#27:                                # %if.then.24
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-56(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_19
	.align	16, 0x90
.LBB30_16:                              # %if.else
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	72(%rbx), %rax
	movq	%rax, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jle	.LBB30_17
# BB#18:                                # %if.else.6
                                        #   in Loop: Header=BB30_15 Depth=1
	leal	-1(%rax), %eax
	movl	%eax, 72(%rbx)
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	incl	72(%rbx)
	jmp	.LBB30_19
.LBB30_17:                              # %if.then.5
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-72(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	.align	16, 0x90
.LBB30_19:                              # %cond.false.backedge
                                        #   in Loop: Header=BB30_15 Depth=1
	cmpb	$0, 792(%rbx)
	je	.LBB30_15
.LBB30_20:                              # %while.end
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end30:
	.size	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE, .Lfunc_end30-_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE

	.section	.text.halide_do_task,"ax",@progbits
	.weak	halide_do_task
	.align	16, 0x90
	.type	halide_do_task,@function
halide_do_task:                         # @halide_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end31:
	.size	halide_do_task, .Lfunc_end31-halide_do_task

	.section	.text._ZN6Halide7Runtime8Internal13worker_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13worker_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13worker_threadEPv,@function
_ZN6Halide7Runtime8Internal13worker_threadEPv: # @_ZN6Halide7Runtime8Internal13worker_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	xorl	%edi, %edi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end32:
	.size	_ZN6Halide7Runtime8Internal13worker_threadEPv, .Lfunc_end32-_ZN6Halide7Runtime8Internal13worker_threadEPv

	.section	.text._ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,@function
_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_: # @_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r8, -96(%rbp)          # 8-byte Spill
	movl	%ecx, %r14d
	movl	%edx, %r12d
	movq	%rsi, -104(%rbp)        # 8-byte Spill
	movq	%rdi, %r13
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 793(%rbx)
	je	.LBB33_2
# BB#1:                                 # %entry.while.cond.preheader_crit_edge
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	jmp	.LBB33_5
.LBB33_2:                               # %if.then
	movb	$0, 792(%rbx)
	leaq	80(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	144(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	208(%rbx), %rdi
	callq	halide_cond_init@PLT
	movq	$0, 64(%rbx)
	movl	788(%rbx), %edi
	testl	%edi, %edi
	jne	.LBB33_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %edi
	movl	%edi, 788(%rbx)
.LBB33_4:                               # %if.end
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%rbx)
	movl	$0, 784(%rbx)
	movl	%eax, 72(%rbx)
	movb	$1, 793(%rbx)
	xorl	%ecx, %ecx
.LBB33_5:                               # %while.cond.preheader
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jge	.LBB33_8
# BB#6:
	movq	_ZN6Halide7Runtime8Internal13worker_threadEPv@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB33_7:                               # %while.body
                                        # =>This Inner Loop Header: Depth=1
	xorl	%esi, %esi
	movq	%r15, %rdi
	callq	halide_spawn_thread@PLT
	movslq	784(%rbx), %rcx
	leal	1(%rcx), %edx
	movl	%edx, 784(%rbx)
	movq	%rax, 272(%rbx,%rcx,8)
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jl	.LBB33_7
.LBB33_8:                               # %while.end
	movq	-104(%rbp), %rcx        # 8-byte Reload
	movq	%rcx, -80(%rbp)
	movq	%r13, -72(%rbp)
	movl	%r12d, -64(%rbp)
	leal	(%r12,%r14), %ecx
	movl	%ecx, -60(%rbp)
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	%rcx, -56(%rbp)
	movq	$0, -48(%rbp)
	movq	64(%rbx), %rcx
	testq	%rcx, %rcx
	movl	%eax, %edx
	cmovel	%r14d, %edx
	cmpl	%r14d, %eax
	cmovlel	%eax, %edx
	movl	%edx, 76(%rbx)
	movq	%rcx, -88(%rbp)
	leaq	-88(%rbp), %rax
	movq	%rax, 64(%rbx)
	leaq	144(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
	movl	76(%rbx), %eax
	cmpl	72(%rbx), %eax
	jle	.LBB33_10
# BB#9:                                 # %if.then.14
	movl	$208, %edi
	addq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_cond_broadcast@PLT
.LBB33_10:                              # %if.end.15
	leaq	-88(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	-44(%rbp), %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end33:
	.size	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_, .Lfunc_end33-_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_

	.section	.text.halide_set_num_threads,"ax",@progbits
	.weak	halide_set_num_threads
	.align	16, 0x90
	.type	halide_set_num_threads,@function
halide_set_num_threads:                 # @halide_set_num_threads
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movl	%edi, %ebx
	testl	%ebx, %ebx
	js	.LBB34_1
# BB#2:                                 # %if.end
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	jne	.LBB34_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %ebx
	jmp	.LBB34_4
.LBB34_1:                               # %if.end.thread
	leaq	.L.str.2(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
.LBB34_4:                               # %if.end.3
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movl	788(%r14), %r15d
	movl	%ebx, %edi
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%r14)
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r15d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end34:
	.size	halide_set_num_threads, .Lfunc_end34-halide_set_num_threads

	.section	.text.halide_shutdown_thread_pool,"ax",@progbits
	.weak	halide_shutdown_thread_pool
	.align	16, 0x90
	.type	halide_shutdown_thread_pool,@function
halide_shutdown_thread_pool:            # @halide_shutdown_thread_pool
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	cmpb	$0, 793(%r13)
	je	.LBB35_5
# BB#1:                                 # %if.end
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movb	$1, 792(%r13)
	leaq	80(%r13), %rdi
	movq	%rdi, -48(%rbp)         # 8-byte Spill
	callq	halide_cond_broadcast@PLT
	leaq	144(%r13), %r15
	movq	%r15, %rdi
	callq	halide_cond_broadcast@PLT
	leaq	208(%r13), %r12
	movq	%r12, %rdi
	callq	halide_cond_broadcast@PLT
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%ebx, %ebx
	cmpl	$0, 784(%r13)
	jle	.LBB35_4
# BB#2:
	leaq	272(%r13), %r14
	.align	16, 0x90
.LBB35_3:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rdi
	callq	halide_join_thread@PLT
	addq	$1, %rbx
	movslq	784(%r13), %rax
	addq	$8, %r14
	cmpq	%rax, %rbx
	jl	.LBB35_3
.LBB35_4:                               # %for.cond.cleanup
	movq	%r13, %rdi
	callq	halide_mutex_destroy@PLT
	movq	-48(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_destroy@PLT
	movq	%r15, %rdi
	callq	halide_cond_destroy@PLT
	movq	%r12, %rdi
	callq	halide_cond_destroy@PLT
	movb	$0, 793(%r13)
.LBB35_5:                               # %return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end35:
	.size	halide_shutdown_thread_pool, .Lfunc_end35-halide_shutdown_thread_pool

	.section	.text.halide_thread_pool_cleanup,"ax",@progbits
	.weak	halide_thread_pool_cleanup
	.align	16, 0x90
	.type	halide_thread_pool_cleanup,@function
halide_thread_pool_cleanup:             # @halide_thread_pool_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_thread_pool@PLT # TAILCALL
.Lfunc_end36:
	.size	halide_thread_pool_cleanup, .Lfunc_end36-halide_thread_pool_cleanup

	.section	.text.halide_set_custom_do_task,"ax",@progbits
	.weak	halide_set_custom_do_task
	.align	16, 0x90
	.type	halide_set_custom_do_task,@function
halide_set_custom_do_task:              # @halide_set_custom_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end37:
	.size	halide_set_custom_do_task, .Lfunc_end37-halide_set_custom_do_task

	.section	.text.halide_set_custom_do_par_for,"ax",@progbits
	.weak	halide_set_custom_do_par_for
	.align	16, 0x90
	.type	halide_set_custom_do_par_for,@function
halide_set_custom_do_par_for:           # @halide_set_custom_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end38:
	.size	halide_set_custom_do_par_for, .Lfunc_end38-halide_set_custom_do_par_for

	.section	.text.halide_do_par_for,"ax",@progbits
	.weak	halide_do_par_for
	.align	16, 0x90
	.type	halide_do_par_for,@function
halide_do_par_for:                      # @halide_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end39:
	.size	halide_do_par_for, .Lfunc_end39-halide_do_par_for

	.section	.text._ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,@function
_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc: # @_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end40:
	.size	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc, .Lfunc_end40-_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,@function
_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc: # @_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movl	$1, %esi
	callq	dlopen@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	jne	.LBB41_2
# BB#1:                                 # %if.then
	callq	dlerror@PLT
.LBB41_2:                               # %if.end
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end41:
	.size	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc, .Lfunc_end41-_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,@function
_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc: # @_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end42:
	.size	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc, .Lfunc_end42-_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc

	.section	.text.halide_set_custom_get_symbol,"ax",@progbits
	.weak	halide_set_custom_get_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_symbol,@function
halide_set_custom_get_symbol:           # @halide_set_custom_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end43:
	.size	halide_set_custom_get_symbol, .Lfunc_end43-halide_set_custom_get_symbol

	.section	.text.halide_set_custom_load_library,"ax",@progbits
	.weak	halide_set_custom_load_library
	.align	16, 0x90
	.type	halide_set_custom_load_library,@function
halide_set_custom_load_library:         # @halide_set_custom_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end44:
	.size	halide_set_custom_load_library, .Lfunc_end44-halide_set_custom_load_library

	.section	.text.halide_set_custom_get_library_symbol,"ax",@progbits
	.weak	halide_set_custom_get_library_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_library_symbol,@function
halide_set_custom_get_library_symbol:   # @halide_set_custom_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end45:
	.size	halide_set_custom_get_library_symbol, .Lfunc_end45-halide_set_custom_get_library_symbol

	.section	.text.halide_get_symbol,"ax",@progbits
	.weak	halide_get_symbol
	.align	16, 0x90
	.type	halide_get_symbol,@function
halide_get_symbol:                      # @halide_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end46:
	.size	halide_get_symbol, .Lfunc_end46-halide_get_symbol

	.section	.text.halide_load_library,"ax",@progbits
	.weak	halide_load_library
	.align	16, 0x90
	.type	halide_load_library,@function
halide_load_library:                    # @halide_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end47:
	.size	halide_load_library, .Lfunc_end47-halide_load_library

	.section	.text.halide_get_library_symbol,"ax",@progbits
	.weak	halide_get_library_symbol
	.align	16, 0x90
	.type	halide_get_library_symbol,@function
halide_get_library_symbol:              # @halide_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end48:
	.size	halide_get_library_symbol, .Lfunc_end48-halide_get_library_symbol

	.section	.text.halide_set_gpu_device,"ax",@progbits
	.weak	halide_set_gpu_device
	.align	16, 0x90
	.type	halide_set_gpu_device,@function
halide_set_gpu_device:                  # @halide_set_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end49:
	.size	halide_set_gpu_device, .Lfunc_end49-halide_set_gpu_device

	.section	.text.halide_get_gpu_device,"ax",@progbits
	.weak	halide_get_gpu_device
	.align	16, 0x90
	.type	halide_get_gpu_device,@function
halide_get_gpu_device:                  # @halide_get_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB50_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB50_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %r14
	cmpb	$0, (%r14)
	je	.LBB50_4
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit.if.end.4_crit_edge
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	jmp	.LBB50_7
.LBB50_4:                               # %if.then
	leaq	.L.str.10(%rip), %rdi
	callq	getenv@PLT
	movq	%rax, %rcx
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB50_6
# BB#5:                                 # %if.then.2
	movq	%rcx, %rdi
	callq	atoi@PLT
.LBB50_6:                               # %if.end
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rcx
	movl	%eax, (%rcx)
	movb	$1, (%r14)
.LBB50_7:                               # %if.end.4
	movl	$0, (%rbx)
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end50:
	.size	halide_get_gpu_device, .Lfunc_end50-halide_get_gpu_device

	.section	.text._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,@function
_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t: # @_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4168, %rsp             # imm = 0x1048
	movq	%rsi, %r12
	movq	%rdi, -4160(%rbp)       # 8-byte Spill
	movl	$1, %r14d
	lock		xaddl	%r14d, _ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids(%rip)
	callq	halide_get_trace_file@PLT
	testl	%eax, %eax
	jle	.LBB51_10
# BB#1:                                 # %if.then
	movl	%eax, %r15d
	movzwl	26(%r12), %eax
	movzbl	25(%r12), %r13d
	addl	$7, %r13d
	shrl	$3, %r13d
	imulq	%rax, %r13
	movl	40(%r12), %ebx
	leal	(,%rbx,4), %eax
	movl	%eax, -4192(%rbp)       # 4-byte Spill
	movq	(%r12), %rdi
	callq	strlen@PLT
	addq	$1, %rax
	movq	%rax, -4168(%rbp)       # 8-byte Spill
	leal	(%r13,%rbx,4), %ecx
	leal	28(%rax,%rcx), %edx
	movl	%edx, -4196(%rbp)       # 4-byte Spill
	leal	31(%rax,%rcx), %eax
	andl	$-4, %eax
	movl	%eax, -4152(%rbp)       # 4-byte Spill
	movl	%eax, -4144(%rbp)
	movl	%r14d, -4140(%rbp)
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	vmovupd	24(%r12), %xmm0
	vmovupd	%xmm0, -4136(%rbp)
	movl	%ebx, -4120(%rbp)
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %r14
	.align	16, 0x90
.LBB51_2:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%r14)
	testl	%eax, %eax
	jne	.LBB51_2
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	leaq	-4144(%rbp), %rsi
	movl	$28, %edx
	movl	%r15d, %ebx
	movl	%ebx, %edi
	callq	write@PLT
	movq	%rax, %r15
	movq	%r12, %rcx
	movq	16(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_5
# BB#4:                                 # %if.then.19
	movl	-4192(%rbp), %edx       # 4-byte Reload
	movq	%rcx, %r12
	movl	%ebx, %edi
	callq	write@PLT
	movq	%r12, %rcx
	addq	%rax, %r15
.LBB51_5:                               # %if.end
	movq	%r15, -4192(%rbp)       # 8-byte Spill
	movl	%ebx, %r15d
	movl	-4152(%rbp), %r12d      # 4-byte Reload
	subl	-4196(%rbp), %r12d      # 4-byte Folded Reload
	movq	8(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_6
# BB#7:                                 # %if.then.25
	movq	%rcx, %rbx
	movl	%r15d, %edi
	movq	%r13, %rdx
	callq	write@PLT
	movq	%rbx, %rcx
	movq	-4192(%rbp), %r13       # 8-byte Reload
	addq	%rax, %r13
	jmp	.LBB51_8
.LBB51_10:                              # %if.else
	leaq	-49(%rbp), %r15
	movb	$0, -49(%rbp)
	movzbl	25(%r12), %eax
	movl	$8, %ecx
	.align	16, 0x90
.LBB51_11:                              # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movl	%ecx, %edx
	leal	(%rdx,%rdx), %ecx
	cmpl	%eax, %edx
	jl	.LBB51_11
# BB#12:                                # %while.end
	cmpl	$65, %edx
	movq	%rdx, -4168(%rbp)       # 8-byte Spill
	jl	.LBB51_14
# BB#13:                                # %if.then.46
	leaq	.L.str.1.15(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB51_14:                              # %if.end.47
	movl	28(%r12), %r13d
	leaq	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types(%rip), %rax
	movq	(%rax,%r13,8), %rdx
	leaq	-4144(%rbp), %rdi
	movq	%r12, %rbx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.123(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	36(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.22.125(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_15
# BB#16:                                # %if.then.63
	movq	%r13, -4152(%rbp)       # 8-byte Spill
	leaq	.L.str.15(%rip), %rdx
	movq	%rsi, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	jmp	.LBB51_17
.LBB51_6:
	movq	-4192(%rbp), %r13       # 8-byte Reload
.LBB51_8:                               # %if.end.30
	movq	(%rcx), %rsi
	movq	-4168(%rbp), %rax       # 8-byte Reload
	movl	%eax, %edx
	movl	%r15d, %edi
	callq	write@PLT
	movq	%rax, %rbx
	addq	%r13, %rbx
	movl	$0, -44(%rbp)
	movl	%r12d, %edx
	leaq	-44(%rbp), %rsi
	movl	%r15d, %edi
	callq	write@PLT
	addq	%rbx, %rax
	movl	$0, (%r14)
	movl	-4152(%rbp), %ecx       # 4-byte Reload
	cmpq	%rcx, %rax
	je	.LBB51_45
# BB#9:                                 # %if.then.40
	leaq	.L.str.14(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB51_45
.LBB51_15:
	movq	%r13, -4152(%rbp)       # 8-byte Spill
.LBB51_17:                              # %for.cond.preheader
	movq	%r15, %r13
	cmpl	$0, 40(%rbx)
	jle	.LBB51_18
# BB#21:                                # %for.body.lr.ph
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	xorl	%r12d, %r12d
	xorl	%r15d, %r15d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_22:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_28
# BB#23:                                # %if.then.69
                                        #   in Loop: Header=BB51_22 Depth=1
	movzwl	26(%rbx), %ecx
	cmpl	$2, %ecx
	jb	.LBB51_26
# BB#24:                                # %land.lhs.true
                                        #   in Loop: Header=BB51_22 Depth=1
	movl	%r12d, %eax
	cltd
	idivl	%ecx
	testl	%edx, %edx
	je	.LBB51_25
.LBB51_26:                              # %if.else.80
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.17(%rip), %rdx
	jmp	.LBB51_27
.LBB51_25:                              # %if.then.78
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.16(%rip), %rdx
	.align	16, 0x90
.LBB51_27:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
.LBB51_28:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	16(%rbx), %rax
	movslq	(%rax,%r15), %rdx
	movl	$1, %ecx
	movq	%rsi, %rdi
	movq	%r13, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rsi
	addq	$1, %r14
	movslq	40(%rbx), %rax
	addq	$4, %r15
	addl	$1, %r12d
	cmpq	%rax, %r14
	jl	.LBB51_22
	jmp	.LBB51_19
.LBB51_18:
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
.LBB51_19:                              # %for.cond.cleanup
	movzwl	26(%rbx), %eax
	cmpl	$1, %eax
	jbe	.LBB51_29
# BB#20:                                # %if.then.92
	leaq	.L.str.18(%rip), %rdx
	jmp	.LBB51_30
.LBB51_29:                              # %if.else.94
	leaq	.L.str.8.76(%rip), %rdx
.LBB51_30:                              # %if.end.96
	movq	%rsi, %rdi
	movq	%r13, %r15
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	-4160(%rbp), %r12       # 8-byte Reload
	movq	-4152(%rbp), %rax       # 8-byte Reload
	cmpl	$1, %eax
	jg	.LBB51_42
# BB#31:                                # %if.then.98
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_33
# BB#32:                                # %if.then.103
	leaq	.L.str.20(%rip), %rdx
	jmp	.LBB51_34
.LBB51_33:                              # %if.else.105
	leaq	.L.str.21(%rip), %rdx
.LBB51_34:                              # %for.cond.109.preheader
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	cmpw	$0, 26(%rbx)
	je	.LBB51_42
# BB#35:                                # %for.body.115.lr.ph
	movq	%r12, -4160(%rbp)       # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -4152(%rbp)       # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_36:                              # %for.body.115
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_38
# BB#37:                                # %if.then.117
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%r15, %rsi
	leaq	.L.str.17(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_38:                              # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	movzbl	24(%rbx), %eax
	cmpq	$3, %rax
	ja	.LBB51_74
# BB#39:                                # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	leaq	.LJTI51_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	movq	-4168(%rbp), %rdx       # 8-byte Reload
	jmpq	*%rax
.LBB51_46:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_47
# BB#50:                                # %if.then.133
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movswq	(%rax,%r12), %rdx
	jmp	.LBB51_49
.LBB51_56:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_57
# BB#59:                                # %if.then.169
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzwl	(%rax,%r12), %edx
	jmp	.LBB51_49
.LBB51_64:                              # %if.then.195
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	movq	%rdi, %rbx
	cmpl	$15, %edx
	jle	.LBB51_65
# BB#67:                                # %if.end.198
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$32, %edx
	movq	-4184(%rbp), %rax       # 8-byte Reload
	jne	.LBB51_70
# BB#68:                                # %if.then.200
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	vcvtss2sd	(%rax,%rcx), %xmm0, %xmm0
	xorl	%edx, %edx
	jmp	.LBB51_69
.LBB51_72:                              # %if.then.224
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	(%rax,%r13), %rdx
	movq	%r15, %rsi
	callq	halide_pointer_to_string@PLT
	jmp	.LBB51_73
.LBB51_47:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_51
# BB#48:                                # %if.then.125
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movsbq	(%rax,%r14), %rdx
	jmp	.LBB51_49
.LBB51_57:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_60
# BB#58:                                # %if.then.161
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzbl	(%rax,%r14), %edx
.LBB51_49:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_73
.LBB51_65:                              # %if.else.205.thread
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	leaq	.L.str.22(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movq	-4184(%rbp), %rax       # 8-byte Reload
	movq	8(%rax), %rax
	jmp	.LBB51_66
.LBB51_70:                              # %if.else.205
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	cmpl	$16, %edx
	jne	.LBB51_66
# BB#71:                                # %if.then.207
                                        #   in Loop: Header=BB51_36 Depth=1
	movzwl	(%rax,%r12), %edi
	callq	halide_float16_bits_to_double@PLT
	movl	$1, %edx
.LBB51_69:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_73:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	jmp	.LBB51_74
.LBB51_66:                              # %if.else.212
                                        #   in Loop: Header=BB51_36 Depth=1
	vmovsd	(%rax,%r13), %xmm0      # xmm0 = mem[0],zero
	movl	$1, %edx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB51_55
.LBB51_51:                              # %if.else.139
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_53
# BB#52:                                # %if.then.141
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movslq	(%rax,%rcx), %rdx
	jmp	.LBB51_54
.LBB51_60:                              # %if.else.175
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_62
# BB#61:                                # %if.then.177
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movl	(%rax,%rcx), %edx
	jmp	.LBB51_63
.LBB51_53:                              # %if.else.146
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_54:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_55
.LBB51_62:                              # %if.else.182
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_63:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
.LBB51_55:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_74:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	addq	$1, %r14
	movzwl	26(%rbx), %eax
	addq	$8, %r13
	addq	$2, %r12
	addq	$4, -4152(%rbp)         # 8-byte Folded Spill
	cmpq	%rax, %r14
	jl	.LBB51_36
# BB#40:                                # %for.cond.cleanup.114
	movzwl	%ax, %eax
	cmpl	$1, %eax
	movq	-4160(%rbp), %r12       # 8-byte Reload
	jbe	.LBB51_42
# BB#41:                                # %if.then.240
	leaq	.L.str.23(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_42:                              # %if.end.243
	leaq	.L.str.7.110(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %r15d
	leaq	-4144(%rbp), %rsi
	subq	%rsi, %r15
	addq	%rax, %r15
	movq	%r12, %rdi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB51_43:                              # %while.cond.i.353
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB51_43
# BB#44:                                # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy4096EED2Ev.exit
	leaq	-4144(%rbp), %r14
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	movl	$0, (%rbx)
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
.LBB51_45:                              # %if.end.247
	movl	-4172(%rbp), %eax       # 4-byte Reload
	addq	$4168, %rsp             # imm = 0x1048
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end51:
	.size	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t, .Lfunc_end51-_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.section	.rodata._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"a",@progbits
	.align	4
.LJTI51_0:
	.long	.LBB51_46-.LJTI51_0
	.long	.LBB51_56-.LJTI51_0
	.long	.LBB51_64-.LJTI51_0
	.long	.LBB51_72-.LJTI51_0

	.section	.text.halide_get_trace_file,"ax",@progbits
	.weak	halide_get_trace_file
	.align	16, 0x90
	.type	halide_get_trace_file,@function
halide_get_trace_file:                  # @halide_get_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB52_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB52_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	cmpb	$0, (%rax)
	jne	.LBB52_8
# BB#3:                                 # %if.then
	leaq	.L.str.25(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB52_7
# BB#4:                                 # %if.then.2
	movl	$1089, %esi             # imm = 0x441
	movl	$420, %edx              # imm = 0x1A4
	movq	%rax, %rdi
	callq	open@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jg	.LBB52_6
# BB#5:                                 # %if.then.4
	leaq	.L.str.26(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB52_6:                               # %if.end
	movl	%r15d, %edi
	callq	halide_set_trace_file@PLT
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	jmp	.LBB52_8
.LBB52_7:                               # %if.else
	xorl	%edi, %edi
	callq	halide_set_trace_file@PLT
.LBB52_8:                               # %if.end.6
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	movl	$0, (%rbx)
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end52:
	.size	halide_get_trace_file, .Lfunc_end52-halide_get_trace_file

	.section	.text.halide_set_custom_trace,"ax",@progbits
	.weak	halide_set_custom_trace
	.align	16, 0x90
	.type	halide_set_custom_trace,@function
halide_set_custom_trace:                # @halide_set_custom_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end53:
	.size	halide_set_custom_trace, .Lfunc_end53-halide_set_custom_trace

	.section	.text.halide_set_trace_file,"ax",@progbits
	.weak	halide_set_trace_file
	.align	16, 0x90
	.type	halide_set_trace_file,@function
halide_set_trace_file:                  # @halide_set_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end54:
	.size	halide_set_trace_file, .Lfunc_end54-halide_set_trace_file

	.section	.text.halide_trace,"ax",@progbits
	.weak	halide_trace
	.align	16, 0x90
	.type	halide_trace,@function
halide_trace:                           # @halide_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end55:
	.size	halide_trace, .Lfunc_end55-halide_trace

	.section	.text.halide_shutdown_trace,"ax",@progbits
	.weak	halide_shutdown_trace
	.align	16, 0x90
	.type	halide_shutdown_trace,@function
halide_shutdown_trace:                  # @halide_shutdown_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	je	.LBB56_2
# BB#1:                                 # %if.then
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %r14
	movl	(%r14), %edi
	callq	close@PLT
	movl	$0, (%r14)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rcx
	movb	$0, (%rcx)
	movb	$0, (%rbx)
	jmp	.LBB56_3
.LBB56_2:                               # %return
	xorl	%eax, %eax
.LBB56_3:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end56:
	.size	halide_shutdown_trace, .Lfunc_end56-halide_shutdown_trace

	.section	.text.halide_trace_cleanup,"ax",@progbits
	.weak	halide_trace_cleanup
	.align	16, 0x90
	.type	halide_trace_cleanup,@function
halide_trace_cleanup:                   # @halide_trace_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_trace@PLT # TAILCALL
.Lfunc_end57:
	.size	halide_trace_cleanup, .Lfunc_end57-halide_trace_cleanup

	.section	.text.halide_trace_helper,"ax",@progbits
	.weak	halide_trace_helper
	.align	16, 0x90
	.type	halide_trace_helper,@function
halide_trace_helper:                    # @halide_trace_helper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$56, %rsp
	movl	40(%rbp), %r10d
	movl	32(%rbp), %r11d
	movl	24(%rbp), %eax
	movl	16(%rbp), %ebx
	movq	%rsi, -56(%rbp)
	movq	%rcx, -40(%rbp)
	movq	%rdx, -48(%rbp)
	movb	%r8b, -32(%rbp)
	movb	%r9b, -31(%rbp)
	movw	%bx, -30(%rbp)
	movl	%eax, -28(%rbp)
	movl	%r11d, -24(%rbp)
	movl	%r10d, -20(%rbp)
	movzwl	%bx, %eax
	cmpl	$1, %eax
	movl	$1, %ecx
	cmoval	%eax, %ecx
	imull	48(%rbp), %ecx
	movl	%ecx, -16(%rbp)
	leaq	-56(%rbp), %rsi
	callq	halide_trace@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end58:
	.size	halide_trace_helper, .Lfunc_end58-halide_trace_helper

	.section	.text._ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,@function
_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc: # @_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	.align	16, 0x90
.LBB59_1:                               # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	leaq	1(%rcx), %rax
	cmpb	$0, (%rcx)
	jne	.LBB59_1
# BB#2:                                 # %while.cond.1.preheader
	xorl	%eax, %eax
	cmpq	%rdi, %rcx
	je	.LBB59_19
	.align	16, 0x90
.LBB59_3:                               # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	jne	.LBB59_4
# BB#7:                                 # %while.body.5
                                        #   in Loop: Header=BB59_3 Depth=1
	movzbl	-1(%rcx), %edx
	addq	$-1, %rcx
	cmpl	$46, %edx
	sete	%al
	cmpq	%rcx, %rdi
	jne	.LBB59_3
# BB#8:                                 # %while.end.7
	movzbl	%dl, %eax
	cmpl	$46, %eax
	je	.LBB59_5
# BB#9:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_4:
	movq	%rcx, %rdi
.LBB59_5:                               # %if.end
	movb	1(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$116, %eax
	jne	.LBB59_6
# BB#10:                                # %if.end.16
	movb	2(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$105, %eax
	jne	.LBB59_11
# BB#12:                                # %if.end.24
	movb	3(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$102, %eax
	jne	.LBB59_13
# BB#14:                                # %if.end.32
	movb	4(%rdi), %cl
	movb	$1, %al
	testb	%cl, %cl
	je	.LBB59_19
# BB#15:                                # %if.end.32
	movzbl	%cl, %eax
	cmpl	$70, %eax
	je	.LBB59_18
# BB#16:                                # %if.end.32
	cmpl	$102, %eax
	jne	.LBB59_17
.LBB59_18:                              # %if.end.44
	cmpb	$0, 5(%rdi)
	sete	%al
.LBB59_19:                              # %cleanup
	popq	%rbp
	retq
.LBB59_6:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_11:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_13:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_17:                              # %if.then.43
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end59:
	.size	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc, .Lfunc_end59-_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc

	.section	.text._ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,@function
_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t: # @_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%r8), %r9
	imull	32(%r8), %edi
	imull	36(%r8), %esi
	addl	%edi, %esi
	imull	40(%r8), %edx
	addl	%esi, %edx
	imull	44(%r8), %ecx
	addl	%edx, %ecx
	movslq	%ecx, %rax
	imulq	%r9, %rax
	addq	8(%r8), %rax
	popq	%rbp
	retq
.Lfunc_end60:
	.size	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t, .Lfunc_end60-_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI61_0:
	.long	1                       # 0x1
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI61_1:
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	1                       # 0x1
	.long	1                       # 0x1
	.section	.text.halide_debug_to_file,"ax",@progbits
	.weak	halide_debug_to_file
	.align	16, 0x90
	.type	halide_debug_to_file,@function
halide_debug_to_file:                   # @halide_debug_to_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4232, %rsp             # imm = 0x1088
	movq	%rcx, %r12
	movq	%r12, -4152(%rbp)       # 8-byte Spill
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%r12, %rsi
	callq	halide_copy_to_host@PLT
	leaq	.L.str.27(%rip), %rsi
	movq	%rbx, %rdi
	callq	fopen@PLT
	movq	%rax, %r14
	movl	$-1, %r13d
	testq	%r14, %r14
	je	.LBB61_44
# BB#1:                                 # %if.end
	vmovdqu	16(%r12), %xmm1
	vmovdqa	%xmm1, -4208(%rbp)      # 16-byte Spill
	vpbroadcastd	.LCPI61_0(%rip), %xmm0
	vpmaxsd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, -4176(%rbp)      # 16-byte Spill
	movslq	64(%r12), %r12
	movq	%r12, -4184(%rbp)       # 8-byte Spill
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc@PLT
	testb	%al, %al
	je	.LBB61_18
# BB#2:                                 # %if.then.19
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %ecx
	cmpl	$2, %ecx
	setb	%bl
	vpextrd	$2, %xmm0, %eax
	cmpl	$5, %eax
	setl	%dl
	andb	%bl, %dl
	movl	$1, %esi
	cmovel	%eax, %esi
	movl	%esi, -4212(%rbp)       # 4-byte Spill
	movl	%ecx, %r13d
	cmovnel	%eax, %r13d
	movw	$18761, -4144(%rbp)     # imm = 0x4949
	movw	$42, -4142(%rbp)
	movl	$8, -4140(%rbp)
	movw	$15, -4136(%rbp)
	movw	$256, -4134(%rbp)       # imm = 0x100
	movw	$4, -4132(%rbp)
	movl	$1, -4130(%rbp)
	vmovd	%xmm0, %r8d
	vmovd	%xmm0, -4126(%rbp)
	movw	$257, -4122(%rbp)       # imm = 0x101
	movw	$4, -4120(%rbp)
	movl	$1, -4118(%rbp)
	vpextrd	$1, %xmm0, -4114(%rbp)
	leal	(,%r12,8), %edi
	movw	$258, -4110(%rbp)       # imm = 0x102
	movw	$3, -4108(%rbp)
	movl	$1, -4106(%rbp)
	movw	%di, -4102(%rbp)
	movw	$259, -4098(%rbp)       # imm = 0x103
	movw	$3, -4096(%rbp)
	movl	$1, -4094(%rbp)
	movw	$1, -4090(%rbp)
	cmpl	$2, %r13d
	setg	%bl
	movzbl	%bl, %edi
	addl	$1, %edi
	movw	$262, -4086(%rbp)       # imm = 0x106
	movw	$3, -4084(%rbp)
	movl	$1, -4082(%rbp)
	movw	%di, -4078(%rbp)
	movw	$273, -4074(%rbp)       # imm = 0x111
	movw	$4, -4072(%rbp)
	movl	%r13d, -4070(%rbp)
	movl	$210, -4066(%rbp)
	testb	%dl, %dl
	movw	%cx, %dx
	cmovnew	%ax, %dx
	movw	$277, -4062(%rbp)       # imm = 0x115
	movw	$3, -4060(%rbp)
	movl	$1, -4058(%rbp)
	movw	%dx, -4054(%rbp)
	movw	$278, -4050(%rbp)       # imm = 0x116
	movw	$4, -4048(%rbp)
	movl	$1, -4046(%rbp)
	vpextrd	$1, %xmm0, -4042(%rbp)
	vpextrd	$1, %xmm0, %ebx
	imull	%r8d, %ebx
	imull	%ebx, %eax
	imull	%r12d, %eax
	imull	%ecx, %eax
	cmpl	$1, %r13d
	leal	210(,%r13,4), %ecx
	cmovel	%eax, %ecx
	movw	$279, -4038(%rbp)       # imm = 0x117
	movw	$4, -4036(%rbp)
	movl	%r13d, -4034(%rbp)
	movl	%ecx, -4030(%rbp)
	movw	$282, -4026(%rbp)       # imm = 0x11A
	movw	$5, -4024(%rbp)
	movl	$1, -4022(%rbp)
	movl	$194, -4018(%rbp)
	movw	$283, -4014(%rbp)       # imm = 0x11B
	movw	$5, -4012(%rbp)
	movl	$1, -4010(%rbp)
	movl	$202, -4006(%rbp)
	movw	$284, -4002(%rbp)       # imm = 0x11C
	movw	$3, -4000(%rbp)
	movl	$1, -3998(%rbp)
	movw	$2, -3994(%rbp)
	movw	$296, -3990(%rbp)       # imm = 0x128
	movw	$3, -3988(%rbp)
	movl	$1, -3986(%rbp)
	movw	$1, -3982(%rbp)
	movslq	%r15d, %rax
	movq	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE@GOTPCREL(%rip), %rcx
	movw	(%rcx,%rax,2), %ax
	movw	$339, -3978(%rbp)       # imm = 0x153
	movw	$3, -3976(%rbp)
	movl	$1, -3974(%rbp)
	movw	%ax, -3970(%rbp)
	movw	$-32539, -3966(%rbp)    # imm = 0xFFFFFFFFFFFF80E5
	movw	$4, -3964(%rbp)
	movl	$1, -3962(%rbp)
	movl	%esi, -3958(%rbp)
	vmovdqa	.LCPI61_1(%rip), %xmm0  # xmm0 = [0,1,1,1]
	vmovdqu	%xmm0, -3954(%rbp)
	movl	$1, -3938(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$210, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#4:                                 # %if.end.67
	cmpl	$2, %r13d
	jl	.LBB61_11
# BB#5:                                 # %for.body.lr.ph
	movq	%r14, %r12
	movl	%ebx, -4216(%rbp)       # 4-byte Spill
	movq	-4184(%rbp), %rax       # 8-byte Reload
	imull	%eax, %ebx
	imull	-4212(%rbp), %ebx       # 4-byte Folded Reload
	leal	210(,%r13,8), %eax
	movl	%eax, -44(%rbp)
	xorl	%r15d, %r15d
	leaq	-44(%rbp), %r14
	.align	16, 0x90
.LBB61_6:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r14, %rdi
	movq	%r12, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_16
# BB#7:                                 # %if.end.80
                                        #   in Loop: Header=BB61_6 Depth=1
	addl	%ebx, -44(%rbp)
	addl	$1, %r15d
	cmpl	%r13d, %r15d
	jl	.LBB61_6
# BB#8:                                 # %for.body.91.lr.ph
	movl	-4212(%rbp), %eax       # 4-byte Reload
	imull	-4216(%rbp), %eax       # 4-byte Folded Reload
	movl	%eax, -48(%rbp)
	xorl	%ebx, %ebx
	leaq	-48(%rbp), %r15
	movq	%r12, %r14
	.align	16, 0x90
.LBB61_10:                              # %for.body.91
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r15, %rdi
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#9:                                 # %for.cond.88
                                        #   in Loop: Header=BB61_10 Depth=1
	addl	$1, %ebx
	cmpl	%r13d, %ebx
	jl	.LBB61_10
.LBB61_11:                              # %cleanup.106.thread
	movq	-4184(%rbp), %r12       # 8-byte Reload
	jmp	.LBB61_12
.LBB61_18:                              # %if.else.116
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vmovdqa	%xmm0, -4144(%rbp)
	movl	%r15d, -4128(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$20, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_19
.LBB61_12:                              # %if.end.130
	movl	$4096, %eax             # imm = 0x1000
	xorl	%edx, %edx
	idivl	%r12d
	xorl	%r13d, %r13d
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %r10d
	testl	%r10d, %r10d
	jle	.LBB61_43
# BB#13:                                # %for.cond.135.preheader.lr.ph
	vmovd	%xmm0, %r9d
	movl	%r9d, -4216(%rbp)       # 4-byte Spill
	movl	%eax, %ecx
	imull	%r12d, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, -4232(%rbp)       # 8-byte Spill
	vmovdqa	-4208(%rbp), %xmm1      # 16-byte Reload
	vpextrd	$2, %xmm1, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovgl	%ecx, %ebx
	movq	%rbx, -4248(%rbp)       # 8-byte Spill
	leal	-128(%rbx), %r8d
	movl	%r8d, -4252(%rbp)       # 4-byte Spill
	movl	%r8d, %edi
	shrl	$7, %edi
	addl	$1, %edi
	vpextrd	$1, %xmm0, -4236(%rbp)  # 4-byte Folded Spill
	vpextrd	$2, %xmm0, %esi
	movl	%esi, -4260(%rbp)       # 4-byte Spill
	movl	%ebx, %r11d
	andl	$-128, %r11d
	movl	%r11d, -4264(%rbp)      # 4-byte Spill
	andl	$7, %edi
	movl	%edi, -4256(%rbp)       # 4-byte Spill
	movl	%edi, %r12d
	negl	%r12d
	movl	%r12d, -4268(%rbp)      # 4-byte Spill
	movl	$0, -4208(%rbp)         # 4-byte Folded Spill
	xorl	%r15d, %r15d
.LBB61_14:                              # %for.cond.143.preheader.lr.ph.us.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB61_23 Depth 2
                                        #     Child Loop BB61_26 Depth 2
                                        #     Child Loop BB61_29 Depth 2
                                        #     Child Loop BB61_40 Depth 2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	testl	%r9d, %r9d
	jle	.LBB61_20
# BB#15:                                #   in Loop: Header=BB61_14 Depth=1
	movl	$0, -4212(%rbp)         # 4-byte Folded Spill
.LBB61_40:                              # %for.body.146.lr.ph.us.us.preheader.us
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	movl	%r10d, -4240(%rbp)      # 4-byte Spill
	movl	%eax, %r12d
	movq	%r14, -4224(%rbp)       # 8-byte Spill
	movl	$0, -4176(%rbp)         # 4-byte Folded Spill
.LBB61_34:                              # %for.body.146.lr.ph.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB61_35 Depth 4
	xorl	%r13d, %r13d
	.align	16, 0x90
.LBB61_35:                              # %for.body.146.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        #       Parent Loop BB61_34 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	leal	1(%r15), %r14d
	movl	%r13d, %edi
	movl	-4176(%rbp), %esi       # 4-byte Reload
	movl	-4212(%rbp), %edx       # 4-byte Reload
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	movq	-4152(%rbp), %r8        # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t@PLT
	movq	-4184(%rbp), %rdx       # 8-byte Reload
	movl	%edx, %ecx
	imull	%r15d, %ecx
	movslq	%ecx, %rcx
	leaq	-4144(%rbp,%rcx), %rdi
	movq	%rax, %rsi
	callq	memcpy@PLT
	movl	%r14d, %r15d
	cmpl	%r12d, %r14d
	jne	.LBB61_37
# BB#36:                                # %if.then.153.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	movl	$1, %edx
	leaq	-4144(%rbp), %rdi
	movq	-4232(%rbp), %rsi       # 8-byte Reload
	movq	-4224(%rbp), %rcx       # 8-byte Reload
	callq	fwrite@PLT
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB61_41
.LBB61_37:                              # %for.inc.167.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	addl	$1, %r13d
	cmpl	-4216(%rbp), %r13d      # 4-byte Folded Reload
	jl	.LBB61_35
# BB#38:                                # %for.inc.172.us.us.us
                                        #   in Loop: Header=BB61_34 Depth=3
	movl	-4176(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4176(%rbp)       # 4-byte Spill
	cmpl	-4236(%rbp), %eax       # 4-byte Folded Reload
	jl	.LBB61_34
# BB#39:                                # %for.inc.177.us.us
                                        #   in Loop: Header=BB61_40 Depth=2
	movl	-4212(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4212(%rbp)       # 4-byte Spill
	movl	-4260(%rbp), %esi       # 4-byte Reload
	cmpl	%esi, %eax
	movq	-4224(%rbp), %r14       # 8-byte Reload
	movl	$0, %r13d
	movl	%r12d, %eax
	movl	-4240(%rbp), %r10d      # 4-byte Reload
	movl	-4216(%rbp), %r9d       # 4-byte Reload
	movq	-4248(%rbp), %rbx       # 8-byte Reload
	movl	-4252(%rbp), %r8d       # 4-byte Reload
	movl	-4256(%rbp), %edi       # 4-byte Reload
	movl	-4264(%rbp), %r11d      # 4-byte Reload
	movl	-4268(%rbp), %r12d      # 4-byte Reload
	jl	.LBB61_40
	jmp	.LBB61_30
.LBB61_20:                              # %overflow.checked.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%ebx, %ebx
	je	.LBB61_29
# BB#21:                                # %overflow.checked61
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%r11d, %r11d
	je	.LBB61_28
# BB#22:                                # %vector.body.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%ecx, %ecx
	movl	%r12d, %edx
	testl	%edi, %edi
	je	.LBB61_24
	.align	16, 0x90
.LBB61_23:                              # %vector.body.prol
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	subl	$-128, %ecx
	addl	$1, %edx
	jne	.LBB61_23
.LBB61_24:                              # %vector.body.preheader.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	cmpl	$896, %r8d              # imm = 0x380
	jb	.LBB61_28
# BB#25:                                # %vector.body.preheader.split.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	subl	%ecx, %edx
	.align	16, 0x90
.LBB61_26:                              # %vector.body
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$-1024, %edx            # imm = 0xFFFFFFFFFFFFFC00
	jne	.LBB61_26
# BB#27:                                #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
.LBB61_28:                              # %middle.block
                                        #   in Loop: Header=BB61_14 Depth=1
	cmpl	%edx, %ebx
	je	.LBB61_30
	.align	16, 0x90
.LBB61_29:                              # %overflow.checked
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$1, %edx
	cmpl	%esi, %edx
	jl	.LBB61_29
.LBB61_30:                              # %for.inc.182
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, -4208(%rbp)       # 4-byte Spill
	cmpl	%r10d, %ecx
	movq	-4184(%rbp), %rcx       # 8-byte Reload
	jl	.LBB61_14
# BB#31:                                # %for.end.186
	testl	%r15d, %r15d
	jle	.LBB61_43
# BB#32:                                # %if.then.188
	imull	%ecx, %r15d
	movslq	%r15d, %rsi
	leaq	-4144(%rbp), %rdi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_33
.LBB61_43:                              # %if.end.197
	movq	%r14, %rdi
	callq	fclose@PLT
	jmp	.LBB61_44
.LBB61_41:                              # %cleanup.184
	movq	-4224(%rbp), %rdi       # 8-byte Reload
.LBB61_42:                              # %cleanup.199
	callq	fclose@PLT
	movl	$-1, %r13d
	jmp	.LBB61_44
.LBB61_3:                               # %if.then.65
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_19:                              # %if.then.124
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_16:                              # %cleanup
	movq	%r12, %rdi
.LBB61_17:                              # %cleanup.106.thread415
	callq	fclose@PLT
	movl	$-2, %r13d
.LBB61_44:                              # %cleanup.209
	movl	%r13d, %eax
	addq	$4232, %rsp             # imm = 0x1088
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB61_33:                              # %if.then.194
	movq	%r14, %rdi
	jmp	.LBB61_42
.Lfunc_end61:
	.size	halide_debug_to_file, .Lfunc_end61-halide_debug_to_file

	.section	.text._ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,@function
_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t: # @_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%rdi), %r8
	movl	32(%rdi), %eax
	movl	36(%rdi), %edx
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	%esi, %rax
	movslq	16(%rdi), %rsi
	imulq	%r8, %rsi
	imulq	%rax, %rsi
	cmpq	%r8, %rsi
	cmovbeq	%r8, %rsi
	movl	%edx, %eax
	negl	%eax
	cmovll	%edx, %eax
	cltq
	movslq	20(%rdi), %rdx
	imulq	%r8, %rdx
	imulq	%rax, %rdx
	cmpq	%rsi, %rdx
	cmovbeq	%rsi, %rdx
	movl	40(%rdi), %eax
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	24(%rdi), %rcx
	imulq	%r8, %rcx
	movslq	%esi, %rax
	imulq	%rax, %rcx
	cmpq	%rdx, %rcx
	cmovbeq	%rdx, %rcx
	movl	44(%rdi), %eax
	movl	%eax, %edx
	negl	%edx
	cmovll	%eax, %edx
	movslq	%edx, %rdx
	movslq	28(%rdi), %rax
	imulq	%r8, %rax
	imulq	%rdx, %rax
	cmpq	%rcx, %rax
	cmovbeq	%rcx, %rax
	popq	%rbp
	retq
.Lfunc_end62:
	.size	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t, .Lfunc_end62-_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t

	.section	.text._ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,@function
_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m: # @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	memcmp@PLT
	testl	%eax, %eax
	sete	%al
	popq	%rbp
	retq
.Lfunc_end63:
	.size	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m, .Lfunc_end63-_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m

	.section	.text._ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,@function
_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_: # @_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	64(%rdi), %eax
	cmpl	64(%rsi), %eax
	jne	.LBB64_13
# BB#1:                                 # %for.cond.preheader
	movl	48(%rdi), %eax
	cmpl	48(%rsi), %eax
	jne	.LBB64_13
# BB#2:                                 # %lor.lhs.false
	movl	16(%rdi), %eax
	cmpl	16(%rsi), %eax
	jne	.LBB64_13
# BB#3:                                 # %lor.lhs.false.10
	movl	32(%rdi), %eax
	cmpl	32(%rsi), %eax
	jne	.LBB64_13
# BB#4:                                 # %for.cond
	movl	52(%rdi), %eax
	cmpl	52(%rsi), %eax
	jne	.LBB64_13
# BB#5:                                 # %lor.lhs.false.1
	movl	20(%rdi), %eax
	cmpl	20(%rsi), %eax
	jne	.LBB64_13
# BB#6:                                 # %lor.lhs.false.10.1
	movl	36(%rdi), %eax
	cmpl	36(%rsi), %eax
	jne	.LBB64_13
# BB#7:                                 # %for.cond.1
	movl	56(%rdi), %eax
	cmpl	56(%rsi), %eax
	jne	.LBB64_13
# BB#8:                                 # %lor.lhs.false.2
	movl	24(%rdi), %eax
	cmpl	24(%rsi), %eax
	jne	.LBB64_13
# BB#9:                                 # %lor.lhs.false.10.2
	movl	40(%rdi), %eax
	cmpl	40(%rsi), %eax
	jne	.LBB64_13
# BB#10:                                # %for.cond.2
	movl	60(%rdi), %eax
	cmpl	60(%rsi), %eax
	jne	.LBB64_13
# BB#11:                                # %lor.lhs.false.3
	movl	28(%rdi), %eax
	cmpl	28(%rsi), %eax
	jne	.LBB64_13
# BB#12:                                # %lor.lhs.false.10.3
	movl	44(%rdi), %eax
	cmpl	44(%rsi), %eax
	sete	%al
	popq	%rbp
	retq
.LBB64_13:                              # %cleanup
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end64:
	.size	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_, .Lfunc_end64-_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_

	.section	.text._ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,@function
_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh: # @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	-16(%rdi), %rax
	popq	%rbp
	retq
.Lfunc_end65:
	.size	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh, .Lfunc_end65-_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI66_0:
	.zero	16
	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,@function
_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_: # @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%r8, %r12
	movq	%rsi, %r14
	movq	%rdi, %r13
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r13)
	movq	$0, 16(%r13)
	movq	%rdx, 24(%r13)
	movl	%ecx, 40(%r13)
	movl	$0, 44(%r13)
	movl	%r9d, 48(%r13)
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 32(%r13)
	testq	%rax, %rax
	je	.LBB66_7
# BB#1:                                 # %if.end
	movq	64(%r12), %rcx
	movq	%rcx, 120(%r13)
	vmovups	(%r12), %ymm0
	vmovups	32(%r12), %ymm1
	vmovups	%ymm1, 88(%r13)
	vmovups	%ymm0, 56(%r13)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 56(%r13)
	movq	24(%r13), %rcx
	testq	%rcx, %rcx
	je	.LBB66_4
# BB#2:                                 # %for.body.preheader
	movb	(%r14), %dl
	movb	%dl, (%rax)
	cmpq	$2, %rcx
	jb	.LBB66_4
# BB#3:                                 # %for.body.for.body_crit_edge.preheader
	movb	1(%r14), %cl
	movb	%cl, 1(%rax)
	movl	$2, %eax
	cmpq	$2, 24(%r13)
	jbe	.LBB66_4
	.align	16, 0x90
.LBB66_8:                               # %for.body.for.body_crit_edge.for.body.for.body_crit_edge_crit_edge
                                        # =>This Inner Loop Header: Depth=1
	movq	32(%r13), %rcx
	movb	(%r14,%rax), %dl
	movb	%dl, (%rcx,%rax)
	addq	$1, %rax
	cmpq	24(%r13), %rax
	jb	.LBB66_8
.LBB66_4:                               # %for.cond.11.preheader
	movb	$1, %r15b
	cmpl	$0, 48(%r13)
	je	.LBB66_7
# BB#5:
	movq	16(%rbp), %r14
	xorl	%ebx, %ebx
	.align	16, 0x90
.LBB66_6:                               # %for.body.15
                                        # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	movl	%ebx, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movl	%ebx, %ecx
	movq	(%r14,%rcx,8), %rcx
	movq	64(%rcx), %rdx
	movq	%rdx, 64(%rax)
	vmovups	(%rcx), %ymm0
	vmovups	32(%rcx), %ymm1
	vmovups	%ymm1, 32(%rax)
	vmovups	%ymm0, (%rax)
	addl	$1, %ebx
	cmpl	48(%r13), %ebx
	jb	.LBB66_6
.LBB66_7:                               # %return
	movb	%r15b, %al
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end66:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_, .Lfunc_end66-_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,@function
_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi: # @_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	leaq	128(%rdi,%rax,8), %rax
	popq	%rbp
	retq
.Lfunc_end67:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi, .Lfunc_end67-_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,@function
_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv: # @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	32(%r14), %rsi
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	callq	halide_free@PLT
	cmpl	$0, 48(%r14)
	je	.LBB68_2
	.align	16, 0x90
.LBB68_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_device_free@PLT
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_free@PLT
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB68_1
.LBB68_2:                               # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end68:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv, .Lfunc_end68-_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv

	.section	.text._ZN6Halide7Runtime8Internal8djb_hashEPKhm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8djb_hashEPKhm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8djb_hashEPKhm,@function
_ZN6Halide7Runtime8Internal8djb_hashEPKhm: # @_ZN6Halide7Runtime8Internal8djb_hashEPKhm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$5381, %eax             # imm = 0x1505
	testq	%rsi, %rsi
	je	.LBB69_8
# BB#1:                                 # %for.body.preheader
	leaq	-1(%rsi), %r8
	xorl	%ecx, %ecx
	testb	$7, %sil
	je	.LBB69_2
# BB#3:                                 # %for.body.prol.preheader
	movl	%esi, %r9d
	andl	$7, %r9d
	movl	$5381, %eax             # imm = 0x1505
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB69_4:                               # %for.body.prol
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %edx
	movzbl	(%rdi,%rcx), %eax
	addl	%edx, %eax
	addq	$1, %rcx
	cmpq	%rcx, %r9
	jne	.LBB69_4
	jmp	.LBB69_5
.LBB69_2:
	movl	$5381, %eax             # imm = 0x1505
.LBB69_5:                               # %for.body.preheader.split
	cmpq	$7, %r8
	jb	.LBB69_8
# BB#6:                                 # %for.body.preheader.split.split
	movq	%rcx, %r9
	subq	%rsi, %r9
	leaq	(%rdi,%rcx), %r8
	xorl	%esi, %esi
	.align	16, 0x90
.LBB69_7:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %eax
	movzbl	(%r8,%rsi), %edx
	addl	%eax, %edx
	imull	$33, %edx, %r10d
	leaq	(%rcx,%rsi), %rdx
	movzbl	1(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	2(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	3(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	4(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	5(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	6(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	7(%rdi,%rdx), %eax
	addl	%r10d, %eax
	addq	$8, %rsi
	movq	%r9, %rdx
	addq	%rsi, %rdx
	jne	.LBB69_7
.LBB69_8:                               # %for.cond.cleanup
	popq	%rbp
	retq
.Lfunc_end69:
	.size	_ZN6Halide7Runtime8Internal8djb_hashEPKhm, .Lfunc_end69-_ZN6Halide7Runtime8Internal8djb_hashEPKhm

	.section	.text._ZN6Halide7Runtime8Internal11prune_cacheEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11prune_cacheEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal11prune_cacheEv,@function
_ZN6Halide7Runtime8Internal11prune_cacheEv: # @_ZN6Halide7Runtime8Internal11prune_cacheEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	movq	(%r15), %r14
	testq	%r14, %r14
	je	.LBB70_21
# BB#1:                                 # %entry
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r12
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
	jmp	.LBB70_2
	.align	16, 0x90
.LBB70_20:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, %r14
.LBB70_2:                               # %entry
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB70_6 Depth 2
                                        #     Child Loop BB70_23 Depth 2
	cmpq	%rcx, %rax
	jle	.LBB70_21
# BB#3:                                 # %while.body
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	8(%r14), %r13
	cmpl	$0, 44(%r14)
	jne	.LBB70_19
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB70_2 Depth=1
	movzbl	40(%r14), %eax
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rcx
	cmpq	%r14, %rcx
	je	.LBB70_5
	.align	16, 0x90
.LBB70_6:                               # %while.cond.9
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rcx, %rax
	testq	%rax, %rax
	je	.LBB70_22
# BB#7:                                 # %land.rhs.11
                                        #   in Loop: Header=BB70_6 Depth=2
	movq	(%rax), %rcx
	cmpq	%r14, %rcx
	jne	.LBB70_6
# BB#8:                                 # %if.end
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	%rcx, (%rax)
	jmp	.LBB70_9
.LBB70_5:                               # %if.then.6
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rdx
	movq	%rcx, (%rdx,%rax,8)
.LBB70_9:                               # %if.end.21
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	%r14, (%r15)
	jne	.LBB70_11
# BB#10:                                # %if.then.23
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, (%r15)
.LBB70_11:                              # %if.end.24
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	je	.LBB70_13
# BB#12:                                # %if.then.26
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	%rax, 16(%r13)
.LBB70_13:                              # %if.end.28
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	%r14, (%rax)
	jne	.LBB70_15
# BB#14:                                # %if.then.30
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rcx
	movq	%rax, (%rcx)
.LBB70_15:                              # %if.end.32
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	$0, 16(%r14)
	je	.LBB70_17
# BB#16:                                # %if.then.35
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, 16(%r14)
.LBB70_17:                              # %for.cond.preheader
                                        #   in Loop: Header=BB70_2 Depth=1
	xorl	%ebx, %ebx
	cmpl	$0, 48(%r14)
	je	.LBB70_18
	.align	16, 0x90
.LBB70_23:                              # %for.body
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	subq	%rax, (%r12)
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB70_23
.LBB70_18:                              # %for.cond.cleanup
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r14, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
.LBB70_19:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	jne	.LBB70_20
.LBB70_21:                              # %while.end.41
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB70_22:                              # %if.then.18
	leaq	.L.str.3.29(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_print@PLT
	callq	abort@PLT
.Lfunc_end70:
	.size	_ZN6Halide7Runtime8Internal11prune_cacheEv, .Lfunc_end70-_ZN6Halide7Runtime8Internal11prune_cacheEv

	.section	.text.halide_memoization_cache_set_size,"ax",@progbits
	.weak	halide_memoization_cache_set_size
	.align	16, 0x90
	.type	halide_memoization_cache_set_size,@function
halide_memoization_cache_set_size:      # @halide_memoization_cache_set_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	testq	%rdi, %rdi
	movl	$1048576, %ebx          # imm = 0x100000
	cmovneq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rax
	movq	%rbx, (%rax)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end71:
	.size	halide_memoization_cache_set_size, .Lfunc_end71-halide_memoization_cache_set_size

	.section	.text.halide_memoization_cache_lookup,"ax",@progbits
	.weak	halide_memoization_cache_lookup
	.align	16, 0x90
	.type	halide_memoization_cache_lookup,@function
halide_memoization_cache_lookup:        # @halide_memoization_cache_lookup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r9, -104(%rbp)         # 8-byte Spill
	movl	%r8d, -84(%rbp)         # 4-byte Spill
	movq	%rcx, -96(%rbp)         # 8-byte Spill
	movq	%rsi, -80(%rbp)         # 8-byte Spill
	movq	%rdi, -72(%rbp)         # 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %rdi
	movq	%rax, %rsi
	callq	_ZN6Halide7Runtime8Internal8djb_hashEPKhm@PLT
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movzbl	%al, %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	(%rax,%rbx,8), %rbx
	testq	%rbx, %rbx
	je	.LBB72_11
# BB#1:                                 # %while.body.lr.ph
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_18
# BB#2:                                 # %while.body.lr.ph.split.us
	cltq
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB72_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB72_8 Depth 2
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_10
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_10
# BB#5:                                 # %land.lhs.true.7.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#6:                                 # %land.lhs.true.10.us
                                        #   in Loop: Header=BB72_3 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#7:                                 # %land.lhs.true.13.us
                                        #   in Loop: Header=BB72_3 Depth=1
	xorl	%r15d, %r15d
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	movq	-104(%rbp), %r14        # 8-byte Reload
	movl	$1, %r13d
	jne	.LBB72_10
	.align	16, 0x90
.LBB72_8:                               # %for.body.us
                                        #   Parent Loop BB72_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%r14), %r12
	movq	%rbx, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	jge	.LBB72_9
# BB#43:                                # %for.body.us
                                        #   in Loop: Header=BB72_8 Depth=2
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %r14
	testb	%al, %al
	jne	.LBB72_8
.LBB72_9:                               # %for.cond.cleanup.us
                                        #   in Loop: Header=BB72_3 Depth=1
	testb	%al, %al
	jne	.LBB72_23
	.align	16, 0x90
.LBB72_10:                              # %if.end.64.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_3
	jmp	.LBB72_11
	.align	16, 0x90
.LBB72_18:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_17
# BB#19:                                # %land.lhs.true
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_17
# BB#20:                                # %land.lhs.true.7
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#21:                                # %land.lhs.true.10
                                        #   in Loop: Header=BB72_18 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#22:                                # %land.lhs.true.13
                                        #   in Loop: Header=BB72_18 Depth=1
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	je	.LBB72_23
	.align	16, 0x90
.LBB72_17:                              # %if.end.64
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_18
.LBB72_11:                              # %for.cond.66.preheader
	movl	$1, %r12d
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_42
# BB#12:                                # %for.body.69.lr.ph
	movslq	%eax, %r12
	xorl	%r15d, %r15d
	movq	-104(%rbp), %r13        # 8-byte Reload
	movq	%r13, %r14
	.align	16, 0x90
.LBB72_13:                              # %for.body.69
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rbx
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	leaq	16(%rax), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, 8(%rbx)
	testq	%rax, %rax
	je	.LBB72_14
# BB#40:                                # %for.inc.103
                                        #   in Loop: Header=BB72_13 Depth=1
	addq	$16, %rax
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	-44(%rbp), %ecx         # 4-byte Reload
	movl	%ecx, 8(%rax)
	movq	$0, (%rax)
	addq	$1, %r15
	addq	$8, %r14
	cmpq	%r12, %r15
	jl	.LBB72_13
# BB#41:
	movl	$1, %r12d
	jmp	.LBB72_42
.LBB72_14:                              # %for.cond.79.preheader
	movl	$-1, %r12d
	testl	%r15d, %r15d
	jle	.LBB72_42
# BB#15:                                # %for.body.82.lr.ph
	movslq	%r15d, %r14
	leaq	-8(%r13,%r14,8), %rbx
	addq	$1, %r14
	movq	-72(%rbp), %r15         # 8-byte Reload
	.align	16, 0x90
.LBB72_16:                              # %for.body.82
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	halide_free@PLT
	movq	(%rbx), %rax
	movq	$0, 8(%rax)
	addq	$-1, %r14
	addq	$-8, %rbx
	cmpq	$1, %r14
	jg	.LBB72_16
	jmp	.LBB72_42
.LBB72_23:                              # %if.then.22
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %r14
	cmpq	(%r14), %rbx
	movq	-104(%rbp), %r12        # 8-byte Reload
	movl	-84(%rbp), %r13d        # 4-byte Reload
	je	.LBB72_36
# BB#24:                                # %if.then.24
	cmpq	$0, 8(%rbx)
	jne	.LBB72_26
# BB#25:                                # %if.then.26
	leaq	.L.str.4.30(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_26:                              # %if.end
	movq	16(%rbx), %rax
	testq	%rax, %rax
	je	.LBB72_28
# BB#27:                                # %if.then.28
	movq	8(%rbx), %rcx
	movq	%rcx, 8(%rax)
	jmp	.LBB72_31
.LBB72_28:                              # %if.else
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	cmpq	%rbx, (%r15)
	je	.LBB72_30
# BB#29:                                # %if.then.33
	leaq	.L.str.5.31(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_30:                              # %if.end.34
	movq	8(%rbx), %rax
	movq	%rax, (%r15)
.LBB72_31:                              # %if.end.36
	movq	8(%rbx), %rax
	testq	%rax, %rax
	jne	.LBB72_33
# BB#32:                                # %if.then.39
	leaq	.L.str.6.32(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	movq	8(%rbx), %rax
.LBB72_33:                              # %if.end.40
	movq	16(%rbx), %rcx
	movq	%rcx, 16(%rax)
	movq	$0, 8(%rbx)
	movq	(%r14), %rax
	movq	%rax, 16(%rbx)
	movq	(%r14), %rax
	testq	%rax, %rax
	je	.LBB72_35
# BB#34:                                # %if.then.47
	movq	%rbx, 8(%rax)
.LBB72_35:                              # %if.end.49
	movq	%rbx, (%r14)
.LBB72_36:                              # %for.cond.52.preheader
	testl	%r13d, %r13d
	jle	.LBB72_39
# BB#37:
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB72_38:                              # %for.body.55
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12), %r15
	movq	%rbx, %rdi
	movl	%r14d, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	64(%rax), %rcx
	movq	%rcx, 64(%r15)
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	%ymm1, 32(%r15)
	vmovups	%ymm0, (%r15)
	addl	$1, %r14d
	addq	$8, %r12
	cmpl	%r14d, %r13d
	jne	.LBB72_38
.LBB72_39:                              # %for.cond.cleanup.54
	addl	%r13d, 44(%rbx)
	xorl	%r12d, %r12d
.LBB72_42:                              # %cleanup.108
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	vzeroupper
	callq	halide_mutex_unlock@PLT
	movl	%r12d, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end72:
	.size	halide_memoization_cache_lookup, .Lfunc_end72-halide_memoization_cache_lookup

	.section	.text.halide_memoization_cache_store,"ax",@progbits
	.weak	halide_memoization_cache_store
	.align	16, 0x90
	.type	halide_memoization_cache_store,@function
halide_memoization_cache_store:         # @halide_memoization_cache_store
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movq	%r9, %r13
	movl	%r8d, %r14d
	movq	%rcx, -80(%rbp)         # 8-byte Spill
	movl	%edx, -100(%rbp)        # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, -120(%rbp)        # 8-byte Spill
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	8(%rax), %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movzbl	%bl, %eax
	movq	%rax, -112(%rbp)        # 8-byte Spill
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %r12
	testq	%r12, %r12
	je	.LBB73_14
# BB#1:                                 # %while.body.lr.ph
	movslq	-100(%rbp), %rax        # 4-byte Folded Reload
	movq	%rax, -56(%rbp)         # 8-byte Spill
	testl	%r14d, %r14d
	jle	.LBB73_16
# BB#2:                                 # %while.body.lr.ph.split.us
	movslq	%r14d, %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB73_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB73_9 Depth 2
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_13
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_13
# BB#5:                                 # %land.lhs.true.12.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#6:                                 # %land.lhs.true.15.us
                                        #   in Loop: Header=BB73_3 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#7:                                 # %land.lhs.true.18.us
                                        #   in Loop: Header=BB73_3 Depth=1
	cmpl	%r14d, 48(%r12)
	jne	.LBB73_13
# BB#8:                                 #   in Loop: Header=BB73_3 Depth=1
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	movq	%r14, -136(%rbp)        # 8-byte Spill
	movb	$1, %al
	movl	%eax, -68(%rbp)         # 4-byte Spill
	xorl	%r15d, %r15d
	movq	%r13, %rbx
	movq	%r13, -128(%rbp)        # 8-byte Spill
	movl	$1, %r13d
	.align	16, 0x90
.LBB73_9:                               # %for.body.us
                                        #   Parent Loop BB73_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r14
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	movb	%al, -41(%rbp)          # 1-byte Spill
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rax
	cmpq	8(%r14), %rax
	jne	.LBB73_11
# BB#10:                                #   in Loop: Header=BB73_9 Depth=2
	movl	$0, -68(%rbp)           # 4-byte Folded Spill
.LBB73_11:                              # %select.mid
                                        #   in Loop: Header=BB73_9 Depth=2
	movb	-41(%rbp), %cl          # 1-byte Reload
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	setl	%al
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %rbx
	testb	%cl, %al
	jne	.LBB73_9
# BB#12:                                # %for.cond.cleanup.us
                                        #   in Loop: Header=BB73_3 Depth=1
	testb	%cl, %cl
	movq	-128(%rbp), %r13        # 8-byte Reload
	movq	-136(%rbp), %r14        # 8-byte Reload
	movq	-88(%rbp), %r15         # 8-byte Reload
	movq	-96(%rbp), %rbx         # 8-byte Reload
	jne	.LBB73_22
	.align	16, 0x90
.LBB73_13:                              # %if.end.56.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_3
	jmp	.LBB73_14
	.align	16, 0x90
.LBB73_16:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_21
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_21
# BB#18:                                # %land.lhs.true.12
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#19:                                # %land.lhs.true.15
                                        #   in Loop: Header=BB73_16 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#20:                                # %land.lhs.true.18
                                        #   in Loop: Header=BB73_16 Depth=1
	cmpl	%r14d, 48(%r12)
	je	.LBB73_24
	.align	16, 0x90
.LBB73_21:                              # %if.end.56
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_16
.LBB73_14:                              # %for.cond.60.preheader
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	xorl	%r15d, %r15d
	testl	%r14d, %r14d
	movq	%r14, %rax
	jle	.LBB73_15
# BB#26:
	movq	%r13, %rbx
	movl	%eax, %r14d
	movq	%rax, %r12
	.align	16, 0x90
.LBB73_27:                              # %for.body.63
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	addq	%rax, %r15
	addq	$8, %rbx
	addl	$-1, %r14d
	jne	.LBB73_27
	jmp	.LBB73_28
.LBB73_15:
	movq	%rax, %r12
.LBB73_28:                              # %for.cond.cleanup.62
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r14
	addq	%r15, (%r14)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r12, %rbx
	leal	-1(%rbx), %eax
	cltq
	leaq	(%rax,%rax,8), %rax
	leaq	200(,%rax,8), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB73_29
# BB#31:                                # %if.end.96
	movslq	-100(%rbp), %rdx        # 4-byte Folded Reload
	movq	%r13, (%rsp)
	movq	%r12, %rdi
	movq	-88(%rbp), %rsi         # 8-byte Reload
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	-80(%rbp), %r8          # 8-byte Reload
	movl	%ebx, %r9d
	callq	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_@PLT
	testb	%al, %al
	je	.LBB73_32
# BB#35:                                # %if.end.120
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rax
	movq	%rax, (%r12)
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	movq	%rcx, 16(%r12)
	testq	%rcx, %rcx
	je	.LBB73_37
# BB#36:                                # %if.then.125
	movq	%r12, 8(%rcx)
.LBB73_37:                              # %if.end.126
	movq	%r12, (%rax)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	$0, (%rax)
	jne	.LBB73_39
# BB#38:                                # %if.then.128
	movq	%r12, (%rax)
.LBB73_39:                              # %if.end.129
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	%r12, (%rcx,%rax,8)
	movl	%ebx, 44(%r12)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_40:                              # %for.body.137
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r12, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_40
	jmp	.LBB73_41
.LBB73_29:                              # %if.then.79
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_30:                              # %for.body.86
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_30
	jmp	.LBB73_41
.LBB73_32:                              # %if.then.103
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_33
	.align	16, 0x90
.LBB73_34:                              # %for.body.110
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_34
.LBB73_33:                              # %for.cond.cleanup.109
	movq	-120(%rbp), %rdi        # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB73_41:                              # %cleanup.153
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%eax, %eax
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB73_22:                              # %if.then.36
	movl	-68(%rbp), %eax         # 4-byte Reload
	testb	$1, %al
	jne	.LBB73_24
# BB#23:                                # %if.then.38
	leaq	.L.str.8.33(%rip), %rsi
	movq	-120(%rbp), %rdi        # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB73_24:                              # %for.cond.42.preheader
	testl	%r14d, %r14d
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_25:                              # %for.body.45
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %r14d
	jne	.LBB73_25
	jmp	.LBB73_41
.Lfunc_end73:
	.size	halide_memoization_cache_store, .Lfunc_end73-halide_memoization_cache_store

	.section	.text.halide_memoization_cache_release,"ax",@progbits
	.weak	halide_memoization_cache_release
	.align	16, 0x90
	.type	halide_memoization_cache_release,@function
halide_memoization_cache_release:       # @halide_memoization_cache_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	%rsi, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	(%rax), %rbx
	testq	%rbx, %rbx
	je	.LBB74_4
# BB#1:                                 # %if.else
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movl	44(%rbx), %eax
	testl	%eax, %eax
	jne	.LBB74_3
# BB#2:                                 # %if.then.6
	leaq	.L.str.11.34(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	44(%rbx), %eax
.LBB74_3:                               # %if.end
	addl	$-1, %eax
	movl	%eax, 44(%rbx)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.LBB74_4:                               # %if.then
	movq	%r14, %rdi
	movq	%rax, %rsi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_free@PLT         # TAILCALL
.Lfunc_end74:
	.size	halide_memoization_cache_release, .Lfunc_end74-halide_memoization_cache_release

	.section	.text.halide_memoization_cache_cleanup,"ax",@progbits
	.weak	halide_memoization_cache_cleanup
	.align	16, 0x90
	.type	halide_memoization_cache_cleanup,@function
halide_memoization_cache_cleanup:       # @halide_memoization_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	xorl	%r14d, %r14d
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB75_1:                               # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB75_2 Depth 2
	movq	(%r15,%r14,8), %rbx
	movq	$0, (%r15,%r14,8)
	testq	%rbx, %rbx
	je	.LBB75_3
	.align	16, 0x90
.LBB75_2:                               # %while.body
                                        #   Parent Loop BB75_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r12
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	%r12, %rbx
	testq	%r12, %r12
	jne	.LBB75_2
.LBB75_3:                               # %while.end
                                        #   in Loop: Header=BB75_1 Depth=1
	addq	$1, %r14
	cmpq	$256, %r14              # imm = 0x100
	jne	.LBB75_1
# BB#4:                                 # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_mutex_destroy@PLT # TAILCALL
.Lfunc_end75:
	.size	halide_memoization_cache_cleanup, .Lfunc_end75-halide_memoization_cache_cleanup

	.section	.text.halide_cache_cleanup,"ax",@progbits
	.weak	halide_cache_cleanup
	.align	16, 0x90
	.type	halide_cache_cleanup,@function
halide_cache_cleanup:                   # @halide_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_memoization_cache_cleanup@PLT # TAILCALL
.Lfunc_end76:
	.size	halide_cache_cleanup, .Lfunc_end76-halide_cache_cleanup

	.section	.text.halide_string_to_string,"ax",@progbits
	.weak	halide_string_to_string
	.align	16, 0x90
	.type	halide_string_to_string,@function
halide_string_to_string:                # @halide_string_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB77_3
# BB#1:                                 # %while.body.preheader
	je	.LBB77_2
	.align	16, 0x90
.LBB77_4:                               # %if.end.3
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rdx), %al
	movb	%al, (%rdi)
	testb	%al, %al
	je	.LBB77_3
# BB#5:                                 # %if.end.6
                                        #   in Loop: Header=BB77_4 Depth=1
	addq	$1, %rdi
	addq	$1, %rdx
	cmpq	%rdi, %rsi
	jne	.LBB77_4
# BB#6:
	movq	%rsi, %rdi
.LBB77_2:                               # %if.then.2
	movb	$0, -1(%rdi)
.LBB77_3:                               # %return
	movq	%rdi, %rax
	popq	%rbp
	retq
.Lfunc_end77:
	.size	halide_string_to_string, .Lfunc_end77-halide_string_to_string

	.section	.text.halide_uint64_to_string,"ax",@progbits
	.weak	halide_uint64_to_string
	.align	16, 0x90
	.type	halide_uint64_to_string,@function
halide_uint64_to_string:                # @halide_uint64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	movb	$0, -1(%rbp)
	leaq	-2(%rbp), %r8
	movl	$1, %r9d
	testq	%rdx, %rdx
	jne	.LBB78_2
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jle	.LBB78_4
	.align	16, 0x90
.LBB78_2:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdx, %r11
	movl	%r9d, %r10d
	movabsq	$-3689348814741910323, %r9 # imm = 0xCCCCCCCCCCCCCCCD
	mulxq	%r9, %rax, %rdx
	shrq	$3, %rdx
	imull	$-10, %edx, %eax
	leal	48(%r11,%rax), %eax
	movb	%al, (%r8)
	addq	$-1, %r8
	leal	1(%r10), %r9d
	cmpq	$9, %r11
	ja	.LBB78_2
# BB#3:                                 # %for.body
                                        #   in Loop: Header=BB78_2 Depth=1
	cmpl	%ecx, %r10d
	jl	.LBB78_2
.LBB78_4:                               # %for.cond.cleanup
	addq	$1, %r8
	movq	%r8, %rdx
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end78:
	.size	halide_uint64_to_string, .Lfunc_end78-halide_uint64_to_string

	.section	.text.halide_int64_to_string,"ax",@progbits
	.weak	halide_int64_to_string
	.align	16, 0x90
	.type	halide_int64_to_string,@function
halide_int64_to_string:                 # @halide_int64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB79_3
# BB#1:                                 # %entry
	testq	%rdx, %rdx
	jns	.LBB79_3
# BB#2:                                 # %if.then
	movb	$45, (%rdi)
	addq	$1, %rdi
	negq	%rdx
.LBB79_3:                               # %if.end
	popq	%rbp
	jmp	halide_uint64_to_string@PLT # TAILCALL
.Lfunc_end79:
	.size	halide_int64_to_string, .Lfunc_end79-halide_int64_to_string

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI80_0:
	.quad	-9223372036854775808    # 0x8000000000000000
	.quad	-9223372036854775808    # 0x8000000000000000
.LCPI80_6:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI80_7:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI80_1:
	.quad	4607182418800017408     # double 1
.LCPI80_2:
	.quad	4621819117588971520     # double 10
.LCPI80_3:
	.quad	4696837146684686336     # double 1.0E+6
.LCPI80_4:
	.quad	4602678819172646912     # double 0.5
.LCPI80_5:
	.quad	4890909195324358656     # double 9.2233720368547758E+18
	.section	.text.halide_double_to_string,"ax",@progbits
	.weak	halide_double_to_string
	.align	16, 0x90
	.type	halide_double_to_string,@function
halide_double_to_string:                # @halide_double_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$552, %rsp              # imm = 0x228
	movl	%edx, %ebx
	vmovapd	%xmm0, -592(%rbp)       # 16-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	vmovsd	%xmm0, -48(%rbp)
	movq	$0, -56(%rbp)
	leaq	-56(%rbp), %rdi
	leaq	-48(%rbp), %rsi
	movl	$8, %edx
	callq	memcpy@PLT
	movq	-56(%rbp), %rax
	movb	$52, %cl
	bzhiq	%rcx, %rax, %r13
	movq	%rax, %r15
	shrq	$52, %r15
	andl	$2047, %r15d            # imm = 0x7FF
	shrq	$63, %rax
	cmpl	$2047, %r15d            # imm = 0x7FF
	jne	.LBB80_9
# BB#1:                                 # %if.then
	testq	%r13, %r13
	je	.LBB80_6
# BB#2:                                 # %if.then.4
	testl	%eax, %eax
	je	.LBB80_5
# BB#3:                                 # %if.then.6
	leaq	.L.str.45(%rip), %rdx
	jmp	.LBB80_4
.LBB80_9:                               # %if.else.15
	testq	%r13, %r13
	jne	.LBB80_18
# BB#10:                                # %if.else.15
	testl	%r15d, %r15d
	jne	.LBB80_18
# BB#11:                                # %if.then.18
	testl	%ebx, %ebx
	je	.LBB80_15
# BB#12:                                # %if.then.20
	testl	%eax, %eax
	je	.LBB80_14
# BB#13:                                # %if.then.22
	leaq	.L.str.4.49(%rip), %rdx
	jmp	.LBB80_4
.LBB80_18:                              # %if.end.32
	testl	%eax, %eax
	je	.LBB80_19
# BB#20:                                # %if.then.34
	leaq	.L.str.8.53(%rip), %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r14
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
	vxorpd	.LCPI80_0(%rip), %xmm2, %xmm2
	vmovlpd	%xmm2, -48(%rbp)
	jmp	.LBB80_21
.LBB80_6:                               # %if.else.9
	testl	%eax, %eax
	je	.LBB80_8
# BB#7:                                 # %if.then.11
	leaq	.L.str.2.47(%rip), %rdx
	jmp	.LBB80_4
.LBB80_5:                               # %if.else
	leaq	.L.str.1.46(%rip), %rdx
	jmp	.LBB80_4
.LBB80_15:                              # %if.else.26
	testl	%eax, %eax
	je	.LBB80_17
# BB#16:                                # %if.then.28
	leaq	.L.str.6.51(%rip), %rdx
	jmp	.LBB80_4
.LBB80_19:
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
.LBB80_21:                              # %if.end.37
	testl	%ebx, %ebx
	je	.LBB80_36
# BB#22:                                # %while.condthread-pre-split
	xorl	%ebx, %ebx
	vmovsd	.LCPI80_1(%rip), %xmm0  # xmm0 = mem[0],zero
	vucomisd	%xmm2, %xmm0
	jbe	.LBB80_26
# BB#23:
	vmovsd	.LCPI80_2(%rip), %xmm1  # xmm1 = mem[0],zero
	.align	16, 0x90
.LBB80_24:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	vmulsd	%xmm1, %xmm2, %xmm2
	addl	$-1, %ebx
	vucomisd	%xmm2, %xmm0
	ja	.LBB80_24
# BB#25:                                # %while.cond.while.cond.41thread-pre-split_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_26:                              # %while.cond.41thread-pre-split
	vucomisd	.LCPI80_2(%rip), %xmm2
	jb	.LBB80_30
# BB#27:
	vmovsd	.LCPI80_2(%rip), %xmm0  # xmm0 = mem[0],zero
	.align	16, 0x90
.LBB80_28:                              # %while.body.43
                                        # =>This Inner Loop Header: Depth=1
	vdivsd	%xmm0, %xmm2, %xmm2
	addl	$1, %ebx
	vucomisd	%xmm0, %xmm2
	jae	.LBB80_28
# BB#29:                                # %while.cond.41.while.end.44_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_30:                              # %while.end.44
	vmovsd	.LCPI80_3(%rip), %xmm0  # xmm0 = mem[0],zero
	vfmadd213sd	.LCPI80_4(%rip), %xmm2, %xmm0
	vmovsd	.LCPI80_5(%rip), %xmm1  # xmm1 = mem[0],zero
	vsubsd	%xmm1, %xmm0, %xmm2
	vcvttsd2si	%xmm2, %rax
	movabsq	$-9223372036854775808, %rcx # imm = 0x8000000000000000
	xorq	%rax, %rcx
	vcvttsd2si	%xmm0, %rdx
	vucomisd	%xmm1, %xmm0
	cmovaeq	%rcx, %rdx
	movabsq	$4835703278458516699, %rax # imm = 0x431BDE82D7B634DB
	mulxq	%rax, %rcx, %rax
	shrq	$18, %rax
	imulq	$-1000000, %rax, %r15   # imm = 0xFFFFFFFFFFF0BDC0
	addq	%rdx, %r15
	movl	$1, %ecx
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	js	.LBB80_32
# BB#31:                                # %if.then.54
	leaq	.L.str.10.55(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	jmp	.LBB80_33
.LBB80_36:                              # %if.else.62
	testl	%r15d, %r15d
	je	.LBB80_37
# BB#38:                                # %if.end.66
	movabsq	$4503599627370496, %rax # imm = 0x10000000000000
	orq	%rax, %r13
	leal	-1075(%r15), %ecx
	cmpl	$1074, %r15d            # imm = 0x432
	ja	.LBB80_39
# BB#40:                                # %if.then.72
	cmpl	$-52, %ecx
	jge	.LBB80_42
# BB#41:
	xorl	%eax, %eax
	jmp	.LBB80_43
.LBB80_8:                               # %if.else.13
	leaq	.L.str.3.48(%rip), %rdx
	jmp	.LBB80_4
.LBB80_14:                              # %if.else.24
	leaq	.L.str.5.50(%rip), %rdx
	jmp	.LBB80_4
.LBB80_32:                              # %if.else.56
	leaq	.L.str.11.56(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	negl	%ebx
.LBB80_33:                              # %if.end.59
	movslq	%ebx, %rdx
	movl	$2, %ecx
	movq	%r12, %rsi
	jmp	.LBB80_34
.LBB80_17:                              # %if.else.30
	leaq	.L.str.7.52(%rip), %rdx
.LBB80_4:                               # %cleanup.148
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	jmp	.LBB80_35
.LBB80_37:                              # %if.then.64
	vxorpd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB80_35
.LBB80_39:
	xorl	%eax, %eax
	movq	%rax, -592(%rbp)        # 8-byte Spill
	movl	%ecx, %ebx
	jmp	.LBB80_44
.LBB80_42:                              # %if.else.76
	movl	$1075, %edx             # imm = 0x433
	subl	%r15d, %edx
	shrxq	%rdx, %r13, %rax
	shlxq	%rdx, %rax, %rdx
	subq	%rdx, %r13
.LBB80_43:                              # %if.end.84
	vmovq	%r13, %xmm0
	vmovdqa	.LCPI80_6(%rip), %xmm1  # xmm1 = [1127219200,1160773632,0,0]
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vmovapd	.LCPI80_7(%rip), %xmm2  # xmm2 = [4.503600e+15,1.934281e+25]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	shlq	$52, %rcx
	movabsq	$4696837146684686336, %rdx # imm = 0x412E848000000000
	addq	%rcx, %rdx
	vmovq	%rdx, %xmm3
	vfmadd213sd	.LCPI80_4(%rip), %xmm0, %xmm3
	vmovsd	.LCPI80_5(%rip), %xmm0  # xmm0 = mem[0],zero
	vsubsd	%xmm0, %xmm3, %xmm4
	vcvttsd2si	%xmm4, %rcx
	movabsq	$-9223372036854775808, %rdx # imm = 0x8000000000000000
	xorq	%rcx, %rdx
	vcvttsd2si	%xmm3, %rcx
	vucomisd	%xmm0, %xmm3
	cmovaeq	%rdx, %rcx
	vmovq	%rcx, %xmm0
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vucomisd	%xmm3, %xmm0
	setnp	%dl
	sete	%bl
	andb	%dl, %bl
	andb	%cl, %bl
	movzbl	%bl, %edx
	subq	%rdx, %rcx
	cmpq	$1000000, %rcx          # imm = 0xF4240
	sete	%dl
	movzbl	%dl, %r13d
	movl	$0, %edx
	cmovneq	%rcx, %rdx
	movq	%rdx, -592(%rbp)        # 8-byte Spill
	addq	%rax, %r13
	xorl	%ebx, %ebx
.LBB80_44:                              # %if.end.105
	leaq	-56(%rbp), %rsi
	leaq	-88(%rbp), %r15
	movl	$1, %ecx
	movq	%r15, %rdi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	movq	%rbx, %r13
	jle	.LBB80_65
# BB#45:                                # %for.cond.112.preheader.preheader
	testb	$1, %r13b
	jne	.LBB80_47
# BB#46:
	xorl	%r8d, %r8d
	jmp	.LBB80_53
.LBB80_47:                              # %for.cond.112.preheader.prol
	movl	$1, %r8d
	cmpq	%r15, %rax
	je	.LBB80_48
# BB#49:                                # %for.body.116.preheader.prol
	movl	$480, %ecx              # imm = 0x1E0
	subq	%rax, %rcx
	leaq	-568(%rbp,%rcx), %r11
	leaq	-1(%rax), %rsi
	xorl	%r9d, %r9d
	.align	16, 0x90
.LBB80_50:                              # %for.body.116.prol
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edi
	addl	%edi, %edi
	orl	%r9d, %edi
	movsbl	%dil, %r10d
	leal	246(%rdi), %ecx
	cmpl	$9, %r10d
	setg	%dl
	movzbl	%dl, %r9d
	cmovlel	%edi, %ecx
	addl	$48, %ecx
	movb	%cl, (%rsi)
	addq	$-1, %rsi
	addq	$1, %r11
	jne	.LBB80_50
# BB#51:                                # %for.cond.cleanup.115.prol
	cmpl	$10, %r10d
	jl	.LBB80_53
# BB#52:                                # %if.then.136.prol
	leaq	-89(%rbp), %r15
	movb	$49, -89(%rbp)
	jmp	.LBB80_53
.LBB80_48:
	movq	%rax, %r15
.LBB80_53:                              # %for.cond.112.preheader.preheader.split
	cmpl	$1, %r13d
	je	.LBB80_65
	.align	16, 0x90
.LBB80_54:                              # %for.cond.112.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB80_55 Depth 2
                                        #     Child Loop BB80_60 Depth 2
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r10
	cmpq	%r15, %rax
	je	.LBB80_59
	.align	16, 0x90
.LBB80_55:                              # %for.body.116
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edx
	addl	%edx, %edx
	orl	%r9d, %edx
	movsbl	%dl, %edi
	leal	246(%rdx), %ebx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%edx, %ebx
	addl	$48, %ebx
	movb	%bl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r15
	jne	.LBB80_55
# BB#56:                                # %for.cond.cleanup.115
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$9, %edi
	jle	.LBB80_58
# BB#57:                                # %if.then.136
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r15)
	addq	$-1, %r15
.LBB80_58:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r15, %r10
.LBB80_59:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r15
	cmpq	%r10, %rax
	je	.LBB80_64
	.align	16, 0x90
.LBB80_60:                              # %for.body.116.1
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %ebx
	addl	%ebx, %ebx
	orl	%r9d, %ebx
	movsbl	%bl, %edi
	leal	246(%rbx), %edx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%ebx, %edx
	addl	$48, %edx
	movb	%dl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r10
	jne	.LBB80_60
# BB#61:                                # %for.cond.cleanup.115.1
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$10, %edi
	jl	.LBB80_63
# BB#62:                                # %if.then.136.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r10)
	addq	$-1, %r10
.LBB80_63:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r10, %r15
.LBB80_64:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	addl	$2, %r8d
	cmpl	%r13d, %r8d
	jne	.LBB80_54
.LBB80_65:                              # %for.cond.cleanup
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	-592(%rbp), %rdx        # 8-byte Reload
.LBB80_34:                              # %cleanup.148
	callq	halide_int64_to_string@PLT
.LBB80_35:                              # %cleanup.148
	addq	$552, %rsp              # imm = 0x228
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end80:
	.size	halide_double_to_string, .Lfunc_end80-halide_double_to_string

	.section	.text.halide_pointer_to_string,"ax",@progbits
	.weak	halide_pointer_to_string
	.align	16, 0x90
	.type	halide_pointer_to_string,@function
halide_pointer_to_string:               # @halide_pointer_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, -32(%rbp)
	movl	$0, -16(%rbp)
	leaq	-14(%rbp), %r10
	leaq	-13(%rbp), %r8
	movl	$1, %ecx
	leaq	.L.str.12.57(%rip), %r9
	.align	16, 0x90
.LBB81_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	%edx, %eax
	andl	$15, %eax
	movb	(%rax,%r9), %al
	movb	%al, (%r10)
	addq	$-1, %r10
	addq	$-1, %r8
	cmpl	$15, %ecx
	jg	.LBB81_3
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB81_1 Depth=1
	shrq	$4, %rdx
	addl	$1, %ecx
	testq	%rdx, %rdx
	jne	.LBB81_1
.LBB81_3:                               # %cleanup
	movq	%r8, %rdx
	addq	$-2, %rdx
	movb	$120, (%r10)
	movb	$48, -2(%r8)
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end81:
	.size	halide_pointer_to_string, .Lfunc_end81-halide_pointer_to_string

	.section	.text.halide_get_device_handle,"ax",@progbits
	.weak	halide_get_device_handle
	.align	16, 0x90
	.type	halide_get_device_handle,@function
halide_get_device_handle:               # @halide_get_device_handle
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB82_2
# BB#1:                                 # %if.end
	movq	(%rdi), %rax
.LBB82_2:                               # %cleanup
	popq	%rbp
	retq
.Lfunc_end82:
	.size	halide_get_device_handle, .Lfunc_end82-halide_get_device_handle

	.section	.text._ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,@function
_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t: # @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %rbx
	movq	%rdi, %r14
	xorl	%eax, %eax
	cmpb	$0, 69(%rbx)
	je	.LBB83_5
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %rcx
	movl	$-14, %eax
	cmpb	$0, 68(%rbx)
	jne	.LBB83_5
# BB#2:                                 # %if.end.10
	movl	$-19, %eax
	testq	%rcx, %rcx
	je	.LBB83_5
# BB#3:                                 # %if.end.16
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	*48(%rcx)
	movl	%eax, %ecx
	movl	$-14, %eax
	testl	%ecx, %ecx
	jne	.LBB83_5
# BB#4:                                 # %if.end.25
	movb	$0, 69(%rbx)
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_buffer_is_initialized@PLT
	xorl	%eax, %eax
.LBB83_5:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end83:
	.size	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t, .Lfunc_end83-_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t

	.section	.text.halide_get_device_interface,"ax",@progbits
	.weak	halide_get_device_interface
	.align	16, 0x90
	.type	halide_get_device_interface,@function
halide_get_device_interface:            # @halide_get_device_interface
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB84_2
# BB#1:                                 # %if.end
	movq	8(%rdi), %rax
.LBB84_2:                               # %return
	popq	%rbp
	retq
.Lfunc_end84:
	.size	halide_get_device_interface, .Lfunc_end84-halide_get_device_interface

	.section	.text.halide_new_device_wrapper,"ax",@progbits
	.weak	halide_new_device_wrapper
	.align	16, 0x90
	.type	halide_new_device_wrapper,@function
halide_new_device_wrapper:              # @halide_new_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$16, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	xorl	%eax, %eax
	testq	%rbx, %rbx
	je	.LBB85_2
# BB#1:                                 # %if.end
	movq	%r14, (%rbx)
	movq	%r15, 8(%rbx)
	callq	*(%r15)
	movq	%rbx, %rax
.LBB85_2:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end85:
	.size	halide_new_device_wrapper, .Lfunc_end85-halide_new_device_wrapper

	.section	.text.halide_delete_device_wrapper,"ax",@progbits
	.weak	halide_delete_device_wrapper
	.align	16, 0x90
	.type	halide_delete_device_wrapper,@function
halide_delete_device_wrapper:           # @halide_delete_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	8(%rbx), %rax
	callq	*8(%rax)
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end86:
	.size	halide_delete_device_wrapper, .Lfunc_end86-halide_delete_device_wrapper

	.section	.text.halide_device_release,"ax",@progbits
	.weak	halide_device_release
	.align	16, 0x90
	.type	halide_device_release,@function
halide_device_release:                  # @halide_device_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmpq	*40(%rsi)               # TAILCALL
.Lfunc_end87:
	.size	halide_device_release, .Lfunc_end87-halide_device_release

	.section	.text.halide_copy_to_host,"ax",@progbits
	.weak	halide_copy_to_host
	.align	16, 0x90
	.type	halide_copy_to_host,@function
halide_copy_to_host:                    # @halide_copy_to_host
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %r15
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end88:
	.size	halide_copy_to_host, .Lfunc_end88-halide_copy_to_host

	.section	.text.halide_copy_to_device,"ax",@progbits
	.weak	halide_copy_to_device
	.align	16, 0x90
	.type	halide_copy_to_device,@function
halide_copy_to_device:                  # @halide_copy_to_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	(%r12), %rdi
	callq	halide_get_device_interface@PLT
	testq	%r15, %r15
	jne	.LBB89_2
# BB#1:                                 # %if.then
	movl	$-19, %ebx
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB89_17
.LBB89_2:                               # %if.end.24
	movq	(%r12), %rcx
	cmpq	%r15, %rax
	je	.LBB89_11
# BB#3:                                 # %if.end.24
	testq	%rcx, %rcx
	je	.LBB89_11
# BB#4:                                 # %if.then.28
	testq	%rax, %rax
	je	.LBB89_9
# BB#5:                                 # %land.lhs.true.34
	cmpb	$0, 69(%r12)
	je	.LBB89_9
# BB#6:                                 # %if.then.37
	cmpb	$0, 68(%r12)
	je	.LBB89_8
# BB#7:                                 # %if.then.40
	leaq	.L.str.25.64(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB89_8:                               # %if.end.41
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_9:                               # %if.end.50
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_device_free@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
# BB#10:                                # %if.end.58
	movb	$1, 68(%r12)
	movq	(%r12), %rcx
.LBB89_11:                              # %if.end.60
	testq	%rcx, %rcx
	jne	.LBB89_13
# BB#12:                                # %if.then.63
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_13:                              # %if.end.72
	xorl	%ebx, %ebx
	cmpb	$0, 68(%r12)
	je	.LBB89_17
# BB#14:                                # %if.then.75
	movl	$-15, %ebx
	cmpb	$0, 69(%r12)
	jne	.LBB89_17
# BB#15:                                # %if.else
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*56(%r15)
	testl	%eax, %eax
	jne	.LBB89_17
# BB#16:                                # %if.then.89
	movb	$0, 68(%r12)
	xorl	%ebx, %ebx
.LBB89_17:                              # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end89:
	.size	halide_copy_to_device, .Lfunc_end89-halide_copy_to_device

	.section	.text.halide_device_free,"ax",@progbits
	.weak	halide_device_free
	.align	16, 0x90
	.type	halide_device_free,@function
halide_device_free:                     # @halide_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r13
	movq	%rdi, %r14
	testq	%r13, %r13
	je	.LBB90_1
# BB#3:                                 # %if.then.8
	movq	(%r13), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB90_2
# BB#4:                                 # %if.then.12
	callq	*(%r12)
	movq	%r14, %rdi
	movq	%r13, %rsi
	callq	*24(%r12)
	movl	%eax, %r15d
	callq	*8(%r12)
	cmpq	$0, (%r13)
	je	.LBB90_6
# BB#5:                                 # %if.then.17
	leaq	.L.str.40(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB90_6:                               # %cleanup.22
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB90_7
.LBB90_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
.LBB90_2:                               # %if.end.23
	movb	$0, 69(%r13)
	xorl	%eax, %eax
.LBB90_7:                               # %cleanup.24
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end90:
	.size	halide_device_free, .Lfunc_end90-halide_device_free

	.section	.text.halide_device_malloc,"ax",@progbits
	.weak	halide_device_malloc
	.align	16, 0x90
	.type	halide_device_malloc,@function
halide_device_malloc:                   # @halide_device_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB91_6
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB91_6
# BB#2:                                 # %if.then
	movl	$1024, %esi             # imm = 0x400
	movq	%r14, %rdi
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB91_3
# BB#4:                                 # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.37(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB91_5
.LBB91_6:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*16(%rbx)
	movl	%eax, %r14d
	callq	*8(%rbx)
	testl	%r14d, %r14d
	movl	$-16, %eax
	cmovel	%r14d, %eax
	jmp	.LBB91_7
.LBB91_3:                               # %if.then.i
	leaq	.L.str.37(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB91_5:                               # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-16, %eax
.LBB91_7:                               # %cleanup.23
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end91:
	.size	halide_device_malloc, .Lfunc_end91-halide_device_malloc

	.section	.text.halide_device_sync,"ax",@progbits
	.weak	halide_device_sync
	.align	16, 0x90
	.type	halide_device_sync,@function
halide_device_sync:                     # @halide_device_sync
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r15
	movl	$-19, %r14d
	testq	%rbx, %rbx
	je	.LBB92_3
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB92_3
# BB#2:                                 # %if.end.2
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	*32(%rax)
	testl	%eax, %eax
	movl	$-17, %r14d
	cmovel	%eax, %r14d
.LBB92_3:                               # %cleanup.7
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end92:
	.size	halide_device_sync, .Lfunc_end92-halide_device_sync

	.section	.text.halide_weak_device_free,"ax",@progbits
	.weak	halide_weak_device_free
	.align	16, 0x90
	.type	halide_weak_device_free,@function
halide_weak_device_free:                # @halide_weak_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end93:
	.size	halide_weak_device_free, .Lfunc_end93-halide_weak_device_free

	.section	.text.halide_device_free_as_destructor,"ax",@progbits
	.weak	halide_device_free_as_destructor
	.align	16, 0x90
	.type	halide_device_free_as_destructor,@function
halide_device_free_as_destructor:       # @halide_device_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end94:
	.size	halide_device_free_as_destructor, .Lfunc_end94-halide_device_free_as_destructor

	.section	.text.halide_device_and_host_malloc,"ax",@progbits
	.weak	halide_device_and_host_malloc
	.align	16, 0x90
	.type	halide_device_and_host_malloc,@function
halide_device_and_host_malloc:          # @halide_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB95_3
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB95_3
# BB#2:                                 # %if.then
	leaq	.L.str.42(%rip), %rsi
	jmp	.LBB95_5
.LBB95_3:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*64(%rbx)
	movl	%eax, %r15d
	callq	*8(%rbx)
	xorl	%eax, %eax
	testl	%r15d, %r15d
	je	.LBB95_6
# BB#4:                                 # %if.then.21
	leaq	.L.str.43(%rip), %rsi
.LBB95_5:                               # %cleanup.22
	movq	%r14, %rdi
	callq	halide_error@PLT
	movl	$-16, %eax
.LBB95_6:                               # %cleanup.22
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end95:
	.size	halide_device_and_host_malloc, .Lfunc_end95-halide_device_and_host_malloc

	.section	.text.halide_device_and_host_free,"ax",@progbits
	.weak	halide_device_and_host_free
	.align	16, 0x90
	.type	halide_device_and_host_free,@function
halide_device_and_host_free:            # @halide_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%r12, %r12
	je	.LBB96_1
# BB#2:                                 # %if.then.8
	movq	(%r12), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB96_6
# BB#3:                                 # %if.then.12
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*72(%rbx)
	movl	%eax, %r15d
	callq	*8(%rbx)
	cmpq	$0, (%r12)
	je	.LBB96_5
# BB#4:                                 # %if.then.17
	leaq	.L.str.45.65(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB96_5:                               # %cleanup.28
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB96_9
.LBB96_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
	jmp	.LBB96_8
.LBB96_6:                               # %if.else.21
	movq	8(%r12), %rsi
	testq	%rsi, %rsi
	je	.LBB96_8
# BB#7:                                 # %if.then.23
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%r12)
.LBB96_8:                               # %if.end.29
	movb	$0, 69(%r12)
	xorl	%eax, %eax
.LBB96_9:                               # %cleanup.30
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end96:
	.size	halide_device_and_host_free, .Lfunc_end96-halide_device_and_host_free

	.section	.text.halide_default_device_and_host_malloc,"ax",@progbits
	.weak	halide_default_device_and_host_malloc
	.align	16, 0x90
	.type	halide_default_device_and_host_malloc,@function
halide_default_device_and_host_malloc:  # @halide_default_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	movq	%r14, %rdi
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %rcx
	movq	%rcx, 8(%rbx)
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB97_3
# BB#1:                                 # %if.end
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %r15d
	xorl	%eax, %eax
	testl	%r15d, %r15d
	je	.LBB97_3
# BB#2:                                 # %if.then.5
	movq	8(%rbx), %rsi
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
	movl	%r15d, %eax
.LBB97_3:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end97:
	.size	halide_default_device_and_host_malloc, .Lfunc_end97-halide_default_device_and_host_malloc

	.section	.text.halide_default_device_and_host_free,"ax",@progbits
	.weak	halide_default_device_and_host_free
	.align	16, 0x90
	.type	halide_default_device_and_host_free,@function
halide_default_device_and_host_free:    # @halide_default_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r15
	callq	halide_device_free@PLT
	movl	%eax, %r14d
	movq	8(%rbx), %rsi
	testq	%rsi, %rsi
	je	.LBB98_2
# BB#1:                                 # %if.then
	movq	%r15, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
.LBB98_2:                               # %if.end
	movw	$0, 68(%rbx)
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end98:
	.size	halide_default_device_and_host_free, .Lfunc_end98-halide_default_device_and_host_free

	.section	.text.halide_device_and_host_free_as_destructor,"ax",@progbits
	.weak	halide_device_and_host_free_as_destructor
	.align	16, 0x90
	.type	halide_device_and_host_free_as_destructor,@function
halide_device_and_host_free_as_destructor: # @halide_device_and_host_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_and_host_free@PLT # TAILCALL
.Lfunc_end99:
	.size	halide_device_and_host_free_as_destructor, .Lfunc_end99-halide_device_and_host_free_as_destructor

	.section	.text.halide_device_host_nop_free,"ax",@progbits
	.weak	halide_device_host_nop_free
	.align	16, 0x90
	.type	halide_device_host_nop_free,@function
halide_device_host_nop_free:            # @halide_device_host_nop_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end100:
	.size	halide_device_host_nop_free, .Lfunc_end100-halide_device_host_nop_free

	.section	.text.halide_float16_bits_to_float,"ax",@progbits
	.weak	halide_float16_bits_to_float
	.align	16, 0x90
	.type	halide_float16_bits_to_float,@function
halide_float16_bits_to_float:           # @halide_float16_bits_to_float
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%edi, %eax
	shll	$16, %eax
	andl	$-2147483648, %eax      # imm = 0xFFFFFFFF80000000
	movl	$1290, %ecx             # imm = 0x50A
	bextrl	%ecx, %edi, %ecx
	andl	$1023, %edi             # imm = 0x3FF
	je	.LBB101_3
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jne	.LBB101_3
# BB#2:                                 # %if.then
	lzcntl	%edi, %ecx
	xorl	$31, %ecx
	movl	$-2, %edx
	roll	%cl, %edx
	andl	%edi, %edx
	movl	$23, %esi
	subl	%ecx, %esi
	shlxl	%esi, %edx, %edx
	shll	$23, %ecx
	addl	$864026624, %ecx        # imm = 0x33800000
	orl	%eax, %ecx
	orl	%edx, %ecx
	movl	%ecx, %edi
	jmp	.LBB101_7
.LBB101_3:                              # %if.else
	shll	$13, %edi
	xorl	%edx, %edx
	testl	%ecx, %ecx
	je	.LBB101_6
# BB#4:                                 # %if.else.18
	movl	$2139095040, %edx       # imm = 0x7F800000
	cmpl	$31, %ecx
	je	.LBB101_6
# BB#5:                                 # %if.else.21
	shll	$23, %ecx
	addl	$939524096, %ecx        # imm = 0x38000000
	movl	%ecx, %edx
.LBB101_6:                              # %if.end.23
	orl	%eax, %edi
	orl	%edx, %edi
.LBB101_7:                              # %if.end.28
	vmovd	%edi, %xmm0
	popq	%rbp
	retq
.Lfunc_end101:
	.size	halide_float16_bits_to_float, .Lfunc_end101-halide_float16_bits_to_float

	.section	.text.halide_float16_bits_to_double,"ax",@progbits
	.weak	halide_float16_bits_to_double
	.align	16, 0x90
	.type	halide_float16_bits_to_double,@function
halide_float16_bits_to_double:          # @halide_float16_bits_to_double
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_float16_bits_to_float@PLT
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	popq	%rbp
	retq
.Lfunc_end102:
	.size	halide_float16_bits_to_double, .Lfunc_end102-halide_float16_bits_to_double

	.section	.text.halide_error_bounds_inference_call_failed,"ax",@progbits
	.weak	halide_error_bounds_inference_call_failed
	.align	16, 0x90
	.type	halide_error_bounds_inference_call_failed,@function
halide_error_bounds_inference_call_failed: # @halide_error_bounds_inference_call_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB103_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB103_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.68(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB103_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB103_5
.LBB103_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB103_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end103:
	.size	halide_error_bounds_inference_call_failed, .Lfunc_end103-halide_error_bounds_inference_call_failed

	.section	.text.halide_error_extern_stage_failed,"ax",@progbits
	.weak	halide_error_extern_stage_failed
	.align	16, 0x90
	.type	halide_error_extern_stage_failed,@function
halide_error_extern_stage_failed:       # @halide_error_extern_stage_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB104_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB104_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.2.70(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB104_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB104_5
.LBB104_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB104_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end104:
	.size	halide_error_extern_stage_failed, .Lfunc_end104-halide_error_extern_stage_failed

	.section	.text.halide_error_explicit_bounds_too_small,"ax",@progbits
	.weak	halide_error_explicit_bounds_too_small
	.align	16, 0x90
	.type	halide_error_explicit_bounds_too_small,@function
halide_error_explicit_bounds_too_small: # @halide_error_explicit_bounds_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movl	%r8d, -48(%rbp)         # 4-byte Spill
	movl	%ecx, -52(%rbp)         # 4-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB105_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB105_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.3.71(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.4.72(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.5.73(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r12
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.7.75(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	16(%rbp), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB105_3
# BB#4:                                 # %if.else.i.58
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB105_5
.LBB105_3:                              # %if.then.i.57
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB105_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-2, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end105:
	.size	halide_error_explicit_bounds_too_small, .Lfunc_end105-halide_error_explicit_bounds_too_small

	.section	.text.halide_error_bad_elem_size,"ax",@progbits
	.weak	halide_error_bad_elem_size
	.align	16, 0x90
	.type	halide_error_bad_elem_size,@function
halide_error_bad_elem_size:             # @halide_error_bad_elem_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%r8d, -44(%rbp)         # 4-byte Spill
	movl	%ecx, -48(%rbp)         # 4-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB106_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB106_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.9.77(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.10.78(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.11.79(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB106_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB106_5
.LBB106_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB106_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-3, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end106:
	.size	halide_error_bad_elem_size, .Lfunc_end106-halide_error_bad_elem_size

	.section	.text.halide_error_access_out_of_bounds,"ax",@progbits
	.weak	halide_error_access_out_of_bounds
	.align	16, 0x90
	.type	halide_error_access_out_of_bounds,@function
halide_error_access_out_of_bounds:      # @halide_error_access_out_of_bounds
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movl	%r8d, %r15d
	movl	%ecx, %r14d
	cmpl	%r13d, %r14d
	jge	.LBB107_6
# BB#1:                                 # %if.then
	movq	%rsi, %r15
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_3
# BB#2:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.13.81(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	jmp	.LBB107_4
.LBB107_6:                              # %if.else
	movl	16(%rbp), %r14d
	cmpl	%r14d, %r15d
	jle	.LBB107_12
# BB#7:                                 # %if.then.8
	movq	%rsi, %r13
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_9
# BB#8:                                 # %if.then.i.61
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_9:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit65
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.15.83(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
.LBB107_4:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.14.82(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB107_5
# BB#10:                                # %if.else.i.98
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB107_11
.LBB107_5:                              # %if.then.i.50
	leaq	.L.str.53(%rip), %rsi
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
.LBB107_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit103
	callq	halide_error@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB107_12:                             # %if.end.17
	movl	$-4, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end107:
	.size	halide_error_access_out_of_bounds, .Lfunc_end107-halide_error_access_out_of_bounds

	.section	.text.halide_error_buffer_allocation_too_large,"ax",@progbits
	.weak	halide_error_buffer_allocation_too_large
	.align	16, 0x90
	.type	halide_error_buffer_allocation_too_large,@function
halide_error_buffer_allocation_too_large: # @halide_error_buffer_allocation_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB108_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB108_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.16.84(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB108_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB108_5
.LBB108_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB108_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-5, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end108:
	.size	halide_error_buffer_allocation_too_large, .Lfunc_end108-halide_error_buffer_allocation_too_large

	.section	.text.halide_error_buffer_extents_negative,"ax",@progbits
	.weak	halide_error_buffer_extents_negative
	.align	16, 0x90
	.type	halide_error_buffer_extents_negative,@function
halide_error_buffer_extents_negative:   # @halide_error_buffer_extents_negative
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movl	%edx, %r13d
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB109_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB109_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.19.87(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.88(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.21.89(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB109_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB109_5
.LBB109_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB109_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-28, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end109:
	.size	halide_error_buffer_extents_negative, .Lfunc_end109-halide_error_buffer_extents_negative

	.section	.text.halide_error_buffer_extents_too_large,"ax",@progbits
	.weak	halide_error_buffer_extents_too_large
	.align	16, 0x90
	.type	halide_error_buffer_extents_too_large,@function
halide_error_buffer_extents_too_large:  # @halide_error_buffer_extents_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB110_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB110_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.22.90(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB110_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB110_5
.LBB110_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB110_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-6, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end110:
	.size	halide_error_buffer_extents_too_large, .Lfunc_end110-halide_error_buffer_extents_too_large

	.section	.text.halide_error_constraints_make_required_region_smaller,"ax",@progbits
	.weak	halide_error_constraints_make_required_region_smaller
	.align	16, 0x90
	.type	halide_error_constraints_make_required_region_smaller,@function
halide_error_constraints_make_required_region_smaller: # @halide_error_constraints_make_required_region_smaller
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	16(%rbp), %eax
	leal	-1(%r13,%rax), %edx
	movl	%edx, -60(%rbp)         # 4-byte Spill
	leal	-1(%rcx,%rax), %eax
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB111_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB111_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.23.91(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.24.92(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.25.93(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-60(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.26.94(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.27.95(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	-56(%rbp), %rcx         # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB111_3
# BB#4:                                 # %if.else.i.65
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB111_5
.LBB111_3:                              # %if.then.i.64
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB111_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-7, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end111:
	.size	halide_error_constraints_make_required_region_smaller, .Lfunc_end111-halide_error_constraints_make_required_region_smaller

	.section	.text.halide_error_constraint_violated,"ax",@progbits
	.weak	halide_error_constraint_violated
	.align	16, 0x90
	.type	halide_error_constraint_violated,@function
halide_error_constraint_violated:       # @halide_error_constraint_violated
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movl	%edx, -52(%rbp)         # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB112_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB112_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.29(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.31(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %r15         # 8-byte Reload
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB112_3
# BB#4:                                 # %if.else.i.40
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB112_5
.LBB112_3:                              # %if.then.i.39
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB112_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-8, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end112:
	.size	halide_error_constraint_violated, .Lfunc_end112-halide_error_constraint_violated

	.section	.text.halide_error_param_too_small_i64,"ax",@progbits
	.weak	halide_error_param_too_small_i64
	.align	16, 0x90
	.type	halide_error_param_too_small_i64,@function
halide_error_param_too_small_i64:       # @halide_error_param_too_small_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB113_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB113_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB113_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB113_5
.LBB113_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB113_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end113:
	.size	halide_error_param_too_small_i64, .Lfunc_end113-halide_error_param_too_small_i64

	.section	.text.halide_error_param_too_small_u64,"ax",@progbits
	.weak	halide_error_param_too_small_u64
	.align	16, 0x90
	.type	halide_error_param_too_small_u64,@function
halide_error_param_too_small_u64:       # @halide_error_param_too_small_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB114_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB114_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB114_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB114_5
.LBB114_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB114_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end114:
	.size	halide_error_param_too_small_u64, .Lfunc_end114-halide_error_param_too_small_u64

	.section	.text.halide_error_param_too_small_f64,"ax",@progbits
	.weak	halide_error_param_too_small_f64
	.align	16, 0x90
	.type	halide_error_param_too_small_f64,@function
halide_error_param_too_small_f64:       # @halide_error_param_too_small_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB115_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB115_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB115_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB115_5
.LBB115_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB115_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end115:
	.size	halide_error_param_too_small_f64, .Lfunc_end115-halide_error_param_too_small_f64

	.section	.text.halide_error_param_too_large_i64,"ax",@progbits
	.weak	halide_error_param_too_large_i64
	.align	16, 0x90
	.type	halide_error_param_too_large_i64,@function
halide_error_param_too_large_i64:       # @halide_error_param_too_large_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB116_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB116_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB116_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB116_5
.LBB116_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB116_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end116:
	.size	halide_error_param_too_large_i64, .Lfunc_end116-halide_error_param_too_large_i64

	.section	.text.halide_error_param_too_large_u64,"ax",@progbits
	.weak	halide_error_param_too_large_u64
	.align	16, 0x90
	.type	halide_error_param_too_large_u64,@function
halide_error_param_too_large_u64:       # @halide_error_param_too_large_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB117_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB117_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB117_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB117_5
.LBB117_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB117_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end117:
	.size	halide_error_param_too_large_u64, .Lfunc_end117-halide_error_param_too_large_u64

	.section	.text.halide_error_param_too_large_f64,"ax",@progbits
	.weak	halide_error_param_too_large_f64
	.align	16, 0x90
	.type	halide_error_param_too_large_f64,@function
halide_error_param_too_large_f64:       # @halide_error_param_too_large_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB118_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB118_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB118_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB118_5
.LBB118_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB118_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end118:
	.size	halide_error_param_too_large_f64, .Lfunc_end118-halide_error_param_too_large_f64

	.section	.text.halide_error_out_of_memory,"ax",@progbits
	.weak	halide_error_out_of_memory
	.align	16, 0x90
	.type	halide_error_out_of_memory,@function
halide_error_out_of_memory:             # @halide_error_out_of_memory
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.35(%rip), %rsi
	callq	halide_error@PLT
	movl	$-11, %eax
	popq	%rbp
	retq
.Lfunc_end119:
	.size	halide_error_out_of_memory, .Lfunc_end119-halide_error_out_of_memory

	.section	.text.halide_error_buffer_argument_is_null,"ax",@progbits
	.weak	halide_error_buffer_argument_is_null
	.align	16, 0x90
	.type	halide_error_buffer_argument_is_null,@function
halide_error_buffer_argument_is_null:   # @halide_error_buffer_argument_is_null
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	xorl	%r12d, %r12d
	testq	%rbx, %rbx
	je	.LBB120_2
# BB#1:                                 # %if.then.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
.LBB120_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.36(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.37.96(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB120_3
# BB#4:                                 # %if.else.i.15
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB120_5
.LBB120_3:                              # %if.then.i.14
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB120_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-12, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end120:
	.size	halide_error_buffer_argument_is_null, .Lfunc_end120-halide_error_buffer_argument_is_null

	.section	.text.halide_error_debug_to_file_failed,"ax",@progbits
	.weak	halide_error_debug_to_file_failed
	.align	16, 0x90
	.type	halide_error_debug_to_file_failed,@function
halide_error_debug_to_file_failed:      # @halide_error_debug_to_file_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB121_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB121_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.38(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.39(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.40.97(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB121_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB121_5
.LBB121_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB121_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-13, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end121:
	.size	halide_error_debug_to_file_failed, .Lfunc_end121-halide_error_debug_to_file_failed

	.section	.text.halide_error_unaligned_host_ptr,"ax",@progbits
	.weak	halide_error_unaligned_host_ptr
	.align	16, 0x90
	.type	halide_error_unaligned_host_ptr,@function
halide_error_unaligned_host_ptr:        # @halide_error_unaligned_host_ptr
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r15d
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB122_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB122_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.41(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.42.98(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.43.99(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB122_3
# BB#4:                                 # %if.else.i.23
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB122_5
.LBB122_3:                              # %if.then.i.22
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB122_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-24, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end122:
	.size	halide_error_unaligned_host_ptr, .Lfunc_end122-halide_error_unaligned_host_ptr

	.section	.text.halide_error_bad_fold,"ax",@progbits
	.weak	halide_error_bad_fold
	.align	16, 0x90
	.type	halide_error_bad_fold,@function
halide_error_bad_fold:                  # @halide_error_bad_fold
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB123_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB123_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.44(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.46.101(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB123_3
# BB#4:                                 # %if.else.i.31
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB123_5
.LBB123_3:                              # %if.then.i.30
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB123_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-25, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end123:
	.size	halide_error_bad_fold, .Lfunc_end123-halide_error_bad_fold

	.section	.text.halide_error_fold_factor_too_small,"ax",@progbits
	.weak	halide_error_fold_factor_too_small
	.align	16, 0x90
	.type	halide_error_fold_factor_too_small,@function
halide_error_fold_factor_too_small:     # @halide_error_fold_factor_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movq	%r8, -56(%rbp)          # 8-byte Spill
	movl	%ecx, %r15d
	movq	%rdx, %r13
	movq	%rsi, -64(%rbp)         # 8-byte Spill
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB124_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB124_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.47(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.48(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-64(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.49(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.50(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB124_3
# BB#4:                                 # %if.else.i.48
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB124_5
.LBB124_3:                              # %if.then.i.47
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB124_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-26, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end124:
	.size	halide_error_fold_factor_too_small, .Lfunc_end124-halide_error_fold_factor_too_small

	.section	.text.halide_error_requirement_failed,"ax",@progbits
	.weak	halide_error_requirement_failed
	.align	16, 0x90
	.type	halide_error_requirement_failed,@function
halide_error_requirement_failed:        # @halide_error_requirement_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB125_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB125_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.51(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.52(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB125_3
# BB#4:                                 # %if.else.i.19
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB125_5
.LBB125_3:                              # %if.then.i.18
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB125_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-27, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end125:
	.size	halide_error_requirement_failed, .Lfunc_end125-halide_error_requirement_failed

	.section	.text.halide_profiler_get_state,"ax",@progbits
	.weak	halide_profiler_get_state
	.align	16, 0x90
	.type	halide_profiler_get_state,@function
halide_profiler_get_state:              # @halide_profiler_get_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	_ZZ25halide_profiler_get_stateE1s(%rip), %rax
	popq	%rbp
	retq
.Lfunc_end126:
	.size	halide_profiler_get_state, .Lfunc_end126-halide_profiler_get_state

	.section	.text._ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,@function
_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy: # @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r12
	movl	%esi, %r13d
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	80(%r14), %rax
	jmp	.LBB127_1
	.align	16, 0x90
.LBB127_4:                              # %for.inc
                                        #   in Loop: Header=BB127_1 Depth=1
	movq	64(%rax), %rax
.LBB127_1:                              # %entry
                                        # =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	je	.LBB127_5
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpq	%rbx, 48(%rax)
	jne	.LBB127_4
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpl	%r13d, 72(%rax)
	jne	.LBB127_4
	jmp	.LBB127_16
.LBB127_5:                              # %for.end.critedge
	movl	$96, %edi
	callq	malloc@PLT
	movq	%rax, %r15
	xorl	%eax, %eax
	testq	%r15, %r15
	je	.LBB127_16
# BB#6:                                 # %if.end.7
	movq	80(%r14), %rax
	movq	%rax, 64(%r15)
	movq	%rbx, 48(%r15)
	movl	68(%r14), %eax
	movl	%eax, 76(%r15)
	movl	%r13d, 72(%r15)
	movl	$0, 80(%r15)
	movl	$0, 84(%r15)
	movl	$0, 88(%r15)
	movslq	%r13d, %rax
	shlq	$3, %rax
	leaq	(%rax,%rax,8), %rdi
	vxorps	%ymm0, %ymm0, %ymm0
	vmovups	%ymm0, 16(%r15)
	vmovups	%ymm0, (%r15)
	vzeroupper
	callq	malloc@PLT
	movq	%rax, 56(%r15)
	testq	%rax, %rax
	je	.LBB127_15
# BB#7:                                 # %for.cond.17.preheader
	testl	%r13d, %r13d
	jle	.LBB127_14
# BB#8:                                 # %for.body.20.preheader
	leal	-1(%r13), %r8d
	xorl	%ecx, %ecx
	testb	$3, %r13b
	je	.LBB127_11
# BB#9:                                 # %for.body.20.prol.preheader
	movl	%r13d, %esi
	andl	$3, %esi
	negl	%esi
	xorl	%ecx, %ecx
	vxorps	%ymm0, %ymm0, %ymm0
	movq	%r12, %rdi
	movq	%rax, %rbx
	.align	16, 0x90
.LBB127_10:                             # %for.body.20.prol
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rbx)
	movq	(%rdi), %rdx
	movq	%rdx, 56(%rbx)
	movl	$0, 64(%rbx)
	addq	$1, %rcx
	vmovups	%ymm0, 24(%rbx)
	vmovups	%ymm0, 8(%rbx)
	addq	$72, %rbx
	addq	$8, %rdi
	addl	$1, %esi
	jne	.LBB127_10
.LBB127_11:                             # %for.body.20.preheader.split
	cmpl	$3, %r8d
	jb	.LBB127_14
# BB#12:                                # %for.body.20.preheader.split.split
	movl	%r13d, %edx
	subl	%ecx, %edx
	leaq	(%rcx,%rcx,8), %rsi
	leaq	(%rax,%rsi,8), %rax
	leaq	(%r12,%rcx,8), %rcx
	vxorps	%ymm0, %ymm0, %ymm0
	.align	16, 0x90
.LBB127_13:                             # %for.body.20
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rax)
	movq	(%rcx), %rsi
	movq	%rsi, 56(%rax)
	movl	$0, 64(%rax)
	vmovups	%ymm0, 24(%rax)
	vmovups	%ymm0, 8(%rax)
	movq	$0, 72(%rax)
	movq	8(%rcx), %rsi
	movq	%rsi, 128(%rax)
	movl	$0, 136(%rax)
	vmovups	%ymm0, 96(%rax)
	vmovups	%ymm0, 80(%rax)
	movq	$0, 144(%rax)
	movq	16(%rcx), %rsi
	movq	%rsi, 200(%rax)
	movl	$0, 208(%rax)
	vmovups	%ymm0, 168(%rax)
	vmovups	%ymm0, 152(%rax)
	movq	$0, 216(%rax)
	movq	24(%rcx), %rsi
	movq	%rsi, 272(%rax)
	movl	$0, 280(%rax)
	vmovups	%ymm0, 240(%rax)
	vmovups	%ymm0, 224(%rax)
	addq	$288, %rax              # imm = 0x120
	addq	$32, %rcx
	addl	$-4, %edx
	jne	.LBB127_13
.LBB127_14:                             # %for.cond.cleanup.19
	addl	%r13d, 68(%r14)
	movq	%r15, 80(%r14)
	movq	%r15, %rax
	jmp	.LBB127_16
.LBB127_15:                             # %if.then.15
	movq	%r15, %rdi
	callq	free@PLT
	xorl	%eax, %eax
.LBB127_16:                             # %cleanup.62
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end127:
	.size	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy, .Lfunc_end127-_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy

	.section	.text._ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,@function
_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi: # @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	80(%rdi), %r8
	xorl	%r9d, %r9d
	testq	%r8, %r8
	je	.LBB128_8
# BB#1:
	movq	%r8, %rax
	.align	16, 0x90
.LBB128_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %r11
	movslq	76(%r11), %r10
	cmpl	%esi, %r10d
	jg	.LBB128_7
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB128_2 Depth=1
	movl	72(%r11), %eax
	addl	%r10d, %eax
	cmpl	%esi, %eax
	jg	.LBB128_4
.LBB128_7:                              # %if.end.23
                                        #   in Loop: Header=BB128_2 Depth=1
	movq	64(%r11), %rax
	movq	%r11, %r9
	testq	%rax, %rax
	jne	.LBB128_2
.LBB128_8:                              # %cleanup.25
	popq	%rbp
	retq
.LBB128_4:                              # %if.then
	testq	%r9, %r9
	je	.LBB128_6
# BB#5:                                 # %if.then.4
	movq	64(%r11), %rax
	movq	%rax, 64(%r9)
	movq	%r8, 64(%r11)
	movq	%r11, 80(%rdi)
.LBB128_6:                              # %if.end
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	shlq	$3, %rax
	addq	56(%r11), %rax
	negq	%r10
	leaq	(%r10,%r10,8), %rsi
	addq	%rdx, (%rax,%rsi,8)
	movslq	%ecx, %rcx
	movl	$1, %edi
	vmovq	%rdi, %xmm0
	vmovq	%rcx, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0]
	vpaddq	40(%rax,%rsi,8), %xmm0, %xmm1
	vmovdqu	%xmm1, 40(%rax,%rsi,8)
	addq	%rdx, (%r11)
	incl	84(%r11)
	vpaddq	32(%r11), %xmm0, %xmm0
	vmovdqu	%xmm0, 32(%r11)
	popq	%rbp
	retq
.Lfunc_end128:
	.size	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi, .Lfunc_end128-_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi

	.section	.text._ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,@function
_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv: # @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r13
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	cmpl	$-2, 72(%r13)
	je	.LBB129_11
# BB#1:                                 # %while.body.lr.ph
	leaq	-44(%rbp), %r14
	leaq	-48(%rbp), %r15
	.align	16, 0x90
.LBB129_2:                              # %while.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB129_3 Depth 2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %r12
	jmp	.LBB129_3
	.align	16, 0x90
.LBB129_9:                              # %cleanup.thread
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	64(%r13), %r12d
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%edi, %edi
	movl	%r12d, %esi
	callq	halide_sleep_ms@PLT
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %r12
.LBB129_3:                              # %while.body.3
                                        #   Parent Loop BB129_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	88(%r13), %rax
	testq	%rax, %rax
	je	.LBB129_5
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*%rax
	jmp	.LBB129_6
	.align	16, 0x90
.LBB129_5:                              # %if.else
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	72(%r13), %eax
	movl	%eax, -44(%rbp)
	movl	76(%r13), %eax
	movl	%eax, -48(%rbp)
.LBB129_6:                              # %if.end
                                        #   in Loop: Header=BB129_3 Depth=2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %rbx
	movl	-44(%rbp), %esi
	cmpl	$-2, %esi
	je	.LBB129_10
# BB#7:                                 # %if.else.10
                                        #   in Loop: Header=BB129_3 Depth=2
	testl	%esi, %esi
	js	.LBB129_9
# BB#8:                                 # %if.then.12
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%rbx, %rdx
	subq	%r12, %rdx
	movl	-48(%rbp), %ecx
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi@PLT
	jmp	.LBB129_9
	.align	16, 0x90
.LBB129_10:                             # %cleanup
                                        #   in Loop: Header=BB129_2 Depth=1
	cmpl	$-2, 72(%r13)
	jne	.LBB129_2
.LBB129_11:                             # %while.end.19
	movb	$0, 96(%r13)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end129:
	.size	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv, .Lfunc_end129-_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv

	.section	.text.halide_profiler_get_pipeline_state,"ax",@progbits
	.weak	halide_profiler_get_pipeline_state
	.align	16, 0x90
	.type	halide_profiler_get_pipeline_state,@function
halide_profiler_get_pipeline_state:     # @halide_profiler_get_pipeline_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	80(%r14), %rax
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB130_5
# BB#1:
	xorl	%r15d, %r15d
	.align	16, 0x90
.LBB130_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rbx, 48(%rax)
	je	.LBB130_3
# BB#4:                                 # %for.inc
                                        #   in Loop: Header=BB130_2 Depth=1
	movq	64(%rax), %rax
	testq	%rax, %rax
	jne	.LBB130_2
	jmp	.LBB130_5
.LBB130_3:
	movq	%rax, %r15
.LBB130_5:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end130:
	.size	halide_profiler_get_pipeline_state, .Lfunc_end130-halide_profiler_get_pipeline_state

	.section	.text.halide_profiler_pipeline_start,"ax",@progbits
	.weak	halide_profiler_pipeline_start
	.align	16, 0x90
	.type	halide_profiler_pipeline_start,@function
halide_profiler_pipeline_start:         # @halide_profiler_pipeline_start
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, %r15
	movl	%edx, %r12d
	movq	%rsi, %r13
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 96(%rbx)
	jne	.LBB131_2
# BB#1:                                 # %if.then
	movq	%r14, %rdi
	callq	halide_start_clock@PLT
	movq	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv@GOTPCREL(%rip), %rdi
	xorl	%esi, %esi
	callq	halide_spawn_thread@PLT
	movb	$1, 96(%rbx)
.LBB131_2:                              # %if.end
	movq	%r13, %rdi
	movl	%r12d, %esi
	movq	%r15, %rdx
	callq	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy@PLT
	testq	%rax, %rax
	je	.LBB131_3
# BB#4:                                 # %if.end.9
	incl	80(%rax)
	movl	76(%rax), %r14d
	jmp	.LBB131_5
.LBB131_3:                              # %if.then.7
	movq	%r14, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r14d
.LBB131_5:                              # %cleanup
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end131:
	.size	halide_profiler_pipeline_start, .Lfunc_end131-halide_profiler_pipeline_start

	.section	.text.halide_profiler_stack_peak_update,"ax",@progbits
	.weak	halide_profiler_stack_peak_update
	.align	16, 0x90
	.type	halide_profiler_stack_peak_update,@function
halide_profiler_stack_peak_update:      # @halide_profiler_stack_peak_update
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdx, %r14
	movq	%rsi, %rbx
	testq	%rbx, %rbx
	jne	.LBB132_2
# BB#1:                                 # %if.then
	leaq	.L.str.103(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB132_2:                              # %for.cond.preheader
	movl	72(%rbx), %eax
	testl	%eax, %eax
	jle	.LBB132_10
# BB#3:                                 # %for.body.lr.ph
	xorl	%edx, %edx
	.align	16, 0x90
.LBB132_4:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB132_6 Depth 2
	movq	(%r14,%rdx,8), %rsi
	testq	%rsi, %rsi
	je	.LBB132_9
# BB#5:                                 # %if.then.3
                                        #   in Loop: Header=BB132_4 Depth=1
	movq	56(%rbx), %rax
	leaq	(%rdx,%rdx,8), %rcx
	leaq	32(%rax,%rcx,8), %rdi
	movq	32(%rax,%rcx,8), %rcx
	.align	16, 0x90
.LBB132_6:                              # %while.cond.i
                                        #   Parent Loop BB132_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rsi, %rcx
	jae	.LBB132_8
# BB#7:                                 # %while.body.i
                                        #   in Loop: Header=BB132_6 Depth=2
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB132_6
.LBB132_8:                              # %for.inc.loopexit
                                        #   in Loop: Header=BB132_4 Depth=1
	movl	72(%rbx), %eax
.LBB132_9:                              # %for.inc
                                        #   in Loop: Header=BB132_4 Depth=1
	addq	$1, %rdx
	movslq	%eax, %rcx
	cmpq	%rcx, %rdx
	jl	.LBB132_4
.LBB132_10:                             # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end132:
	.size	halide_profiler_stack_peak_update, .Lfunc_end132-halide_profiler_stack_peak_update

	.section	.text.halide_profiler_memory_allocate,"ax",@progbits
	.weak	halide_profiler_memory_allocate
	.align	16, 0x90
	.type	halide_profiler_memory_allocate,@function
halide_profiler_memory_allocate:        # @halide_profiler_memory_allocate
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%r14, %r14
	je	.LBB133_13
# BB#1:                                 # %if.end
	testq	%rbx, %rbx
	jne	.LBB133_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.1.104(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_3:                              # %if.end.3
	testl	%r15d, %r15d
	jns	.LBB133_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.2.105(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_5:                              # %if.end.6
	cmpl	%r15d, 72(%rbx)
	jg	.LBB133_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.3.106(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_7:                              # %if.end.9
	movq	56(%rbx), %rdx
	lock		addl	$1, 88(%rbx)
	lock		addq	%r14, 24(%rbx)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rbx)
	movslq	%r15d, %rdi
	addq	%r14, %rsi
	movq	16(%rbx), %rcx
	addq	$16, %rbx
	.align	16, 0x90
.LBB133_8:                              # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_10
# BB#9:                                 # %while.body.i
                                        #   in Loop: Header=BB133_8 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rbx)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_8
.LBB133_10:                             # %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit
	leaq	(%rdi,%rdi,8), %rax
	lock		addl	$1, 64(%rdx,%rax,8)
	lock		addq	%r14, 24(%rdx,%rax,8)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rdx,%rax,8)
	addq	%r14, %rsi
	leaq	16(%rdx,%rax,8), %rdi
	movq	16(%rdx,%rax,8), %rcx
	.align	16, 0x90
.LBB133_11:                             # %while.cond.i.37
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_13
# BB#12:                                # %while.body.i.39
                                        #   in Loop: Header=BB133_11 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_11
.LBB133_13:                             # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end133:
	.size	halide_profiler_memory_allocate, .Lfunc_end133-halide_profiler_memory_allocate

	.section	.text.halide_profiler_memory_free,"ax",@progbits
	.weak	halide_profiler_memory_free
	.align	16, 0x90
	.type	halide_profiler_memory_free,@function
halide_profiler_memory_free:            # @halide_profiler_memory_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %rbx
	movl	%edx, %r14d
	movq	%rsi, %r15
	movq	%rdi, %r12
	testq	%rbx, %rbx
	je	.LBB134_8
# BB#1:                                 # %if.end
	testq	%r15, %r15
	jne	.LBB134_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.4.107(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_3:                              # %if.end.3
	testl	%r14d, %r14d
	jns	.LBB134_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.5.108(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_5:                              # %if.end.6
	cmpl	%r14d, 72(%r15)
	jg	.LBB134_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.6.109(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_7:                              # %if.end.9
	movq	56(%r15), %rax
	negq	%rbx
	lock		addq	%rbx, 8(%r15)
	movslq	%r14d, %rcx
	leaq	(%rcx,%rcx,8), %rcx
	lock		addq	%rbx, 8(%rax,%rcx,8)
.LBB134_8:                              # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end134:
	.size	halide_profiler_memory_free, .Lfunc_end134-halide_profiler_memory_free

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI135_0:
	.long	1232348160              # float 1.0E+6
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI135_1:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI135_2:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI135_3:
	.quad	4457293557087583675     # double 1.0E-10
	.section	.text.halide_profiler_report_unlocked,"ax",@progbits
	.weak	halide_profiler_report_unlocked
	.align	16, 0x90
	.type	halide_profiler_report_unlocked,@function
halide_profiler_report_unlocked:        # @halide_profiler_report_unlocked
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1144, %rsp             # imm = 0x478
	movq	%rdi, -1136(%rbp)       # 8-byte Spill
	leaq	-1064(%rbp), %rbx
	movb	$0, -41(%rbp)
	movq	80(%rsi), %r13
	testq	%r13, %r13
	je	.LBB135_50
# BB#1:
	leaq	-41(%rbp), %r15
	leaq	-1064(%rbp), %r12
	leaq	.L.str.20.123(%rip), %r14
	leaq	-1064(%rbp), %rbx
	.align	16, 0x90
.LBB135_2:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB135_13 Depth 2
                                        #     Child Loop BB135_16 Depth 2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movq	%r13, -1072(%rbp)       # 8-byte Spill
	movq	(%r13), %rax
	movl	%eax, %ecx
	andl	$1, %ecx
	testq	%rax, %rax
	js	.LBB135_3
# BB#4:                                 # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	vcvtsi2ssq	%rax, %xmm0, %xmm0
	jmp	.LBB135_5
	.align	16, 0x90
.LBB135_3:                              #   in Loop: Header=BB135_2 Depth=1
	shrq	%rax
	orq	%rax, %rcx
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_5:                              # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 80(%r13)
	je	.LBB135_49
# BB#6:                                 # %if.end
                                        #   in Loop: Header=BB135_2 Depth=1
	vdivss	.LCPI135_0(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movb	$0, -1064(%rbp)
	movq	32(%r13), %rax
	movq	%rax, -1120(%rbp)       # 8-byte Spill
	movq	40(%r13), %rax
	movq	%rax, -1128(%rbp)       # 8-byte Spill
	movq	48(%r13), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.7.110(%rip), %rbx
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.8.111(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.9.112(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.10.113(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	84(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.11.114(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	80(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.12.115(%rip), %rdx
	callq	halide_string_to_string@PLT
	vcvtsi2ssl	80(%r13), %xmm0, %xmm0
	vmovss	-1080(%rbp), %xmm1      # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.13.116(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1128(%rbp), %rdx       # 8-byte Reload
	movq	-1120(%rbp), %rcx       # 8-byte Reload
	cmpq	%rdx, %rcx
	je	.LBB135_8
# BB#7:                                 # %if.then.31
                                        #   in Loop: Header=BB135_2 Depth=1
	vmovq	%rcx, %xmm0
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	%rdx, %xmm1
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.14.117(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
.LBB135_8:                              # %if.end.35
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.15.118(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	88(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.16.119(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	16(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.17.120(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	cmpq	$0, (%r13)
	jne	.LBB135_14
# BB#9:                                 # %lor.end
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpq	$0, 24(%r13)
	jne	.LBB135_14
# BB#10:                                # %for.cond.50.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	movslq	72(%r13), %rax
	testq	%rax, %rax
	jle	.LBB135_49
# BB#11:                                # %for.body.53.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	56(%r13), %rcx
	addq	$32, %rcx
	xorl	%edx, %edx
	.align	16, 0x90
.LBB135_13:                             # %for.body.53
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	$0, (%rcx)
	jne	.LBB135_14
# BB#12:                                # %for.cond.50
                                        #   in Loop: Header=BB135_13 Depth=2
	addq	$1, %rdx
	addq	$72, %rcx
	cmpq	%rax, %rdx
	jl	.LBB135_13
	jmp	.LBB135_49
	.align	16, 0x90
.LBB135_14:                             # %for.cond.62.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 72(%r13)
	jle	.LBB135_49
# BB#15:                                # %if.then.i.352.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB135_16:                             # %if.then.i.352
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movb	$0, -1064(%rbp)
	movq	56(%r13), %rax
	leaq	(%rcx,%rcx,8), %r8
	leaq	(%rax,%r8,8), %rdx
	testl	%ecx, %ecx
	jne	.LBB135_18
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, (%rdx)
	movq	%r12, %rbx
	je	.LBB135_48
.LBB135_18:                             # %if.end.75
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rdx, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, -1096(%rbp)       # 8-byte Spill
	movq	%rax, %r13
	movq	%r12, %rdi
	movq	%r15, %rsi
	leaq	.L.str.18.121(%rip), %rdx
	movq	%r8, %rbx
	movq	%rbx, -1080(%rbp)       # 8-byte Spill
	callq	halide_string_to_string@PLT
	movq	56(%r13,%rbx,8), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.19.122(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$24, %rcx
	ja	.LBB135_20
	.align	16, 0x90
.LBB135_19:                             # %while.body
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$25, %rcx
	jb	.LBB135_19
.LBB135_20:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rcx       # 8-byte Reload
	movq	(%rcx), %rcx
	movl	%ecx, %edx
	andl	$1, %edx
	testq	%rcx, %rcx
	js	.LBB135_21
# BB#22:                                # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	jmp	.LBB135_23
	.align	16, 0x90
.LBB135_21:                             #   in Loop: Header=BB135_16 Depth=2
	shrq	%rcx
	orq	%rcx, %rdx
	vcvtsi2ssq	%rdx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_23:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rcx       # 8-byte Reload
	vcvtsi2ssl	80(%rcx), %xmm0, %xmm1
	vmulss	.LCPI135_0(%rip), %xmm1, %xmm1
	vdivss	%xmm1, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %edi
	testq	%rax, %rax
	je	.LBB135_25
# BB#24:                                # %if.then.i.382
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rdi
.LBB135_25:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy1024EE5eraseEi.exit
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r15, %rsi
	leaq	.L.str.21.124(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$34, %rax
	ja	.LBB135_27
	.align	16, 0x90
.LBB135_26:                             # %while.body.95
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$35, %rax
	jb	.LBB135_26
.LBB135_27:                             # %while.end.97
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rax       # 8-byte Reload
	movq	(%rax), %rsi
	movl	$0, %ebx
	testq	%rsi, %rsi
	je	.LBB135_29
# BB#28:                                # %if.then.100
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rax       # 8-byte Reload
	imulq	$100, (%rax), %rax
	xorl	%edx, %edx
	divq	%rsi
	movq	%rax, %rbx
.LBB135_29:                             # %if.end.106
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.22.125(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	%ebx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.23.126(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$42, %rax
	ja	.LBB135_31
	.align	16, 0x90
.LBB135_30:                             # %while.body.114
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$43, %rax
	jb	.LBB135_30
.LBB135_31:                             # %while.end.116
                                        #   in Loop: Header=BB135_16 Depth=2
	movl	$58, %ebx
	movq	-1128(%rbp), %rax       # 8-byte Reload
	cmpq	%rax, -1120(%rbp)       # 8-byte Folded Reload
	je	.LBB135_36
# BB#32:                                # %if.then.118
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1080(%rbp), %rax       # 8-byte Reload
	vmovq	40(%r13,%rax,8), %xmm0  # xmm0 = mem[0],zero
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	48(%r13,%rax,8), %xmm1  # xmm1 = mem[0],zero
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1088(%rbp)      # 4-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.24.127(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1088(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %ecx
	testq	%rax, %rax
	je	.LBB135_34
# BB#33:                                # %if.then.i.429
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rcx
.LBB135_34:                             # %while.cond.130.preheader
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rax
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$57, %rax
	ja	.LBB135_36
	.align	16, 0x90
.LBB135_35:                             # %while.body.133
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$58, %rax
	jb	.LBB135_35
.LBB135_36:                             # %if.end.136
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r13, %rdi
	movq	-1080(%rbp), %rsi       # 8-byte Reload
	movslq	64(%rdi,%rsi,8), %r8
	movl	$0, %eax
	testq	%r8, %r8
	je	.LBB135_38
# BB#37:                                # %if.then.140
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	24(%rdi,%rsi,8), %rax
	xorl	%edx, %edx
	divq	%r8
.LBB135_38:                             # %if.end.146
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 16(%rdi,%rsi,8)
	movq	-1072(%rbp), %r13       # 8-byte Reload
	je	.LBB135_45
# BB#39:                                # %if.then.149
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, -1104(%rbp)       # 8-byte Spill
	leaq	64(%rdi,%rsi,8), %rax
	movq	%rax, -1112(%rbp)       # 8-byte Spill
	leaq	16(%rdi,%rsi,8), %r13
	movq	%rdi, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.25.128(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	jmp	.LBB135_41
	.align	16, 0x90
.LBB135_40:                             # %while.body.157
                                        #   in Loop: Header=BB135_41 Depth=3
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
.LBB135_41:                             # %while.body.157
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rcx
	subq	%r12, %rcx
	movq	%rax, %rdi
	movq	%r15, %rsi
	cmpq	%rbx, %rcx
	jb	.LBB135_40
# BB#42:                                # %while.end.159
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.26.129(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1112(%rbp), %rcx       # 8-byte Reload
	movslq	(%rcx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	addq	$15, %rbx
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	movq	-1072(%rbp), %r13       # 8-byte Reload
	jae	.LBB135_44
	.align	16, 0x90
.LBB135_43:                             # %while.body.167
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	jb	.LBB135_43
.LBB135_44:                             # %while.end.169
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.27.130(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1104(%rbp), %rcx       # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rcx
	movq	-1088(%rbp), %rdi       # 8-byte Reload
	movq	-1080(%rbp), %rsi       # 8-byte Reload
.LBB135_45:                             # %if.end.172
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 32(%rdi,%rsi,8)
	je	.LBB135_47
# BB#46:                                # %if.then.175
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	32(%rdi,%rsi,8), %rbx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.28.131(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rcx
.LBB135_47:                             # %if.end.179
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.7.110(%rip), %rdx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	movq	-1096(%rbp), %rcx       # 8-byte Reload
.LBB135_48:                             # %cleanup.182
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$1, %rcx
	movslq	72(%r13), %rax
	cmpq	%rax, %rcx
	jl	.LBB135_16
.LBB135_49:                             # %cleanup.191
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	64(%r13), %r13
	testq	%r13, %r13
	jne	.LBB135_2
.LBB135_50:                             # %if.else.i
	movl	$1, %edx
	leaq	-1064(%rbp), %rsi
	subq	%rsi, %rdx
	addq	%rbx, %rdx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	callq	halide_msan_annotate_memory_is_initialized@PLT
	addq	$1144, %rsp             # imm = 0x478
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end135:
	.size	halide_profiler_report_unlocked, .Lfunc_end135-halide_profiler_report_unlocked

	.section	.text.halide_profiler_report,"ax",@progbits
	.weak	halide_profiler_report
	.align	16, 0x90
	.type	halide_profiler_report,@function
halide_profiler_report:                 # @halide_profiler_report
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_profiler_report_unlocked@PLT
	movq	%rbx, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end136:
	.size	halide_profiler_report, .Lfunc_end136-halide_profiler_report

	.section	.text.halide_profiler_reset,"ax",@progbits
	.weak	halide_profiler_reset
	.align	16, 0x90
	.type	halide_profiler_reset,@function
halide_profiler_reset:                  # @halide_profiler_reset
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	jmp	.LBB137_2
	.align	16, 0x90
.LBB137_1:                              # %while.body
                                        #   in Loop: Header=BB137_2 Depth=1
	movq	64(%rbx), %rax
	movq	%rax, 80(%r14)
	movq	56(%rbx), %rdi
	callq	free@PLT
	movq	%rbx, %rdi
	callq	free@PLT
.LBB137_2:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	80(%r14), %rbx
	testq	%rbx, %rbx
	jne	.LBB137_1
# BB#3:                                 # %while.end
	movl	$0, 68(%r14)
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end137:
	.size	halide_profiler_reset, .Lfunc_end137-halide_profiler_reset

	.section	.text.halide_profiler_shutdown,"ax",@progbits
	.weak	halide_profiler_shutdown
	.align	16, 0x90
	.type	halide_profiler_shutdown,@function
halide_profiler_shutdown:               # @halide_profiler_shutdown
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_profiler_get_state@PLT
	cmpb	$0, 96(%rax)
	je	.LBB138_4
# BB#1:                                 # %if.end
	movl	$-2, 72(%rax)
	.align	16, 0x90
.LBB138_2:                              # %do.body
                                        # =>This Inner Loop Header: Depth=1
	mfence
	cmpb	$0, 96(%rax)
	jne	.LBB138_2
# BB#3:                                 # %do.end
	movl	$-1, 72(%rax)
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	halide_profiler_report_unlocked@PLT # TAILCALL
.LBB138_4:                              # %cleanup
	popq	%rbp
	retq
.Lfunc_end138:
	.size	halide_profiler_shutdown, .Lfunc_end138-halide_profiler_shutdown

	.section	.text.halide_profiler_pipeline_end,"ax",@progbits
	.weak	halide_profiler_pipeline_end
	.align	16, 0x90
	.type	halide_profiler_pipeline_end,@function
halide_profiler_pipeline_end:           # @halide_profiler_pipeline_end
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$-1, 72(%rsi)
	popq	%rbp
	retq
.Lfunc_end139:
	.size	halide_profiler_pipeline_end, .Lfunc_end139-halide_profiler_pipeline_end

	.section	.text.halide_msan_annotate_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_memory_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_memory_is_initialized,@function
halide_msan_annotate_memory_is_initialized: # @halide_msan_annotate_memory_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end140:
	.size	halide_msan_annotate_memory_is_initialized, .Lfunc_end140-halide_msan_annotate_memory_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized,@function
halide_msan_annotate_buffer_is_initialized: # @halide_msan_annotate_buffer_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end141:
	.size	halide_msan_annotate_buffer_is_initialized, .Lfunc_end141-halide_msan_annotate_buffer_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized_as_destructor,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized_as_destructor
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized_as_destructor,@function
halide_msan_annotate_buffer_is_initialized_as_destructor: # @halide_msan_annotate_buffer_is_initialized_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end142:
	.size	halide_msan_annotate_buffer_is_initialized_as_destructor, .Lfunc_end142-halide_msan_annotate_buffer_is_initialized_as_destructor

	.section	.text.halide_default_can_use_target_features,"ax",@progbits
	.weak	halide_default_can_use_target_features
	.align	16, 0x90
	.type	halide_default_can_use_target_features,@function
halide_default_can_use_target_features: # @halide_default_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$24, %rsp
	movq	%rdi, %rbx
	movb	_ZZ38halide_default_can_use_target_featuresE11initialized(%rip), %al
	andb	$1, %al
	jne	.LBB143_2
# BB#1:                                 # %if.then
	leaq	-24(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv@PLT
	vmovups	-24(%rbp), %xmm0
	vmovups	%xmm0, _ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip)
	movb	$1, _ZZ38halide_default_can_use_target_featuresE11initialized(%rip)
.LBB143_2:                              # %if.end
	andq	_ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip), %rbx
	je	.LBB143_4
# BB#3:                                 # %if.then.1
	movq	_ZZ38halide_default_can_use_target_featuresE12cpu_features+8(%rip), %rcx
	andq	%rbx, %rcx
	xorl	%eax, %eax
	cmpq	%rbx, %rcx
	jne	.LBB143_5
.LBB143_4:                              # %if.end.6
	movl	$1, %eax
.LBB143_5:                              # %cleanup
	addq	$24, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end143:
	.size	halide_default_can_use_target_features, .Lfunc_end143-halide_default_can_use_target_features

	.section	.text.halide_set_custom_can_use_target_features,"ax",@progbits
	.weak	halide_set_custom_can_use_target_features
	.align	16, 0x90
	.type	halide_set_custom_can_use_target_features,@function
halide_set_custom_can_use_target_features: # @halide_set_custom_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end144:
	.size	halide_set_custom_can_use_target_features, .Lfunc_end144-halide_set_custom_can_use_target_features

	.section	.text.halide_can_use_target_features,"ax",@progbits
	.weak	halide_can_use_target_features
	.align	16, 0x90
	.type	halide_can_use_target_features,@function
halide_can_use_target_features:         # @halide_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end145:
	.size	halide_can_use_target_features, .Lfunc_end145-halide_can_use_target_features

	.section	.text._ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,@function
_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv: # @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	movl	$1, -24(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-24(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -24(%rbp)
	movl	%ebx, -20(%rbp)
	movl	%ecx, -16(%rbp)
	movl	%edx, -12(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-16(%rbp), %eax
	movl	%eax, %ecx
	andl	$524288, %ecx           # imm = 0x80000
	shrq	$15, %rcx
	movl	%eax, %edx
	shrl	$23, %edx
	andl	$32, %edx
	orq	%rcx, %rdx
	movl	%eax, %ecx
	shrl	$20, %ecx
	andl	$512, %ecx              # imm = 0x200
	orq	%rdx, %rcx
	movl	%eax, %r8d
	shrl	$5, %r8d
	andl	$128, %r8d
	orq	%rcx, %r8
	andl	$1879048192, %eax       # imm = 0x70000000
	cmpl	$1879048192, %eax       # imm = 0x70000000
	jne	.LBB146_4
# BB#1:                                 # %if.then.33
	movl	$7, -40(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-40(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -40(%rbp)
	movl	%ebx, -36(%rbp)
	movl	%ecx, -32(%rbp)
	movl	%edx, -28(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-36(%rbp), %ecx
	movl	%ecx, %eax
	andl	$32, %eax
	addq	%rax, %rax
	orq	%r8, %rax
	movl	%ecx, %edx
	andl	$268500992, %edx        # imm = 0x10010000
	cmpl	$268500992, %edx        # imm = 0x10010000
	jne	.LBB146_3
# BB#2:                                 # %if.then.44
	movl	%ecx, %edx
	andl	$469827584, %edx        # imm = 0x1C010000
	cmpl	$469827584, %edx        # imm = 0x1C010000
	movabsq	$824633720832, %rdx     # imm = 0xC000000000
	movabsq	$274877906944, %rsi     # imm = 0x4000000000
	cmoveq	%rdx, %rsi
	orq	%rsi, %rax
	movl	%ecx, %edx
	andl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	movabsq	$1099511627776, %rsi    # imm = 0x10000000000
	orq	%rax, %rsi
	cmpl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	cmovneq	%rax, %rsi
	andl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	movabsq	$2199023255552, %rax    # imm = 0x20000000000
	orq	%rsi, %rax
	cmpl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	cmovneq	%rsi, %rax
.LBB146_3:                              # %if.end.64
	movq	%rax, %r8
.LBB146_4:                              # %if.end.65
	movabsq	$4123168604912, %rax    # imm = 0x3C0000002F0
	movq	%rax, (%rdi)
	movq	%r8, 8(%rdi)
	movq	%rdi, %rax
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end146:
	.size	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv, .Lfunc_end146-_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv

	.section	.rodata,"a",@progbits
	.align	32
.LCPI147_0:
	.long	3                       # 0x3
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI147_1:
	.long	3                       # 0x3
	.long	4096                    # 0x1000
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI147_2:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
	.long	4294967288              # 0xfffffff8
	.long	4294967286              # 0xfffffff6
	.long	4294967284              # 0xfffffff4
	.long	4294967282              # 0xfffffff2
.LCPI147_3:
	.long	4294967280              # 0xfffffff0
	.long	4294967278              # 0xffffffee
	.long	4294967276              # 0xffffffec
	.long	4294967274              # 0xffffffea
	.long	4294967272              # 0xffffffe8
	.long	4294967270              # 0xffffffe6
	.long	4294967268              # 0xffffffe4
	.long	4294967266              # 0xffffffe2
.LCPI147_7:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
.LCPI147_10:
	.long	16                      # 0x10
	.long	18                      # 0x12
	.long	20                      # 0x14
	.long	22                      # 0x16
	.long	24                      # 0x18
	.long	26                      # 0x1a
	.long	28                      # 0x1c
	.long	30                      # 0x1e
.LCPI147_11:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	8                       # 0x8
	.long	10                      # 0xa
	.long	12                      # 0xc
	.long	14                      # 0xe
.LCPI147_12:
	.zero	4
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
.LCPI147_13:
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
	.zero	4
.LCPI147_14:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI147_15:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
.LCPI147_26:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
.LCPI147_27:
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI147_4:
	.long	1                       # 0x1
.LCPI147_5:
	.long	1199570688              # float 65535
.LCPI147_17:
	.long	1065353216              # float 1
.LCPI147_18:
	.long	1073741824              # float 2
.LCPI147_19:
	.long	1048576000              # float 0.25
.LCPI147_20:
	.long	1056964608              # float 0.5
.LCPI147_21:
	.long	2147483647              # 0x7fffffff
.LCPI147_22:
	.long	40                      # 0x28
.LCPI147_23:
	.long	1045220557              # float 0.200000003
.LCPI147_24:
	.long	1042983595              # float 0.166666672
.LCPI147_25:
	.long	1166012416              # float 4095
.LCPI147_28:
	.long	3212836864              # float -1
.LCPI147_29:
	.long	3186723360              # float -0.117939234
.LCPI147_30:
	.long	3190598332              # float -0.16862005
.LCPI147_31:
	.long	3196053750              # float -0.249912113
.LCPI147_32:
	.long	3204448274              # float -0.500001073
.LCPI147_33:
	.long	1028743925              # float 0.0511197634
.LCPI147_34:
	.long	1041846319              # float 0.149719939
.LCPI147_35:
	.long	1045207583              # float 0.199806675
.LCPI147_36:
	.long	1051372237              # float 0.333334357
.LCPI147_37:
	.long	1060205080              # float 0.693147182
.LCPI147_39:
	.long	1069066811              # float 1.44269502
.LCPI147_40:
	.long	3207688704              # float -0.693145751
.LCPI147_41:
	.long	3049242254              # float -1.42860677E-6
.LCPI147_42:
	.long	983314022               # float 0.00119156833
.LCPI147_43:
	.long	1026188988              # float 0.0416018814
.LCPI147_44:
	.long	1056964574              # float 0.499998987
.LCPI147_45:
	.long	967284723               # float 3.19659332E-4
.LCPI147_46:
	.long	1007360298              # float 0.00848988629
.LCPI147_47:
	.long	1042984479              # float 0.166679844
.LCPI147_48:
	.long	2139095040              # float +Inf
.LCPI147_49:
	.long	901758606               # float 1.42860677E-6
.LCPI147_50:
	.long	1060205056              # float 0.693145751
.LCPI147_51:
	.long	127                     # 0x7f
.LCPI147_52:
	.long	255                     # 0xff
.LCPI147_53:
	.long	4286578688              # float -Inf
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI147_6:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI147_8:
	.byte	0                       # 0x0
	.byte	2                       # 0x2
	.byte	4                       # 0x4
	.byte	6                       # 0x6
	.byte	8                       # 0x8
	.byte	10                      # 0xa
	.byte	12                      # 0xc
	.byte	14                      # 0xe
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
.LCPI147_9:
	.zero	16,1
.LCPI147_16:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI147_54:
	.zero	16,255
	.section	.rodata.cst8,"aM",@progbits,8
	.align	4
.LCPI147_38:
	.long	4286578688              # float -inf
	.long	2143289344              # float nan
	.section	.text.__sharpi,"ax",@progbits
	.globl	__sharpi
	.align	16, 0x90
	.type	__sharpi,@function
__sharpi:                               # @__sharpi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$6592, %rsp             # imm = 0x19C0
	testq	%rdi, %rdi
	je	.LBB147_1
# BB#29:                                # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB147_30
# BB#31:                                # %assert succeeded11
	testq	%r8, %r8
	je	.LBB147_32
# BB#33:                                # %assert succeeded30
	testq	%r9, %r9
	je	.LBB147_34
# BB#35:                                # %assert succeeded49
	movq	%rcx, 5672(%rsp)        # 8-byte Spill
	movq	%r9, 5624(%rsp)         # 8-byte Spill
	cmpq	$0, 88(%rbp)
	je	.LBB147_36
# BB#37:                                # %assert succeeded68
	movq	112(%rbp), %r11
	testq	%r11, %r11
	je	.LBB147_38
# BB#39:                                # %assert succeeded87
	movslq	16(%rdi), %r13
	movq	%r13, 4904(%rsp)        # 8-byte Spill
	movl	48(%rdi), %r12d
	movq	%r12, 5536(%rsp)        # 8-byte Spill
	movl	16(%r11), %eax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	movslq	20(%r11), %r10
	movq	%r10, 472(%rsp)         # 8-byte Spill
	movl	48(%r11), %ebx
	movq	%rbx, 480(%rsp)         # 8-byte Spill
	movslq	52(%r11), %r14
	movq	%r14, 1016(%rsp)        # 8-byte Spill
	leal	(%r14,%r10), %ecx
	movq	%rcx, 424(%rsp)         # 8-byte Spill
	cmpl	%edx, %ecx
	movl	%ecx, %r9d
	cmovll	%edx, %r9d
	movq	%r9, 5456(%rsp)         # 8-byte Spill
	movq	%rdx, 752(%rsp)         # 8-byte Spill
	leal	(%rbx,%rax), %eax
	movq	%rax, 440(%rsp)         # 8-byte Spill
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	%rsi, 760(%rsp)         # 8-byte Spill
	movl	%ebx, %r15d
	sarl	$31, %r15d
	andl	%ebx, %r15d
	movq	%r15, 5352(%rsp)        # 8-byte Spill
	leal	-8(%r15), %edx
	movq	%rdx, 3064(%rsp)        # 8-byte Spill
	leal	-1(%r12,%r13), %ebx
	movl	%ebx, 4352(%rsp)        # 4-byte Spill
	cmpl	%edx, %ebx
	movl	%ebx, %ecx
	cmovgl	%edx, %ecx
	cmpl	%r12d, %ecx
	cmovll	%r12d, %ecx
	leal	(%r13,%r13), %edx
	movl	$2, %esi
	subl	%edx, %esi
	cmpl	$1, %edx
	leal	-2(%r13,%r13), %edx
	movl	%edx, 4336(%rsp)        # 4-byte Spill
	cmovgl	%edx, %esi
	subl	%r13d, %esi
	leal	-1(%r13), %edx
	cmpl	%esi, %edx
	cmoval	%edx, %esi
	leal	-1(%r13,%r12), %edx
	subl	%esi, %edx
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	movl	%edx, 4976(%rsp)        # 4-byte Spill
	cmpl	$1, %eax
	leal	-1(%rax), %eax
	movl	%eax, 400(%rsp)         # 4-byte Spill
	movl	$0, %ecx
	cmovgl	%eax, %ecx
	subl	%r15d, %ecx
	movq	%rcx, 5504(%rsp)        # 8-byte Spill
	leal	16(%rcx), %eax
	andl	$-32, %eax
	leal	23(%r15,%rax), %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%r12d, %eax
	cmovll	%r12d, %eax
	leal	(%r12,%r13), %ecx
	movq	%rcx, 2160(%rsp)        # 8-byte Spill
	cmpl	%ecx, %eax
	leal	1(%rax), %eax
	cmovll	%ecx, %eax
	movl	%eax, 4960(%rsp)        # 4-byte Spill
	movl	%r14d, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	movq	%rax, 672(%rsp)         # 8-byte Spill
	movslq	20(%rdi), %rbx
	movq	%rbx, 1400(%rsp)        # 8-byte Spill
	movslq	52(%rdi), %rsi
	movq	%rsi, 1408(%rsp)        # 8-byte Spill
	leal	-8(%rax), %ecx
	movl	%ecx, 5760(%rsp)        # 4-byte Spill
	leal	-1(%rsi,%rbx), %r12d
	movl	%r12d, 1396(%rsp)       # 4-byte Spill
	cmpl	%ecx, %r12d
	movl	%r12d, %eax
	cmovgl	%ecx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	leal	(%rbx,%rbx), %ecx
	movl	$2, %edx
	subl	%ecx, %edx
	cmpl	$1, %ecx
	leal	-2(%rbx,%rbx), %ecx
	movl	%ecx, 1388(%rsp)        # 4-byte Spill
	cmovgl	%ecx, %edx
	movl	%edx, 1392(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	subl	%ebx, %ecx
	leal	-1(%rbx), %edx
	cmpl	%ecx, %edx
	cmoval	%edx, %ecx
	leal	-1(%rbx,%rsi), %edx
	movl	%edx, 1384(%rsp)        # 4-byte Spill
	subl	%ecx, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	movl	%edx, 4944(%rsp)        # 4-byte Spill
	cmpl	$1, %r9d
	leal	-1(%r9), %eax
	movl	%eax, 404(%rsp)         # 4-byte Spill
	movl	$0, %ecx
	cmovgl	%eax, %ecx
	movq	%rcx, 5696(%rsp)        # 8-byte Spill
	movl	$0, %eax
	cmovgl	%r9d, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	leal	8(%rcx), %edx
	movl	%edx, 5248(%rsp)        # 4-byte Spill
	cmpl	%edx, %r12d
	movl	%r12d, %eax
	cmovgl	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	leal	(%rsi,%rbx), %ecx
	movl	%ecx, 740(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	leal	1(%rax), %eax
	cmovll	%ecx, %eax
	movl	%eax, 4936(%rsp)        # 4-byte Spill
	leal	-32(%r14,%r10), %eax
	movl	%eax, 396(%rsp)         # 4-byte Spill
	cmpl	%r14d, %eax
	cmovgl	%r14d, %eax
	movl	%eax, 468(%rsp)         # 4-byte Spill
	leal	-1(%r10), %eax
	movq	%rax, 456(%rsp)         # 8-byte Spill
	orl	$31, %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	leal	(%rax,%r14), %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	leal	-1(%r14,%r10), %ecx
	movl	%ecx, 420(%rsp)         # 4-byte Spill
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 4992(%rsp)        # 4-byte Spill
	movq	5672(%rsp), %rdx        # 8-byte Reload
	movl	48(%rdx), %r13d
	movq	%r13, 4712(%rsp)        # 8-byte Spill
	movl	$1, %eax
	subl	%r13d, %eax
	movq	(%rdi), %rcx
	movq	%rcx, 5360(%rsp)        # 8-byte Spill
	movq	8(%rdi), %rcx
	movq	%rcx, 5048(%rsp)        # 8-byte Spill
	movl	32(%rdi), %ecx
	movl	%ecx, 4448(%rsp)        # 4-byte Spill
	movslq	36(%rdi), %rcx
	movq	%rcx, 1744(%rsp)        # 8-byte Spill
	movl	64(%rdi), %ecx
	movl	%ecx, 4632(%rsp)        # 4-byte Spill
	movq	%rdi, 4800(%rsp)        # 8-byte Spill
	movq	(%rdx), %rcx
	movq	%rcx, 5408(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 5528(%rsp)        # 8-byte Spill
	movslq	16(%rdx), %r9
	movq	%r9, 4696(%rsp)         # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 5568(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4576(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 4664(%rsp)        # 8-byte Spill
	movl	52(%rdx), %r10d
	movq	%r10, 4728(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4672(%rsp)        # 4-byte Spill
	movq	(%r8), %rcx
	movq	%rcx, 5424(%rsp)        # 8-byte Spill
	movq	8(%r8), %rcx
	movq	%rcx, 1888(%rsp)        # 8-byte Spill
	movslq	16(%r8), %rcx
	movq	%rcx, 5440(%rsp)        # 8-byte Spill
	movslq	20(%r8), %rcx
	movq	%rcx, 1848(%rsp)        # 8-byte Spill
	movl	32(%r8), %ecx
	movl	%ecx, 4544(%rsp)        # 4-byte Spill
	movslq	36(%r8), %rcx
	movq	%rcx, 1880(%rsp)        # 8-byte Spill
	movl	48(%r8), %ecx
	movq	%rcx, 5472(%rsp)        # 8-byte Spill
	movl	52(%r8), %ecx
	movq	%rcx, 1816(%rsp)        # 8-byte Spill
	movl	64(%r8), %ecx
	movl	%ecx, 4640(%rsp)        # 4-byte Spill
	movq	%r8, 4832(%rsp)         # 8-byte Spill
	movq	5624(%rsp), %rdx        # 8-byte Reload
	movq	(%rdx), %rcx
	movq	%rcx, 5280(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 896(%rsp)         # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 4848(%rsp)        # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 4760(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4384(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 888(%rsp)         # 8-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 4736(%rsp)        # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 4744(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4616(%rsp)        # 4-byte Spill
	movq	88(%rbp), %rcx
	movq	%rcx, %rdx
	movq	(%rdx), %rcx
	movq	%rcx, 5392(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 5632(%rsp)        # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 4688(%rsp)        # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 4680(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4480(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 4896(%rsp)        # 8-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 4704(%rsp)        # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 4880(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4624(%rsp)        # 4-byte Spill
	movq	(%r11), %rcx
	movq	%rcx, 5376(%rsp)        # 8-byte Spill
	movq	8(%r11), %rcx
	movq	%rcx, 744(%rsp)         # 8-byte Spill
	movl	24(%r11), %ecx
	movq	%rcx, 4512(%rsp)        # 8-byte Spill
	movl	32(%r11), %ecx
	movl	%ecx, 4368(%rsp)        # 4-byte Spill
	movslq	36(%r11), %rcx
	movq	%rcx, 408(%rsp)         # 8-byte Spill
	movl	40(%r11), %ecx
	movl	%ecx, 4416(%rsp)        # 4-byte Spill
	movl	56(%r11), %ecx
	movq	%rcx, 4816(%rsp)        # 8-byte Spill
	movl	64(%r11), %ecx
	movl	%ecx, 4608(%rsp)        # 4-byte Spill
	leal	(%r9,%r9), %r11d
	cltd
	idivl	%r11d
	movl	%r11d, %eax
	negl	%eax
	movl	%r9d, %edi
	sarl	$31, %edi
	andnl	%r11d, %edi, %ecx
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	movl	%ebx, 4192(%rsp)        # 4-byte Spill
	movl	%r11d, %esi
	subl	%ebx, %esi
	addl	$-1, %esi
	movl	%esi, 4312(%rsp)        # 4-byte Spill
	cmpl	%ebx, %esi
	movl	%ebx, %eax
	cmovgel	%esi, %eax
	movl	%eax, 5728(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	movl	%r13d, %eax
	negl	%eax
	cltd
	idivl	%r11d
	movl	%ecx, %r14d
	sarl	$31, %r14d
	andl	%edi, %r14d
	addl	%ecx, %r14d
	movl	%r14d, 4168(%rsp)       # 4-byte Spill
	movl	%edx, %r15d
	sarl	$31, %r15d
	andl	%edi, %r15d
	addl	%edx, %r15d
	movl	%r15d, 4160(%rsp)       # 4-byte Spill
	movl	%r11d, %r12d
	subl	%r14d, %r12d
	addl	$-1, %r12d
	movl	%r12d, 4224(%rsp)       # 4-byte Spill
	cmpl	%r12d, %r14d
	movl	%r14d, %r8d
	cmovgl	%r12d, %r8d
	addl	%r13d, %r8d
	leal	-1(%r13,%r9), %eax
	movl	%eax, 5680(%rsp)        # 4-byte Spill
	cmpl	%r8d, %eax
	cmovlel	%eax, %r8d
	subl	%r15d, %r11d
	addl	$-1, %r11d
	movl	%r11d, 4888(%rsp)       # 4-byte Spill
	cmpl	%r11d, %r15d
	movl	%r15d, %edx
	cmovgl	%r11d, %edx
	addl	%r13d, %edx
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	cmpl	%esi, %ebx
	movl	%ebx, %ecx
	cmovgl	%esi, %ecx
	addl	%r13d, %ecx
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	%ecx, %eax
	sarl	$31, %eax
	cmpl	$3, %r8d
	movl	$2, %esi
	cmovll	%r8d, %esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	movl	%edx, %r9d
	sarl	$31, %r9d
	andl	%edx, %r9d
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	movl	%esi, %ebx
	sarl	$31, %ebx
	testl	%esi, %esi
	setg	%dil
	andl	%esi, %ebx
	movzbl	%dil, %esi
	orl	%esi, %ebx
	cmpl	%edx, %ebx
	cmovgl	%edx, %ebx
	movl	%ebx, %edi
	sarl	$31, %edi
	andl	%ebx, %edi
	cmpl	%ecx, %edi
	cmovgl	%ecx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	testl	%edi, %edi
	setg	%bl
	andl	%edi, %esi
	movzbl	%bl, %edi
	orl	%edi, %esi
	cmpl	%r8d, %esi
	cmovgl	%r8d, %esi
	cmpl	$3, %esi
	movl	$2, %edi
	cmovgel	%edi, %esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	movl	%esi, %edi
	sarl	$31, %edi
	testl	%esi, %esi
	setg	%bl
	andl	%esi, %edi
	movzbl	%bl, %esi
	orl	%esi, %edi
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	movl	%edi, %edx
	sarl	$31, %edx
	andl	%edi, %edx
	cmpl	%r8d, %edx
	cmovgl	%r8d, %edx
	testl	%ecx, %ecx
	setg	%bl
	andl	%ecx, %eax
	movzbl	%bl, %ecx
	orl	%ecx, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	$3, %eax
	movl	$2, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r13d, %eax
	cmovll	%r13d, %eax
	movl	%eax, 4784(%rsp)        # 4-byte Spill
	movl	5728(%rsp), %edx        # 4-byte Reload
	addl	%r13d, %edx
	cmpl	%r14d, %r12d
	cmovgel	%r12d, %r14d
	addl	%r13d, %r14d
	cmpl	$1, %r14d
	cmovlel	%ecx, %r14d
	cmpl	%r14d, %edx
	cmovgel	%edx, %r14d
	cmpl	%r15d, %r11d
	cmovgel	%r11d, %r15d
	addl	%r13d, %r15d
	movl	$0, %ecx
	cmovsl	%ecx, %r15d
	cmpl	%r15d, %r14d
	cmovgel	%r14d, %r15d
	cmpl	%edx, %r15d
	cmovll	%edx, %r15d
	cmpl	$2, %r15d
	setl	%al
	cmpl	$1, %r15d
	cmovlel	%ecx, %r15d
	movzbl	%al, %eax
	orl	%eax, %r15d
	movl	5680(%rsp), %eax        # 4-byte Reload
	cmpl	%r15d, %eax
	cmovlel	%eax, %r15d
	cmpl	%r13d, %r15d
	cmovll	%r13d, %r15d
	movl	%r15d, 4928(%rsp)       # 4-byte Spill
	movq	5568(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rdx), %esi
	movl	%esi, 4320(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%esi, %ecx, %edi
	andl	%eax, %ecx
	orl	%edi, %ecx
	movq	5504(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %edi
	andl	$-8, %edi
	movq	%rdi, 5200(%rsp)        # 8-byte Spill
	movl	%esi, %r9d
	subl	%ecx, %r9d
	cmovgel	%esi, %ecx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	movq	5352(%rsp), %r12        # 8-byte Reload
	leal	7(%rdi,%r12), %eax
	movq	%rdi, %r8
	leal	-1(%r10,%rdx), %esi
	movq	%rdx, %r15
	movl	%esi, 4256(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	leal	-1(%rcx,%r10), %edi
	movl	%edi, 5072(%rsp)        # 4-byte Spill
	addl	%r10d, %ecx
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	leal	12(%rbx), %eax
	movq	%rbx, %r11
	andl	$-8, %eax
	movq	%rax, 5168(%rsp)        # 8-byte Spill
	movl	%eax, %r14d
	movq	%r12, %rbx
	addl	%ebx, %r14d
	leal	1(%rax,%rbx), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5184(%rsp)        # 4-byte Spill
	leal	3(%rax,%rbx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5136(%rsp)        # 4-byte Spill
	leal	(%r10,%r15), %edx
	movq	%rdx, 5488(%rsp)        # 8-byte Spill
	cmpl	%r14d, %edx
	movl	%edx, %eax
	movq	%rdx, %r12
	cmovgl	%r14d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	addl	$-1, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5120(%rsp)        # 4-byte Spill
	leal	4(%r11), %ecx
	andl	$-8, %ecx
	movq	%rcx, 5104(%rsp)        # 8-byte Spill
	movq	%rbx, %rax
	leal	5(%rcx,%rax), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	movl	%edi, %ecx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5096(%rsp)        # 4-byte Spill
	leal	9(%r8,%rax), %edx
	movq	%rax, %rdi
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5152(%rsp)        # 4-byte Spill
	movl	%r9d, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%r10d, %eax
	leal	2(%rdi), %ecx
	movq	%rcx, 4656(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 5056(%rsp)        # 4-byte Spill
	movq	%rdi, %rcx
	cmpl	%eax, %ecx
	movq	%rcx, %rdx
	cmovgl	%eax, %edi
	leal	1(%rdx), %ecx
	movq	%rcx, 4648(%rsp)        # 8-byte Spill
	cmovgel	%eax, %ecx
	movl	%ecx, 5728(%rsp)        # 4-byte Spill
	cmpl	%edi, %esi
	cmovlel	%esi, %edi
	cmpl	%r10d, %edi
	cmovll	%r10d, %edi
	movl	%edi, 5024(%rsp)        # 4-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 4920(%rsp)        # 8-byte Spill
	movq	%rdx, %r8
	cmpl	%eax, %ecx
	movl	%ecx, %r9d
	cmovgl	%eax, %r9d
	cmpl	%r9d, %esi
	cmovlel	%esi, %r9d
	cmpl	%r10d, %r9d
	cmovll	%r10d, %r9d
	cmpl	%eax, %esi
	movl	%esi, %edi
	cmovgl	%eax, %edi
	movl	%edi, 5008(%rsp)        # 4-byte Spill
	movq	480(%rsp), %rbx         # 8-byte Reload
	cmpl	%ebx, %r12d
	movl	%r12d, %ecx
	movq	%r12, %r15
	cmovgl	%ebx, %ecx
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	addl	$-1, %ebx
	cmpl	%edi, %ebx
	cmovgl	%edi, %ebx
	cmpl	%r10d, %ebx
	cmovll	%r10d, %ebx
	leal	-6(%r8), %ecx
	movq	%rcx, 4184(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	movl	%ecx, %r11d
	cmovgl	%eax, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	cmpl	%r10d, %r11d
	cmovll	%r10d, %r11d
	leal	-4(%r8), %r12d
	cmpl	%eax, %r12d
	cmovgl	%eax, %r12d
	cmpl	%r12d, %esi
	cmovlel	%esi, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	movq	3064(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	movl	%edx, %r13d
	cmovgl	%eax, %r13d
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	movl	5728(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5728(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r15d
	movl	%r15d, %r8d
	cmovgl	%ecx, %r8d
	addl	$-1, %r8d
	cmpl	%edi, %r8d
	cmovgl	%edi, %r8d
	cmpl	%r10d, %r8d
	cmovll	%r10d, %r8d
	movq	5352(%rsp), %r15        # 8-byte Reload
	leal	-5(%r15), %ecx
	movq	%rcx, 4176(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	movl	%ecx, %edx
	cmovgl	%eax, %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movq	4920(%rsp), %rdi        # 8-byte Reload
	movq	5488(%rsp), %rcx        # 8-byte Reload
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	addl	$-1, %ecx
	movl	5008(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	leal	-1(%r15), %edi
	movq	%rdi, 4912(%rsp)        # 8-byte Spill
	cmpl	%eax, %edi
	cmovlel	%edi, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	5024(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%r8d, %eax
	cmovgl	%r8d, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%ecx, %edx
	movl	5056(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	movl	5728(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, 5056(%rsp)        # 4-byte Spill
	movq	5104(%rsp), %rdx        # 8-byte Reload
	leal	6(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	5072(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	5096(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	leal	4(%rdx,%r15), %ecx
	movq	%rdx, %rbx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	7(%rbx,%r15), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	leal	3(%rbx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movq	5168(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx,%r15), %ecx
	movl	5184(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5136(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	5120(%rsp), %r8d        # 4-byte Reload
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	cmpl	%r14d, %esi
	cmovlel	%esi, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	cmpl	%r10d, %r14d
	cmovll	%r10d, %r14d
	cmpl	%r14d, %ecx
	cmovgel	%ecx, %r14d
	cmpl	%edx, %r14d
	cmovll	%edx, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	movl	5216(%rsp), %eax        # 4-byte Reload
	cmpl	%r14d, %eax
	cmovgel	%eax, %r14d
	movl	%eax, %r8d
	movl	5152(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %ebx
	movq	5200(%rsp), %rdx        # 8-byte Reload
	leal	5(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %r14d
	cmovgel	%r14d, %eax
	leal	6(%rdx,%r15), %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	8(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5072(%rsp)        # 4-byte Spill
	movq	5472(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %eax
	negl	%eax
	movq	5440(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx), %r9d
	cltd
	idivl	%r9d
	movl	%r9d, %eax
	negl	%eax
	movl	%ecx, %esi
	movq	%rcx, %r11
	sarl	$31, %esi
	andnl	%r9d, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%edi, 3984(%rsp)        # 4-byte Spill
	movl	%r9d, %ecx
	subl	%edi, %ecx
	addl	$-1, %ecx
	movl	%ecx, 5008(%rsp)        # 4-byte Spill
	cmpl	%edi, %ecx
	movl	%edi, %r8d
	cmovgel	%ecx, %r8d
	movl	$1, %eax
	subl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	sarl	$31, %r14d
	andl	%esi, %r14d
	addl	%edx, %r14d
	movl	%r14d, 3992(%rsp)       # 4-byte Spill
	movl	$2, %eax
	subl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	sarl	$31, %r15d
	andl	%esi, %r15d
	addl	%edx, %r15d
	movl	%r15d, 3968(%rsp)       # 4-byte Spill
	movl	%r9d, %r10d
	subl	%r15d, %r10d
	addl	$-1, %r10d
	movl	%r10d, 4872(%rsp)       # 4-byte Spill
	cmpl	%r10d, %r15d
	movl	%r15d, %eax
	cmovgl	%r10d, %eax
	movq	%rbx, %rsi
	addl	%esi, %eax
	leal	-1(%rsi,%r11), %r11d
	movl	%r11d, 4144(%rsp)       # 4-byte Spill
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%ecx, %edi
	movl	%edi, %edx
	cmovgl	%ecx, %edx
	addl	%esi, %edx
	movq	%rsi, %r12
	cmpl	%edx, %r11d
	cmovlel	%r11d, %edx
	subl	%r14d, %r9d
	addl	$-1, %r9d
	movl	%r9d, 4152(%rsp)        # 4-byte Spill
	cmpl	%r9d, %r14d
	movl	%r14d, %esi
	cmovgl	%r9d, %esi
	addl	%r12d, %esi
	cmpl	%esi, %r11d
	cmovlel	%r11d, %esi
	movl	%esi, %edi
	sarl	$31, %edi
	testl	%esi, %esi
	setg	%bl
	andl	%esi, %edi
	movzbl	%bl, %ebx
	orl	%ebx, %edi
	cmpl	%edx, %edi
	movl	%edi, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebx
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%edx, %edi
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	cmpl	$3, %ebx
	movl	$2, %r13d
	cmovgel	%r13d, %ebx
	cmpl	%esi, %ebx
	cmovgl	%esi, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	testl	%ebx, %ebx
	setg	%sil
	andl	%ebx, %ecx
	movzbl	%sil, %esi
	orl	%esi, %ecx
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, %edx
	sarl	$31, %edx
	andl	%ecx, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	cmpl	$3, %edx
	cmovgel	%r13d, %edx
	movl	$2, %esi
	movq	%r12, %rcx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%edx, 4768(%rsp)        # 4-byte Spill
	addl	%ecx, %r8d
	cmpl	%r14d, %r9d
	movl	%r14d, %eax
	cmovgel	%r9d, %eax
	addl	%ecx, %eax
	movq	%rcx, %rdx
	cmpl	$2, %eax
	setl	%cl
	cmpl	$1, %eax
	movl	$0, %edi
	cmovlel	%edi, %eax
	movzbl	%cl, %ecx
	orl	%ecx, %eax
	cmpl	%eax, %r8d
	cmovgel	%r8d, %eax
	cmpl	%r15d, %r10d
	cmovgel	%r10d, %r15d
	addl	%edx, %r15d
	cmpl	$1, %r15d
	cmovlel	%esi, %r15d
	cmpl	%r15d, %eax
	cmovgel	%eax, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	testl	%r15d, %r15d
	cmovsl	%edi, %r15d
	cmpl	%r15d, %r11d
	cmovlel	%r11d, %r15d
	cmpl	%edx, %r15d
	cmovll	%edx, %r15d
	movl	%r15d, 5024(%rsp)       # 4-byte Spill
	movq	1848(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx), %edx
	movq	%rdx, 1824(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%ecx, %esi
	movq	%rcx, %rdi
	sarl	$31, %esi
	andnl	%edx, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 1836(%rsp)        # 4-byte Spill
	movl	%edx, %r9d
	subl	%esi, %r9d
	cmovgel	%edx, %esi
	movq	%rdi, %rcx
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movq	%rsi, %rax
	movq	%rcx, %r8
	movq	5696(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx), %r14d
	movq	1816(%rsp), %rdx        # 8-byte Reload
	leal	-1(%rdx,%r8), %esi
	movl	%esi, 1804(%rsp)        # 4-byte Spill
	cmpl	%r14d, %esi
	movl	%esi, %edi
	cmovgl	%r14d, %edi
	leal	-1(%rax,%rdx), %ebx
	movl	%ebx, 5136(%rsp)        # 4-byte Spill
	addl	%edx, %eax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	movl	%edi, 5216(%rsp)        # 4-byte Spill
	leal	4(%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5200(%rsp)        # 4-byte Spill
	cmpl	%ecx, %esi
	movl	%esi, %eax
	cmovgl	%ecx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	leal	6(%rcx), %r12d
	cmpl	%r12d, %esi
	movl	%esi, %eax
	cmovgl	%r12d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5168(%rsp)        # 4-byte Spill
	movl	%r9d, %r15d
	sarl	$31, %r15d
	andl	%r9d, %r15d
	addl	%edx, %r15d
	movq	672(%rsp), %rax         # 8-byte Reload
	leal	2(%rax), %ecx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	%ecx, 5120(%rsp)        # 4-byte Spill
	cmpl	%r15d, %eax
	movl	%eax, %edi
	cmovgl	%r15d, %edi
	leal	1(%rax), %ebx
	movq	%rax, %rcx
	cmovgel	%r15d, %ebx
	movl	%ebx, 5728(%rsp)        # 4-byte Spill
	cmpl	%edi, %esi
	cmovlel	%esi, %edi
	movl	%edi, 5104(%rsp)        # 4-byte Spill
	leal	-2(%rcx), %r13d
	cmpl	%r15d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r15d, %ebx
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%r15d, %esi
	movl	%esi, %r9d
	cmovgl	%r15d, %r9d
	leal	(%rdx,%r8), %edx
	movq	%rdx, 1808(%rsp)        # 8-byte Spill
	movq	1016(%rsp), %r8         # 8-byte Reload
	cmpl	%r8d, %edx
	movl	%edx, %eax
	movq	%rdx, %rdi
	cmovgl	%r8d, %eax
	movl	%eax, %r8d
	sarl	$31, %r8d
	andl	%eax, %r8d
	addl	$-1, %r8d
	cmpl	%r9d, %r8d
	cmovgl	%r9d, %r8d
	leal	-4(%rcx), %r10d
	cmpl	%r15d, %r10d
	cmovgl	%r15d, %r10d
	cmpl	%r10d, %esi
	cmovlel	%esi, %r10d
	leal	-6(%rcx), %edx
	cmpl	%r15d, %edx
	movl	%edx, %r11d
	cmovgl	%r15d, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%r15d, %eax
	cmovgl	%r15d, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	%eax, 5760(%rsp)        # 4-byte Spill
	movl	5728(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	%eax, 5728(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	addl	$-1, %edx
	cmpl	%r9d, %edx
	cmovgl	%r9d, %edx
	leal	-5(%rcx), %ecx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	addl	$-1, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%r13d, %r15d
	cmovlel	%r15d, %r13d
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	cmpl	%r13d, %ebx
	cmovlel	%ebx, %r13d
	movl	5104(%rsp), %eax        # 4-byte Reload
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	movl	%eax, %edi
	cmpl	%r13d, %r10d
	cmovlel	%r10d, %r13d
	cmpl	%r13d, %r11d
	cmovlel	%r11d, %r13d
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r13d
	cmpl	%edx, %r13d
	cmovgl	%edx, %r13d
	cmpl	%r13d, %r10d
	cmovlel	%r10d, %r13d
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	cmpl	%r13d, %r11d
	cmovlel	%r11d, %r13d
	cmpl	%edi, %r13d
	cmovgl	%edi, %r13d
	movl	5120(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	cmpl	%ebx, %r13d
	cmovgl	%ebx, %r13d
	cmpl	%r8d, %r13d
	cmovgl	%r8d, %r13d
	movl	5728(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r13d
	cmpl	%r8d, %r13d
	cmovgl	%r8d, %r13d
	cmpl	%ebx, %r13d
	cmovgl	%ebx, %r13d
	cmpl	%edi, %r13d
	cmovgl	%edi, %r13d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	movq	1816(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movq	5696(%rsp), %rdx        # 8-byte Reload
	leal	3(%rdx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	5136(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	%ecx, %edi
	movq	1808(%rsp), %r8         # 8-byte Reload
	cmpl	%r14d, %r8d
	cmovlel	%r8d, %r14d
	movq	5152(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	movq	%rcx, %rbx
	addl	$-1, %r14d
	cmpl	%r14d, %eax
	cmovgel	%eax, %r14d
	movl	5216(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r9d
	movl	5200(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r15d
	movl	5184(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r10d
	movl	5168(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r11d
	movl	5248(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %r14d
	cmovgel	%r14d, %eax
	movl	%eax, %ecx
	leal	7(%rdx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r12d, %r8d
	cmovlel	%r8d, %r12d
	cmpl	%ebx, %r12d
	cmovll	%ebx, %r12d
	addl	$-1, %r12d
	cmpl	%r12d, %eax
	cmovgel	%eax, %r12d
	cmpl	%r15d, %r12d
	cmovll	%r15d, %r12d
	cmpl	%r11d, %r12d
	cmovll	%r11d, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	cmpl	%r9d, %r12d
	cmovll	%r9d, %r12d
	movq	%rdx, %rcx
	leal	-2(%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	cmpl	%ecx, %r8d
	movq	%rcx, %rdx
	movl	%r8d, %ecx
	cmovgl	%edx, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	addl	$-1, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	5456(%rsp), %rax        # 8-byte Reload
	cmpl	$2, %eax
	setl	%al
	movzbl	%al, %eax
	orl	5312(%rsp), %eax        # 4-byte Folded Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movq	1816(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movq	896(%rsp), %rcx         # 8-byte Reload
	orq	%rcx, 5280(%rsp)        # 8-byte Folded Spill
	sete	%r9b
	jne	.LBB147_41
# BB#40:                                # %true_bb
	vxorps	%xmm8, %xmm8, %xmm8
	movq	5624(%rsp), %rcx        # 8-byte Reload
	vmovups	%xmm8, (%rcx)
	movl	$4, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	$0, 48(%rcx)
	movl	$0, 52(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 60(%rcx)
	vmovaps	.LCPI147_0(%rip), %ymm8 # ymm8 = [3,3,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rcx)
.LBB147_41:                             # %after_bb
	movq	5360(%rsp), %rcx        # 8-byte Reload
	orq	5048(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r8b
	jne	.LBB147_43
# BB#42:                                # %true_bb105
	movl	4960(%rsp), %ecx        # 4-byte Reload
	movl	4976(%rsp), %esi        # 4-byte Reload
	subl	%esi, %ecx
	movl	4936(%rsp), %edx        # 4-byte Reload
	movl	4944(%rsp), %edi        # 4-byte Reload
	subl	%edi, %edx
	vxorps	%xmm8, %xmm8, %xmm8
	movq	4800(%rsp), %rbx        # 8-byte Reload
	vmovups	%xmm8, (%rbx)
	movl	$2, 64(%rbx)
	movb	$0, 68(%rbx)
	movb	$0, 69(%rbx)
	movl	%esi, 48(%rbx)
	movl	%ecx, 16(%rbx)
	movl	$1, 32(%rbx)
	movl	%edi, 52(%rbx)
	movl	%edx, 20(%rbx)
	movl	%ecx, 36(%rbx)
	movl	$0, 56(%rbx)
	movl	$0, 24(%rbx)
	movl	$0, 40(%rbx)
	movl	$0, 60(%rbx)
	movl	$0, 28(%rbx)
	movl	$0, 44(%rbx)
.LBB147_43:                             # %after_bb107
	movq	5376(%rsp), %rcx        # 8-byte Reload
	orq	744(%rsp), %rcx         # 8-byte Folded Reload
	sete	%r11b
	movq	1016(%rsp), %r14        # 8-byte Reload
	jne	.LBB147_45
# BB#44:                                # %true_bb108
	movl	$1, %ecx
	movl	468(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %ecx
	addl	4992(%rsp), %ecx        # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	112(%rbp), %rdx
	movq	%rdx, %rsi
	vmovups	%xmm8, (%rsi)
	movl	$2, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movq	480(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 48(%rsi)
	movq	1200(%rsp), %rdx        # 8-byte Reload
	movl	%edx, 16(%rsi)
	movl	$3, 32(%rsi)
	movl	%ebx, 52(%rsi)
	movl	%ecx, 20(%rsi)
	movl	%edx, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$3, 24(%rsi)
	movl	$1, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_45:                             # %after_bb110
	movq	5392(%rsp), %rcx        # 8-byte Reload
	orq	5632(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r10b
	jne	.LBB147_47
# BB#46:                                # %true_bb111
	vxorps	%xmm8, %xmm8, %xmm8
	movq	88(%rbp), %rcx
	vmovups	%xmm8, (%rcx)
	movl	$2, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	$0, 48(%rcx)
	movl	$0, 52(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 60(%rcx)
	vmovaps	.LCPI147_1(%rip), %ymm8 # ymm8 = [3,4096,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rcx)
.LBB147_47:                             # %after_bb113
	movq	5408(%rsp), %rcx        # 8-byte Reload
	orq	5528(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r15b
	jne	.LBB147_49
# BB#48:                                # %true_bb114
	movl	4928(%rsp), %ecx        # 4-byte Reload
	movl	4784(%rsp), %edi        # 4-byte Reload
	subl	%edi, %ecx
	addl	$1, %ecx
	movl	$1, %ebx
	movl	5056(%rsp), %edx        # 4-byte Reload
	subl	%edx, %ebx
	addl	5072(%rsp), %ebx        # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	%xmm8, (%rsi)
	movl	$4, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movl	%edi, 48(%rsi)
	movl	%ecx, 16(%rsi)
	movl	$1, 32(%rsi)
	movl	%edx, 52(%rsi)
	movl	%ebx, 20(%rsi)
	movl	%ecx, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$0, 24(%rsi)
	movl	$0, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_49:                             # %after_bb116
	movq	5424(%rsp), %rcx        # 8-byte Reload
	orq	1888(%rsp), %rcx        # 8-byte Folded Reload
	sete	%bl
	jne	.LBB147_51
# BB#50:                                # %after_bb119.thread
	movl	4768(%rsp), %edx        # 4-byte Reload
	movl	5024(%rsp), %esi        # 4-byte Reload
	subl	%edx, %esi
	addl	$1, %esi
	addl	$1, %eax
	subl	%r13d, %eax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	4832(%rsp), %rcx        # 8-byte Reload
	vmovups	%xmm0, (%rcx)
	movl	$4, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	%edx, 48(%rcx)
	movl	%esi, 16(%rcx)
	movl	$1, 32(%rcx)
	movl	%r13d, 52(%rcx)
	movl	%eax, 20(%rcx)
	movl	%esi, 36(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 24(%rcx)
	movl	$0, 40(%rcx)
	movl	$0, 60(%rcx)
	movl	$0, 28(%rcx)
	movl	$0, 44(%rcx)
	jmp	.LBB147_1559
.LBB147_51:                             # %after_bb119
	orb	%r9b, %r8b
	orb	%r11b, %r8b
	orb	%r8b, %r10b
	orb	%r10b, %r15b
	xorl	%r8d, %r8d
	orb	%r15b, %bl
	movl	$0, %ecx
	movl	$0, %edx
	movq	%rdx, 5096(%rsp)        # 8-byte Spill
	movl	$0, %edx
	movq	%rdx, 4720(%rsp)        # 8-byte Spill
	movl	$0, %edx
	movq	%rdx, 4752(%rsp)        # 8-byte Spill
	movl	$0, %edx
	movl	$0, %r12d
	movl	$0, %esi
	movq	%rsi, 5672(%rsp)        # 8-byte Spill
	movl	$0, %edi
	movl	$0, %ebx
	jne	.LBB147_207
# BB#52:                                # %true_bb120
	movl	4616(%rsp), %ecx        # 4-byte Reload
	cmpl	$4, %ecx
	jne	.LBB147_53
# BB#57:                                # %assert succeeded124
	movl	4632(%rsp), %ebx        # 4-byte Reload
	cmpl	$2, %ebx
	movq	1400(%rsp), %r15        # 8-byte Reload
	movq	5536(%rsp), %rdi        # 8-byte Reload
	movq	4904(%rsp), %rsi        # 8-byte Reload
	movq	4744(%rsp), %r11        # 8-byte Reload
	movq	4736(%rsp), %r9         # 8-byte Reload
	movq	4680(%rsp), %r12        # 8-byte Reload
	movq	4848(%rsp), %rdx        # 8-byte Reload
	movq	4816(%rsp), %r8         # 8-byte Reload
	movl	4672(%rsp), %ecx        # 4-byte Reload
	jne	.LBB147_58
# BB#60:                                # %assert succeeded126
	movl	4608(%rsp), %ebx        # 4-byte Reload
	cmpl	$2, %ebx
	jne	.LBB147_61
# BB#62:                                # %assert succeeded128
	movl	4624(%rsp), %ebx        # 4-byte Reload
	cmpl	$2, %ebx
	jne	.LBB147_63
# BB#64:                                # %assert succeeded130
	cmpl	$4, %ecx
	jne	.LBB147_65
# BB#66:                                # %assert succeeded132
	movl	4640(%rsp), %ecx        # 4-byte Reload
	cmpl	$4, %ecx
	jne	.LBB147_67
# BB#68:                                # %assert succeeded134
	testl	%r9d, %r9d
	jg	.LBB147_70
# BB#69:                                # %assert succeeded134
	movl	$3, %ecx
	subl	%edx, %ecx
	cmpl	%r9d, %ecx
	jg	.LBB147_70
# BB#73:                                # %assert succeeded136
	testl	%edx, %edx
	js	.LBB147_74
# BB#75:                                # %assert succeeded138
	testl	%r11d, %r11d
	movq	4688(%rsp), %rbx        # 8-byte Reload
	movq	4760(%rsp), %r10        # 8-byte Reload
	movq	4512(%rsp), %rdx        # 8-byte Reload
	jg	.LBB147_77
# BB#76:                                # %assert succeeded138
	movl	$3, %ecx
	subl	%r10d, %ecx
	cmpl	%r11d, %ecx
	jg	.LBB147_77
# BB#79:                                # %assert succeeded140
	testl	%r10d, %r10d
	js	.LBB147_80
# BB#82:                                # %assert succeeded142
	cmpl	4976(%rsp), %edi        # 4-byte Folded Reload
	movq	1408(%rsp), %r9         # 8-byte Reload
	jg	.LBB147_84
# BB#83:                                # %assert succeeded142
	movl	4960(%rsp), %ecx        # 4-byte Reload
	subl	%esi, %ecx
	cmpl	%edi, %ecx
	jg	.LBB147_84
# BB#85:                                # %assert succeeded144
	testl	%esi, %esi
	js	.LBB147_86
# BB#87:                                # %assert succeeded146
	cmpl	4944(%rsp), %r9d        # 4-byte Folded Reload
	jg	.LBB147_89
# BB#88:                                # %assert succeeded146
	movl	4936(%rsp), %ecx        # 4-byte Reload
	subl	%r15d, %ecx
	cmpl	%r9d, %ecx
	jg	.LBB147_89
# BB#90:                                # %assert succeeded148
	testl	%r15d, %r15d
	js	.LBB147_91
# BB#92:                                # %assert succeeded150
	movq	1200(%rsp), %rcx        # 8-byte Reload
	testl	%ecx, %ecx
	movq	5568(%rsp), %r10        # 8-byte Reload
	movq	4704(%rsp), %r11        # 8-byte Reload
	js	.LBB147_93
# BB#94:                                # %assert succeeded152
	cmpl	468(%rsp), %r14d        # 4-byte Folded Reload
	movq	472(%rsp), %rcx         # 8-byte Reload
	jg	.LBB147_96
# BB#95:                                # %assert succeeded152
	movl	4992(%rsp), %edi        # 4-byte Reload
	subl	%ecx, %edi
	cmpl	%r14d, %edi
	jge	.LBB147_96
# BB#97:                                # %assert succeeded154
	testl	%ecx, %ecx
	js	.LBB147_98
# BB#101:                               # %assert succeeded156
	testl	%r8d, %r8d
	jg	.LBB147_103
# BB#102:                               # %assert succeeded156
	movl	$3, %ecx
	subl	%edx, %ecx
	cmpl	%r8d, %ecx
	jg	.LBB147_103
# BB#104:                               # %assert succeeded158
	testl	%edx, %edx
	js	.LBB147_105
# BB#106:                               # %assert succeeded160
	testl	%r11d, %r11d
	jg	.LBB147_108
# BB#107:                               # %assert succeeded160
	movl	$3, %ecx
	subl	%ebx, %ecx
	cmpl	%r11d, %ecx
	jg	.LBB147_108
# BB#109:                               # %assert succeeded162
	testl	%ebx, %ebx
	js	.LBB147_110
# BB#112:                               # %assert succeeded164
	movq	4880(%rsp), %rbx        # 8-byte Reload
	testl	%ebx, %ebx
	movq	4696(%rsp), %r14        # 8-byte Reload
	jg	.LBB147_114
# BB#113:                               # %assert succeeded164
	movl	$4096, %ecx             # imm = 0x1000
	subl	%r12d, %ecx
	cmpl	%ebx, %ecx
	jg	.LBB147_114
# BB#115:                               # %assert succeeded166
	movq	%rdx, %r15
	testl	%r12d, %r12d
	js	.LBB147_116
# BB#117:                               # %assert succeeded168
	movl	4928(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %ecx
	subl	%r14d, %ecx
	movq	4712(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	movq	4728(%rsp), %r9         # 8-byte Reload
	jge	.LBB147_118
# BB#119:                               # %assert succeeded170
	testl	%r14d, %r14d
	js	.LBB147_120
# BB#121:                               # %assert succeeded172
	cmpl	5056(%rsp), %r9d        # 4-byte Folded Reload
	movl	5024(%rsp), %r8d        # 4-byte Reload
	jg	.LBB147_123
# BB#122:                               # %assert succeeded172
	movl	5072(%rsp), %ecx        # 4-byte Reload
	subl	%r10d, %ecx
	cmpl	%r9d, %ecx
	jge	.LBB147_123
# BB#124:                               # %assert succeeded174
	testl	%r10d, %r10d
	js	.LBB147_125
# BB#126:                               # %assert succeeded176
	movq	4760(%rsp), %r11        # 8-byte Reload
	movl	%r8d, %ecx
	movq	5440(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %ecx
	movq	5472(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	jge	.LBB147_127
# BB#128:                               # %assert succeeded178
	movq	%rsi, %rdi
	testl	%ebx, %ebx
	js	.LBB147_129
# BB#130:                               # %assert succeeded180
	movl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %ecx
	movq	1816(%rsp), %rsi        # 8-byte Reload
	cmpl	%esi, %ecx
	jge	.LBB147_131
# BB#132:                               # %assert succeeded182
	testl	%edx, %edx
	movq	%rdx, %rcx
	movq	%rdi, %rsi
	js	.LBB147_133
# BB#134:                               # %assert succeeded184
	movl	4384(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	jne	.LBB147_135
# BB#137:                               # %assert succeeded186
	movl	4448(%rsp), %eax        # 4-byte Reload
	cmpl	$1, %eax
	movl	4576(%rsp), %edx        # 4-byte Reload
	jne	.LBB147_138
# BB#142:                               # %assert succeeded188
	movl	4368(%rsp), %eax        # 4-byte Reload
	cmpl	$3, %eax
	jne	.LBB147_143
# BB#144:                               # %assert succeeded190
	movl	4416(%rsp), %eax        # 4-byte Reload
	cmpl	$1, %eax
	jne	.LBB147_145
# BB#146:                               # %assert succeeded192
	movq	4816(%rsp), %rax        # 8-byte Reload
	testl	%eax, %eax
	jne	.LBB147_147
# BB#148:                               # %assert succeeded194
	cmpl	$3, %r15d
	jne	.LBB147_149
# BB#150:                               # %assert succeeded196
	movl	4480(%rsp), %eax        # 4-byte Reload
	cmpl	$1, %eax
	jne	.LBB147_151
# BB#152:                               # %assert succeeded198
	cmpl	$1, %edx
	jne	.LBB147_153
# BB#154:                               # %assert succeeded200
	movl	4544(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	jne	.LBB147_155
# BB#156:                               # %assert succeeded204
	movq	888(%rsp), %rax         # 8-byte Reload
	imulq	%r11, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_157
# BB#160:                               # %assert succeeded206
	imulq	4848(%rsp), %r11        # 8-byte Folded Reload
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r11
	jge	.LBB147_161
# BB#162:                               # %assert succeeded210
	movq	1744(%rsp), %rax        # 8-byte Reload
	movq	1400(%rsp), %rdi        # 8-byte Reload
	imulq	%rdi, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_163
# BB#164:                               # %assert succeeded212
	movq	%rdi, %rdx
	imulq	%rsi, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_165
# BB#168:                               # %assert succeeded214
	movq	1200(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %r8
	leaq	(%r8,%r8,2), %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	movq	4664(%rsp), %rsi        # 8-byte Reload
	jne	.LBB147_169
# BB#170:                               # %assert succeeded216
	movq	408(%rsp), %rax         # 8-byte Reload
	movq	472(%rsp), %rdi         # 8-byte Reload
	imulq	%rdi, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_169
# BB#171:                               # %assert succeeded218
	movq	%rdi, %rdx
	imulq	%r8, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_172
# BB#173:                               # %assert succeeded220
	cmpq	$715827883, %rdx        # imm = 0x2AAAAAAB
	jge	.LBB147_174
# BB#175:                               # %assert succeeded224
	movq	4896(%rsp), %rax        # 8-byte Reload
	imulq	%r12, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_176
# BB#177:                               # %assert succeeded226
	imulq	4688(%rsp), %r12        # 8-byte Folded Reload
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r12
	jge	.LBB147_178
# BB#179:                               # %assert succeeded230
	movq	%rsi, %rax
	imulq	%r10, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_180
# BB#181:                               # %assert succeeded232
	imulq	%r14, %r10
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r10
	movq	1016(%rsp), %rdi        # 8-byte Reload
	jge	.LBB147_182
# BB#183:                               # %assert succeeded236
	movq	1880(%rsp), %rax        # 8-byte Reload
	imulq	%rcx, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	movq	760(%rsp), %r9          # 8-byte Reload
	jne	.LBB147_184
# BB#185:                               # %assert succeeded238
	movq	%rcx, %rdx
	imulq	%rbx, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_186
# BB#187:                               # %assert succeeded240
	movq	%rbx, %r14
	vmovss	%xmm1, 5376(%rsp)       # 4-byte Spill
	vmovss	%xmm4, 5392(%rsp)       # 4-byte Spill
	vmovss	%xmm7, 5408(%rsp)       # 4-byte Spill
	vmovss	%xmm2, 5424(%rsp)       # 4-byte Spill
	vmovss	%xmm5, 5624(%rsp)       # 4-byte Spill
	vmovss	%xmm0, 5672(%rsp)       # 4-byte Spill
	vmovss	%xmm6, 5728(%rsp)       # 4-byte Spill
	vmovss	%xmm3, 5760(%rsp)       # 4-byte Spill
	movq	5696(%rsp), %rsi        # 8-byte Reload
	leal	-31(%rsi), %eax
	movl	%eax, 612(%rsp)         # 4-byte Spill
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	movl	%ecx, 376(%rsp)         # 4-byte Spill
	movl	468(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	%eax, 468(%rsp)         # 4-byte Spill
	movl	%eax, %edx
	sarl	$31, %edx
	andl	%eax, %edx
	movl	%edx, 736(%rsp)         # 4-byte Spill
	movl	%esi, %r13d
	movq	672(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %r13d
	movl	%r13d, %eax
	andl	$-32, %eax
	leal	31(%rcx,%rax), %eax
	cmpl	%esi, %eax
	cmovgl	%esi, %eax
	movq	752(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rcx), %esi
	movl	%esi, 972(%rsp)         # 4-byte Spill
	xorl	%ebx, %ebx
	cmpl	$1, %ecx
	movl	$0, %edi
	cmovgl	%esi, %edi
	movl	4992(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovgel	%ecx, %edi
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	subl	%edx, %edi
	leal	-1(%r9), %eax
	movl	%eax, 968(%rsp)         # 4-byte Spill
	cmpl	$1, %r9d
	cmovgl	%eax, %ebx
	movq	%rbx, 872(%rsp)         # 8-byte Spill
	leal	-7(%rbx), %ecx
	movl	%ecx, 704(%rsp)         # 4-byte Spill
	movq	480(%rsp), %rax         # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%eax, %r10d
	cmovgl	%ecx, %r10d
	movl	%r10d, %ecx
	sarl	$31, %ecx
	andl	%r10d, %ecx
	movl	%ecx, 380(%rsp)         # 4-byte Spill
	movl	%ebx, %eax
	orl	$7, %eax
	movl	%eax, 392(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	movl	%eax, %esi
	cmovgl	%ebx, %esi
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	-1(%rax), %eax
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movq	5504(%rsp), %r15        # 8-byte Reload
	movl	%r15d, %edx
	andl	$-32, %edx
	movq	%rdx, 5696(%rsp)        # 8-byte Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	31(%rdx,%rax), %eax
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	subl	%ecx, %esi
	addl	$1, %edi
	leal	1(%rsi), %r9d
	leaq	(%r9,%r9), %rcx
	movl	%ecx, %eax
	imulq	%rdi, %rcx
	leaq	(%rcx,%rcx,2), %rbx
	movl	%ecx, %ecx
	cmpq	$2147483647, %rbx       # imm = 0x7FFFFFFF
	ja	.LBB147_189
# BB#188:                               # %assert succeeded240
	imulq	%rdi, %rax
	shrq	$32, %rax
	movq	%r9, %rdx
	shrq	$31, %rdx
	negq	%rdx
	andq	%rdi, %rdx
	addq	%rax, %rdx
	leaq	(%rcx,%rcx,2), %rax
	shrq	$32, %rax
	leaq	(%rdx,%rdx,2), %rcx
	addq	%rax, %rcx
	orq	%rdx, %rcx
	shrq	$32, %rcx
	jne	.LBB147_189
# BB#190:                               # %assert succeeded242
	movl	%r10d, 356(%rsp)        # 4-byte Spill
	movq	%rsi, 360(%rsp)         # 8-byte Spill
	movq	%rdi, 384(%rsp)         # 8-byte Spill
	movq	%r8, 344(%rsp)          # 8-byte Spill
	movq	%r9, 368(%rsp)          # 8-byte Spill
	leaq	2(%rbx), %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	addq	$2, %rbx
	je	.LBB147_193
# BB#191:                               # %assert succeeded242
	testq	%rax, %rax
	je	.LBB147_192
.LBB147_193:                            # %assert succeeded244
	movq	%rax, 1416(%rsp)        # 8-byte Spill
	sarl	$5, %r13d
	movl	%r13d, 648(%rsp)        # 4-byte Spill
	movl	%r15d, %eax
	sarl	$5, %eax
	movq	%rax, 880(%rsp)         # 8-byte Spill
	testl	%r13d, %r13d
	movq	5536(%rsp), %rsi        # 8-byte Reload
	movq	4904(%rsp), %rcx        # 8-byte Reload
	vmovss	5408(%rsp), %xmm6       # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	5376(%rsp), %xmm7       # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	movq	%r14, %rdx
	js	.LBB147_1470
# BB#194:                               # %for f0.s0.v11.v14.preheader
	movq	2160(%rsp), %rdi        # 8-byte Reload
	leal	8(%rdi), %eax
	vmovd	%eax, %xmm0
	leal	8(%rsi), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_2(%rip), %ymm2 # ymm2 = [0,4294967294,4294967292,4294967290,4294967288,4294967286,4294967284,4294967282]
	vpaddd	%ymm2, %ymm0, %ymm3
	vmovdqa	%ymm3, 4576(%rsp)       # 32-byte Spill
	vmovdqa	.LCPI147_3(%rip), %ymm3 # ymm3 = [4294967280,4294967278,4294967276,4294967274,4294967272,4294967270,4294967268,4294967266]
	vpaddd	%ymm3, %ymm0, %ymm0
	vmovdqa	%ymm0, 4544(%rsp)       # 32-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm4
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm4, %ymm0, %ymm4
	vpcmpeqd	%ymm5, %ymm5, %ymm5
	vpaddd	%ymm5, %ymm4, %ymm4
	vmovdqa	%ymm4, 4512(%rsp)       # 32-byte Spill
	vpaddd	%ymm2, %ymm1, %ymm4
	vmovdqa	%ymm4, 4096(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 4064(%rsp)       # 32-byte Spill
	leal	7(%rdi), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm4
	vmovdqa	%ymm4, 4480(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 4448(%rsp)       # 32-byte Spill
	leal	7(%rsi), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm2
	vmovdqa	%ymm2, 4032(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 4000(%rsp)       # 32-byte Spill
	vmovss	.LCPI147_5(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm1, %xmm2
	vmulss	%xmm6, %xmm2, %xmm3
	vmovss	5392(%rsp), %xmm4       # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm7, %xmm3, %xmm8
	vmovss	32(%rbp), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm3, %xmm3
	vmulss	%xmm3, %xmm2, %xmm2
	vdivss	%xmm2, %xmm4, %xmm9
	vmovss	16(%rbp), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	5424(%rsp), %xmm5       # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm4
	vmulss	%xmm2, %xmm4, %xmm3
	vmovss	5624(%rsp), %xmm6       # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vdivss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm10
	vmovss	40(%rbp), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm2, %xmm4, %xmm2
	vdivss	%xmm2, %xmm6, %xmm11
	vmovss	5672(%rsp), %xmm5       # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm1
	vmovss	5728(%rsp), %xmm4       # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm2
	vmovss	5760(%rsp), %xmm3       # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm12
	vmovss	24(%rbp), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm2, %xmm2
	vmulss	%xmm2, %xmm1, %xmm1
	movl	3984(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %edx
	movl	5008(%rsp), %ecx        # 4-byte Reload
	cmovgl	%eax, %ecx
	movq	5472(%rsp), %r12        # 8-byte Reload
	addl	%r12d, %ecx
	movl	4144(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%r12d, %ecx
	cmovll	%r12d, %ecx
	movl	%ecx, 5008(%rsp)        # 4-byte Spill
	leal	(%r12,%rdx), %esi
	testl	%esi, %esi
	movl	$0, %r13d
	cmovlel	%eax, %r13d
	cmpl	%r12d, %r13d
	cmovll	%r12d, %r13d
	testl	%esi, %esi
	cmovlel	%ecx, %r13d
	movq	4696(%rsp), %r14        # 8-byte Reload
	movl	4160(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r14d
	movq	%rdx, %r10
	movl	4888(%rsp), %ebx        # 4-byte Reload
	cmovgl	%ecx, %ebx
	movq	4712(%rsp), %r8         # 8-byte Reload
	addl	%r8d, %ebx
	movl	5680(%rsp), %edx        # 4-byte Reload
	cmpl	%ebx, %edx
	cmovlel	%edx, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	movl	%ebx, 4888(%rsp)        # 4-byte Spill
	leal	(%r8,%r14), %edi
	testl	%edi, %edi
	movl	$0, %ecx
	cmovlel	%edx, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	testl	%edi, %edi
	cmovlel	%ebx, %ecx
	movl	%ecx, 5424(%rsp)        # 4-byte Spill
	movl	3968(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r10d
	movl	4872(%rsp), %ebx        # 4-byte Reload
	cmovgl	%ecx, %ebx
	addl	%r12d, %ebx
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	cmpl	%r12d, %ebx
	cmovll	%r12d, %ebx
	movl	%ebx, 4872(%rsp)        # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %r9d
	cmovll	%eax, %r9d
	cmpl	%r12d, %r9d
	cmovll	%r12d, %r9d
	cmpl	$3, %esi
	cmovll	%ebx, %r9d
	movl	4168(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r14d
	movl	4224(%rsp), %r15d       # 4-byte Reload
	cmovgl	%ecx, %r15d
	addl	%r8d, %r15d
	cmpl	%r15d, %edx
	cmovlel	%edx, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	cmpl	$3, %edi
	movl	$2, %ecx
	cmovll	%edx, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	$3, %edi
	cmovll	%r15d, %ecx
	movl	%ecx, 5624(%rsp)        # 4-byte Spill
	movl	3992(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r10d
	movl	4152(%rsp), %r10d       # 4-byte Reload
	cmovgl	%ecx, %r10d
	addl	%r12d, %r10d
	cmpl	%r10d, %eax
	cmovlel	%eax, %r10d
	cmpl	%r12d, %r10d
	cmovll	%r12d, %r10d
	cmpl	$1, %esi
	setg	%bl
	cmpl	$2, %esi
	movl	$0, %r11d
	cmovgel	%r11d, %eax
	movzbl	%bl, %ebx
	orl	%ebx, %eax
	cmpl	%r12d, %eax
	cmovll	%r12d, %eax
	cmpl	$2, %esi
	cmovll	%r10d, %eax
	movl	4192(%rsp), %esi        # 4-byte Reload
	cmpl	%esi, %r14d
	movl	4312(%rsp), %ecx        # 4-byte Reload
	cmovgl	%esi, %ecx
	addl	%r8d, %ecx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	$1, %edi
	setg	%bl
	cmpl	$2, %edi
	cmovgel	%r11d, %edx
	movzbl	%bl, %esi
	orl	%esi, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	cmpl	$2, %edi
	cmovll	%ecx, %edx
	movl	%edx, 5680(%rsp)        # 4-byte Spill
	movq	5696(%rsp), %r14        # 8-byte Reload
	leal	64(%r14), %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	movq	%rdi, %rdx
	shlq	$6, %rdx
	movabsq	$68719474688, %rsi      # imm = 0xFFFFFF800
	andq	%rdx, %rsi
	leaq	(%rsi,%rsi,2), %rsi
	shrq	$32, %rsi
	movq	%rdi, %rbx
	shrq	$30, %rbx
	leaq	(%rbx,%rbx,2), %rbx
	shlq	$4, %rbx
	addq	%rsi, %rbx
	movl	%edi, %esi
	shll	$6, %esi
	leal	(%rsi,%rsi,2), %esi
	andl	$-2048, %esi            # imm = 0xFFFFFFFFFFFFF800
	leaq	(%rsi,%rsi,2), %rsi
	leaq	(%rbx,%rbx,2), %rbx
	shrq	$32, %rsi
	addq	%rbx, %rsi
	movabsq	$64424509440, %rbx      # imm = 0xF00000000
	andq	%rsi, %rbx
	leaq	(%rdx,%rdx,8), %rsi
	movq	%rsi, 336(%rsp)         # 8-byte Spill
	movq	%rsi, %rdx
	shrq	$31, %rdx
	orq	%rdx, %rbx
	sete	611(%rsp)               # 1-byte Folded Spill
	orq	$4, %rsi
	movq	%rsi, 600(%rsp)         # 8-byte Spill
	leal	40(%r14), %edx
	movq	%rdx, 1232(%rsp)        # 8-byte Spill
	shlq	$8, %rdx
	movq	%rdx, 640(%rsp)         # 8-byte Spill
	leal	48(%r14), %edx
	movq	%rdx, 864(%rsp)         # 8-byte Spill
	shlq	$8, %rdx
	movq	%rdx, 632(%rsp)         # 8-byte Spill
	movl	%r14d, %edx
	orl	$31, %edx
	movq	5352(%rsp), %rdi        # 8-byte Reload
	leal	(%r14,%rdi), %esi
	movq	%rdi, %r11
	addl	%esi, %edx
	leal	33(%rsi), %ebx
	cmpl	%edx, %ebx
	cmovgel	%ebx, %edx
	leal	37(%rsi), %ebx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movq	5504(%rsp), %rdi        # 8-byte Reload
	sarl	$31, %edi
	andnl	%esi, %edi, %esi
	movq	%r11, %rbx
	andl	%ebx, %edi
	orl	%esi, %edi
	movq	%r14, %r11
	subl	%ebx, %r14d
	movq	%rbx, %rsi
	leal	32(%r14,%rdi), %ebx
	movq	%rbx, 664(%rsp)         # 8-byte Spill
	addl	%edi, %r14d
	movq	480(%rsp), %rdi         # 8-byte Reload
	movslq	%edi, %rbx
	movq	%rbx, %rdi
	sarq	$63, %rdi
	andq	%rbx, %rdi
	movq	%rdi, 4760(%rsp)        # 8-byte Spill
	subl	%esi, %edx
	leal	3(%rdx), %edi
	movq	%rdi, 1344(%rsp)        # 8-byte Spill
	shlq	$8, %rdi
	movq	%rdi, 624(%rsp)         # 8-byte Spill
	shlq	$8, 664(%rsp)           # 8-byte Folded Spill
	leal	79(%r11), %esi
	sarl	$5, %esi
	movl	%esi, 1740(%rsp)        # 4-byte Spill
	movq	1408(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %esi
	movq	1744(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %esi
	movq	5536(%rsp), %rdi        # 8-byte Reload
	addl	%edi, %esi
	movl	%esi, 4752(%rsp)        # 4-byte Spill
	movq	880(%rsp), %rsi         # 8-byte Reload
	movslq	%esi, %rbx
	movq	%rbx, 5280(%rsp)        # 8-byte Spill
	movq	%rbx, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdi
	shlq	$9, %rdi
	movq	%rdi, 4976(%rsp)        # 8-byte Spill
	movq	%rbx, %rdi
	shlq	$10, %rdi
	movq	%rdi, 4960(%rsp)        # 8-byte Spill
	movq	1816(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %ebx
	movq	1880(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %ebx
	leal	43(%r11), %edi
	sarl	$3, %edi
	movl	%edi, 2248(%rsp)        # 4-byte Spill
	addl	%r12d, %ebx
	cmpl	$1, %r8d
	movl	5680(%rsp), %edi        # 4-byte Reload
	cmovgl	%ecx, %edi
	movl	%edi, 5680(%rsp)        # 4-byte Spill
	movq	4664(%rsp), %rcx        # 8-byte Reload
	movq	4728(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %ecx
	vdivss	%xmm1, %xmm3, %xmm13
	addl	%r8d, %ecx
	movl	%ecx, 5408(%rsp)        # 4-byte Spill
	cmpl	$1, %r12d
	cmovgl	%r10d, %eax
	movslq	%ebx, %r10
	movslq	%eax, %rdi
	subq	%r10, %rdi
	movq	%rdi, 1840(%rsp)        # 8-byte Spill
	cmpl	$2, %r8d
	movl	5624(%rsp), %eax        # 4-byte Reload
	cmovgl	%r15d, %eax
	movl	%eax, 5624(%rsp)        # 4-byte Spill
	cmpl	$2, %r12d
	cmovgl	4872(%rsp), %r9d        # 4-byte Folded Reload
	movslq	%r9d, %rcx
	subq	%r10, %rcx
	movq	%rcx, 1872(%rsp)        # 8-byte Spill
	testl	%r8d, %r8d
	movl	5424(%rsp), %eax        # 4-byte Reload
	cmovgl	4888(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 5424(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	cmovgl	5008(%rsp), %r13d       # 4-byte Folded Reload
	movq	888(%rsp), %rcx         # 8-byte Reload
	movq	4744(%rsp), %r11        # 8-byte Reload
	imull	%ecx, %r11d
	movslq	%r13d, %rdi
	movq	4880(%rsp), %rax        # 8-byte Reload
	movq	4896(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %eax
	movq	%rax, %r8
	subq	%r10, %rdi
	movq	%rdi, 1864(%rsp)        # 8-byte Spill
	movq	4736(%rsp), %rax        # 8-byte Reload
	addl	%eax, %r11d
	movq	4704(%rsp), %rax        # 8-byte Reload
	addl	%eax, %r8d
	movq	%r8, 4880(%rsp)         # 8-byte Spill
	movslq	468(%rsp), %rdi         # 4-byte Folded Reload
	movq	%rdi, %rbx
	sarq	$63, %rbx
	andq	%rdi, %rbx
	movq	%rbx, 552(%rsp)         # 8-byte Spill
	movq	5696(%rsp), %r9         # 8-byte Reload
	leal	51(%r9), %eax
	sarl	$3, %eax
	movl	%eax, 2252(%rsp)        # 4-byte Spill
	leaq	64(%rsi), %rdi
	movq	%rdi, 1720(%rsp)        # 8-byte Spill
	leaq	40(%rsi), %rdi
	movq	%rdi, 1776(%rsp)        # 8-byte Spill
	addq	$48, %rsi
	movq	%rsi, 1784(%rsp)        # 8-byte Spill
	movslq	%edx, %rax
	addq	$3, %rax
	movq	%rax, 1264(%rsp)        # 8-byte Spill
	leal	39(%r9), %eax
	sarl	$3, %eax
	movl	%eax, 1800(%rsp)        # 4-byte Spill
	movslq	%r14d, %rax
	addq	$32, %rax
	movq	%rax, 1792(%rsp)        # 8-byte Spill
	movslq	%r11d, %rax
	leaq	(%rcx,%rcx), %rdx
	subq	%rax, %rdx
	movq	%rdx, 848(%rsp)         # 8-byte Spill
	subq	%rax, %rcx
	movq	%rcx, 888(%rsp)         # 8-byte Spill
	movl	$2, %ecx
	subq	%rax, %rcx
	movq	%rcx, 840(%rsp)         # 8-byte Spill
	movl	$1, %ecx
	subq	%rax, %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	negq	%rax
	movq	%rax, 856(%rsp)         # 8-byte Spill
	movq	5536(%rsp), %rsi        # 8-byte Reload
	leal	39(%rsi), %eax
	movq	5352(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	sarl	$5, %eax
	movl	$0, %ebx
	cmovsl	%ebx, %eax
	movq	%rax, 4992(%rsp)        # 8-byte Spill
	movq	2160(%rsp), %rdx        # 8-byte Reload
	subl	%ecx, %edx
	movq	4904(%rsp), %rax        # 8-byte Reload
	subl	%ecx, %eax
	movq	%rcx, %rdi
	addl	%esi, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	addl	$-24, %eax
	leal	-23(%rdx), %ecx
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	6(%rdx), %ecx
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	addl	$7, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	leal	47(%r9), %eax
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	sarl	$5, %edx
	addl	$1, %edx
	movq	%rdx, 2160(%rsp)        # 8-byte Spill
	movq	4728(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %r10d
	subl	%edi, %r10d
	movl	%r10d, %ecx
	sarl	$3, %ecx
	movq	%rcx, 4784(%rsp)        # 8-byte Spill
	leal	6(%r10), %eax
	sarl	$3, %eax
	movl	%eax, 5096(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	leal	1(%rcx), %edx
	movl	%edx, 4768(%rsp)        # 4-byte Spill
	movl	%eax, %ecx
	cmovgel	%edx, %ecx
	leal	7(%r10), %eax
	sarl	$3, %eax
	movl	%eax, 4800(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	leal	9(%r10), %eax
	sarl	$3, %eax
	movl	%eax, 4872(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 1192(%rsp)        # 8-byte Spill
	leal	10(%r10), %eax
	sarl	$3, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	leal	11(%r10), %ecx
	sarl	$3, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	testl	%ecx, %ecx
	cmovsl	%ebx, %ecx
	movq	%rcx, 4936(%rsp)        # 8-byte Spill
	movq	5568(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r12d
	subl	%edi, %r12d
	movq	5488(%rsp), %r11        # 8-byte Reload
	subl	%edi, %r11d
	movq	%rsi, %rcx
	leal	(%r12,%rcx), %eax
	movl	%eax, 5024(%rsp)        # 4-byte Spill
	cmpl	%eax, %r11d
	movl	%r11d, %r14d
	cmovgl	%eax, %r14d
	movl	%r14d, %eax
	sarl	$3, %eax
	movq	%rax, 5056(%rsp)        # 8-byte Spill
	leal	-7(%r12,%rcx), %edx
	sarl	$3, %edx
	movl	%edx, 5312(%rsp)        # 4-byte Spill
	leal	-1(%rax), %eax
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	leal	-6(%r12,%rcx), %esi
	sarl	$3, %esi
	movl	%esi, 5104(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-5(%r12,%rcx), %esi
	sarl	$3, %esi
	movl	%esi, 5072(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-4(%r12,%rcx), %esi
	sarl	$3, %esi
	movl	%esi, 5216(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-3(%r12,%rcx), %esi
	sarl	$3, %esi
	movl	%esi, 5200(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-7(%r11), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5728(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-6(%r11), %r15d
	movl	%r15d, %esi
	sarl	$3, %esi
	movl	%esi, 5120(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-5(%r11), %r8d
	movl	%r8d, %esi
	sarl	$3, %esi
	movl	%esi, 5136(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-4(%r11), %esi
	sarl	$3, %esi
	movl	%esi, 5152(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-3(%r11), %esi
	sarl	$3, %esi
	movl	%esi, 5440(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	-2(%r11), %r13d
	movl	%r13d, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5760(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-1(%r11), %edi
	movl	%edi, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5672(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	1(%r11), %ecx
	movl	%ecx, %esi
	sarl	$3, %esi
	movl	%esi, 5472(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	2(%r11), %esi
	sarl	$3, %esi
	movl	%esi, 5504(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	leal	3(%r11), %esi
	sarl	$3, %esi
	movl	%esi, 5360(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	movl	%r11d, %esi
	sarl	$3, %esi
	movl	%esi, 5184(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	addl	$35, %r9d
	movq	%r9, 5696(%rsp)         # 8-byte Spill
	movl	%r9d, %esi
	sarl	$3, %esi
	movl	%esi, 4928(%rsp)        # 4-byte Spill
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	addl	$1, %edx
	movslq	5408(%rsp), %rsi        # 4-byte Folded Reload
	movq	%rsi, 4848(%rsp)        # 8-byte Spill
	movslq	5680(%rsp), %rbx        # 4-byte Folded Reload
	movq	%rbx, 4832(%rsp)        # 8-byte Spill
	subq	%rsi, %rbx
	movq	%rbx, 4312(%rsp)        # 8-byte Spill
	movslq	5624(%rsp), %rbx        # 4-byte Folded Reload
	movq	%rbx, 5624(%rsp)        # 8-byte Spill
	subq	%rsi, %rbx
	movq	%rbx, 4744(%rsp)        # 8-byte Spill
	movslq	5424(%rsp), %rbx        # 4-byte Folded Reload
	movq	%rbx, 4816(%rsp)        # 8-byte Spill
	subq	%rsi, %rbx
	movq	%rbx, 4888(%rsp)        # 8-byte Spill
	movq	1192(%rsp), %rsi        # 8-byte Reload
	testl	%esi, %esi
	movl	$0, %ebx
	cmovsl	%ebx, %esi
	movq	%rsi, 1192(%rsp)        # 8-byte Spill
	addl	$-14, %r14d
	leal	-13(%r11), %r9d
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	leal	-12(%r11), %r9d
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	leal	-11(%r11), %r9d
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	cmpl	%r14d, %r15d
	cmovlel	%r15d, %r14d
	cmpl	%r14d, %r8d
	cmovlel	%r8d, %r14d
	cmpl	%r14d, %r13d
	cmovlel	%r13d, %r14d
	cmpl	%r14d, %edi
	cmovlel	%edi, %r14d
	cmpl	%r14d, %r11d
	cmovll	%ecx, %r14d
	cmpl	%r14d, %r11d
	cmovlel	%r11d, %r14d
	movq	5696(%rsp), %rcx        # 8-byte Reload
	cmpl	%r14d, %ecx
	cmovlel	%ecx, %r14d
	sarl	$3, %r14d
	addl	$1, %r14d
	movq	4728(%rsp), %r8         # 8-byte Reload
	leal	13(%r8), %esi
	movq	5352(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %esi
	sarl	$3, %esi
	cmovsl	%ebx, %esi
	movq	%rsi, 5008(%rsp)        # 8-byte Spill
	leal	-10(%r12,%r8), %esi
	sarl	$3, %esi
	movl	%esi, 5248(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovgl	%eax, %esi
	leal	-9(%r12,%r8), %edi
	sarl	$3, %edi
	movl	%edi, 5168(%rsp)        # 4-byte Spill
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	movl	5312(%rsp), %edi        # 4-byte Reload
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	leal	-2(%r12,%r8), %edi
	sarl	$3, %edi
	movl	%edi, 5424(%rsp)        # 4-byte Spill
	cmpl	%esi, %edi
	cmovgl	%esi, %edi
	leal	-1(%r12,%r8), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5408(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	leal	-10(%r11), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5680(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	leal	-9(%r11), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5696(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	movl	5728(%rsp), %ebx        # 4-byte Reload
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	movl	5760(%rsp), %ebx        # 4-byte Reload
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	movl	5672(%rsp), %r15d       # 4-byte Reload
	cmpl	%edi, %r15d
	cmovlel	%r15d, %edi
	movl	5504(%rsp), %ebx        # 4-byte Reload
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	movl	5360(%rsp), %r13d       # 4-byte Reload
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	leal	4(%r11), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5392(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	leal	5(%r11), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5376(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	leal	15(%r8), %ebx
	subl	%ecx, %ebx
	sarl	$3, %ebx
	movl	$0, %ecx
	cmovsl	%ecx, %ebx
	movq	%rbx, 4904(%rsp)        # 8-byte Spill
	movl	5024(%rsp), %ebx        # 4-byte Reload
	sarl	$3, %ebx
	movl	%ebx, 5024(%rsp)        # 4-byte Spill
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5248(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5168(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5312(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5216(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5200(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5424(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	movl	5408(%rsp), %ebx        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	leal	1(%r12,%r8), %r12d
	sarl	$3, %r12d
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	movl	5680(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5696(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5728(%rsp), %r9d        # 4-byte Reload
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	5152(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	%ecx, %r8d
	movl	5440(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5760(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r15d
	cmovlel	%r15d, %eax
	movl	5472(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5504(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	movl	5392(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5376(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	6(%r11), %ecx
	sarl	$3, %ecx
	movl	%ecx, 4944(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	addl	$7, %r11d
	sarl	$3, %r11d
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	movl	5184(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	%ecx, %r13d
	leal	4(%r10), %ecx
	sarl	$3, %ecx
	movq	4784(%rsp), %rbx        # 8-byte Reload
	cmpl	%ecx, %ebx
	cmovgel	4768(%rsp), %ecx        # 4-byte Folded Reload
	addl	$5, %r10d
	sarl	$3, %r10d
	cmpl	%r10d, %ecx
	cmovgel	%ecx, %r10d
	movl	5096(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r10d
	cmovll	%ecx, %r10d
	movl	4800(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r10d
	cmovll	%ecx, %r10d
	movl	4872(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r10d
	cmovll	%ecx, %r10d
	testl	%r10d, %r10d
	movl	$0, %ecx
	cmovsl	%ecx, %r10d
	movq	%r10, 5096(%rsp)        # 8-byte Spill
	movl	5104(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5072(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5680(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5696(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	movl	%r9d, %r10d
	movl	5120(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5136(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5440(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	5760(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	cmpl	%esi, %r15d
	cmovlel	%r15d, %esi
	movl	5472(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movl	%r13d, %ebx
	cmpl	%esi, %ebx
	cmovlel	%ebx, %esi
	movq	880(%rsp), %rcx         # 8-byte Reload
	leal	3(,%rcx,4), %ecx
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	addl	$1, %esi
	movl	$31, %r8d
	movq	5456(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %r8d
	movq	5280(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %ecx
	shll	$10, %ecx
	movq	%rcx, 4736(%rsp)        # 8-byte Spill
	movl	%ebx, %ecx
	shll	$9, %ecx
	movq	%rcx, 656(%rsp)         # 8-byte Spill
	cmpl	$1, %r9d
	movl	$30, %ebx
	cmovgl	%r8d, %ebx
	movl	%ebx, 596(%rsp)         # 4-byte Spill
	movl	2248(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %r9d
	notl	%r9d
	movl	%r9d, 5456(%rsp)        # 4-byte Spill
	movq	5056(%rsp), %rcx        # 8-byte Reload
	negl	%ecx
	movq	%rcx, 5056(%rsp)        # 8-byte Spill
	movq	%rcx, %r13
	movl	5312(%rsp), %r15d       # 4-byte Reload
	notl	%r15d
	movl	%r15d, 5312(%rsp)       # 4-byte Spill
	notl	%r10d
	movl	%r10d, 5728(%rsp)       # 4-byte Spill
	notl	5760(%rsp)              # 4-byte Folded Spill
	notl	5672(%rsp)              # 4-byte Folded Spill
	notl	5504(%rsp)              # 4-byte Folded Spill
	notl	5360(%rsp)              # 4-byte Folded Spill
	movq	1192(%rsp), %rcx        # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovgl	%ebx, %ecx
	cmpl	%r14d, %ecx
	cmovll	%r14d, %ecx
	movl	%ecx, 4720(%rsp)        # 4-byte Spill
	movq	5008(%rsp), %rcx        # 8-byte Reload
	movl	2252(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %ecx
	movl	%ecx, %r10d
	cmovgl	%ebx, %r10d
	movl	5248(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5248(%rsp)        # 4-byte Spill
	movq	%r13, %rbx
	cmpl	%ecx, %ebx
	movl	%ecx, %r13d
	cmovgel	%ebx, %r13d
	movl	5168(%rsp), %r8d        # 4-byte Reload
	notl	%r8d
	movl	%r8d, 5168(%rsp)        # 4-byte Spill
	cmpl	%r8d, %r13d
	cmovgel	%r13d, %r8d
	cmpl	%r15d, %r8d
	movl	5424(%rsp), %ebx        # 4-byte Reload
	notl	%ebx
	movl	%ebx, 5424(%rsp)        # 4-byte Spill
	cmovll	%r15d, %r8d
	cmpl	%ebx, %r8d
	movl	5408(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5408(%rsp)        # 4-byte Spill
	cmovll	%ebx, %r8d
	cmpl	%ecx, %r8d
	movl	5680(%rsp), %ebx        # 4-byte Reload
	notl	%ebx
	movl	%ebx, 5680(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r8d
	cmpl	%ebx, %r8d
	movl	5696(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5696(%rsp)        # 4-byte Spill
	cmovll	%ebx, %r8d
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	5728(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	5760(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	5672(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	5504(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	5360(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %r8d
	movl	5392(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5392(%rsp)        # 4-byte Spill
	cmovll	%ebx, %r8d
	cmpl	%ecx, %r8d
	movl	5376(%rsp), %ebx        # 4-byte Reload
	notl	%ebx
	movl	%ebx, 5376(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r8d
	cmpl	%ebx, %r8d
	cmovll	%ebx, %r8d
	cmpl	%r9d, %r8d
	cmovll	%r9d, %r8d
	negl	%r8d
	cmpl	%r8d, %r10d
	cmovgel	%r10d, %r8d
	movl	$2, %ecx
	subq	4760(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 1224(%rsp)        # 8-byte Spill
	movl	1740(%rsp), %ecx        # 4-byte Reload
	movq	4992(%rsp), %rbx        # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovgl	%ebx, %ecx
	movl	%ecx, 1328(%rsp)        # 4-byte Spill
	movq	2160(%rsp), %rbx        # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovgel	%ecx, %ebx
	movl	%ebx, 1324(%rsp)        # 4-byte Spill
	movl	2248(%rsp), %ebx        # 4-byte Reload
	movq	4936(%rsp), %r9         # 8-byte Reload
	cmpl	%r9d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r9d, %ecx
	movl	%ecx, 2236(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	%edx, 1336(%rsp)        # 4-byte Spill
	movq	1192(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ebx
	movl	%ebx, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, 1320(%rsp)        # 4-byte Spill
	cmpl	%r14d, %ecx
	movl	%r14d, %edx
	cmovgel	%ecx, %edx
	movl	%edx, 1316(%rsp)        # 4-byte Spill
	movl	2252(%rsp), %edx        # 4-byte Reload
	movq	5008(%rsp), %r9         # 8-byte Reload
	cmpl	%r9d, %edx
	movl	%edx, %ecx
	cmovgl	%r9d, %ecx
	movl	%ecx, 1312(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	addl	$1, %edi
	cmpl	%edi, %ecx
	cmovgel	%ecx, %edi
	movl	%edi, 1332(%rsp)        # 4-byte Spill
	movq	4904(%rsp), %rdi        # 8-byte Reload
	cmpl	%edi, %edx
	movl	%edx, %ecx
	cmovgl	%edi, %ecx
	movl	%ecx, 1308(%rsp)        # 4-byte Spill
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	addl	$1, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	%eax, 1340(%rsp)        # 4-byte Spill
	movl	1800(%rsp), %eax        # 4-byte Reload
	movq	5096(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, 3164(%rsp)        # 4-byte Spill
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 1452(%rsp)        # 4-byte Spill
	movl	$-7, %eax
	movq	5536(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	movq	%rax, 3992(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	subl	%ecx, %eax
	movq	%rax, 3984(%rsp)        # 8-byte Spill
	movq	5280(%rsp), %rcx        # 8-byte Reload
	imull	$3264, %ecx, %eax       # imm = 0xCC0
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 548(%rsp)         # 4-byte Spill
	movl	%ecx, %eax
	shll	$5, %eax
	movq	%rax, 4872(%rsp)        # 8-byte Spill
	imull	$1728, %ecx, %eax       # imm = 0x6C0
	movq	%rcx, %rsi
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 544(%rsp)         # 4-byte Spill
	movq	880(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movl	%eax, 652(%rsp)         # 4-byte Spill
	shll	$6, %eax
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 540(%rsp)         # 4-byte Spill
	movq	4728(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	negl	%eax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	movl	$1, %eax
	subl	%ecx, %eax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	movl	$-4, %eax
	subl	%ecx, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	movl	$-3, %eax
	subl	%ecx, %eax
	movq	%rax, 4688(%rsp)        # 8-byte Spill
	movl	$-2, %eax
	subl	%ecx, %eax
	movq	%rax, 3144(%rsp)        # 8-byte Spill
	movq	%rcx, %rdx
	movq	1016(%rsp), %r15        # 8-byte Reload
	movq	472(%rsp), %rax         # 8-byte Reload
	leal	(%r15,%rax), %edi
	movq	752(%rsp), %rax         # 8-byte Reload
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	testl	%edi, %edi
	movl	$1, %r9d
	cmovlel	%r9d, %edi
	movl	$31, %eax
	subl	%edi, %eax
	movl	%eax, 592(%rsp)         # 4-byte Spill
	movq	5352(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %ecx
	imull	$3136, %esi, %eax       # imm = 0xC40
	subl	%edx, %ecx
	movq	%rcx, 4640(%rsp)        # 8-byte Spill
	imull	$1600, %esi, %ecx       # imm = 0x640
	subl	$-128, %eax
	movl	%eax, 536(%rsp)         # 4-byte Spill
	subl	$-128, %ecx
	movl	%ecx, 532(%rsp)         # 4-byte Spill
	movl	%esi, %ebx
	shll	$6, %ebx
	movl	%ebx, %ecx
	subl	$-128, %ecx
	movl	%ecx, 828(%rsp)         # 4-byte Spill
	movl	$-7, %eax
	subl	%edx, %eax
	movq	%rax, 4168(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	subl	%edx, %eax
	movq	%rax, 4160(%rsp)        # 8-byte Spill
	movl	$-6, %eax
	subl	%edx, %eax
	movq	%rax, 4152(%rsp)        # 8-byte Spill
	movl	$-5, %eax
	subl	%edx, %eax
	movq	%rax, 4144(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	37(%r10,%rax), %edx
	leal	33(%r10,%rax), %ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leal	31(%r10,%rbx), %eax
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	addl	$3, %ecx
	subl	%r10d, %ecx
	movl	%ecx, 1764(%rsp)        # 4-byte Spill
	movq	480(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %edx
	notl	%edx
	movq	760(%rsp), %rbx         # 8-byte Reload
	testl	%ebx, %ebx
	movl	$1, %eax
	cmovgl	%ebx, %eax
	movl	$7, %ebx
	subl	%eax, %ebx
	cmpl	%ebx, %edx
	cmovgel	%edx, %ebx
	notl	%ebx
	movslq	%ebx, %rcx
	notq	%rcx
	cmpq	$-2, %rcx
	movq	$-1, %rdx
	cmovleq	%rdx, %rcx
	movq	%rcx, 616(%rsp)         # 8-byte Spill
	movq	1200(%rsp), %rdx        # 8-byte Reload
	leal	-1(%rsi,%rdx), %ebx
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leal	31(%r10,%rcx), %edx
	cmpl	%edx, %ebx
	cmovgel	%ebx, %edx
	movl	392(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	negl	%eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	%eax, %edx
	movl	380(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %edx
	movl	%edx, 4712(%rsp)        # 4-byte Spill
	movl	%r15d, %edx
	notl	%edx
	movl	$31, %ebx
	subl	%r15d, %ebx
	movq	472(%rsp), %r10         # 8-byte Reload
	subl	%r10d, %ebx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	movl	376(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	notl	%ebx
	movslq	%ebx, %rdx
	notq	%rdx
	cmpq	$-2, %rdx
	movq	$-1, %rsi
	cmovleq	%rsi, %rdx
	addq	$1, %rdx
	movq	%rdx, 520(%rsp)         # 8-byte Spill
	addl	$1, %eax
	subl	%ecx, %eax
	movq	752(%rsp), %rcx         # 8-byte Reload
	testl	%ecx, %ecx
	cmovgl	%ecx, %r9d
	movl	$-32, %edx
	movq	672(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %edx
	movl	648(%rsp), %ebx         # 4-byte Reload
	shll	$5, %ebx
	subl	%ebx, %edx
	addl	$-1, %r9d
	negl	%edi
	cmpl	%edx, %edi
	cmovgel	%edi, %edx
	notl	%edx
	cmpl	%edx, %r9d
	cmovgel	%r9d, %edx
	movq	448(%rsp), %rsi         # 8-byte Reload
	leal	(%r15,%rsi), %esi
	notl	%esi
	movl	%r10d, %ebx
	negl	%ebx
	subl	%r15d, %ebx
	cmpl	%esi, %ebx
	cmovll	%esi, %ebx
	notl	%ebx
	cmpl	%ebx, %edx
	cmovgel	%edx, %ebx
	addl	$1, %ebx
	subl	736(%rsp), %ebx         # 4-byte Folded Reload
	imull	%eax, %ebx
	movq	4832(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm1
	movq	4848(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm7
	vpsubd	%xmm7, %xmm1, %xmm2
	movq	5624(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm1
	vpsubd	%xmm7, %xmm1, %xmm1
	movq	4816(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm3
	vpsubd	%xmm7, %xmm3, %xmm7
	movslq	%ebx, %rax
	addq	%rax, %rax
	movq	%rax, 5624(%rsp)        # 8-byte Spill
	movl	1740(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %eax
	notl	%eax
	movq	4992(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	%ecx, %eax
	movq	%rcx, %rdi
	shll	$5, %eax
	movq	5352(%rsp), %rsi        # 8-byte Reload
	leal	-32(%rsi), %ecx
	movl	%ecx, %edx
	subl	%eax, %edx
	movl	%edx, 1188(%rsp)        # 4-byte Spill
	subl	4752(%rsp), %ecx        # 4-byte Folded Reload
	subl	%eax, %ecx
	movq	%rcx, 728(%rsp)         # 8-byte Spill
	movl	%edi, %eax
	notl	%eax
	movl	%eax, 1184(%rsp)        # 4-byte Spill
	movq	2160(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	1(%rdi,%rcx), %eax
	movl	%eax, 1180(%rsp)        # 4-byte Spill
	movl	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, 1176(%rsp)        # 4-byte Spill
	shll	$5, %ecx
	movq	%rcx, 2160(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	movq	5536(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	leal	-7(%rax), %ecx
	movq	%rcx, 2136(%rsp)        # 8-byte Spill
	movq	5280(%rsp), %rdx        # 8-byte Reload
	imull	$3456, %edx, %ecx       # imm = 0xD80
	addl	$-8, %eax
	movq	%rax, 2144(%rsp)        # 8-byte Spill
	imull	$1920, %edx, %eax       # imm = 0x780
	addl	$768, %ecx              # imm = 0x300
	movl	%ecx, 516(%rsp)         # 4-byte Spill
	addl	$768, %eax              # imm = 0x300
	movl	%eax, 512(%rsp)         # 4-byte Spill
	movl	652(%rsp), %eax         # 4-byte Reload
	shll	$7, %eax
	addl	$768, %eax              # imm = 0x300
	movl	%eax, 652(%rsp)         # 4-byte Spill
	movq	4936(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	movl	5456(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 4936(%rsp)        # 8-byte Spill
	movl	%eax, %edi
	leal	(,%rcx,8), %eax
	movq	%rcx, %rbx
	movl	$-8, %ecx
	subl	%eax, %ecx
	movq	%rcx, 720(%rsp)         # 8-byte Spill
	movq	3064(%rsp), %rcx        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 1172(%rsp)        # 4-byte Spill
	movq	5056(%rsp), %rcx        # 8-byte Reload
	movl	5312(%rsp), %r9d        # 4-byte Reload
	cmpl	%r9d, %ecx
	movl	5104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5104(%rsp)        # 4-byte Spill
	movl	%r9d, %r15d
	cmovgel	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5072(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5072(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	movl	5216(%rsp), %edx        # 4-byte Reload
	notl	%edx
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%edx, %r15d
	movl	5200(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5200(%rsp)        # 4-byte Spill
	cmovll	%edx, %r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5728(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5120(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5120(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5136(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5136(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	movl	5152(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5152(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5440(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5440(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5672(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5472(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5472(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5504(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5360(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5184(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	4928(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	movl	%ebx, %eax
	notl	%eax
	negl	%r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	leal	(%rsi,%r15,8), %eax
	movq	%rax, 2152(%rsp)        # 8-byte Spill
	movq	4728(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	movq	%rax, 2128(%rsp)        # 8-byte Spill
	movl	2248(%rsp), %eax        # 4-byte Reload
	subl	%r15d, %eax
	movl	%eax, 1168(%rsp)        # 4-byte Spill
	movq	1192(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %edi
	cmovgel	%edi, %ecx
	movq	%rcx, 1192(%rsp)        # 8-byte Spill
	leal	(,%rcx,8), %eax
	negl	%eax
	movl	%eax, 1164(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	notl	%eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	movl	2252(%rsp), %edi        # 4-byte Reload
	notl	%edi
	movq	5008(%rsp), %rsi        # 8-byte Reload
	notl	%esi
	cmpl	%esi, %edi
	cmovgel	%edi, %esi
	movl	%esi, %ecx
	notl	%ecx
	movl	%ecx, 1160(%rsp)        # 4-byte Spill
	leal	(,%rsi,8), %edx
	movq	%rsi, %rbx
	negl	%edx
	movl	%edx, 1156(%rsp)        # 4-byte Spill
	movl	5168(%rsp), %r10d       # 4-byte Reload
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	cmpl	%r9d, %r13d
	cmovll	%r9d, %r13d
	movl	5424(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	movl	%eax, %esi
	cmovgel	%r13d, %esi
	movl	5408(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5680(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5728(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5672(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5504(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5360(%rsp), %r14d       # 4-byte Reload
	cmpl	%r14d, %esi
	cmovll	%r14d, %esi
	movl	5392(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5376(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	movl	5456(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	negl	%esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	movq	5280(%rsp), %rax        # 8-byte Reload
	imull	$3584, %eax, %ecx       # imm = 0xE00
	leal	1(%rsi,%rbx), %edx
	movl	%edx, 1152(%rsp)        # 4-byte Spill
	addl	$1024, %ecx             # imm = 0x400
	movl	%ecx, 508(%rsp)         # 4-byte Spill
	movl	%eax, %ecx
	shll	$11, %ecx
	orl	$1024, %ecx             # imm = 0x400
	movl	%ecx, 504(%rsp)         # 4-byte Spill
	movq	656(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx,2), %ecx
	movq	%rcx, 4928(%rsp)        # 8-byte Spill
	leal	-8(%rdx,%rdx,2), %ecx
	movq	%rcx, 2224(%rsp)        # 8-byte Spill
	addl	$1024, %edx             # imm = 0x400
	movq	%rdx, 656(%rsp)         # 8-byte Spill
	movq	4904(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %edi
	cmovgel	%edi, %ecx
	movq	%rcx, 4904(%rsp)        # 8-byte Spill
	leal	(,%rcx,8), %eax
	movq	%rcx, %rsi
	movq	5352(%rsp), %rcx        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 1148(%rsp)        # 4-byte Spill
	negl	%eax
	movq	%rax, 712(%rsp)         # 8-byte Spill
	movl	5024(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movq	5056(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	5248(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movl	5216(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	5200(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	5424(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	5408(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	notl	%r12d
	cmpl	%r12d, %eax
	cmovgel	%eax, %r12d
	movl	5680(%rsp), %r10d       # 4-byte Reload
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5728(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5152(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %r12d
	cmovll	%ebx, %r12d
	movl	5440(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	%eax, %r9d
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5672(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5472(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %r12d
	cmovll	%edx, %r12d
	movl	5504(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	cmpl	%r14d, %r12d
	cmovll	%r14d, %r12d
	movl	5392(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5376(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	4944(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	notl	%r11d
	cmpl	%r11d, %eax
	cmovgel	%eax, %r11d
	movl	5184(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r11d
	cmovll	%eax, %r11d
	movl	%eax, %r14d
	movl	5456(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r11d
	cmovll	%eax, %r11d
	movl	%esi, %eax
	notl	%eax
	movl	%eax, 1144(%rsp)        # 4-byte Spill
	negl	%r11d
	cmpl	%eax, %r11d
	cmovll	%eax, %r11d
	movl	2252(%rsp), %eax        # 4-byte Reload
	subl	%r11d, %eax
	movl	%eax, 1140(%rsp)        # 4-byte Spill
	movl	1800(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movq	5096(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 5096(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	leal	(,%rcx,8), %esi
	movq	%rcx, %rdi
	subl	%esi, %eax
	movq	%rax, 2200(%rsp)        # 8-byte Spill
	movl	5104(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5072(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5728(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5120(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5136(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	cmpl	%ebx, %r13d
	cmovll	%ebx, %r13d
	cmpl	%r9d, %r13d
	cmovll	%r9d, %r13d
	movl	5760(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5672(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	cmpl	%edx, %r13d
	cmovll	%edx, %r13d
	cmpl	%r14d, %r13d
	cmovll	%r14d, %r13d
	movq	5280(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %eax
	movl	%eax, 1220(%rsp)        # 4-byte Spill
	shll	$2, %ecx
	movl	$-4, %eax
	subl	%ecx, %eax
	cmpl	%eax, %r13d
	cmovgel	%r13d, %eax
	negl	%eax
	movl	%edi, %ecx
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	5352(%rsp), %r13        # 8-byte Reload
	subl	%esi, %r13d
	movl	4336(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %ymm3
	vmovaps	%ymm3, 4416(%rsp)       # 32-byte Spill
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1648(%rsp)       # 16-byte Spill
	vpbroadcastd	.LCPI147_4(%rip), %ymm3
	vmovdqa	%ymm3, 768(%rsp)        # 32-byte Spill
	vpsubd	%ymm0, %ymm3, %ymm0
	vmovdqa	%ymm0, 4384(%rsp)       # 32-byte Spill
	vmovd	4352(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4368(%rsp)       # 16-byte Spill
	movq	5536(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4352(%rsp)       # 16-byte Spill
	movq	4976(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	movq	4960(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 4672(%rsp)        # 8-byte Spill
	movl	4320(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm4
	vmovdqa	%xmm4, 5456(%rsp)       # 16-byte Spill
	movq	4664(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5424(%rsp)       # 16-byte Spill
	movq	5488(%rsp), %rdx        # 8-byte Reload
	leal	2(%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm3
	vmovdqa	.LCPI147_6(%rip), %xmm0 # xmm0 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5200(%rsp)       # 16-byte Spill
	movq	4728(%rsp), %r9         # 8-byte Reload
	leal	2(%r9), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovd	%r9d, %xmm3
	vpbroadcastd	%xmm3, %xmm5
	vmovdqa	%xmm5, 5408(%rsp)       # 16-byte Spill
	movq	5568(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 5392(%rsp)       # 16-byte Spill
	movl	4256(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm6
	vmovdqa	%xmm6, 5376(%rsp)       # 16-byte Spill
	leal	1(%rdx), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5184(%rsp)       # 16-byte Spill
	leal	1(%r9), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5136(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 5168(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm2
	vmovaps	%xmm2, 5504(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm2
	vmovaps	%xmm2, 5472(%rsp)       # 16-byte Spill
	leal	3(%rdx), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5024(%rsp)       # 16-byte Spill
	leal	3(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4848(%rsp)       # 16-byte Spill
	movq	%rdx, %rdi
	leal	4(%rdi), %eax
	leal	6(%rdi), %edx
	leal	5(%rdi), %esi
	leal	8(%rdi), %ebx
	leal	7(%rdi), %r10d
	leal	-2(%rdi), %r12d
	leal	-3(%rdi), %r14d
	vmovd	%edi, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5120(%rsp)       # 16-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5008(%rsp)       # 16-byte Spill
	leal	4(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4832(%rsp)       # 16-byte Spill
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5072(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4992(%rsp)       # 16-byte Spill
	leal	6(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4816(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4976(%rsp)       # 16-byte Spill
	leal	5(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4800(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4336(%rsp)       # 16-byte Spill
	leal	8(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 3968(%rsp)       # 16-byte Spill
	vmovd	%r10d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4320(%rsp)       # 16-byte Spill
	leal	7(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 3952(%rsp)       # 16-byte Spill
	vmovd	%r12d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4960(%rsp)       # 16-byte Spill
	leal	-2(%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4784(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4944(%rsp)       # 16-byte Spill
	leal	-3(%r9), %eax
	movq	%r9, %r12
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4768(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm5, %xmm2
	vmovdqa	%xmm2, 5056(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm6, %xmm0
	vmovdqa	%xmm0, 5104(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vmovdqa	%xmm0, 5440(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 5760(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 5728(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vmovdqa	%xmm0, 5488(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 5696(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 5680(%rsp)       # 16-byte Spill
	movq	4896(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 5568(%rsp)       # 32-byte Spill
	movslq	4720(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1128(%rsp)        # 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	movq	672(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	notl	%edx
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	64(%rax), %r8d
	movl	%r8d, 1832(%rsp)        # 4-byte Spill
	movl	%r12d, %r10d
	notl	%r10d
	movq	%r10, 3128(%rsp)        # 8-byte Spill
	movslq	4712(%rsp), %rax        # 4-byte Folded Reload
	leaq	2(%rax,%rax), %rsi
	movq	%rsi, 2208(%rsp)        # 8-byte Spill
	leaq	1(%rax), %rax
	movq	%rax, 696(%rsp)         # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	1(%rax,%r15), %eax
	movl	%eax, 1116(%rsp)        # 4-byte Spill
	movq	4736(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%rdi,2), %eax
	movq	%rax, 4936(%rsp)        # 8-byte Spill
	leal	(,%r15,8), %esi
	movq	%rsi, 2112(%rsp)        # 8-byte Spill
	leal	(%rax,%r15,8), %esi
	movq	%rsi, 2104(%rsp)        # 8-byte Spill
	movq	4928(%rsp), %r9         # 8-byte Reload
	leal	(%r9,%r15,8), %esi
	movq	%rsi, 2096(%rsp)        # 8-byte Spill
	movq	5352(%rsp), %rbx        # 8-byte Reload
	leal	1(%rbx,%r15,8), %esi
	movq	%rsi, 2088(%rsp)        # 8-byte Spill
	leal	-4(%rbx,%r15,8), %esi
	movq	%rsi, 2080(%rsp)        # 8-byte Spill
	leal	-3(%rbx,%r15,8), %esi
	movq	%rsi, 2072(%rsp)        # 8-byte Spill
	leal	-1(%rbx,%r15,8), %esi
	movq	%rsi, 2064(%rsp)        # 8-byte Spill
	leal	-2(%rbx,%r15,8), %esi
	movq	%rsi, 2056(%rsp)        # 8-byte Spill
	leal	-8(%rdi,%rdi,2), %esi
	movq	%rsi, 2216(%rsp)        # 8-byte Spill
	movq	4704(%rsp), %rsi        # 8-byte Reload
	movq	1192(%rsp), %rdi        # 8-byte Reload
	leal	1(%rdi,%rsi), %esi
	movl	%esi, 1112(%rsp)        # 4-byte Spill
	movq	4904(%rsp), %rsi        # 8-byte Reload
	leal	1(%r11,%rsi), %esi
	movl	%esi, 1108(%rsp)        # 4-byte Spill
	leal	(,%r11,8), %esi
	movq	%rsi, 1096(%rsp)        # 8-byte Spill
	leal	(%rax,%r11,8), %eax
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	leal	(%r9,%r11,8), %eax
	movq	%rax, 1080(%rsp)        # 8-byte Spill
	leal	(%rbx,%r11,8), %esi
	movq	%rsi, 2048(%rsp)        # 8-byte Spill
	leal	-3(%rbx,%r11,8), %eax
	movq	%rax, 2040(%rsp)        # 8-byte Spill
	leal	-7(%rbx,%r11,8), %eax
	movq	%rax, 2032(%rsp)        # 8-byte Spill
	leal	-5(%rbx,%r11,8), %eax
	movq	%rax, 2024(%rsp)        # 8-byte Spill
	leal	-8(%rbx,%r11,8), %eax
	movq	%rax, 2016(%rsp)        # 8-byte Spill
	leal	-4(%rbx,%r11,8), %eax
	movq	%rax, 2008(%rsp)        # 8-byte Spill
	leal	-6(%rbx,%r11,8), %eax
	movq	%rax, 2000(%rsp)        # 8-byte Spill
	movq	5096(%rsp), %rax        # 8-byte Reload
	leal	1(%rax,%rcx), %eax
	movl	%eax, 1304(%rsp)        # 4-byte Spill
	movl	1800(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	movl	%eax, 1300(%rsp)        # 4-byte Spill
	leal	(,%rcx,8), %eax
	movq	%rax, 1288(%rsp)        # 8-byte Spill
	leal	(%rbx,%rcx,8), %edi
	movq	%rdi, 3056(%rsp)        # 8-byte Spill
	leal	3(%rbx,%rcx,8), %eax
	movq	%rax, 3048(%rsp)        # 8-byte Spill
	leal	1(%rbx,%rcx,8), %eax
	movq	%rax, 3040(%rsp)        # 8-byte Spill
	leal	-1(%rbx,%rcx,8), %eax
	movq	%rax, 3032(%rsp)        # 8-byte Spill
	leal	-2(%rbx,%rcx,8), %eax
	movq	%rax, 3024(%rsp)        # 8-byte Spill
	leal	2(%rbx,%rcx,8), %eax
	movq	%rbx, %r9
	movq	%rax, 3016(%rsp)        # 8-byte Spill
	movl	$3, %eax
	subl	%r12d, %eax
	leal	(%rax,%rdi), %eax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	movl	$2, %eax
	subl	%r12d, %eax
	leal	(%rax,%rdi), %eax
	movq	%rax, 3000(%rsp)        # 8-byte Spill
	movl	%edi, %eax
	subl	%r12d, %eax
	movq	%rax, 2992(%rsp)        # 8-byte Spill
	addl	$-8, %r13d
	movq	%r13, 2120(%rsp)        # 8-byte Spill
	movq	640(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 584(%rsp)         # 8-byte Spill
	movq	632(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 576(%rsp)         # 8-byte Spill
	movq	624(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 568(%rsp)         # 8-byte Spill
	movq	664(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	movq	1824(%rsp), %rax        # 8-byte Reload
	leal	-1(%rax), %eax
	movl	%eax, 1860(%rsp)        # 4-byte Spill
	movq	1816(%rsp), %rbx        # 8-byte Reload
	leal	-1(%rbx), %eax
	movl	%eax, 1704(%rsp)        # 4-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax), %ecx
	movl	%ecx, 1772(%rsp)        # 4-byte Spill
	leal	-2(%rbx), %ecx
	movl	%ecx, 1700(%rsp)        # 4-byte Spill
	leal	2(%rax), %ecx
	movl	%ecx, 1768(%rsp)        # 4-byte Spill
	leal	2(%rbx), %ecx
	movl	%ecx, 1708(%rsp)        # 4-byte Spill
	movq	4664(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rcx), %rcx
	movq	%rcx, 4736(%rsp)        # 8-byte Spill
	leal	-3(%rax), %eax
	movl	%eax, 1044(%rsp)        # 4-byte Spill
	movslq	1396(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1376(%rsp)        # 8-byte Spill
	movslq	740(%rsp), %rax         # 4-byte Folded Reload
	movq	%rax, 1368(%rsp)        # 8-byte Spill
	movq	4640(%rsp), %rcx        # 8-byte Reload
	leal	-2(%rcx), %eax
	movq	%rax, 4904(%rsp)        # 8-byte Spill
	leal	-1(%rcx), %eax
	movq	%rax, 4896(%rsp)        # 8-byte Spill
	leal	-6(%rcx), %eax
	movq	%rax, 3944(%rsp)        # 8-byte Spill
	leal	-5(%rcx), %eax
	movq	%rax, 3936(%rsp)        # 8-byte Spill
	leal	3(%r9), %eax
	movq	%rax, 4632(%rsp)        # 8-byte Spill
	leal	3(%rcx), %eax
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	leal	1(%rcx), %eax
	movq	%rax, 4616(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	movq	%rax, 4608(%rsp)        # 8-byte Spill
	movq	2160(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	movq	%rax, 1992(%rsp)        # 8-byte Spill
	leal	-8(%rcx), %eax
	movq	%rax, 1984(%rsp)        # 8-byte Spill
	movq	3152(%rsp), %r9         # 8-byte Reload
	movq	2152(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %eax
	movq	%rax, 1976(%rsp)        # 8-byte Spill
	movq	4696(%rsp), %r11        # 8-byte Reload
	leal	(%rcx,%r11), %eax
	movq	%rax, 1968(%rsp)        # 8-byte Spill
	movq	4688(%rsp), %r15        # 8-byte Reload
	leal	(%rcx,%r15), %eax
	movq	%rax, 1960(%rsp)        # 8-byte Spill
	leal	(%rcx,%r10), %eax
	movq	%rax, 1952(%rsp)        # 8-byte Spill
	movq	3144(%rsp), %r14        # 8-byte Reload
	leal	(%rcx,%r14), %eax
	movq	%rax, 1944(%rsp)        # 8-byte Spill
	movq	1848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rbx), %eax
	movl	%eax, 708(%rsp)         # 4-byte Spill
	movl	$0, %eax
	movslq	%r8d, %rcx
	movq	%rcx, 1752(%rsp)        # 8-byte Spill
	leal	(%rsi,%r15), %ecx
	movq	%rcx, 1936(%rsp)        # 8-byte Spill
	movq	4168(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movq	%rcx, 1928(%rsp)        # 8-byte Spill
	movq	4160(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movq	%rcx, 1920(%rsp)        # 8-byte Spill
	leal	(%rsi,%r11), %ecx
	movq	%rcx, 1912(%rsp)        # 8-byte Spill
	movq	4152(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movq	%rcx, 1904(%rsp)        # 8-byte Spill
	movq	4144(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movq	%rcx, 1896(%rsp)        # 8-byte Spill
	leal	(%rdi,%r9), %ecx
	movq	%rcx, 2984(%rsp)        # 8-byte Spill
	leal	(%rdi,%r10), %ecx
	movq	%rcx, 2976(%rsp)        # 8-byte Spill
	leal	(%rdi,%r14), %ecx
	movq	%rcx, 2968(%rsp)        # 8-byte Spill
	movq	616(%rsp), %rsi         # 8-byte Reload
	leaq	17(%rsi), %rcx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	leaq	1(%rsi), %rcx
	movq	%rcx, 488(%rsp)         # 8-byte Spill
	vmovd	4752(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovaps	%ymm0, 1600(%rsp)       # 32-byte Spill
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovd	%ecx, %xmm0
	vmovdqa	%ymm0, 5536(%rsp)       # 32-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	%xmm0, 5360(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB147_195:                            # %for f0.s0.v11.v14
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_230 Depth 2
                                        #       Child Loop BB147_232 Depth 3
                                        #     Child Loop BB147_259 Depth 2
                                        #       Child Loop BB147_261 Depth 3
                                        #     Child Loop BB147_296 Depth 2
                                        #       Child Loop BB147_299 Depth 3
                                        #     Child Loop BB147_318 Depth 2
                                        #       Child Loop BB147_321 Depth 3
                                        #     Child Loop BB147_340 Depth 2
                                        #       Child Loop BB147_342 Depth 3
                                        #     Child Loop BB147_368 Depth 2
                                        #       Child Loop BB147_364 Depth 3
                                        #         Child Loop BB147_365 Depth 4
                                        #     Child Loop BB147_362 Depth 2
                                        #       Child Loop BB147_370 Depth 3
                                        #     Child Loop BB147_405 Depth 2
                                        #       Child Loop BB147_408 Depth 3
                                        #     Child Loop BB147_445 Depth 2
                                        #       Child Loop BB147_446 Depth 3
                                        #         Child Loop BB147_447 Depth 4
                                        #     Child Loop BB147_467 Depth 2
                                        #       Child Loop BB147_469 Depth 3
                                        #         Child Loop BB147_471 Depth 4
                                        #       Child Loop BB147_499 Depth 3
                                        #         Child Loop BB147_501 Depth 4
                                        #         Child Loop BB147_528 Depth 4
                                        #         Child Loop BB147_555 Depth 4
                                        #       Child Loop BB147_583 Depth 3
                                        #         Child Loop BB147_585 Depth 4
                                        #       Child Loop BB147_612 Depth 3
                                        #         Child Loop BB147_614 Depth 4
                                        #       Child Loop BB147_650 Depth 3
                                        #         Child Loop BB147_652 Depth 4
                                        #         Child Loop BB147_687 Depth 4
                                        #         Child Loop BB147_706 Depth 4
                                        #       Child Loop BB147_742 Depth 3
                                        #         Child Loop BB147_744 Depth 4
                                        #       Child Loop BB147_779 Depth 3
                                        #         Child Loop BB147_824 Depth 4
                                        #       Child Loop BB147_783 Depth 3
                                        #         Child Loop BB147_785 Depth 4
                                        #         Child Loop BB147_804 Depth 4
                                        #         Child Loop BB147_844 Depth 4
                                        #       Child Loop BB147_864 Depth 3
                                        #         Child Loop BB147_867 Depth 4
                                        #       Child Loop BB147_887 Depth 3
                                        #         Child Loop BB147_932 Depth 4
                                        #       Child Loop BB147_891 Depth 3
                                        #         Child Loop BB147_893 Depth 4
                                        #         Child Loop BB147_912 Depth 4
                                        #         Child Loop BB147_952 Depth 4
                                        #       Child Loop BB147_972 Depth 3
                                        #         Child Loop BB147_975 Depth 4
                                        #       Child Loop BB147_995 Depth 3
                                        #         Child Loop BB147_997 Depth 4
                                        #       Child Loop BB147_1017 Depth 3
                                        #         Child Loop BB147_1019 Depth 4
                                        #         Child Loop BB147_1038 Depth 4
                                        #         Child Loop BB147_1057 Depth 4
                                        #       Child Loop BB147_1077 Depth 3
                                        #         Child Loop BB147_1079 Depth 4
                                        #       Child Loop BB147_1105 Depth 3
                                        #         Child Loop BB147_1101 Depth 4
                                        #           Child Loop BB147_1102 Depth 5
                                        #       Child Loop BB147_1099 Depth 3
                                        #         Child Loop BB147_1107 Depth 4
                                        #       Child Loop BB147_1143 Depth 3
                                        #         Child Loop BB147_1145 Depth 4
                                        #         Child Loop BB147_1180 Depth 4
                                        #         Child Loop BB147_1200 Depth 4
                                        #       Child Loop BB147_1236 Depth 3
                                        #         Child Loop BB147_1239 Depth 4
                                        #       Child Loop BB147_1275 Depth 3
                                        #         Child Loop BB147_1278 Depth 4
                                        #       Child Loop BB147_1314 Depth 3
                                        #         Child Loop BB147_1316 Depth 4
                                        #         Child Loop BB147_1351 Depth 4
                                        #         Child Loop BB147_1371 Depth 4
                                        #       Child Loop BB147_1407 Depth 3
                                        #         Child Loop BB147_1410 Depth 4
                                        #       Child Loop BB147_1447 Depth 3
                                        #         Child Loop BB147_1448 Depth 4
                                        #           Child Loop BB147_1449 Depth 5
	movl	%edx, 684(%rsp)         # 4-byte Spill
	movq	%rax, 688(%rsp)         # 8-byte Spill
	movl	592(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1592(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 912(%rsp)         # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1584(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movl	%esi, 908(%rsp)         # 4-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1672(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	%rcx, 1568(%rsp)        # 8-byte Spill
	movl	596(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %edx
	movl	%ecx, %r13d
	cmovgel	%edx, %r13d
	movl	%eax, %ecx
	shll	$5, %ecx
	movq	672(%rsp), %rax         # 8-byte Reload
	addl	%eax, %ecx
	movl	612(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movq	%rcx, 1688(%rsp)        # 8-byte Spill
	cmpb	$0, 611(%rsp)           # 1-byte Folded Reload
	je	.LBB147_196
# BB#197:                               # %assert succeeded246
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	600(%rsp), %rsi         # 8-byte Reload
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, %r14
	testq	%r14, %r14
	je	.LBB147_198
# BB#199:                               # %assert succeeded248
                                        #   in Loop: Header=BB147_195 Depth=1
	testq	$-2147483648, 640(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	584(%rsp), %rax         # 8-byte Reload
	jne	.LBB147_200
# BB#201:                               # %assert succeeded250
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB147_202
# BB#204:                               # %assert succeeded252
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB147_205
# BB#208:                               # %assert succeeded256
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r12, 4872(%rsp)        # 8-byte Spill
	movq	%rax, 4880(%rsp)        # 8-byte Spill
	testq	$-2147483648, 632(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	576(%rsp), %rax         # 8-byte Reload
	jne	.LBB147_209
# BB#210:                               # %assert succeeded258
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB147_211
# BB#213:                               # %assert succeeded260
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r15
	testq	%r15, %r15
	je	.LBB147_214
# BB#215:                               # %assert succeeded264
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	testq	$-2147483648, 624(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_216
# BB#217:                               # %assert succeeded266
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	568(%rsp), %rsi         # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, %r14
	testq	%r14, %r14
	je	.LBB147_218
# BB#220:                               # %assert succeeded268
                                        #   in Loop: Header=BB147_195 Depth=1
	testq	$-2147483648, 664(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	560(%rsp), %rax         # 8-byte Reload
	jne	.LBB147_221
# BB#222:                               # %assert succeeded270
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 4712(%rsp)        # 8-byte Spill
	testq	%rax, %rax
	je	.LBB147_223
# BB#227:                               # %assert succeeded272
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	movq	%r15, 4720(%rsp)        # 8-byte Spill
	movq	%r14, 5096(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	testq	%rax, %rax
	je	.LBB147_228
# BB#229:                               # %assert succeeded276
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1592(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	notl	%eax
	cltq
	movq	%rax, 1496(%rsp)        # 8-byte Spill
	movl	$63, %eax
	subl	%ecx, %eax
	movl	%eax, 1012(%rsp)        # 4-byte Spill
	movl	$7, %ecx
	movq	912(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 1024(%rsp)        # 4-byte Spill
	movq	1584(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	notl	%eax
	cltq
	movq	%rax, 1528(%rsp)        # 8-byte Spill
	movl	$3, %eax
	subl	%ecx, %eax
	movl	%eax, 1068(%rsp)        # 4-byte Spill
	movl	908(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	520(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	696(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1520(%rsp)        # 8-byte Spill
	movl	$57, %ecx
	movq	1672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 2240(%rsp)        # 4-byte Spill
	movl	$1, %ecx
	subl	%r13d, %ecx
	movl	%ecx, 1064(%rsp)        # 4-byte Spill
	leal	-4(%r13), %eax
	movl	%eax, 1060(%rsp)        # 4-byte Spill
	movl	$7, %eax
	subl	%r13d, %eax
	movl	%eax, 1056(%rsp)        # 4-byte Spill
	movl	$3, %eax
	subl	%r13d, %eax
	movl	%eax, 1052(%rsp)        # 4-byte Spill
	movl	$9, %eax
	subl	%r13d, %eax
	movl	%eax, 1284(%rsp)        # 4-byte Spill
	movl	$-9, %eax
	subl	%r13d, %eax
	leal	-10(%r13), %edx
	movl	%edx, 1048(%rsp)        # 4-byte Spill
	leal	-6(%r13), %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	leal	-12(%r13), %edx
	movl	%edx, 1276(%rsp)        # 4-byte Spill
	notl	%r13d
	movslq	%r13d, %rdx
	movq	%rdx, 1536(%rsp)        # 8-byte Spill
	cltq
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	movq	1688(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rsi
	movq	%rsi, 1440(%rsp)        # 8-byte Spill
	movl	$8, %edx
	subq	%rsi, %rdx
	movq	%rdx, 1728(%rsp)        # 8-byte Spill
	movslq	%ecx, %rcx
	movq	%rcx, 1000(%rsp)        # 8-byte Spill
	leal	9(%rax), %eax
	movl	%eax, 2896(%rsp)        # 4-byte Spill
	movq	5672(%rsp), %rdx        # 8-byte Reload
	.align	16, 0x90
.LBB147_230:                            # %for deinterleaved$1.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_232 Depth 3
	cmpl	$0, 1740(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_257
# BB#231:                               # %for deinterleaved$1.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_230 Depth=2
	movq	3168(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3600(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	movq	1408(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	1388(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1392(%rsp), %eax        # 4-byte Folded Reload
	movq	1400(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movl	1384(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	cmpq	%rdi, 1376(%rsp)        # 8-byte Folded Reload
	movl	1396(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%rdi, 1368(%rsp)        # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %rdi
	cmovll	%eax, %ecx
	movq	1744(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	vmovd	%ecx, %xmm0
	vpabsd	1648(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3072(%rsp)       # 32-byte Spill
	vpsubd	1600(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3552(%rsp)       # 32-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	movl	1740(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_232:                            # %for deinterleaved$1.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_230 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3520(%rsp)        # 4-byte Spill
	cmpl	$0, 3600(%rsp)          # 4-byte Folded Reload
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%eax, %esi
	andl	$1, %esi
	sete	%r12b
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	3168(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	movq	3984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4416(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3872(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vmovdqa	.LCPI147_10(%rip), %ymm1 # ymm1 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovdqa	%ymm1, %ymm12
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3776(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4256(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 5216(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4224(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 4192(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	3872(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3904(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r15d, %xmm0, %xmm11
	vmovd	%ebx, %xmm0
	vpinsrd	$1, %r14d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r10d, %xmm0
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r11d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm6
	andb	5280(%rsp), %r12b       # 1-byte Folded Reload
	movl	%r12d, 3488(%rsp)       # 4-byte Spill
	andb	5248(%rsp), %sil        # 1-byte Folded Reload
	movl	%esi, 3472(%rsp)        # 4-byte Spill
	vmovd	%ecx, %xmm0
	movq	%rcx, %r14
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4576(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4544(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4096(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4064(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3072(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4384(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4512(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm12, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4368(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4352(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3552(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5280(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3872(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3904(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4224(%rsp)        # 8-byte Spill
	testl	3600(%rsp), %r14d       # 4-byte Folded Reload
	vpbroadcastd	%xmm6, %ymm1
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3312(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3248(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3232(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3264(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3280(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3296(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, 3184(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vmovd	%xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vpaddd	.LCPI147_10(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r8d
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vmovd	%xmm1, %eax
	cltd
	idivl	3808(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3776(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r11d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3744(%rsp)              # 4-byte Folded Reload
	vmovd	3216(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3232(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3200(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3184(%rsp), %xmm1, %xmm3 # 4-byte Folded Reload
	vmovd	%r12d, %xmm1
	vpinsrd	$1, %r15d, %xmm1, %xmm1
	vpinsrd	$2, %r13d, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %r8d, %xmm1, %xmm1
	vmovd	%r10d, %xmm2
	vpinsrd	$1, %r9d, %xmm2, %xmm2
	vpinsrd	$2, %r11d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	leal	-7(%r14), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4480(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4448(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	4032(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3840(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3552(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r8
	movq	%r8, %rax
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3840(%rsp)        # 8-byte Spill
	movslq	%r14d, %rax
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	addq	2912(%rsp), %rax        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4680(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	je	.LBB147_234
# BB#233:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_234:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovd	%ebx, %xmm0
	movl	3488(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm1
	movl	3472(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_236
# BB#235:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_236:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movl	3312(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_238
# BB#237:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_238:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_240
# BB#239:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_240:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm1, 3216(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_242
# BB#241:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_242:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_244
# BB#243:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_244:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm1, 3312(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_246
# BB#245:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm2, %xmm0
.LBB147_246:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rsi
	movq	%rsi, 3488(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rdx,%rsi,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3344(%rsp)        # 8-byte Spill
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3440(%rsp)        # 8-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rsi
	movq	%rsi, 3456(%rsp)        # 8-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r14
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r13
	movq	3872(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rdx,%r9,2), %xmm1, %xmm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdx,%r14,2), %xmm1, %xmm1
	movq	3904(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rdx,%r13,2), %xmm1, %xmm1
	movq	4224(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rdx,%rdi,2), %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rdx,%rbx,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rdx,%rsi,2), %xmm2, %xmm2
	movq	5280(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_248
# BB#247:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm3, %xmm5
.LBB147_248:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movslq	%r11d, %r11
	movslq	%r10d, %r12
	movslq	%r15d, %r10
	movslq	%r8d, %r8
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	3280(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	movq	%rbx, 3392(%rsp)        # 8-byte Spill
	movq	3264(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rdi
	movq	%rdi, 3408(%rsp)        # 8-byte Spill
	movq	3296(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	movq	%rdx, %rsi
	movzwl	(%rsi,%r15,2), %edx
	vmovd	%edx, %xmm0
	movq	3744(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rsi,%rbx,2), %xmm0, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rsi,%rdi,2), %xmm0, %xmm0
	movq	3776(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$6, (%rsi,%rcx,2), %xmm0, %xmm0
	movq	%r8, %rcx
	movzwl	(%rsi,%r11,2), %edx
	vmovd	%edx, %xmm1
	movq	3840(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %r8d
	vpinsrw	$7, %r8d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3616(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r12,2), %xmm1, %xmm1
	movq	3680(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%r10,2), %xmm1, %xmm1
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rcx,2), %xmm1, %xmm1
	movq	3712(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	%rsi, %rdx
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	%ymm1, (%rsi,%rax,4)
	vmovups	%ymm3, 32(%rsi,%rax,4)
	vmovups	%ymm0, 64(%rsi,%rax,4)
	vmovups	%ymm2, 96(%rsi,%rax,4)
	je	.LBB147_250
# BB#249:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	%xmm8, %xmm7
.LBB147_250:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movq	%rdx, %rsi
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %edx
	vmovd	%edx, %xmm0
	movq	3872(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rsi,%r9,2), %xmm0, %xmm0
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rsi,%r14,2), %xmm0, %xmm0
	movq	3904(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$6, (%rsi,%r13,2), %xmm0, %xmm0
	movq	4224(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm0, %xmm0
	movq	3488(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %edx
	vmovd	%edx, %xmm1
	movq	4256(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3456(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5280(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm1, %xmm2
	movq	%rsi, %rdx
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_252
# BB#251:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	3216(%rsp), %xmm6       # 16-byte Reload
.LBB147_252:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movq	%rdx, %rsi
	movzwl	(%rsi,%r11,2), %edx
	vmovd	%edx, %xmm5
	movq	3616(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$2, (%rsi,%r12,2), %xmm5, %xmm5
	movq	3680(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rsi,%r10,2), %xmm5, %xmm5
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm5, %xmm7
	movzwl	(%rsi,%r15,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$6, (%rsi,%rcx,2), %xmm5, %xmm5
	vpinsrw	$7, %r8d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	5672(%rsp), %rdx        # 8-byte Reload
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rdx,%rcx,4)
	vmovups	%ymm4, 12320(%rdx,%rcx,4)
	vmovups	%ymm2, 12352(%rdx,%rcx,4)
	vmovups	%ymm6, 12384(%rdx,%rcx,4)
	je	.LBB147_254
# BB#253:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	3232(%rsp), %xmm9       # 16-byte Reload
.LBB147_254:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_256
# BB#255:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	vmovdqa	3312(%rsp), %xmm15      # 16-byte Reload
.LBB147_256:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_232 Depth=3
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	3840(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rcx,%rsi,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4672(%rsp), %rax        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rdx,%rax,4)
	vmovups	%ymm2, 24608(%rdx,%rax,4)
	vmovups	%ymm1, 24640(%rdx,%rax,4)
	vmovups	%ymm4, 24672(%rdx,%rax,4)
	movq	5312(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3520(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_232
.LBB147_257:                            # %end for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_230 Depth=2
	movq	3168(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	addq	$1, %rax
	cmpl	2896(%rsp), %ecx        # 4-byte Folded Reload
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	jne	.LBB147_230
# BB#258:                               # %produce gH
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%rdx, 5672(%rsp)        # 8-byte Spill
	movq	1688(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax), %edx
	movl	%edx, 1576(%rsp)        # 4-byte Spill
	leal	4(%rax), %ecx
	movl	%ecx, 1680(%rsp)        # 4-byte Spill
	movl	$8, %ecx
	subl	%eax, %ecx
	movq	%rcx, 1208(%rsp)        # 8-byte Spill
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1560(%rsp)        # 4-byte Spill
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1552(%rsp)        # 4-byte Spill
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1544(%rsp)        # 4-byte Spill
	movl	%edx, 2256(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_259:                            # %for gH.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_261 Depth 3
	cmpl	$0, 2248(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_294
# BB#260:                               # %for gH.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_259 Depth=2
	movl	2256(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1804(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1808(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 3872(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2176(%rsp)       # 16-byte Spill
	cltq
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1776(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2168(%rsp)        # 8-byte Spill
	movl	2248(%rsp), %eax        # 4-byte Reload
	movq	5352(%rsp), %r14        # 8-byte Reload
	movl	1560(%rsp), %ecx        # 4-byte Reload
	movl	1552(%rsp), %r9d        # 4-byte Reload
	movl	1544(%rsp), %edi        # 4-byte Reload
	.align	16, 0x90
.LBB147_261:                            # %for gH.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_259 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 5312(%rsp)        # 4-byte Spill
	movl	%edi, 3680(%rsp)        # 4-byte Spill
	movl	%r9d, 3712(%rsp)        # 4-byte Spill
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 3872(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r14d, %r13d
	andl	$1, %r13d
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	3144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	movq	3128(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3136(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	4224(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4256(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 4192(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3904(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3904(%rsp)       # 16-byte Spill
	leal	-2(%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3520(%rsp)        # 4-byte Spill
	vmovd	%r15d, %xmm0
	vpinsrd	$1, 3600(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3472(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3488(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r11d, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3440(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm3, %xmm5
	leal	-1(%r14), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	leal	-3(%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4224(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2176(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5408(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm1
	vmovdqa	5168(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vmovdqa	5024(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpcmpgtd	%xmm1, %xmm14, %xmm4
	vpsubd	%xmm1, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	4224(%rsp), %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	%xmm12, %xmm1, %xmm0
	vmovdqa	%xmm0, 4224(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm11, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movl	2256(%rsp), %ebx        # 4-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	3152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm4
	vmovdqa	3904(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	5200(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm5
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	4192(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm2, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movb	5280(%rsp), %r8b        # 1-byte Reload
	movb	5216(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, %r8b
	vpaddd	%xmm1, %xmm11, %xmm5
	vmovq	%xmm5, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm5, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	movb	5248(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r13b
	movl	%r13d, 5280(%rsp)       # 4-byte Spill
	movl	3872(%rsp), %r15d       # 4-byte Reload
	testl	%r14d, %r15d
	setne	3904(%rsp)              # 1-byte Folded Spill
	leal	1(%r14), %r13d
	movl	%r13d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	sete	%r12b
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	vmovd	3616(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3648(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3552(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3520(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	3488(%rsp), %xmm5       # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vpinsrd	$1, 3472(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$2, 3440(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$3, 3424(%rsp), %xmm5, %xmm6 # 4-byte Folded Reload
	leal	-4(%r14), %eax
	vmovd	%eax, %xmm5
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpinsrd	$2, %esi, %xmm7, %xmm7
	vpsrad	$31, %xmm4, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpinsrd	$3, %edx, %xmm7, %xmm0
	vpaddd	%xmm4, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	orl	%ebx, %eax
	movb	%r12b, %bl
	testb	$1, %al
	sete	3456(%rsp)              # 1-byte Folded Spill
	andb	%r10b, %bl
	andb	%r11b, %r9b
	testl	%r13d, %r15d
	vmovd	%r13d, %xmm1
	vpsrad	$31, %xmm6, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vmovdqa	5008(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm6
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm6, %xmm6
	vmovdqa	4832(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm2, %xmm5, %xmm2
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	5104(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm2, %xmm5
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm3
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm6
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	setne	%r15b
	vmovq	%xmm0, %r8
	movq	%r8, 2368(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3600(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm1, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 2440(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	5488(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2496(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2512(%rsp)        # 8-byte Spill
	movslq	5312(%rsp), %rcx        # 4-byte Folded Reload
	movq	%rcx, %rdx
	orq	$4, %rdx
	movq	%rdx, 2528(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2656(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2704(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2720(%rsp)        # 8-byte Spill
	movq	%rcx, %rdx
	orq	$6, %rdx
	movq	%rdx, 2736(%rsp)        # 8-byte Spill
	vmovdqa	4224(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2880(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3520(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm10
	vmovaps	%xmm10, 4224(%rsp)      # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_263
# BB#262:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm10, %xmm10, %xmm10
.LBB147_263:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	movzbl	4192(%rsp), %r13d       # 1-byte Folded Reload
	vmovd	%r13d, %xmm0
	movzbl	3904(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm8
	vmovaps	%xmm8, 3904(%rsp)       # 16-byte Spill
	je	.LBB147_265
# BB#264:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_265:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_267
# BB#266:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_267:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_269
# BB#268:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_269:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm1, 2272(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movzbl	%bl, %edi
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	je	.LBB147_271
# BB#270:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_271:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm0, 2304(%rsp)       # 16-byte Spill
	movzbl	3456(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	%r15b, %edi
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, %xmm2
	movq	4872(%rsp), %r15        # 8-byte Reload
	je	.LBB147_273
# BB#272:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_273:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm2, 2320(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	movzbl	%r9b, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_275
# BB#274:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_275:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm2, 2336(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movl	3712(%rsp), %r9d        # 4-byte Reload
	je	.LBB147_277
# BB#276:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_277:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm0, 2352(%rsp)       # 16-byte Spill
	movq	2896(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3200(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2912(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2848(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3072(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3216(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm12 # xmm12 = xmm2[0,1,2],mem[0]
	movq	2752(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2768(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2800(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm15 # xmm15 = xmm3[0,1,2],mem[0]
	movq	2544(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2592(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2576(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2608(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm11 # xmm11 = xmm3[0,1,2],mem[0]
	movslq	%r9d, %rdi
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rdi,4), %xmm3
	vmovups	12328(%rdx,%rdi,4), %xmm6
	vmovups	12304(%rdx,%rdi,4), %xmm7
	vmovups	12320(%rdx,%rdi,4), %xmm1
	vshufps	$136, 12336(%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,2],mem[0,2]
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm0, %xmm9, %xmm0
	vshufps	$221, %xmm6, %xmm3, %xmm2 # xmm2 = xmm3[1,3],xmm6[1,3]
	vmovaps	5472(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$221, %xmm1, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm1[1,3]
	vmulps	%xmm12, %xmm9, %xmm1
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm6, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm15, %xmm9, %xmm3
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm7
	vsubps	%xmm4, %xmm14, %xmm6
	cmpl	$0, 104(%rbp)
	je	.LBB147_279
# BB#278:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm10, 4256(%rsp)      # 16-byte Spill
.LBB147_279:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm9
	vmaxps	%xmm1, %xmm2, %xmm0
	vmulps	3840(%rsp), %xmm11, %xmm12 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm6, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm5
	je	.LBB147_281
# BB#280:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	%xmm8, 4192(%rsp)       # 16-byte Spill
.LBB147_281:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	movq	2368(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2384(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	movq	2440(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r10,4), %xmm6, %xmm15 # xmm15 = xmm6[0,1,2],mem[0]
	movl	3680(%rsp), %edi        # 4-byte Reload
	movslq	%edi, %rax
	vmovups	24592(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovups	24608(%rdx,%rax,4), %xmm10
	vmovups	24624(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 2896(%rsp)       # 16-byte Spill
	vmovups	24600(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3184(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm6
	vmovaps	%xmm6, 3168(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	vaddps	%xmm9, %xmm0, %xmm7
	vmovaps	%xmm7, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm12, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm5, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm0
	vshufps	$136, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm10[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm7, %xmm1
	vshufps	$136, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm15, %xmm7, %xmm2
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_283
# BB#282:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	2272(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
.LBB147_283:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	movq	2464(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rdx
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2496(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2480(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2512(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],mem[0]
	movq	2528(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	2672(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2704(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2688(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2720(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2736(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3072(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm9
	vmovups	48(%rsi,%rcx,4), %xmm11
	vmovaps	%xmm11, 2848(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%rcx,4), %xmm14
	vmovaps	%xmm14, 2912(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm12, %xmm15
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm1
	vshufps	$136, %xmm9, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm9[0,2]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$136, %xmm11, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm12, %xmm4
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vmulps	5280(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm4
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm5, %xmm1, %xmm8
	je	.LBB147_285
# BB#284:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	2288(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3904(%rsp)       # 16-byte Spill
.LBB147_285:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vsubps	%xmm0, %xmm15, %xmm1
	vmovdqa	4224(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vfmadd213ps	%xmm2, %xmm14, %xmm7
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm4, %xmm6
	vmovdqa	3904(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm8, %xmm5, %xmm0
	je	.LBB147_287
# BB#286:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	2304(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 5248(%rsp)       # 16-byte Spill
.LBB147_287:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vfmadd213ps	%xmm2, %xmm14, %xmm1
	vblendvps	%xmm15, %xmm7, %xmm0, %xmm0
	vaddps	%xmm6, %xmm8, %xmm15
	je	.LBB147_289
# BB#288:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovaps	2320(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 5216(%rsp)       # 16-byte Spill
.LBB147_289:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm7
	movq	2816(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2832(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2880(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm1
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 2896(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm10[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3184(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3168(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm2, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm12, %xmm4
	vminps	%xmm13, %xmm1, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm1
	vmulps	5280(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3472(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_291
# BB#290:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovdqa	2336(%rsp), %xmm10      # 16-byte Reload
.LBB147_291:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vblendvps	%xmm0, %xmm8, %xmm7, %xmm8
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 2848(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm9[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3072(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 2912(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3520(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm4, %xmm12
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm9[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vsubps	%xmm0, %xmm12, %xmm0
	vfmadd213ps	%xmm6, %xmm14, %xmm1
	vfmadd213ps	%xmm6, %xmm14, %xmm0
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmovdqa	5216(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vpslld	$31, %xmm10, %xmm5
	vmovdqa	3456(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_293
# BB#292:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vmovdqa	2352(%rsp), %xmm6       # 16-byte Reload
.LBB147_293:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_261 Depth=3
	vpslld	$31, %xmm6, %xmm6
	vmovaps	3216(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm6, %xmm7, %xmm2, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm7, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r14d, %rax
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r15,%rax,4)
	addl	$8, %edi
	addl	$8, %r9d
	movl	5312(%rsp), %ecx        # 4-byte Reload
	addl	$8, %ecx
	addl	$8, %r14d
	movl	3744(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_261
.LBB147_294:                            # %end for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_259 Depth=2
	movl	2256(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2256(%rsp)        # 4-byte Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1544(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1552(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1560(%rsp)        # 4-byte Folded Spill
	cmpl	1680(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_259
# BB#295:                               # %for gV.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1568(%rsp), %rax        # 8-byte Reload
	leal	7(%rax), %ecx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	leal	10(%rax), %ecx
	movq	%rcx, 2368(%rsp)        # 8-byte Spill
	addl	$9, %eax
	movq	%rax, 1568(%rsp)        # 8-byte Spill
	movl	1576(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	movl	2248(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_296:                            # %for gV.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_299 Depth 3
	testl	%eax, %eax
	jle	.LBB147_297
# BB#298:                               # %for gV.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_296 Depth=2
	movq	%r8, 2880(%rsp)         # 8-byte Spill
	movl	%r8d, %ebx
	movq	1816(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ebx
	leal	-1(%rbx), %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edi
	movl	%ecx, %r11d
	subl	%eax, %edi
	movq	1848(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %esi
	cmovgl	%r8d, %esi
	addl	$-1, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ecx
	cmovll	%edi, %esi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r8d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %ecx
	cmovlel	%edx, %ebx
	movl	%r8d, %ecx
	subl	%r9d, %ecx
	cmovll	%edx, %ebx
	cmovlel	%edi, %esi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r12d, %edi
	addl	%edx, %edi
	movl	%r11d, %eax
	subl	%edi, %eax
	cmpl	%edi, %r10d
	cmovgl	%edi, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	leal	1(%r8), %edi
	movl	%edi, 2656(%rsp)        # 4-byte Spill
	cmpl	%edi, %r13d
	movl	%r13d, %edx
	cmovgl	%edi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%eax, %edx
	movl	%r8d, %edi
	andl	$1, %edi
	movl	%edi, 5248(%rsp)        # 4-byte Spill
	movslq	%ebx, %r11
	movq	1880(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2864(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rbx
	movq	%rbx, 5312(%rsp)        # 8-byte Spill
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5312(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r11), %r12
	movl	1860(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %edx
	subl	%esi, %edx
	cmpl	%esi, %r10d
	cmovgl	%esi, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	leal	2(%r8), %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %esi
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rax,%rdx), %r15
	subl	%ecx, %ebx
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	leal	-2(%r8), %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ebx, %ecx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %ecx
	movslq	%ecx, %rcx
	imulq	%r14, %rcx
	leaq	(%rax,%rcx), %rbx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r15,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1776(%rsp), %rcx        # 8-byte Folded Reload
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2800(%rsp)        # 8-byte Spill
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2784(%rsp)        # 8-byte Spill
	subq	4760(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1568(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	xorl	%r9d, %r9d
	movl	2248(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_299:                            # %for gV.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_296 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5248(%rsp)          # 4-byte Folded Reload
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %r15d
	movl	%r15d, 3776(%rsp)       # 4-byte Spill
	movl	%r15d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r11d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2864(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5392(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	2880(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	sete	%r8b
	andb	5280(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5312(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5248(%rsp), %r15d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r14
	movq	%r14, %r15
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r13
	movq	%r13, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	movq	2800(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	movq	2784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movq	2704(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %r10d
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3184(%rsp)        # 4-byte Spill
	movq	2768(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	movq	2752(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3280(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3296(%rsp)        # 4-byte Spill
	je	.LBB147_301
# BB#300:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_301:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	movzbl	%r8b, %r8d
	vmovd	%r8d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_303
# BB#302:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_303:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5312(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_305
# BB#304:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_305:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	%xmm3, 5312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3072(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_307
# BB#306:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_307:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r10d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movslq	3184(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r14d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r13d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5472(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2832(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5312(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_309
# BB#308:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovdqa	2912(%rsp), %xmm9       # 16-byte Reload
.LBB147_309:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5472(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3312(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_311
# BB#310:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovdqa	2896(%rsp), %xmm7       # 16-byte Reload
.LBB147_311:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4224(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5312(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5216(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3424(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_313
# BB#312:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovdqa	3072(%rsp), %xmm6       # 16-byte Reload
.LBB147_313:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	3472(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3472(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	je	.LBB147_315
# BB#314:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5280(%rsp)       # 16-byte Spill
.LBB147_315:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_299 Depth=3
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3488(%rsp), %xmm11      # 16-byte Reload
	vmulps	4224(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4192(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vmulps	3872(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3472(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3840(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3408(%rsp), %xmm2       # 16-byte Reload
	vmulps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3600(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5312(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2816(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r9d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_299
	jmp	.LBB147_316
.LBB147_297:                            # %for gV.s0.v11.end for gV.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_296 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2656(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_316:                            #   in Loop: Header=BB147_296 Depth=2
	movl	2252(%rsp), %ecx        # 4-byte Reload
	movl	1056(%rsp), %edx        # 4-byte Reload
	movl	1052(%rsp), %esi        # 4-byte Reload
	movl	2656(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	1680(%rsp), %eax        # 4-byte Folded Reload
	movl	2248(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_296
# BB#317:                               # %produce dV
                                        #   in Loop: Header=BB147_195 Depth=1
	movl	%esi, 1052(%rsp)        # 4-byte Spill
	movl	%edx, 1056(%rsp)        # 4-byte Spill
	movq	1688(%rsp), %rax        # 8-byte Reload
	leal	-6(%rax), %edx
	movl	%edx, 2608(%rsp)        # 4-byte Spill
	leal	8(%rax), %eax
	movl	%eax, 2440(%rsp)        # 4-byte Spill
	movl	%edx, %r9d
	.align	16, 0x90
.LBB147_318:                            # %for dV.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_321 Depth 3
	testl	%ecx, %ecx
	jle	.LBB147_319
# BB#320:                               # %for dV.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_318 Depth=2
	movq	%r9, 2752(%rsp)         # 8-byte Spill
	movl	%r9d, %edi
	movq	1816(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1824(%rsp), %r13        # 8-byte Reload
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %r15d
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r11d
	subl	%eax, %esi
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1804(%rsp), %r8d        # 4-byte Reload
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %r10d
	cmovgl	%r9d, %r10d
	addl	$-1, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	cmpl	%r9d, %ecx
	cmovll	%esi, %r10d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, %r8d
	movl	%r8d, %edi
	cmovgl	%r9d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r9d, %ecx
	cmovlel	%edx, %edi
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r10d
	leal	1(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	leal	1(%r9), %eax
	movl	%eax, 2352(%rsp)        # 4-byte Spill
	cmpl	%eax, %r8d
	movl	%r8d, %r14d
	cmovgl	%eax, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r9d, %r8d
	cmovlel	%esi, %r14d
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movslq	%edi, %rdi
	movq	1880(%rsp), %r12        # 8-byte Reload
	imulq	%r12, %rdi
	movq	%rdi, 5312(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %r11d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2736(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%rdi), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%r11d, %edi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r13d
	sarl	$31, %edi
	movl	1836(%rsp), %eax        # 4-byte Reload
	andl	%eax, %edi
	addl	%r11d, %edi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	addl	%edx, %ecx
	movl	1860(%rsp), %eax        # 4-byte Reload
	subl	%edi, %eax
	movq	1848(%rsp), %r13        # 8-byte Reload
	cmpl	%edi, %r13d
	cmovgl	%edi, %eax
	addl	%ebx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	2(%r9), %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%eax, %edx
	cmpl	%r9d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r11
	movq	1888(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	imulq	%r12, %r11
	leaq	(%r15,%r11), %rdx
	movl	1860(%rsp), %edi        # 4-byte Reload
	subl	%ecx, %edi
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %edi
	addl	%ebx, %edi
	cmpl	%edi, %r8d
	cmovlel	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	leal	-2(%r9), %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%edi, %ecx
	cmpl	%r9d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%edi, %ecx
	movslq	%ecx, %rcx
	imulq	%r12, %rcx
	leaq	(%r15,%rcx), %rdi
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	cmpl	%r9d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r14d
	movslq	%r14d, %rdx
	imulq	%r12, %rdx
	leaq	(%rdx,%r15), %rdx
	movslq	%r10d, %rsi
	imulq	%r12, %rsi
	leaq	(%rsi,%r15), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r10
	leaq	(%rdi,%rcx), %rsi
	movq	5312(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1864(%rsp), %rbx        # 8-byte Reload
	leaq	(%r11,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2640(%rsp)        # 8-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	2416(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2592(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2560(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2544(%rsp)        # 8-byte Spill
	movq	2400(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2528(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2512(%rsp)        # 8-byte Spill
	movq	1568(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2496(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	2252(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_321:                            # %for dV.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_318 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	cmpl	$0, 5280(%rsp)          # 4-byte Folded Reload
	setne	3744(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r15d
	movl	%r15d, 3840(%rsp)       # 4-byte Spill
	movl	%r15d, %r11d
	andl	$1, %r11d
	sete	%r12b
	movq	3936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movq	3944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r14d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r11d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2736(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5360(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4184(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	2752(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	3744(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_54(%rip), %xmm3, %xmm3
	vmovdqa	4800(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5280(%rsp), %r15d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r14
	movq	%r14, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r15
	movq	%r15, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3216(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	movq	2592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r11d
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r10d
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movq	2624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r12d
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movq	2464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3280(%rsp)        # 4-byte Spill
	movq	2560(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	movq	2512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	je	.LBB147_323
# BB#322:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_323:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	je	.LBB147_325
# BB#324:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_325:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_327
# BB#326:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_327:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	%xmm4, 3376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	je	.LBB147_329
# BB#328:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_329:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r11d, %rcx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2912(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2896(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2880(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3072(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movslq	%r12d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5504(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2704(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2864(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2656(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2832(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r14,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2912(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	2896(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3776(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3072(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3312(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3376(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_331
# BB#330:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovdqa	2784(%rsp), %xmm10      # 16-byte Reload
.LBB147_331:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vandps	3424(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3344(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3328(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3392(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_333
# BB#332:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovdqa	2768(%rsp), %xmm11      # 16-byte Reload
.LBB147_333:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3616(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5216(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3360(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3600(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4256(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3408(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3328(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_335
# BB#334:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovdqa	2800(%rsp), %xmm5       # 16-byte Reload
.LBB147_335:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vmulps	4224(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3440(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4192(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm7
	vmovaps	%xmm7, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3904(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm14
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3376(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovdqa	3328(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmulps	3424(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	vmulps	3296(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	je	.LBB147_337
# BB#336:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
.LBB147_337:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_321 Depth=3
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	5248(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3600(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5216(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	4224(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4192(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5312(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3744(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3520(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3456(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3840(%rsp), %rax        # 4-byte Folded Reload
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4752(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	3872(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_321
	jmp	.LBB147_338
.LBB147_319:                            # %for dV.s0.v11.end for dV.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_318 Depth=2
	addl	$1, %r9d
	movl	%r9d, %eax
	movl	%eax, 2352(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_338:                            #   in Loop: Header=BB147_318 Depth=2
	movl	1068(%rsp), %eax        # 4-byte Reload
	movl	1064(%rsp), %edx        # 4-byte Reload
	movl	1060(%rsp), %esi        # 4-byte Reload
	movl	1048(%rsp), %edi        # 4-byte Reload
	movl	2352(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %r9d
	cmpl	2440(%rsp), %ecx        # 4-byte Folded Reload
	movl	2252(%rsp), %ecx        # 4-byte Reload
	jne	.LBB147_318
# BB#339:                               #   in Loop: Header=BB147_195 Depth=1
	movl	%edi, 1048(%rsp)        # 4-byte Spill
	movl	%esi, 1060(%rsp)        # 4-byte Spill
	movl	%edx, 1064(%rsp)        # 4-byte Spill
	movl	%eax, 1068(%rsp)        # 4-byte Spill
	movl	828(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2544(%rsp)        # 4-byte Spill
	movl	532(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2528(%rsp)        # 4-byte Spill
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2512(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_340:                            # %for dh.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_342 Depth 3
	cmpl	$0, 2252(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_359
# BB#341:                               # %for dh.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_340 Depth=2
	movl	2608(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1804(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1808(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 2592(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5312(%rsp)       # 16-byte Spill
	cltq
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movl	2252(%rsp), %eax        # 4-byte Reload
	movq	5352(%rsp), %r13        # 8-byte Reload
	movl	2544(%rsp), %r15d       # 4-byte Reload
	movl	2528(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, 3904(%rsp)        # 4-byte Spill
	movl	2512(%rsp), %r8d        # 4-byte Reload
	.align	16, 0x90
.LBB147_342:                            # %for dh.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_340 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%r8d, 3808(%rsp)        # 4-byte Spill
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	movl	2592(%rsp), %r10d       # 4-byte Reload
	testl	%r10d, %r10d
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r13d, %r11d
	andl	$1, %r11d
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	4144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5312(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4992(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5408(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3840(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5008(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4832(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	movq	4168(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3680(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	leal	-8(%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r9d
	sarq	$32, %rbx
	movq	%rbx, 3440(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3472(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	leal	-7(%r13), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4336(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3968(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3776(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3744(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	vmovdqa	4320(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3952(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	orl	2608(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	2752(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %bl         # 1-byte Reload
	andb	5280(%rsp), %bl         # 1-byte Folded Reload
	andb	5248(%rsp), %r11b       # 1-byte Folded Reload
	movl	%r11d, 5248(%rsp)       # 4-byte Spill
	testl	%r13d, %r10d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5280(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	5024(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %r8
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %r12
	sarq	$32, %r12
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3840(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2688(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r14
	movq	%r14, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vmovdqa	3552(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3600(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_344
# BB#343:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_344:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movzbl	2752(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	5280(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_346
# BB#345:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_346:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	%xmm1, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_348
# BB#347:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_348:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	movl	%r15d, 3840(%rsp)       # 4-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	je	.LBB147_350
# BB#349:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_350:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5528(%rsp), %r15        # 8-byte Reload
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3520(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2832(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2816(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2880(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2864(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2912(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	3904(%rsp), %rbx        # 4-byte Folded Reload
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12296(%rdx,%rbx,4), %xmm7
	vmovups	12312(%rdx,%rbx,4), %xmm0
	vmovups	12304(%rdx,%rbx,4), %xmm3
	vmovups	12320(%rdx,%rbx,4), %xmm14
	vmovups	12288(%rdx,%rbx,4), %xmm5
	movq	2896(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3168(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3200(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	movq	3072(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3248(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r10d, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r11d, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r15,%r12,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2576(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5248(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_352
# BB#351:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	2640(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5280(%rsp)       # 16-byte Spill
.LBB147_352:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movl	3808(%rsp), %r8d        # 4-byte Reload
	vmovdqa	2752(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_354
# BB#353:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovdqa	2624(%rsp), %xmm15      # 16-byte Reload
.LBB147_354:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	movq	2688(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2704(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r14,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2720(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	%r8d, %rcx
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	24584(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3216(%rsp)       # 16-byte Spill
	vmovups	24576(%rsi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	vmovups	24592(%rsi,%rcx,4), %xmm9
	vmovups	24608(%rsi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5760(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3680(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3680(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5280(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3200(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3184(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_356
# BB#355:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_356:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3440(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm7
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm11
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r15, %rdi
	vmovups	(%rsi,%rax,4), %xmm15
	vmovaps	%xmm15, 3424(%rsp)      # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3520(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vaddps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3200(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmulps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovdqa	3184(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	3168(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4256(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movl	3840(%rsp), %r15d       # 4-byte Reload
	je	.LBB147_358
# BB#357:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_358:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_342 Depth=3
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3776(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3712(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3472(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5280(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5488(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3520(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	5216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3600(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4256(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3488(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3552(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4720(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	addl	$8, 3904(%rsp)          # 4-byte Folded Spill
	addl	$8, %r15d
	addl	$8, %r13d
	movl	3872(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_342
.LBB147_359:                            # %end for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_340 Depth=2
	movl	2608(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2608(%rsp)        # 4-byte Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	addl	%eax, 2512(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2528(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2544(%rsp)        # 4-byte Folded Spill
	cmpl	2440(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_340
# BB#360:                               # %for f4.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movl	2248(%rsp), %r14d       # 4-byte Reload
	testl	%r14d, %r14d
	movq	864(%rsp), %r11         # 8-byte Reload
	movq	5096(%rsp), %r15        # 8-byte Reload
	movq	4720(%rsp), %rsi        # 8-byte Reload
	movq	%rsi, %r13
	movq	4752(%rsp), %rsi        # 8-byte Reload
	vmovaps	768(%rsp), %ymm2        # 32-byte Reload
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vmovdqu	.LCPI147_22(%rip), %xmm3
	movl	1576(%rsp), %eax        # 4-byte Reload
	jle	.LBB147_361
	.align	16, 0x90
.LBB147_368:                            # %for f4.s0.v10.v10.preheader.us
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_364 Depth 3
                                        #         Child Loop BB147_365 Depth 4
	movl	%eax, 1576(%rsp)        # 4-byte Spill
	movl	%eax, %r9d
	andl	$63, %r9d
	movl	%r9d, %r8d
	movq	1232(%rsp), %rax        # 8-byte Reload
	imull	%eax, %r8d
	imulq	1264(%rsp), %r9         # 8-byte Folded Reload
	subq	4760(%rsp), %r9         # 8-byte Folded Reload
	xorl	%r10d, %r10d
	.align	16, 0x90
.LBB147_364:                            # %for f4.s0.v10.v10.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_368 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_365 Depth 4
	leal	(,%r10,8), %r12d
	vxorps	%ymm0, %ymm0, %ymm0
	movl	$9, %edi
	movl	2240(%rsp), %ebx        # 4-byte Reload
	.align	16, 0x90
.LBB147_365:                            # %for sum.s1.r4$y.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_368 Depth=2
                                        #       Parent Loop BB147_364 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ebx, %ecx
	andl	$63, %ecx
	imull	%r11d, %ecx
	leal	(%r12,%rcx), %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r13,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$1, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$2, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$3, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$4, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$5, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$6, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r13,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	orl	$7, %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r13,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	leal	8(%r12,%rcx), %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r13,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	addl	$1, %ebx
	addl	$-1, %edi
	jne	.LBB147_365
# BB#366:                               # %consume sum.us
                                        #   in Loop: Header=BB147_364 Depth=3
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%r12,%rax), %eax
	addl	%r8d, %r12d
	vpbroadcastd	%xmm3, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm0
	movslq	%r12d, %rcx
	movq	4872(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm1
	movq	4880(%rsp), %rdx        # 8-byte Reload
	vblendvps	%ymm0, (%rdx,%rcx,4), %ymm1, %ymm0
	cltq
	leaq	(%rax,%r9), %rax
	vmovups	%ymm0, (%r15,%rax,4)
	addq	$1, %r10
	cmpl	%r14d, %r10d
	jne	.LBB147_364
# BB#367:                               # %end for f4.s0.v10.v10.us
                                        #   in Loop: Header=BB147_368 Depth=2
	movl	1576(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	addl	$1, 2240(%rsp)          # 4-byte Folded Spill
	cmpl	1680(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_368
.LBB147_361:                            # %produce f7
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1688(%rsp), %rax        # 8-byte Reload
	leal	2(%rax), %ecx
	movl	%ecx, 2176(%rsp)        # 4-byte Spill
	movq	1672(%rsp), %rcx        # 8-byte Reload
	leal	10(%rcx), %edx
	movq	%rdx, 2168(%rsp)        # 8-byte Spill
	leal	8(%rcx), %edx
	movq	%rdx, 1680(%rsp)        # 8-byte Spill
	leal	7(%rcx), %edx
	movq	%rdx, 1576(%rsp)        # 8-byte Spill
	leal	11(%rcx), %edx
	movq	%rdx, 1568(%rsp)        # 8-byte Spill
	addl	$9, %ecx
	movq	%rcx, 1672(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_362:                            # %for f7.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_370 Depth 3
	testl	%eax, %eax
	jle	.LBB147_363
# BB#369:                               # %for f7.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_362 Depth=2
	movq	%r8, 2440(%rsp)         # 8-byte Spill
	movl	%r8d, %edi
	movq	1816(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r10d
	subl	%eax, %esi
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r9
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %r14d
	cmovgl	%r8d, %r14d
	addl	$-1, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %ecx
	cmovll	%esi, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r9d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %edi
	cmovgl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r8d, %ecx
	cmovlel	%edx, %edi
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r14d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	leal	1(%r8), %eax
	movl	%eax, 2240(%rsp)        # 4-byte Spill
	cmpl	%eax, %r13d
	movl	%r13d, %r9d
	cmovgl	%eax, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	cmpl	%r8d, %r13d
	cmovlel	%r11d, %r9d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2416(%rsp)       # 16-byte Spill
	movslq	%edi, %r10
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%edi, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1880(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %r10
	movq	1864(%rsp), %r12        # 8-byte Reload
	leaq	(%r10,%r12), %rcx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%esi, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%esi, %edx
	cmovgl	%esi, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	2(%r8), %r10d
	cmpl	%r10d, %r13d
	movl	%r13d, %edx
	cmovgl	%r10d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r15, %rcx
	leaq	(%rcx,%r12), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	-2(%r8), %esi
	cmpl	%esi, %r13d
	movl	%r13d, %eax
	cmovgl	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %eax
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %eax
	cltq
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movslq	%r14d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%r11d, %r9d
	movslq	%r9d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movq	2448(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1792(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	movl	2240(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1832(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2336(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2320(%rsp)        # 8-byte Spill
	movq	1680(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2304(%rsp)        # 8-byte Spill
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 2384(%rsp)        # 8-byte Spill
	movq	1576(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2288(%rsp)        # 8-byte Spill
	andl	$63, %r10d
	imull	%eax, %r10d
	movq	%r10, 2400(%rsp)        # 8-byte Spill
	movq	1568(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2256(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_370:                            # %for f7.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_362 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3072(%rsp)        # 4-byte Spill
	cmpl	$0, 3168(%rsp)          # 4-byte Folded Reload
	sete	3840(%rsp)              # 1-byte Folded Spill
	setne	3744(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4256(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r15d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2416(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5216(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5120(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5056(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5360(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5376(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5424(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3808(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r13d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3776(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4960(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4784(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r12d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r15d
	movl	%r15d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2768(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2848(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r12d
	vmovq	%xmm1, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	movb	3744(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5280(%rsp)       # 1-byte Folded Spill
	movl	5216(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2440(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movslq	%eax, %r15
	sete	3216(%rsp)              # 1-byte Folded Spill
	movl	3168(%rsp), %r13d       # 4-byte Reload
	testl	%ecx, %r13d
	setne	3232(%rsp)              # 1-byte Folded Spill
	movb	3840(%rsp), %r10b       # 1-byte Reload
	movl	5312(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	movq	%r15, %rax
	orq	$6, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	movl	%r8d, %r12d
	andl	$1, %r12d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r13d
	movzbl	3216(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r12b
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %ecx
	movslq	%ecx, %r13
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %edi
	movslq	%edi, %rax
	movq	%r13, %rcx
	orq	$6, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	movq	%rax, %rcx
	movq	%rax, %rsi
	orq	$6, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2800(%rsp)        # 4-byte Spill
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r10d
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %ebx
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2816(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	je	.LBB147_372
# BB#371:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_372:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	5280(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5312(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_374
# BB#373:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_374:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3232(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_376
# BB#375:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_376:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_378
# BB#377:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_378:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_380
# BB#379:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_380:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	je	.LBB147_382
# BB#381:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_382:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	5096(%rsp), %rdi        # 8-byte Reload
	movq	%rsi, %r9
	je	.LBB147_384
# BB#383:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_384:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2880(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2912(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_386
# BB#385:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_386:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	3776(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	movq	5528(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2768(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2688(%rsp)       # 16-byte Spill
	movq	2752(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2784(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5672(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovups	48(%rax,%r15,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	movslq	%r10d, %r8
	vmovups	8(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r8,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4224(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rax,%r13,4), %xmm14
	vmovups	48(%rax,%r13,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rax,%r9,4), %xmm6
	vmovups	48(%rax,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	vmovups	40(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovups	56(%rax,%r15,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3472(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2768(%rsp)      # 16-byte Spill
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movslq	%ebx, %rdx
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rax,%rcx,4), %xmm13
	vmovups	48(%rax,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3904(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	40(%rax,%rcx,4), %xmm14
	vmovaps	%xmm14, 3776(%rsp)      # 16-byte Spill
	vmovups	56(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3840(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	3872(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rax,%rcx,4), %xmm1
	vmovups	48(%rax,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovups	40(%rax,%rcx,4), %xmm3
	vmovaps	%xmm3, 3712(%rsp)       # 16-byte Spill
	vmovups	56(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	movq	%rax, %rbx
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4256(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	3248(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	movslq	2800(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5248(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	2816(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	movslq	2832(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4256(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3312(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_388
# BB#387:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	2464(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
.LBB147_388:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vsubps	3472(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vsubps	3488(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm9 # xmm9 = xmm13[1,3],xmm10[1,3]
	vmulps	%xmm0, %xmm15, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vmovaps	3520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3280(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	2816(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3488(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3456(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2816(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3552(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
	vmovaps	%xmm13, 2832(%rsp)      # 16-byte Spill
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3392(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3808(%rsp), %xmm13, %xmm13 # 16-byte Folded Reload
	movq	4712(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_390
# BB#389:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm9, 3648(%rsp)       # 16-byte Spill
	vmovaps	2480(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3184(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm9       # 16-byte Reload
.LBB147_390:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	%xmm9, 3648(%rsp)       # 16-byte Spill
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3424(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2608(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm10, %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vmovaps	3488(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r13,4), %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2768(%rsp), %xmm2       # 16-byte Reload
	vmulps	4224(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	movq	3344(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	movq	%rbx, %rax
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4256(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vaddps	3440(%rsp), %xmm13, %xmm8 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	%xmm9, %xmm5, %xmm9
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3360(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_392
# BB#391:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovdqa	2496(%rsp), %xmm6       # 16-byte Reload
.LBB147_392:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2624(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vsubps	2608(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vmulps	3872(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3520(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3424(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3472(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2880(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_394
# BB#393:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovdqa	2512(%rsp), %xmm4       # 16-byte Reload
.LBB147_394:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3328(%rsp), %xmm4       # 16-byte Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2912(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_396
# BB#395:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	2528(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
.LBB147_396:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4256(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vmulps	5248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3280(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3200(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_398
# BB#397:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	2544(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3216(%rsp)       # 16-byte Spill
.LBB147_398:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3840(%rsp), %xmm4       # 16-byte Reload
	vmulps	4224(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2816(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rax,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2800(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	4192(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rax,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2752(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2832(%rsp), %xmm4       # 16-byte Reload
	vaddps	3440(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3808(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3424(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5488(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_400
# BB#399:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovdqa	2560(%rsp), %xmm15      # 16-byte Reload
.LBB147_400:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vaddps	3552(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3680(%rsp), %xmm6       # 16-byte Reload
	vmulps	3872(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vaddps	3328(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	vmovdqa	2896(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_402
# BB#401:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovdqa	2576(%rsp), %xmm0       # 16-byte Reload
.LBB147_402:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_370 Depth=3
	vmovaps	3456(%rsp), %xmm7       # 16-byte Reload
	vaddps	3488(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3472(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3520(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5216(%rsp), %rax        # 4-byte Folded Reload
	movq	2368(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3072(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_370
	jmp	.LBB147_403
.LBB147_363:                            # %for f7.s0.v11.end for f7.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_362 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2240(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_403:                            #   in Loop: Header=BB147_362 Depth=2
	movq	4720(%rsp), %rcx        # 8-byte Reload
	movq	4752(%rsp), %rdx        # 8-byte Reload
	movl	2240(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	2176(%rsp), %eax        # 4-byte Folded Reload
	movl	1800(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_362
# BB#404:                               # %for f8.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%rdx, 4752(%rsp)        # 8-byte Spill
	movq	%rcx, 4720(%rsp)        # 8-byte Spill
	movq	1688(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	.align	16, 0x90
.LBB147_405:                            # %for f8.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_408 Depth 3
	testl	%eax, %eax
	jle	.LBB147_406
# BB#407:                               # %for f8.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_405 Depth=2
	movq	%r8, 2528(%rsp)         # 8-byte Spill
	movl	%r8d, %eax
	movq	1816(%rsp), %r11        # 8-byte Reload
	subl	%r11d, %eax
	addl	$-1, %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %esi
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	movl	%ecx, %r10d
	subl	%eax, %edx
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movq	1808(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %r9d
	cmovgl	%r8d, %r9d
	addl	$-1, %r9d
	cmpl	%r11d, %r9d
	cmovll	%r11d, %r9d
	cmpl	%r8d, %ebx
	cmovll	%edx, %r9d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovlel	%edx, %r9d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r10d, %esi
	subl	%edi, %esi
	cmpl	%edi, %r14d
	cmovgl	%edi, %esi
	addl	%r11d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	leal	1(%r8), %ecx
	movl	%ecx, 2320(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r13d
	movl	%r13d, %edi
	cmovgl	%ecx, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%esi, %edi
	movl	%r13d, %r10d
	cmovgl	%r8d, %r10d
	cmpl	%r11d, %r10d
	cmovll	%r11d, %r10d
	cmpl	%r8d, %ebx
	cmovlel	%edx, %r10d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovll	%edx, %r10d
	movl	%r8d, %r12d
	andl	$1, %r12d
	movl	%r12d, 2336(%rsp)       # 4-byte Spill
	leal	-2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2512(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movl	%r14d, %ebx
	addl	$2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	sarl	$31, %ebx
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %ebx
	addl	%r14d, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %edi
	movslq	%edi, %rcx
	movq	1880(%rsp), %rdi        # 8-byte Reload
	imulq	%rdi, %rcx
	movq	1872(%rsp), %rsi        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	1888(%rsp), %r14        # 8-byte Reload
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%ebx, %ecx
	movq	1848(%rsp), %r15        # 8-byte Reload
	cmpl	%ebx, %r15d
	cmovgl	%ebx, %ecx
	addl	%r11d, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	leal	-2(%r8), %r9d
	cmpl	%r9d, %r13d
	movl	%r13d, %edx
	cmovgl	%r9d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	movslq	%r10d, %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%r14,%rdx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	leal	2(%r8), %ebx
	cmpl	%ebx, %r13d
	movl	%r13d, %eax
	cmovgl	%ebx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %eax
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %eax
	cltq
	imulq	%rdi, %rax
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movq	2544(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1792(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1764(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2480(%rsp)        # 8-byte Spill
	movq	1568(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1832(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	subq	4760(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r9d
	imull	%esi, %r9d
	movq	%r9, 2496(%rsp)         # 8-byte Spill
	movq	1576(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2440(%rsp)        # 8-byte Spill
	movq	1680(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2416(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	movl	2320(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	movq	2168(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2352(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_408:                            # %for f8.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_405 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3232(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	sete	5216(%rsp)              # 1-byte Folded Spill
	setne	3840(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3520(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3776(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3744(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3712(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3680(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3648(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2512(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3872(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5392(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5360(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5104(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5408(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5440(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3808(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5184(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3744(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5200(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5152(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3184(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3712(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5120(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	5056(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4960(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4784(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	movb	3840(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5248(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2528(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	2336(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3344(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r15b       # 1-byte Reload
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	%r15b, %al
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3600(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3520(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movl	%r11d, %r12d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r15b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3520(%rsp)        # 4-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2896(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %edi
	movq	2440(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %esi
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r15d
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3328(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r11d
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3360(%rsp)        # 4-byte Spill
	je	.LBB147_410
# BB#409:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_410:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5280(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	je	.LBB147_412
# BB#411:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_412:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movzbl	3344(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_414
# BB#413:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_414:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_416
# BB#415:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_416:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm1, 2592(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_418
# BB#417:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_418:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_420
# BB#419:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_420:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm2, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3296(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_422
# BB#421:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_422:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2656(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	je	.LBB147_424
# BB#423:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_424:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	3776(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2912(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3072(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2880(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3168(%rsp)      # 16-byte Spill
	vmovaps	4256(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5096(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4224(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2848(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 2832(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3808(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2848(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2800(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2784(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2816(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3184(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3072(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movslq	2896(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movslq	3840(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3840(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movslq	3328(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	4192(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	movslq	3360(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3904(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	movslq	3520(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2848(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	movslq	%r15d, %rcx
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3552(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2848(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_426
# BB#425:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	2560(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
.LBB147_426:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vsubps	3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vsubps	2752(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vsubps	2768(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2736(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2720(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3488(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_428
# BB#427:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	2576(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3248(%rsp)       # 16-byte Spill
.LBB147_428:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3808(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5728(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmulps	4256(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3616(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4224(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3328(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmulps	5312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2736(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2720(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2704(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2688(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3392(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm1       # 16-byte Reload
	vaddps	3600(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3424(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3616(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_430
# BB#429:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovdqa	2592(%rsp), %xmm7       # 16-byte Reload
.LBB147_430:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3328(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3168(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2800(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3712(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3472(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_432
# BB#431:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovdqa	2608(%rsp), %xmm0       # 16-byte Reload
.LBB147_432:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2864(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3840(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2736(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3344(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3456(%rsp)      # 16-byte Spill
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3328(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5216(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_434
# BB#433:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovdqa	2640(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
.LBB147_434:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3376(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_436
# BB#435:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
.LBB147_436:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3440(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2896(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2880(%rsp), %xmm15      # 16-byte Reload
	vmulps	4256(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3072(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4224(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vaddps	3520(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3472(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3600(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_438
# BB#437:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
.LBB147_438:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vaddps	3344(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3184(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2832(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2800(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rax,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2768(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3408(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3216(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_440
# BB#439:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovdqa	2672(%rsp), %xmm5       # 16-byte Reload
.LBB147_440:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_408 Depth=3
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2912(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5312(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3168(%rsp), %xmm2       # 16-byte Reload
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3872(%rsp), %rax        # 4-byte Folded Reload
	movq	2464(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3232(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_408
	jmp	.LBB147_441
.LBB147_406:                            # %for f8.s0.v11.end for f8.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_405 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2320(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_441:                            # %end for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_405 Depth=2
	movl	2320(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	2176(%rsp), %eax        # 4-byte Folded Reload
	movl	1800(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_405
# BB#442:                               # %consume f8
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	880(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_443
# BB#444:                               # %for f0.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	896(%rsp), %rax         # 8-byte Reload
	movq	848(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	8(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3872(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3840(%rsp)       # 32-byte Spill
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3808(%rsp)       # 32-byte Spill
	movq	888(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	8(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3776(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3744(%rsp)       # 32-byte Spill
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3712(%rsp)       # 32-byte Spill
	movq	840(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3680(%rsp)       # 32-byte Spill
	movq	832(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3648(%rsp)       # 32-byte Spill
	movq	856(%rsp), %rcx         # 8-byte Reload
	vpbroadcastd	(%rax,%rcx,4), %ymm0
	vmovdqa	%ymm0, 3616(%rsp)       # 32-byte Spill
	movq	1536(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %esi
	andl	$63, %esi
	movq	%rsi, %rcx
	movq	1792(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	addq	$1, %rdx
	movl	%edx, %r13d
	andl	$63, %r13d
	movq	%r13, %r12
	imulq	%rax, %r12
	movq	1440(%rsp), %rcx        # 8-byte Reload
	subq	%rcx, %rdx
	shlq	$5, %rdx
	movq	%rdx, 1536(%rsp)        # 8-byte Spill
	leaq	8(%rdx), %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	leaq	16(%rdx), %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	1264(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rsi
	movq	%rsi, 3600(%rsp)        # 8-byte Spill
	imulq	%rax, %r13
	leaq	24(%rdx), %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	subq	552(%rsp), %rcx         # 8-byte Folded Reload
	movq	%rcx, 1440(%rsp)        # 8-byte Spill
	movq	616(%rsp), %rax         # 8-byte Reload
	movq	1520(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r11d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB147_445:                            # %for f0.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_446 Depth 3
                                        #         Child Loop BB147_447 Depth 4
	movl	%r14d, %eax
	shll	$5, %eax
	movq	5352(%rsp), %rcx        # 8-byte Reload
	addl	%ecx, %eax
	cltq
	movq	1224(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rcx
	movq	3600(%rsp), %rdi        # 8-byte Reload
	leaq	(%rcx,%rdi), %rdx
	movq	5096(%rsp), %rsi        # 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm0
	leaq	(%rcx,%r13), %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	leaq	8(%rdi,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm2
	leaq	8(%r13,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm3
	leaq	16(%rdi,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm4
	leaq	16(%r13,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm5
	leaq	24(%rdi,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm6
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	leaq	24(%r13,%rcx), %rcx
	vmovdqu	(%rsi,%rcx,4), %ymm7
	movq	3552(%rsp), %rbx        # 8-byte Reload
	leaq	(%rax,%rbx), %rcx
	movq	4712(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm8
	vmovaps	%ymm8, 5792(%rsp)
	movq	4704(%rsp), %rsi        # 8-byte Reload
	vmovdqu	(%rsi,%rcx,4), %ymm8
	leaq	(%rax,%r12), %rcx
	vmovups	(%rdx,%rcx,4), %ymm9
	movq	1536(%rsp), %rdi        # 8-byte Reload
	vmovaps	%ymm9, 5792(%rsp,%rdi,4)
	vmovups	(%rsi,%rcx,4), %ymm9
	leaq	8(%rbx,%rax), %rcx
	vmovups	(%rdx,%rcx,4), %ymm10
	vmovaps	%ymm10, 5824(%rsp)
	vmovups	(%rsi,%rcx,4), %ymm10
	leaq	8(%rax,%r12), %r8
	vmovups	(%rdx,%r8,4), %ymm11
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm11, 5792(%rsp,%rcx,4)
	vmovdqu	(%rsi,%r8,4), %ymm11
	leaq	16(%rax,%rbx), %rcx
	vmovups	(%rdx,%rcx,4), %ymm12
	vmovaps	%ymm12, 5856(%rsp)
	vmovups	(%rsi,%rcx,4), %ymm12
	leaq	16(%rax,%r12), %r8
	vmovups	(%rdx,%r8,4), %ymm13
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm13, 5792(%rsp,%rcx,4)
	vmovdqu	(%rsi,%r8,4), %ymm13
	leaq	24(%rax,%rbx), %rcx
	vmovups	(%rdx,%rcx,4), %ymm14
	vmovaps	%ymm14, 5888(%rsp)
	leaq	24(%rax,%r12), %rax
	vmovups	(%rdx,%rax,4), %ymm14
	movq	3472(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm14, 5792(%rsp,%rdx,4)
	vmovaps	%ymm0, 6048(%rsp)
	vmovaps	%ymm1, 6048(%rsp,%rdi,4)
	vmovdqa	%ymm2, 6080(%rsp)
	vmovdqa	%ymm3, 6080(%rsp,%rdi,4)
	vmovdqa	%ymm4, 6112(%rsp)
	vmovaps	%ymm5, 6112(%rsp,%rdi,4)
	vmovaps	%ymm6, 6144(%rsp)
	vmovdqa	%ymm7, 6144(%rsp,%rdi,4)
	vmovdqa	%ymm8, 6304(%rsp)
	vmovaps	%ymm9, 6304(%rsp,%rdi,4)
	vmovaps	%ymm10, 6336(%rsp)
	vmovdqa	%ymm11, 6336(%rsp,%rdi,4)
	vmovaps	%ymm12, 6368(%rsp)
	vmovdqa	%ymm13, 6368(%rsp,%rdi,4)
	vmovups	(%rsi,%rcx,4), %ymm0
	movslq	%r11d, %rcx
	movq	3456(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	1416(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rcx,2), %r8
	vmovaps	%ymm0, 6400(%rsp)
	vmovdqu	(%rsi,%rax,4), %ymm0
	vmovdqa	%ymm0, 6400(%rsp,%rdi,4)
	xorl	%eax, %eax
	.align	16, 0x90
.LBB147_446:                            # %for f0.s0.v11.v13.yii
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_445 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_447 Depth 4
	movq	%rax, %rcx
	shlq	$7, %rcx
	vmovaps	5792(%rsp,%rcx), %ymm14
	vmovaps	%ymm14, 4192(%rsp)      # 32-byte Spill
	vmovaps	5824(%rsp,%rcx), %ymm2
	vmovaps	%ymm2, 4256(%rsp)       # 32-byte Spill
	vmovaps	5856(%rsp,%rcx), %ymm0
	vmovaps	%ymm0, 4224(%rsp)       # 32-byte Spill
	vmovaps	5888(%rsp,%rcx), %ymm3
	vmovaps	%ymm3, 3904(%rsp)       # 32-byte Spill
	vmovaps	6048(%rsp,%rcx), %ymm4
	vmovaps	6080(%rsp,%rcx), %ymm7
	vmovaps	6112(%rsp,%rcx), %ymm8
	vmovaps	6144(%rsp,%rcx), %ymm13
	vmovaps	6304(%rsp,%rcx), %ymm9
	vmovaps	6336(%rsp,%rcx), %ymm10
	vmovaps	6368(%rsp,%rcx), %ymm12
	vmovaps	3808(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm0, %ymm0
	vmulps	%ymm5, %ymm2, %ymm1
	vmulps	%ymm5, %ymm14, %ymm2
	vmulps	%ymm5, %ymm3, %ymm5
	vmovaps	3840(%rsp), %ymm11      # 32-byte Reload
	vmovaps	%ymm11, %ymm6
	vfmadd213ps	%ymm5, %ymm13, %ymm6
	vmovaps	%ymm11, %ymm5
	vfmadd213ps	%ymm2, %ymm4, %ymm5
	vmovaps	%ymm11, %ymm2
	vfmadd213ps	%ymm1, %ymm7, %ymm2
	vmovaps	%ymm11, %ymm1
	vfmadd213ps	%ymm0, %ymm8, %ymm1
	vmovaps	3872(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm11
	vfmadd213ps	%ymm1, %ymm12, %ymm11
	vmovaps	%ymm11, 5312(%rsp)      # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm2, %ymm10, %ymm1
	vmovaps	%ymm1, 5280(%rsp)       # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm5, %ymm9, %ymm1
	vmovaps	%ymm1, 5248(%rsp)       # 32-byte Spill
	vmovaps	6400(%rsp,%rcx), %ymm15
	vfmadd213ps	%ymm6, %ymm15, %ymm0
	vmovaps	%ymm0, 5216(%rsp)       # 32-byte Spill
	vmovaps	3712(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm3, %ymm0
	vmovaps	3744(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm11
	vfmadd213ps	%ymm0, %ymm13, %ymm11
	vmulps	%ymm5, %ymm14, %ymm0
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm0, %ymm4, %ymm6
	vmulps	4256(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm0, %ymm7, %ymm1
	vmovaps	4224(%rsp), %ymm3       # 32-byte Reload
	vmulps	%ymm5, %ymm3, %ymm0
	vmovaps	%ymm2, %ymm5
	vfmadd213ps	%ymm0, %ymm8, %ymm5
	vmovaps	3776(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm14
	vfmadd213ps	%ymm5, %ymm12, %ymm14
	vmovaps	%ymm0, %ymm5
	vfmadd213ps	%ymm1, %ymm10, %ymm5
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm6, %ymm9, %ymm1
	vmovaps	%ymm0, %ymm6
	vfmadd213ps	%ymm11, %ymm15, %ymm6
	vmovaps	3616(%rsp), %ymm2       # 32-byte Reload
	vmulps	3904(%rsp), %ymm2, %ymm11 # 32-byte Folded Reload
	vmovaps	3648(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm0, %ymm13
	vmulps	4192(%rsp), %ymm2, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm11, %ymm0, %ymm4
	vmulps	4256(%rsp), %ymm2, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm11, %ymm0, %ymm7
	vmulps	%ymm2, %ymm3, %ymm11
	vfmadd213ps	%ymm11, %ymm0, %ymm8
	vmovaps	3680(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm12
	vfmadd213ps	%ymm7, %ymm0, %ymm10
	vfmadd213ps	%ymm4, %ymm0, %ymm9
	vfmadd213ps	%ymm13, %ymm0, %ymm15
	xorl	%esi, %esi
	movl	$3, %edx
	movq	%r8, %rcx
	.align	16, 0x90
.LBB147_447:                            # %for f0.s0.v12
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_445 Depth=2
                                        #       Parent Loop BB147_446 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovaps	%ymm1, %ymm3
	cmpl	$1, %esi
	je	.LBB147_449
# BB#448:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	5248(%rsp), %ymm3       # 32-byte Reload
.LBB147_449:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm5, %ymm4
	je	.LBB147_451
# BB#450:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	5280(%rsp), %ymm4       # 32-byte Reload
.LBB147_451:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm14, %ymm8
	je	.LBB147_453
# BB#452:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	5312(%rsp), %ymm8       # 32-byte Reload
.LBB147_453:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm6, %ymm11
	je	.LBB147_455
# BB#454:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	5216(%rsp), %ymm11      # 32-byte Reload
.LBB147_455:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm15, %ymm7
	testl	%esi, %esi
	je	.LBB147_457
# BB#456:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm11, %ymm7
.LBB147_457:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm12, %ymm11
	je	.LBB147_459
# BB#458:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm8, %ymm11
.LBB147_459:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm10, %ymm8
	je	.LBB147_461
# BB#460:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm4, %ymm8
.LBB147_461:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm9, %ymm4
	je	.LBB147_463
# BB#462:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vmovaps	%ymm3, %ymm4
.LBB147_463:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_447 Depth=4
	vbroadcastss	.LCPI147_25(%rip), %ymm3
	vminps	%ymm3, %ymm4, %ymm4
	vminps	%ymm3, %ymm8, %ymm8
	vminps	%ymm3, %ymm11, %ymm11
	vminps	%ymm3, %ymm7, %ymm3
	vxorps	%ymm0, %ymm0, %ymm0
	vmaxps	%ymm0, %ymm4, %ymm4
	vmaxps	%ymm0, %ymm8, %ymm7
	vmaxps	%ymm0, %ymm11, %ymm8
	vmaxps	%ymm0, %ymm3, %ymm3
	vcvttps2dq	%ymm4, %ymm4
	vmovdqa	.LCPI147_7(%rip), %ymm0 # ymm0 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm0, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vcvttps2dq	%ymm7, %ymm7
	vpshufb	%ymm0, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vcvttps2dq	%ymm8, %ymm8
	vpshufb	%ymm0, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vcvttps2dq	%ymm3, %ymm3
	vpshufb	%ymm0, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm8, %ymm8    # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero,xmm8[4],zero,xmm8[5],zero,xmm8[6],zero,xmm8[7],zero
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vmovdqa	5568(%rsp), %ymm0       # 32-byte Reload
	vpmulld	%ymm0, %ymm4, %ymm11
	vpmulld	%ymm0, %ymm7, %ymm7
	vpmulld	%ymm0, %ymm8, %ymm8
	vpmulld	%ymm0, %ymm3, %ymm3
	vmovd	%esi, %xmm4
	vpsubd	5536(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpbroadcastd	%xmm4, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm4
	vpaddd	%ymm8, %ymm13, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm3
	vpaddd	%ymm11, %ymm13, %ymm7
	vmovq	%xmm4, %r9
	movslq	%r9d, %r10
	movq	5632(%rsp), %r15        # 8-byte Reload
	movzwl	(%r15,%r10,2), %edi
	vmovd	%edi, %xmm2
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %r9
	vpinsrw	$1, (%r15,%r9,2), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm4, %xmm4
	vpinsrw	$2, (%r15,%rbx,2), %xmm2, %xmm2
	vmovq	%xmm4, %rbx
	vpinsrw	$3, (%r15,%rdi,2), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm2, %xmm2
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rbx
	vpinsrw	$5, (%r15,%rbx,2), %xmm2, %xmm2
	movslq	%edi, %rbx
	vpinsrw	$6, (%r15,%rbx,2), %xmm2, %xmm2
	vmovq	%xmm8, %rbx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm2, %xmm4
	movslq	%ebx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm2
	vpextrq	$1, %xmm8, %rdi
	sarq	$32, %rbx
	vpinsrw	$1, (%r15,%rbx,2), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm8, %xmm0
	vpinsrw	$2, (%r15,%rbx,2), %xmm2, %xmm2
	vmovq	%xmm0, %rbx
	vpinsrw	$3, (%r15,%rdi,2), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm2, %xmm2
	vpextrq	$1, %xmm0, %rdi
	sarq	$32, %rbx
	vpinsrw	$5, (%r15,%rbx,2), %xmm2, %xmm0
	movslq	%edi, %rbx
	vpinsrw	$6, (%r15,%rbx,2), %xmm0, %xmm0
	vmovq	%xmm3, %rbx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm0, %xmm0
	movslq	%ebx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm2
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rbx
	vpinsrw	$1, (%r15,%rbx,2), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm3, %xmm3
	vpinsrw	$2, (%r15,%rbx,2), %xmm2, %xmm2
	vmovq	%xmm3, %rbx
	vpinsrw	$3, (%r15,%rdi,2), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm2, %xmm2
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rbx
	vpinsrw	$5, (%r15,%rbx,2), %xmm2, %xmm2
	movslq	%edi, %rbx
	vpinsrw	$6, (%r15,%rbx,2), %xmm2, %xmm2
	vmovq	%xmm7, %rbx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm2, %xmm2
	movslq	%ebx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm3
	vpextrq	$1, %xmm7, %rdi
	sarq	$32, %rbx
	vpinsrw	$1, (%r15,%rbx,2), %xmm3, %xmm3
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm7, %xmm7
	vpinsrw	$2, (%r15,%rbx,2), %xmm3, %xmm3
	vmovq	%xmm7, %rbx
	vpinsrw	$3, (%r15,%rdi,2), %xmm3, %xmm3
	movslq	%ebx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm3, %xmm3
	vpextrq	$1, %xmm7, %rdi
	sarq	$32, %rbx
	vpinsrw	$5, (%r15,%rbx,2), %xmm3, %xmm3
	movslq	%edi, %rbx
	vpinsrw	$6, (%r15,%rbx,2), %xmm3, %xmm3
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm3, %xmm3
	vinserti128	$1, %xmm4, %ymm0, %ymm0
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovdqu	%ymm2, 2(%rcx)
	vmovdqu	%ymm0, 34(%rcx)
	addq	5624(%rsp), %rcx        # 8-byte Folded Reload
	addl	$1, %esi
	addq	$-1, %rdx
	jne	.LBB147_447
# BB#464:                               # %end for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=3
	addq	$1, %rax
	addq	2208(%rsp), %r8         # 8-byte Folded Reload
	cmpq	$2, %rax
	jne	.LBB147_446
# BB#465:                               # %end for f0.s0.v11.v13.yii
                                        #   in Loop: Header=BB147_445 Depth=2
	addq	$1, %r14
	addl	$32, %r11d
	cmpl	1220(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB147_445
	jmp	.LBB147_466
.LBB147_443:                            # %consume f8.for f0.s0.v11.v13.v13.preheader_crit_edge
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1440(%rsp), %rax        # 8-byte Reload
	subq	552(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_466:                            # %for f0.s0.v11.v13.v13.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	addl	$8, 908(%rsp)           # 4-byte Folded Spill
	movq	1584(%rsp), %rax        # 8-byte Reload
	leal	9(%rax), %ecx
	movq	%rcx, 1680(%rsp)        # 8-byte Spill
	leal	7(%rax), %ecx
	movq	%rcx, 1576(%rsp)        # 8-byte Spill
	movl	$6, %ecx
	movq	1528(%rsp), %rdx        # 8-byte Reload
	subq	%rdx, %rcx
	movq	%rcx, 1568(%rsp)        # 8-byte Spill
	movl	$10, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1560(%rsp)        # 8-byte Spill
	movl	$7, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1552(%rsp)        # 8-byte Spill
	movl	$9, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1544(%rsp)        # 8-byte Spill
	movl	$8, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1536(%rsp)        # 8-byte Spill
	movl	$9, %ecx
	movq	1496(%rsp), %rdx        # 8-byte Reload
	subq	%rdx, %rcx
	movq	%rcx, 1528(%rsp)        # 8-byte Spill
	movl	$7, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1520(%rsp)        # 8-byte Spill
	movl	$6, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1512(%rsp)        # 8-byte Spill
	movl	$10, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1504(%rsp)        # 8-byte Spill
	movl	$8, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1496(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 1488(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 1672(%rsp)        # 8-byte Spill
	addl	$10, %eax
	movq	%rax, 1584(%rsp)        # 8-byte Spill
	movq	912(%rsp), %rax         # 8-byte Reload
	addl	$9, %eax
	movq	%rax, 912(%rsp)         # 8-byte Spill
	movq	1592(%rsp), %rax        # 8-byte Reload
	leal	10(%rax), %ecx
	movq	%rcx, 1480(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 1472(%rsp)        # 8-byte Spill
	leal	7(%rax), %ecx
	movq	%rcx, 1464(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 1456(%rsp)        # 8-byte Spill
	addl	$9, %eax
	movq	%rax, 1592(%rsp)        # 8-byte Spill
	movq	1440(%rsp), %rcx        # 8-byte Reload
	addq	$2, %rcx
	imulq	696(%rsp), %rcx         # 8-byte Folded Reload
	movq	496(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 1248(%rsp)        # 8-byte Spill
	movq	488(%rsp), %rax         # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	%rax, 1240(%rsp)        # 8-byte Spill
	movl	$1, %edx
	movq	656(%rsp), %rax         # 8-byte Reload
	movl	%eax, 996(%rsp)         # 4-byte Spill
	movl	504(%rsp), %eax         # 4-byte Reload
	movl	%eax, 992(%rsp)         # 4-byte Spill
	movl	508(%rsp), %eax         # 4-byte Reload
	movl	%eax, 988(%rsp)         # 4-byte Spill
	movl	652(%rsp), %eax         # 4-byte Reload
	movl	%eax, 984(%rsp)         # 4-byte Spill
	movl	512(%rsp), %eax         # 4-byte Reload
	movl	%eax, 980(%rsp)         # 4-byte Spill
	movl	516(%rsp), %eax         # 4-byte Reload
	movl	%eax, 976(%rsp)         # 4-byte Spill
	movl	$1, %ebx
	movq	5672(%rsp), %rsi        # 8-byte Reload
	movl	1024(%rsp), %ecx        # 4-byte Reload
	movl	1068(%rsp), %edi        # 4-byte Reload
	movl	1064(%rsp), %r9d        # 4-byte Reload
	movl	1060(%rsp), %r8d        # 4-byte Reload
	movl	1056(%rsp), %r13d       # 4-byte Reload
	movl	1052(%rsp), %r10d       # 4-byte Reload
	movl	1284(%rsp), %r11d       # 4-byte Reload
	movl	1048(%rsp), %r14d       # 4-byte Reload
	movl	1280(%rsp), %r15d       # 4-byte Reload
	movl	1276(%rsp), %r12d       # 4-byte Reload
	.align	16, 0x90
.LBB147_467:                            # %for f0.s0.v11.v13.v13
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_469 Depth 3
                                        #         Child Loop BB147_471 Depth 4
                                        #       Child Loop BB147_499 Depth 3
                                        #         Child Loop BB147_501 Depth 4
                                        #         Child Loop BB147_528 Depth 4
                                        #         Child Loop BB147_555 Depth 4
                                        #       Child Loop BB147_583 Depth 3
                                        #         Child Loop BB147_585 Depth 4
                                        #       Child Loop BB147_612 Depth 3
                                        #         Child Loop BB147_614 Depth 4
                                        #       Child Loop BB147_650 Depth 3
                                        #         Child Loop BB147_652 Depth 4
                                        #         Child Loop BB147_687 Depth 4
                                        #         Child Loop BB147_706 Depth 4
                                        #       Child Loop BB147_742 Depth 3
                                        #         Child Loop BB147_744 Depth 4
                                        #       Child Loop BB147_779 Depth 3
                                        #         Child Loop BB147_824 Depth 4
                                        #       Child Loop BB147_783 Depth 3
                                        #         Child Loop BB147_785 Depth 4
                                        #         Child Loop BB147_804 Depth 4
                                        #         Child Loop BB147_844 Depth 4
                                        #       Child Loop BB147_864 Depth 3
                                        #         Child Loop BB147_867 Depth 4
                                        #       Child Loop BB147_887 Depth 3
                                        #         Child Loop BB147_932 Depth 4
                                        #       Child Loop BB147_891 Depth 3
                                        #         Child Loop BB147_893 Depth 4
                                        #         Child Loop BB147_912 Depth 4
                                        #         Child Loop BB147_952 Depth 4
                                        #       Child Loop BB147_972 Depth 3
                                        #         Child Loop BB147_975 Depth 4
                                        #       Child Loop BB147_995 Depth 3
                                        #         Child Loop BB147_997 Depth 4
                                        #       Child Loop BB147_1017 Depth 3
                                        #         Child Loop BB147_1019 Depth 4
                                        #         Child Loop BB147_1038 Depth 4
                                        #         Child Loop BB147_1057 Depth 4
                                        #       Child Loop BB147_1077 Depth 3
                                        #         Child Loop BB147_1079 Depth 4
                                        #       Child Loop BB147_1105 Depth 3
                                        #         Child Loop BB147_1101 Depth 4
                                        #           Child Loop BB147_1102 Depth 5
                                        #       Child Loop BB147_1099 Depth 3
                                        #         Child Loop BB147_1107 Depth 4
                                        #       Child Loop BB147_1143 Depth 3
                                        #         Child Loop BB147_1145 Depth 4
                                        #         Child Loop BB147_1180 Depth 4
                                        #         Child Loop BB147_1200 Depth 4
                                        #       Child Loop BB147_1236 Depth 3
                                        #         Child Loop BB147_1239 Depth 4
                                        #       Child Loop BB147_1275 Depth 3
                                        #         Child Loop BB147_1278 Depth 4
                                        #       Child Loop BB147_1314 Depth 3
                                        #         Child Loop BB147_1316 Depth 4
                                        #         Child Loop BB147_1351 Depth 4
                                        #         Child Loop BB147_1371 Depth 4
                                        #       Child Loop BB147_1407 Depth 3
                                        #         Child Loop BB147_1410 Depth 4
                                        #       Child Loop BB147_1447 Depth 3
                                        #         Child Loop BB147_1448 Depth 4
                                        #           Child Loop BB147_1449 Depth 5
	movq	%rbx, 960(%rsp)         # 8-byte Spill
	movq	%rdx, 1032(%rsp)        # 8-byte Spill
	movl	%r12d, 1276(%rsp)       # 4-byte Spill
	movl	%r15d, 1280(%rsp)       # 4-byte Spill
	movl	%r14d, 1048(%rsp)       # 4-byte Spill
	movl	%r11d, 1284(%rsp)       # 4-byte Spill
	movl	%r10d, 1052(%rsp)       # 4-byte Spill
	movl	%r13d, 1056(%rsp)       # 4-byte Spill
	movl	%r8d, 1060(%rsp)        # 4-byte Spill
	movl	%r9d, 1064(%rsp)        # 4-byte Spill
	movl	%edi, 1068(%rsp)        # 4-byte Spill
	movl	%ecx, 1024(%rsp)        # 4-byte Spill
	movq	%rdx, %rbx
	movq	1816(%rsp), %rdx        # 8-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	708(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 952(%rsp)         # 8-byte Spill
	cmpl	%r13d, %edx
	movl	%r13d, %ecx
	cmovgel	%edx, %ecx
	notl	%ecx
	cmpl	%ecx, %r14d
	cmovgel	%r14d, %ecx
	movl	%ecx, 948(%rsp)         # 4-byte Spill
	cmpl	%edi, %edx
	movl	%edi, %ecx
	cmovgel	%edx, %ecx
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 928(%rsp)         # 8-byte Spill
	cmpl	%r10d, %edx
	cmovgel	%edx, %r10d
	notl	%r10d
	cmpl	%r10d, %r15d
	cmovgel	%r15d, %r10d
	movl	%r10d, 924(%rsp)        # 4-byte Spill
	movq	1408(%rsp), %r10        # 8-byte Reload
	cmpl	%r11d, %r10d
	cmovgel	%r10d, %r11d
	movl	%r11d, %eax
	notl	%eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	movl	1708(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r9d
	movl	%eax, %edi
	cmovgel	%r9d, %edi
	notl	%edi
	cmpl	%edi, %r8d
	cmovgel	%r8d, %edi
	movl	%edi, 1028(%rsp)        # 4-byte Spill
	cmpl	%eax, %r13d
	cmovgel	%r13d, %eax
	notl	%eax
	cmpl	%eax, %r14d
	cmovgel	%r14d, %eax
	movl	%eax, 944(%rsp)         # 4-byte Spill
	cmpl	%r9d, %edx
	movl	%r9d, %eax
	cmovgel	%edx, %eax
	movl	$-3, %edx
	subl	%eax, %edx
	cmpl	%edx, %r15d
	cmovgel	%r15d, %edx
	movl	%edx, 940(%rsp)         # 4-byte Spill
	movl	740(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %r11d
	cmovll	%eax, %r11d
	notl	%r11d
	cmpl	%r11d, %r12d
	cmovgel	%r12d, %r11d
	movl	%r11d, 2800(%rsp)       # 4-byte Spill
	movq	1688(%rsp), %rdx        # 8-byte Reload
	leal	8(%rdx,%rbx,2), %edi
	movl	%edi, 1432(%rsp)        # 4-byte Spill
	cmpl	%edi, %r10d
	movl	%edi, %ecx
	cmovgel	%r10d, %ecx
	leal	10(%rdx,%rbx,2), %edx
	movl	%edx, 2832(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	movl	%edx, %ebx
	cmovgl	%ecx, %ebx
	movl	%ebx, 2880(%rsp)        # 4-byte Spill
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	%ecx, 2864(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	movl	%edx, 2816(%rsp)        # 4-byte Spill
	cmpl	%ebx, %edi
	jge	.LBB147_497
# BB#468:                               # %for deinterleaved$1.s0.v11291.preheader
                                        #   in Loop: Header=BB147_467 Depth=2
	movslq	1284(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	movl	1432(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_469:                            # %for deinterleaved$1.s0.v11291
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_471 Depth 4
	movl	%eax, 2896(%rsp)        # 4-byte Spill
	cmpl	$0, 1740(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_496
# BB#470:                               # %for deinterleaved$1.s0.v10.v10293.preheader
                                        #   in Loop: Header=BB147_469 Depth=3
	movq	3168(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3552(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	movq	1408(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	1388(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1392(%rsp), %eax        # 4-byte Folded Reload
	movq	1400(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movl	1384(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	cmpq	%rdi, 1376(%rsp)        # 8-byte Folded Reload
	movl	1396(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%rdi, 1368(%rsp)        # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %rdi
	cmovll	%eax, %ecx
	movq	1744(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	vmovd	%ecx, %xmm0
	vpabsd	1648(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3072(%rsp)       # 32-byte Spill
	vpsubd	1600(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3520(%rsp)       # 32-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	movl	1740(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_471:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_469 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cmpl	$0, 3552(%rsp)          # 4-byte Folded Reload
	setne	5248(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r15d
	andl	$1, %r15d
	sete	%cl
	movl	%ecx, 5280(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	3168(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	movq	3984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4416(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3776(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3872(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %r9d
	movl	%r9d, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r10d
	movl	%r10d, 3616(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3360(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4224(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4256(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3904(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3872(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	5248(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	andb	5216(%rsp), %r15b       # 1-byte Folded Reload
	movl	%r15d, 3472(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4576(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4544(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4096(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4064(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3072(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4384(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4512(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4368(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4352(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3520(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4224(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5248(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3904(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3872(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	testl	3552(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3248(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3600(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3232(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3344(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3312(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3296(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3776(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vmovd	%xmm2, %eax
	cltd
	idivl	3744(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	vmovd	3216(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3232(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3200(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r14d, %xmm1
	vpinsrd	$1, %r15d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4480(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4448(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	4032(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3808(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3520(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4760(%rsp), %r13        # 8-byte Folded Reload
	addq	2912(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4680(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	je	.LBB147_473
# BB#472:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_473:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovd	%ebx, %xmm0
	movl	5280(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3472(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_475
# BB#474:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_475:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movl	3248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_477
# BB#476:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_477:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_479
# BB#478:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_479:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_481
# BB#480:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_481:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm0, 3200(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_483
# BB#482:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_483:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm1, 3216(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_485
# BB#484:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_485:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5280(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3312(%rsp)        # 8-byte Spill
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3440(%rsp)        # 8-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3456(%rsp)         # 8-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3904(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3872(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4224(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_487
# BB#486:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_487:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3264(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3392(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3408(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_489
# BB#488:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_489:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movq	%rsi, %rcx
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3904(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3248(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3872(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3280(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4224(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3312(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4256(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3456(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_491
# BB#490:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	3184(%rsp), %xmm6       # 16-byte Reload
.LBB147_491:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movq	%rsi, %rdx
	movq	3264(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_493
# BB#492:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	3200(%rsp), %xmm9       # 16-byte Reload
.LBB147_493:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_495
# BB#494:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	vmovdqa	3216(%rsp), %xmm15      # 16-byte Reload
.LBB147_495:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_471 Depth=4
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	3808(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4672(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r13,4)
	vmovups	%ymm2, 24608(%rsi,%r13,4)
	vmovups	%ymm1, 24640(%rsi,%r13,4)
	vmovups	%ymm4, 24672(%rsi,%r13,4)
	movq	5312(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3488(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_471
.LBB147_496:                            # %end for deinterleaved$1.s0.v10.v10294
                                        #   in Loop: Header=BB147_469 Depth=3
	movl	2896(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	addq	$1, 3168(%rsp)          # 8-byte Folded Spill
	cmpl	2880(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_469
.LBB147_497:                            # %end for deinterleaved$1.s0.v11292
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	2816(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 2880(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_581
# BB#498:                               #   in Loop: Header=BB147_467 Depth=2
	movl	2848(%rsp), %edx        # 4-byte Reload
	notl	%edx
	movq	1744(%rsp), %rax        # 8-byte Reload
	imull	%edx, %eax
	movq	728(%rsp), %rcx         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_499:                            # %for deinterleaved$1.s0.v11297
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_501 Depth 4
                                        #         Child Loop BB147_528 Depth 4
                                        #         Child Loop BB147_555 Depth 4
	cmpl	$0, 1328(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_526
# BB#500:                               # %for deinterleaved$1.s0.v10.v10299.preheader
                                        #   in Loop: Header=BB147_499 Depth=3
	movq	5248(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 3520(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1648(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3072(%rsp)       # 32-byte Spill
	vpsubd	1600(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3488(%rsp)       # 32-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	movl	1184(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_501:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_499 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3472(%rsp)        # 4-byte Spill
	cmpl	$0, 3520(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	4256(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r15d
	andl	$1, %r15d
	sete	%cl
	movl	%ecx, 5280(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	movq	3984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4416(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3872(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ebx
	movl	%ebx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r9d
	movl	%r9d, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3344(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4192(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4224(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3904(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3872(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3840(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r14d, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	andb	4256(%rsp), %r15b       # 1-byte Folded Reload
	movl	%r15d, 3456(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4576(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4544(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4096(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4064(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3072(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4384(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4512(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4368(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4352(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3488(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4256(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4224(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3872(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3904(%rsp)        # 8-byte Spill
	testl	3520(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3232(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3232(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3552(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3328(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3296(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3184(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3280(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3744(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vmovd	%xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3600(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3312(%rsp)              # 4-byte Folded Reload
	vmovd	3200(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3216(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3184(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r14d, %xmm1
	vpinsrd	$1, %r15d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4480(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4448(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	4032(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3776(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3488(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4760(%rsp), %r13        # 8-byte Folded Reload
	addq	2912(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4680(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	je	.LBB147_503
# BB#502:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_503:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovd	%ebx, %xmm0
	movl	5280(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3456(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_505
# BB#504:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_505:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movl	3232(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_507
# BB#506:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_507:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_509
# BB#508:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_509:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_511
# BB#510:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_511:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm0, 3184(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_513
# BB#512:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_513:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm1, 3200(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_515
# BB#514:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_515:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5280(%rsp)        # 8-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3296(%rsp)        # 8-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3424(%rsp)        # 8-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3440(%rsp)         # 8-byte Spill
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3872(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	3904(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4224(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_517
# BB#516:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_517:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3248(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3376(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3392(%rsp)        # 8-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3776(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_519
# BB#518:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_519:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movq	%rsi, %rcx
	movq	3456(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3872(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3232(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3264(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3904(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3296(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4256(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4224(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_521
# BB#520:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	3168(%rsp), %xmm6       # 16-byte Reload
.LBB147_521:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movq	%rsi, %rdx
	movq	3248(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_523
# BB#522:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	3184(%rsp), %xmm9       # 16-byte Reload
.LBB147_523:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_525
# BB#524:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	vmovdqa	3200(%rsp), %xmm15      # 16-byte Reload
.LBB147_525:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_501 Depth=4
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	3776(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4672(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r13,4)
	vmovups	%ymm2, 24608(%rsi,%r13,4)
	vmovups	%ymm1, 24640(%rsi,%r13,4)
	vmovups	%ymm4, 24672(%rsi,%r13,4)
	movq	5312(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3472(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_501
.LBB147_526:                            # %end for deinterleaved$1.s0.v10.v10300
                                        #   in Loop: Header=BB147_499 Depth=3
	movl	1324(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1328(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_553
# BB#527:                               # %for deinterleaved$1.s0.v10.v10303.preheader
                                        #   in Loop: Header=BB147_499 Depth=3
	movq	5248(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r9d
	andl	$1, %r9d
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	imulq	1720(%rsp), %r8         # 8-byte Folded Reload
	movl	1180(%rsp), %r10d       # 4-byte Reload
	movl	2848(%rsp), %r11d       # 4-byte Reload
	movl	1188(%rsp), %r15d       # 4-byte Reload
	.align	16, 0x90
.LBB147_528:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_499 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rsi, %r12
	testl	%r9d, %r9d
	setne	%cl
	sete	%dl
	movl	%r15d, %edi
	andl	$1, %edi
	sete	%al
	movl	%r15d, %ebx
	movq	5248(%rsp), %rsi        # 8-byte Reload
	orl	%esi, %ebx
	testb	$1, %bl
	sete	%sil
	andb	%cl, %al
	andb	%dl, %dil
	testl	%r15d, %r9d
	setne	%dl
	movslq	%r15d, %rbx
	subq	4760(%rsp), %rbx        # 8-byte Folded Reload
	addq	%r8, %rbx
	movq	4680(%rsp), %rcx        # 8-byte Reload
	leaq	(%rbx,%rcx), %r14
	movzbl	%sil, %ecx
	vmovd	%esi, %xmm0
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, 5312(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_530
# BB#529:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm5, %xmm5, %xmm5
.LBB147_530:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovd	%ecx, %xmm0
	movzbl	%al, %ecx
	vmovd	%ecx, %xmm2
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm4
	vmovd	%edi, %xmm1
	vpbroadcastb	%xmm1, %xmm3
	vmovdqa	%xmm3, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_532
# BB#531:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_532:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm6
	vpor	%xmm6, %xmm0, %xmm0
	vpor	%xmm4, %xmm2, %xmm2
	vpbroadcastb	%xmm2, %xmm4
	vmovdqa	%xmm4, %xmm1
	je	.LBB147_534
# BB#533:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_534:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm2
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm0
	je	.LBB147_536
# BB#535:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_536:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm0, 4224(%rsp)       # 16-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastb	%xmm2, %xmm2
	vmovdqa	%xmm2, %xmm1
	je	.LBB147_538
# BB#537:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_538:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm1, 4256(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm0, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_540
# BB#539:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_540:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	cmpl	$0, 104(%rbp)
	je	.LBB147_542
# BB#541:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm5, %xmm0
.LBB147_542:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
	movslq	%r11d, %rax
	movq	5048(%rsp), %rcx        # 8-byte Reload
	vmovdqu	-16(%rcx,%rax,2), %ymm5
	vpblendw	$170, 14(%rcx,%rax,2), %ymm5, %ymm5 # ymm5 = ymm5[0],mem[1],ymm5[2],mem[3],ymm5[4],mem[5],ymm5[6],mem[7],ymm5[8],mem[9],ymm5[10],mem[11],ymm5[12],mem[13],ymm5[14],mem[15]
	vpshufb	.LCPI147_26(%rip), %ymm5, %ymm7 # ymm7 = ymm5[0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15,16,17,20,21,24,25,28,29,18,19,22,23,26,27,30,31]
	vperm2i128	$35, %ymm5, %ymm0, %ymm5 # ymm5 = ymm5[2,3,0,1]
	vpshufb	.LCPI147_27(%rip), %ymm5, %ymm5 # ymm5 = ymm5[2,3,6,7,10,11,14,15,0,1,4,5,8,9,12,13,18,19,22,23,26,27,30,31,16,17,20,21,24,25,28,29]
	vpblendd	$60, %ymm5, %ymm7, %ymm7 # ymm7 = ymm7[0,1],ymm5[2,3,4,5],ymm7[6,7]
	vextracti128	$1, %ymm7, %xmm5
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm5, %ymm5
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vpmovzxbd	%xmm0, %ymm10   # ymm10 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm10, %ymm10
	vxorps	%ymm9, %ymm9, %ymm9
	vblendvps	%ymm10, %ymm7, %ymm9, %ymm14
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm15
	je	.LBB147_544
# BB#543:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	%xmm3, %xmm2
.LBB147_544:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqu	-14(%rcx,%rax,2), %ymm0
	vpblendw	$170, 16(%rcx,%rax,2), %ymm0, %ymm0 # ymm0 = ymm0[0],mem[1],ymm0[2],mem[3],ymm0[4],mem[5],ymm0[6],mem[7],ymm0[8],mem[9],ymm0[10],mem[11],ymm0[12],mem[13],ymm0[14],mem[15]
	vpshufb	.LCPI147_26(%rip), %ymm0, %ymm3 # ymm3 = ymm0[0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15,16,17,20,21,24,25,28,29,18,19,22,23,26,27,30,31]
	vperm2i128	$35, %ymm0, %ymm0, %ymm0 # ymm0 = ymm0[2,3,0,1]
	vpshufb	.LCPI147_27(%rip), %ymm0, %ymm0 # ymm0 = ymm0[2,3,6,7,10,11,14,15,0,1,4,5,8,9,12,13,18,19,22,23,26,27,30,31,16,17,20,21,24,25,28,29]
	vpblendd	$60, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0,1],ymm0[2,3,4,5],ymm3[6,7]
	vextracti128	$1, %ymm0, %xmm3
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vcvtdq2ps	%ymm3, %ymm10
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm11
	vpmovzxbd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,zero,zero,xmm2[1],zero,zero,zero,xmm2[2],zero,zero,zero,xmm2[3],zero,zero,zero,xmm2[4],zero,zero,zero,xmm2[5],zero,zero,zero,xmm2[6],zero,zero,zero,xmm2[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm0, %ymm11, %ymm3, %ymm0
	vpunpckhbw	%xmm2, %xmm2, %xmm2 # xmm2 = xmm2[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm10, %ymm3, %ymm2
	vmovaps	.LCPI147_12(%rip), %ymm12 # ymm12 = <u,4,u,5,u,6,u,7>
	vpermps	%ymm2, %ymm12, %ymm3
	vmovaps	.LCPI147_13(%rip), %ymm13 # ymm13 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm15, %ymm13, %ymm9
	vblendps	$170, %ymm3, %ymm9, %ymm3 # ymm3 = ymm9[0],ymm3[1],ymm9[2],ymm3[3],ymm9[4],ymm3[5],ymm9[6],ymm3[7]
	vmovaps	.LCPI147_14(%rip), %ymm8 # ymm8 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm2, %ymm8, %ymm2
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm15, %ymm1, %ymm9
	vblendps	$170, %ymm2, %ymm9, %ymm2 # ymm2 = ymm9[0],ymm2[1],ymm9[2],ymm2[3],ymm9[4],ymm2[5],ymm9[6],ymm2[7]
	vpermps	%ymm0, %ymm12, %ymm9
	vpermps	%ymm14, %ymm13, %ymm15
	vblendps	$170, %ymm9, %ymm15, %ymm9 # ymm9 = ymm15[0],ymm9[1],ymm15[2],ymm9[3],ymm15[4],ymm9[5],ymm15[6],ymm9[7]
	vpermps	%ymm0, %ymm8, %ymm0
	vmovaps	%ymm8, %ymm15
	vpermps	%ymm14, %ymm1, %ymm14
	vblendps	$170, %ymm0, %ymm14, %ymm0 # ymm0 = ymm14[0],ymm0[1],ymm14[2],ymm0[3],ymm14[4],ymm0[5],ymm14[6],ymm0[7]
	vmovups	%ymm0, (%r12,%rbx,4)
	vmovups	%ymm9, 32(%r12,%rbx,4)
	vmovups	%ymm2, 64(%r12,%rbx,4)
	vmovups	%ymm3, 96(%r12,%rbx,4)
	movq	%r12, %rax
	je	.LBB147_546
# BB#545:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	4192(%rsp), %xmm6       # 16-byte Reload
.LBB147_546:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpmovzxbd	%xmm6, %ymm0    # ymm0 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm0, %ymm7, %ymm3, %ymm0
	vpunpckhbw	%xmm6, %xmm6, %xmm2 # xmm2 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm5, %ymm3, %ymm2
	je	.LBB147_548
# BB#547:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	4224(%rsp), %xmm4       # 16-byte Reload
.LBB147_548:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpmovzxbd	%xmm4, %ymm3    # ymm3 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm6, %ymm6, %ymm6
	vblendvps	%ymm3, %ymm11, %ymm6, %ymm3
	vpunpckhbw	%xmm4, %xmm4, %xmm4 # xmm4 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm10, %ymm6, %ymm4
	vpermps	%ymm2, %ymm13, %ymm6
	vpermps	%ymm4, %ymm12, %ymm9
	vblendps	$170, %ymm9, %ymm6, %ymm6 # ymm6 = ymm6[0],ymm9[1],ymm6[2],ymm9[3],ymm6[4],ymm9[5],ymm6[6],ymm9[7]
	vpermps	%ymm2, %ymm1, %ymm2
	vpermps	%ymm4, %ymm15, %ymm4
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm0, %ymm13, %ymm4
	vpermps	%ymm3, %ymm12, %ymm9
	vblendps	$170, %ymm9, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm9[1],ymm4[2],ymm9[3],ymm4[4],ymm9[5],ymm4[6],ymm9[7]
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	%ymm1, %ymm8
	vpermps	%ymm3, %ymm15, %ymm3
	vmovaps	%ymm15, %ymm9
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	vmovups	%ymm0, 12288(%rax,%r14,4)
	vmovups	%ymm4, 12320(%rax,%r14,4)
	vmovups	%ymm2, 12352(%rax,%r14,4)
	vmovups	%ymm6, 12384(%rax,%r14,4)
	movq	%rax, %rsi
	vmovdqa	5280(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_550
# BB#549:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
.LBB147_550:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpmovzxbd	%xmm1, %ymm0    # ymm0 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm2, %ymm2, %ymm2
	vblendvps	%ymm0, %ymm7, %ymm2, %ymm0
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm5, %ymm2, %ymm1
	vmovdqa	5312(%rsp), %xmm3       # 16-byte Reload
	je	.LBB147_552
# BB#551:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vmovdqa	5216(%rsp), %xmm3       # 16-byte Reload
.LBB147_552:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_528 Depth=4
	vpmovzxbd	%xmm3, %ymm2    # ymm2 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm2, %ymm11, %ymm4, %ymm2
	vpunpckhbw	%xmm3, %xmm3, %xmm3 # xmm3 = xmm3[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm10, %ymm4, %ymm3
	vpermps	%ymm1, %ymm13, %ymm4
	vpermps	%ymm3, %ymm12, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm8, %ymm1
	vpermps	%ymm3, %ymm9, %ymm3
	vblendps	$170, %ymm3, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm3[1],ymm1[2],ymm3[3],ymm1[4],ymm3[5],ymm1[6],ymm3[7]
	vpermps	%ymm0, %ymm13, %ymm3
	vpermps	%ymm2, %ymm12, %ymm5
	vblendps	$170, %ymm5, %ymm3, %ymm3 # ymm3 = ymm3[0],ymm5[1],ymm3[2],ymm5[3],ymm3[4],ymm5[5],ymm3[6],ymm5[7]
	vpermps	%ymm0, %ymm8, %ymm0
	vpermps	%ymm2, %ymm9, %ymm2
	vblendps	$170, %ymm2, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm2[1],ymm0[2],ymm2[3],ymm0[4],ymm2[5],ymm0[6],ymm2[7]
	addq	4672(%rsp), %rbx        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%rbx,4)
	vmovups	%ymm3, 24608(%rsi,%rbx,4)
	vmovups	%ymm1, 24640(%rsi,%rbx,4)
	vmovups	%ymm4, 24672(%rsi,%rbx,4)
	addl	$32, %r15d
	addl	$32, %r11d
	addl	$-1, %r10d
	jne	.LBB147_528
.LBB147_553:                            # %end for deinterleaved$1.s0.v10.v10304
                                        #   in Loop: Header=BB147_499 Depth=3
	movl	1324(%rsp), %eax        # 4-byte Reload
	cmpl	1740(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_580
# BB#554:                               # %for deinterleaved$1.s0.v10.v10307.preheader
                                        #   in Loop: Header=BB147_499 Depth=3
	movq	5248(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1648(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3072(%rsp)       # 32-byte Spill
	vpsubd	1600(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 2912(%rsp)       # 32-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	movl	1176(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	.align	16, 0x90
.LBB147_555:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_499 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rdx, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3472(%rsp)        # 4-byte Spill
	cmpl	$0, 3488(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	4256(%rsp)              # 1-byte Folded Spill
	movq	2160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	movl	%eax, %r15d
	andl	$1, %r15d
	sete	%cl
	movl	%ecx, 5280(%rsp)        # 4-byte Spill
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	movq	2144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm13 # ymm13 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm13, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4416(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3872(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %esi
	movl	%esi, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %esi
	movl	%esi, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %esi
	movl	%esi, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %esi
	movl	%esi, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vmovdqa	.LCPI147_10(%rip), %ymm12 # ymm12 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm12, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3520(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movq	1984(%rsp), %rax        # 8-byte Reload
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovd	4192(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 4224(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3904(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3872(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3840(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm2, %xmm2
	vpinsrd	$3, %r14d, %xmm2, %xmm2
	vmovd	%esi, %xmm3
	vpinsrd	$1, %r11d, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vmovd	%r9d, %xmm4
	vpinsrd	$1, %ebx, %xmm4, %xmm4
	vpinsrd	$2, %r10d, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	movq	2136(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm5
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	andb	4256(%rsp), %r15b       # 1-byte Folded Reload
	movl	%r15d, 3456(%rsp)       # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm12, %ymm0, %ymm6
	vpaddd	%ymm13, %ymm0, %ymm0
	vmovdqa	4368(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm7
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm10, %xmm0, %xmm0
	vmovdqa	4352(%rsp), %xmm11      # 16-byte Reload
	vpmaxsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm7, %ymm7
	vpminsd	%xmm10, %xmm6, %xmm0
	vextracti128	$1, %ymm6, %xmm6
	vpminsd	%xmm10, %xmm6, %xmm6
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm11, %xmm6, %xmm6
	vinserti128	$1, %xmm6, %ymm0, %ymm6
	movl	3744(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	4576(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vmovdqa	3072(%rsp), %ymm14      # 32-byte Reload
	vpand	%ymm4, %ymm14, %ymm4
	vmovdqa	4384(%rsp), %ymm9       # 32-byte Reload
	vpaddd	%ymm3, %ymm9, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpsrad	$31, %ymm1, %ymm4
	vpand	%ymm4, %ymm14, %ymm4
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm4, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm4
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm4, %ymm1
	vpabsd	%xmm3, %xmm4
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vmovdqa	4544(%rsp), %ymm4       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm4, %ymm4
	vmovdqa	4512(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm1, %ymm8, %ymm1
	vblendvps	%ymm2, %ymm7, %ymm1, %ymm1
	vblendvps	%ymm4, %ymm6, %ymm3, %ymm2
	vmovdqa	2912(%rsp), %ymm15      # 32-byte Reload
	vpaddd	%ymm2, %ymm15, %ymm2
	vpaddd	%ymm1, %ymm15, %ymm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4256(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4224(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	vmovq	%xmm2, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3872(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3904(%rsp)        # 8-byte Spill
	testl	3488(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	%xmm5, %ymm1
	vpaddd	%ymm13, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3232(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3296(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3280(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3248(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3184(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3264(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, 3168(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vmovd	%xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm12, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3600(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3520(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3312(%rsp)              # 4-byte Folded Reload
	movq	1992(%rsp), %rax        # 8-byte Reload
	movq	5312(%rsp), %r11        # 8-byte Reload
	leal	(%rax,%r11), %eax
	vmovd	%eax, %xmm1
	vmovd	3200(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 3216(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, 3184(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$3, 3168(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vmovd	%r14d, %xmm3
	vpinsrd	$1, %r15d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpinsrd	$3, %r13d, %xmm3, %xmm3
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %ebx, %xmm4, %xmm4
	vmovd	%r9d, %xmm5
	vpinsrd	$1, %r8d, %xmm5, %xmm5
	vpinsrd	$2, %r10d, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpbroadcastd	%xmm1, %ymm6
	vpaddd	%ymm13, %ymm6, %ymm1
	vpminsd	%xmm10, %xmm1, %xmm7
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm11, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm7, %ymm1
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsrad	$31, %ymm2, %ymm3
	vpand	%ymm14, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm3, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovdqa	4480(%rsp), %ymm3       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm3, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	vpaddd	%ymm12, %ymm6, %ymm2
	vpminsd	%xmm10, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm11, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vinserti128	$1, %xmm4, %ymm5, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpand	%ymm14, %ymm4, %ymm4
	vpaddd	%ymm3, %ymm9, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpabsd	%xmm3, %xmm4
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vmovdqa	4448(%rsp), %ymm4       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm4, %ymm0
	vpsubd	%ymm3, %ymm8, %ymm3
	vblendvps	%ymm0, %ymm2, %ymm3, %ymm0
	movl	3776(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vpaddd	%ymm0, %ymm15, %ymm0
	vpaddd	%ymm1, %ymm15, %ymm1
	vmovq	%xmm1, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	movslq	3744(%rsp), %r15        # 4-byte Folded Reload
	subq	4760(%rsp), %r15        # 8-byte Folded Reload
	addq	2896(%rsp), %r15        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4680(%rsp), %rsi        # 8-byte Reload
	leaq	(%r15,%rsi), %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	je	.LBB147_557
# BB#556:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_557:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovd	%ebx, %xmm0
	movl	5280(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3456(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_559
# BB#558:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_559:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movl	3232(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_561
# BB#560:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_561:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_563
# BB#562:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_563:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_565
# BB#564:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_565:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm0, 3200(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_567
# BB#566:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_567:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm1, 3216(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_569
# BB#568:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_569:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5280(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3344(%rsp)        # 8-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3424(%rsp)        # 8-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3872(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%r8,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	3904(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4224(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%rdx,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vpxor	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_571
# BB#570:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_571:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movslq	%r12d, %r13
	movq	%r13, 3328(%rsp)        # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3376(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movq	3280(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %r12
	movq	3264(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3392(%rsp)        # 8-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%rbx, %rax
	movq	%r10, %rbx
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%rax,2), %xmm0, %xmm0
	movzwl	(%r11,%r13,2), %esi
	vmovd	%esi, %xmm1
	movq	3776(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r15,4)
	vmovups	%ymm3, 32(%rcx,%r15,4)
	vmovups	%ymm0, 64(%rcx,%r15,4)
	vmovups	%ymm2, 96(%rcx,%r15,4)
	movq	%rcx, %r11
	je	.LBB147_573
# BB#572:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_573:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movq	%rsi, %rcx
	movq	3456(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3872(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rcx,%r8,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3232(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3904(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4256(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4224(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_575
# BB#574:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	3184(%rsp), %xmm6       # 16-byte Reload
.LBB147_575:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movq	%rsi, %rdx
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %esi
	vmovd	%esi, %xmm5
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%rax,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_577
# BB#576:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	3200(%rsp), %xmm9       # 16-byte Reload
.LBB147_577:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	movq	5312(%rsp), %rdx        # 8-byte Reload
	je	.LBB147_579
# BB#578:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	vmovdqa	3216(%rsp), %xmm15      # 16-byte Reload
.LBB147_579:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_555 Depth=4
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	3776(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rcx,%rdi,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4672(%rsp), %r15        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r15,4)
	vmovups	%ymm2, 24608(%rsi,%r15,4)
	vmovups	%ymm1, 24640(%rsi,%r15,4)
	vmovups	%ymm4, 24672(%rsi,%r15,4)
	addl	$32, %edx
	movl	3472(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_555
.LBB147_580:                            # %end for deinterleaved$1.s0.v10.v10308
                                        #   in Loop: Header=BB147_499 Depth=3
	movl	2880(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2880(%rsp)        # 4-byte Spill
	addq	$1, 5248(%rsp)          # 8-byte Folded Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	addl	%eax, 2848(%rsp)        # 4-byte Folded Spill
	cmpl	2816(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_499
.LBB147_581:                            # %end for deinterleaved$1.s0.v11298
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1688(%rsp), %rax        # 8-byte Reload
	movq	1032(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx,2), %eax
	movq	%rax, 1072(%rsp)        # 8-byte Spill
	movl	2832(%rsp), %eax        # 4-byte Reload
	cmpl	2864(%rsp), %eax        # 4-byte Folded Reload
	jle	.LBB147_611
# BB#582:                               #   in Loop: Header=BB147_467 Depth=2
	movl	2800(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_583:                            # %for deinterleaved$1.s0.v11311
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_585 Depth 4
	cmpl	$0, 1740(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_610
# BB#584:                               # %for deinterleaved$1.s0.v10.v10314.preheader
                                        #   in Loop: Header=BB147_583 Depth=3
	movq	3168(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3552(%rsp)        # 4-byte Spill
	cmpq	%rdi, 1376(%rsp)        # 8-byte Folded Reload
	movl	1396(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	movq	1408(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	%edi, %eax
	subl	%edx, %eax
	cltd
	idivl	1388(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1392(%rsp), %eax        # 4-byte Folded Reload
	movq	1400(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %edx
	leal	(%rdx,%rax), %esi
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %esi
	notl	%esi
	cmovgl	%eax, %esi
	movl	1384(%rsp), %eax        # 4-byte Reload
	subl	%esi, %eax
	cmpq	%rdi, 1368(%rsp)        # 8-byte Folded Reload
	cmovgl	%ecx, %eax
	movq	1744(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1648(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3072(%rsp)       # 32-byte Spill
	vpsubd	1600(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3520(%rsp)       # 32-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	movl	1740(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_585:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_583 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cmpl	$0, 3552(%rsp)          # 4-byte Folded Reload
	setne	5248(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r15d
	andl	$1, %r15d
	sete	%cl
	movl	%ecx, 5280(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	3168(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	movq	3984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4416(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3776(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3872(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ebx
	movl	%ebx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r9d
	movl	%r9d, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3360(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4224(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4256(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3904(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3872(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r14d, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	5248(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	andb	5216(%rsp), %r15b       # 1-byte Folded Reload
	movl	%r15d, 3472(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4576(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4544(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4096(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4064(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3072(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4384(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4512(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4368(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4352(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3520(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4224(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5248(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3904(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3872(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	testl	3552(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3248(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3600(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3232(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3344(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3312(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3296(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3776(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vmovd	%xmm2, %eax
	cltd
	idivl	3744(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	vmovd	3216(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3232(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3200(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r14d, %xmm1
	vpinsrd	$1, %r15d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4480(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4448(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	4032(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3808(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3520(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4760(%rsp), %r13        # 8-byte Folded Reload
	addq	2912(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4680(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	je	.LBB147_587
# BB#586:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_587:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovd	%ebx, %xmm0
	movl	5280(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3472(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_589
# BB#588:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_589:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movl	3248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_591
# BB#590:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_591:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_593
# BB#592:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_593:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_595
# BB#594:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_595:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm0, 3200(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_597
# BB#596:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_597:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm1, 3216(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_599
# BB#598:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_599:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5280(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3312(%rsp)        # 8-byte Spill
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3440(%rsp)        # 8-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3456(%rsp)         # 8-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3904(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3872(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4224(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_601
# BB#600:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_601:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3264(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3392(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3408(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_603
# BB#602:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_603:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movq	%rsi, %rcx
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3904(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3248(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3872(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3280(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4224(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3312(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3440(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4256(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3456(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_605
# BB#604:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	3184(%rsp), %xmm6       # 16-byte Reload
.LBB147_605:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movq	%rsi, %rdx
	movq	3264(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%r11,%rcx,4)
	vmovups	%ymm4, 12320(%r11,%rcx,4)
	vmovups	%ymm2, 12352(%r11,%rcx,4)
	vmovups	%ymm6, 12384(%r11,%rcx,4)
	je	.LBB147_607
# BB#606:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	3200(%rsp), %xmm9       # 16-byte Reload
.LBB147_607:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_609
# BB#608:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	vmovdqa	3216(%rsp), %xmm15      # 16-byte Reload
.LBB147_609:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_585 Depth=4
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	3808(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4672(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%r11,%r13,4)
	vmovups	%ymm2, 24608(%r11,%r13,4)
	vmovups	%ymm1, 24640(%r11,%r13,4)
	vmovups	%ymm4, 24672(%r11,%r13,4)
	movq	5312(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3488(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_585
.LBB147_610:                            # %end for deinterleaved$1.s0.v10.v10315
                                        #   in Loop: Header=BB147_583 Depth=3
	movl	2864(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2864(%rsp)        # 4-byte Spill
	addq	$1, 3168(%rsp)          # 8-byte Folded Spill
	cmpl	2832(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_583
.LBB147_611:                            # %produce gH319
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1072(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rcx
	leal	2(%rcx), %edx
	movl	%edx, 1436(%rsp)        # 4-byte Spill
	movq	1816(%rsp), %rax        # 8-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	leal	4(%rcx), %ecx
	movl	%ecx, 1428(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	movl	%ecx, %edi
	cmovgl	%esi, %edi
	movl	%edi, 1440(%rsp)        # 4-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 2240(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	%ecx, 1260(%rsp)        # 4-byte Spill
	movl	984(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1364(%rsp)        # 4-byte Spill
	movl	980(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	movl	976(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1356(%rsp)        # 4-byte Spill
	movl	%edx, 2256(%rsp)        # 4-byte Spill
	cmpl	%edi, %edx
	jge	.LBB147_648
	.align	16, 0x90
.LBB147_612:                            # %for gH.s0.v11321
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_614 Depth 4
	cmpl	$0, 2248(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_647
# BB#613:                               # %for gH.s0.v10.v10323.preheader
                                        #   in Loop: Header=BB147_612 Depth=3
	movl	2256(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1804(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1808(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 3872(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2176(%rsp)       # 16-byte Spill
	cltq
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1776(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2168(%rsp)        # 8-byte Spill
	movl	2248(%rsp), %eax        # 4-byte Reload
	movq	5352(%rsp), %r10        # 8-byte Reload
	movl	1364(%rsp), %ecx        # 4-byte Reload
	movl	1360(%rsp), %r9d        # 4-byte Reload
	movl	1356(%rsp), %edi        # 4-byte Reload
	.align	16, 0x90
.LBB147_614:                            # %for gH.s0.v10.v10323
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_612 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 5312(%rsp)        # 4-byte Spill
	movl	%edi, 3680(%rsp)        # 4-byte Spill
	movl	%r9d, 3712(%rsp)        # 4-byte Spill
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 3872(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r10d, %r14d
	andl	$1, %r14d
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	3144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3440(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3904(%rsp)        # 4-byte Spill
	movq	3128(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm11, %xmm10
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3136(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	4224(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4256(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 4192(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3904(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3904(%rsp)       # 16-byte Spill
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3520(%rsp)        # 4-byte Spill
	vmovd	%r11d, %xmm0
	vpinsrd	$1, 3552(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3472(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3488(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r15d, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm3, %xmm5
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	leal	-3(%r10), %eax
	vmovd	%eax, %xmm11
	vmovd	%r10d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2176(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5408(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vmovdqa	%xmm10, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm1
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vmovdqa	5024(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpcmpgtd	%xmm1, %xmm14, %xmm4
	vpsubd	%xmm1, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	%xmm11, %xmm4
	vmovdqa	%xmm6, %xmm11
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	%xmm12, %xmm1, %xmm0
	vmovdqa	%xmm0, 4224(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movl	%r10d, %eax
	movl	2256(%rsp), %ebx        # 4-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	3152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm4
	vmovdqa	3904(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	5200(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	4192(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm2, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movb	5280(%rsp), %r13b       # 1-byte Reload
	movb	5216(%rsp), %r15b       # 1-byte Reload
	andb	%r15b, %r13b
	vpaddd	%xmm1, %xmm10, %xmm5
	vmovq	%xmm5, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm5, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	movb	5248(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r14b
	movl	%r14d, 5280(%rsp)       # 4-byte Spill
	movl	3872(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3904(%rsp)              # 1-byte Folded Spill
	leal	1(%r10), %r14d
	movl	%r14d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	sete	%r12b
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	3440(%rsp)              # 4-byte Folded Reload
	vmovd	3616(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3648(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3600(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3520(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	3488(%rsp), %xmm5       # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vpinsrd	$1, 3472(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$2, 3456(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$3, 3424(%rsp), %xmm5, %xmm6 # 4-byte Folded Reload
	leal	-4(%r10), %eax
	vmovd	%eax, %xmm5
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpinsrd	$2, %esi, %xmm7, %xmm7
	vpsrad	$31, %xmm4, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm11, %xmm3, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpinsrd	$3, %edx, %xmm7, %xmm0
	vpaddd	%xmm4, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	orl	%ebx, %eax
	movb	%r12b, %bl
	testb	$1, %al
	sete	3456(%rsp)              # 1-byte Folded Spill
	andb	%r15b, %bl
	andb	%r11b, %r9b
	testl	%r14d, %r8d
	vmovd	%r14d, %xmm1
	vpsrad	$31, %xmm6, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vmovdqa	5008(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm6
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vpxor	%xmm10, %xmm6, %xmm6
	vmovdqa	4832(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm2, %xmm5, %xmm2
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	5104(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm2, %xmm5
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm3
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm6
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	setne	%r15b
	vmovq	%xmm0, %r8
	movq	%r8, 2368(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3552(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm1, %xmm0
	vmovq	%xmm0, %r13
	movq	%r13, 2440(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vpextrq	$1, %xmm0, %r11
	movq	%r11, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vmovdqa	5488(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2496(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2512(%rsp)        # 8-byte Spill
	movslq	5312(%rsp), %rcx        # 4-byte Folded Reload
	movq	%rcx, %rdx
	orq	$4, %rdx
	movq	%rdx, 2528(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2656(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2704(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2720(%rsp)        # 8-byte Spill
	movq	%rcx, %rdx
	orq	$6, %rdx
	movq	%rdx, 2736(%rsp)        # 8-byte Spill
	vmovdqa	4224(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2880(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3520(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm10
	vmovaps	%xmm10, 4224(%rsp)      # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_616
# BB#615:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm10, %xmm10, %xmm10
.LBB147_616:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	movzbl	4192(%rsp), %r14d       # 1-byte Folded Reload
	vmovd	%r14d, %xmm0
	movzbl	3904(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm8
	vmovaps	%xmm8, 3904(%rsp)       # 16-byte Spill
	je	.LBB147_618
# BB#617:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_618:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_620
# BB#619:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_620:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_622
# BB#621:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_622:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm1, 2272(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movzbl	%bl, %edi
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	je	.LBB147_624
# BB#623:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_624:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm0, 2304(%rsp)       # 16-byte Spill
	movzbl	3456(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	%r15b, %edi
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, %xmm2
	movq	4872(%rsp), %r14        # 8-byte Reload
	je	.LBB147_626
# BB#625:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_626:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm2, 2320(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	movzbl	%r9b, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_628
# BB#627:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_628:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm2, 2336(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movl	3712(%rsp), %r9d        # 4-byte Reload
	je	.LBB147_630
# BB#629:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_630:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm0, 2352(%rsp)       # 16-byte Spill
	movq	2896(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3200(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2912(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2848(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3072(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3216(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm12 # xmm12 = xmm2[0,1,2],mem[0]
	movq	2752(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2768(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2800(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm15 # xmm15 = xmm3[0,1,2],mem[0]
	movq	2544(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2592(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2576(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2608(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm11 # xmm11 = xmm3[0,1,2],mem[0]
	movslq	%r9d, %rdi
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rdi,4), %xmm3
	vmovups	12328(%rdx,%rdi,4), %xmm6
	vmovups	12304(%rdx,%rdi,4), %xmm7
	vmovups	12320(%rdx,%rdi,4), %xmm1
	vshufps	$136, 12336(%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,2],mem[0,2]
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm0, %xmm9, %xmm0
	vshufps	$221, %xmm6, %xmm3, %xmm2 # xmm2 = xmm3[1,3],xmm6[1,3]
	vmovaps	5472(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$221, %xmm1, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm1[1,3]
	vmulps	%xmm12, %xmm9, %xmm1
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm6, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm15, %xmm9, %xmm3
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm7
	vsubps	%xmm4, %xmm14, %xmm6
	cmpl	$0, 104(%rbp)
	je	.LBB147_632
# BB#631:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm10, 4256(%rsp)      # 16-byte Spill
.LBB147_632:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm9
	vmaxps	%xmm1, %xmm2, %xmm0
	vmulps	3840(%rsp), %xmm11, %xmm12 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm6, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm5
	je	.LBB147_634
# BB#633:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	%xmm8, 4192(%rsp)       # 16-byte Spill
.LBB147_634:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	movq	2368(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2384(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	movq	2440(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm6, %xmm15 # xmm15 = xmm6[0,1,2],mem[0]
	movl	3680(%rsp), %edi        # 4-byte Reload
	movslq	%edi, %rax
	vmovups	24592(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovups	24608(%rdx,%rax,4), %xmm10
	vmovups	24624(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 2896(%rsp)       # 16-byte Spill
	vmovups	24600(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3184(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm6
	vmovaps	%xmm6, 3168(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	vaddps	%xmm9, %xmm0, %xmm7
	vmovaps	%xmm7, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm12, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm5, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm0
	vshufps	$136, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm10[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm7, %xmm1
	vshufps	$136, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm15, %xmm7, %xmm2
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_636
# BB#635:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	2272(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
.LBB147_636:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	movq	2464(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rdx
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2496(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2480(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2512(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],mem[0]
	movq	2528(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	2672(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2704(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2688(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2720(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2736(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3072(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm9
	vmovups	48(%rsi,%rcx,4), %xmm11
	vmovaps	%xmm11, 2848(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%rcx,4), %xmm14
	vmovaps	%xmm14, 2912(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm12, %xmm15
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm1
	vshufps	$136, %xmm9, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm9[0,2]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$136, %xmm11, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm12, %xmm4
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vmulps	5280(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm4
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm5, %xmm1, %xmm8
	je	.LBB147_638
# BB#637:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	2288(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3904(%rsp)       # 16-byte Spill
.LBB147_638:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vsubps	%xmm0, %xmm15, %xmm1
	vmovdqa	4224(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vfmadd213ps	%xmm2, %xmm14, %xmm7
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm4, %xmm6
	vmovdqa	3904(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm8, %xmm5, %xmm0
	je	.LBB147_640
# BB#639:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	2304(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 5248(%rsp)       # 16-byte Spill
.LBB147_640:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vfmadd213ps	%xmm2, %xmm14, %xmm1
	vblendvps	%xmm15, %xmm7, %xmm0, %xmm0
	vaddps	%xmm6, %xmm8, %xmm15
	je	.LBB147_642
# BB#641:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovaps	2320(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 5216(%rsp)       # 16-byte Spill
.LBB147_642:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm7
	movq	2816(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2832(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2880(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm1
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 2896(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm10[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3184(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3168(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm2, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm12, %xmm4
	vminps	%xmm13, %xmm1, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm1
	vmulps	5280(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3472(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_644
# BB#643:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovdqa	2336(%rsp), %xmm10      # 16-byte Reload
.LBB147_644:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vblendvps	%xmm0, %xmm8, %xmm7, %xmm8
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 2848(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm9[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3072(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 2912(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3520(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm4, %xmm12
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm9[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vsubps	%xmm0, %xmm12, %xmm0
	vfmadd213ps	%xmm6, %xmm14, %xmm1
	vfmadd213ps	%xmm6, %xmm14, %xmm0
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmovdqa	5216(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vpslld	$31, %xmm10, %xmm5
	vmovdqa	3456(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_646
# BB#645:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vmovdqa	2352(%rsp), %xmm6       # 16-byte Reload
.LBB147_646:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_614 Depth=4
	vpslld	$31, %xmm6, %xmm6
	vmovaps	3216(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm6, %xmm7, %xmm2, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm7, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r10d, %rax
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r14,%rax,4)
	addl	$8, %edi
	addl	$8, %r9d
	movl	5312(%rsp), %ecx        # 4-byte Reload
	addl	$8, %ecx
	addl	$8, %r10d
	movl	3744(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_614
.LBB147_647:                            # %end for gH.s0.v10.v10324
                                        #   in Loop: Header=BB147_612 Depth=3
	movl	2256(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2256(%rsp)        # 4-byte Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1356(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1360(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1364(%rsp)        # 4-byte Folded Spill
	cmpl	1440(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_612
.LBB147_648:                            # %end for gH.s0.v11322
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	1260(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1440(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_740
# BB#649:                               #   in Loop: Header=BB147_467 Depth=2
	movl	908(%rsp), %esi         # 4-byte Reload
	movl	924(%rsp), %edx         # 4-byte Reload
	subl	%edx, %esi
	movl	1832(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	imull	%esi, %eax
	imull	%ecx, %esi
	movl	%esi, 1364(%rsp)        # 4-byte Spill
	notl	%edx
	movq	720(%rsp), %rcx         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_650:                            # %for gH.s0.v11327
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_652 Depth 4
                                        #         Child Loop BB147_687 Depth 4
                                        #         Child Loop BB147_706 Depth 4
	cmpl	$0, 2236(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_685
# BB#651:                               # %for gH.s0.v10.v10329.preheader
                                        #   in Loop: Header=BB147_650 Depth=3
	movq	1208(%rsp), %rax        # 8-byte Reload
	movq	5312(%rsp), %rdi        # 8-byte Reload
	leal	(%rax,%rdi), %ecx
	movq	1712(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3904(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2304(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1776(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2288(%rsp)        # 8-byte Spill
	xorl	%r8d, %r8d
	.align	16, 0x90
.LBB147_652:                            # %for gH.s0.v10.v10329
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_650 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r8, 5280(%rsp)         # 8-byte Spill
	cmpl	$0, 3904(%rsp)          # 4-byte Folded Reload
	setne	4224(%rsp)              # 1-byte Folded Spill
	sete	4256(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r8,8), %r15d
	movl	%r15d, 3776(%rsp)       # 4-byte Spill
	movl	%r15d, %r11d
	andl	$1, %r11d
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r13d
	movq	4728(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r13d
	leal	-2(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3472(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	leal	-1(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3440(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3520(%rsp)        # 4-byte Spill
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vmovdqa	%xmm9, %xmm12
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	4192(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpinsrd	$1, 5216(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3744(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3712(%rsp), %xmm1, %xmm10 # 4-byte Folded Reload
	leal	-2(%r12,%r8,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	vmovd	3488(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3552(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3440(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3520(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r10d, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3440(%rsp)        # 4-byte Spill
	vpinsrd	$1, 3424(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r14d, %xmm2, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm2, %xmm5
	leal	-1(%r12,%r8,8), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	leal	-3(%r12,%r8,8), %eax
	movq	%r8, %rbx
	vmovd	%eax, %xmm11
	vmovd	%r15d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2304(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm3, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5408(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vmovdqa	%xmm12, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm1
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm0
	vmovdqa	5168(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm1, %xmm7, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vmovdqa	5024(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm11, %xmm3
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 3744(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	5312(%rsp), %r8         # 8-byte Reload
	orl	%r8d, %eax
	testb	$1, %al
	vpsrad	$31, %xmm10, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpxor	%xmm5, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	5216(%rsp), %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm6, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	sete	3552(%rsp)              # 1-byte Folded Spill
	movb	4224(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, 5248(%rsp)       # 1-byte Folded Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movb	4256(%rsp), %r14b       # 1-byte Reload
	andb	%r14b, %r11b
	movl	%r11d, 5216(%rsp)       # 4-byte Spill
	movl	3904(%rsp), %r11d       # 4-byte Reload
	testl	%r15d, %r11d
	setne	3520(%rsp)              # 1-byte Folded Spill
	leal	1(%r12,%rbx,8), %ecx
	movl	%ecx, %r9d
	andl	$1, %r9d
	sete	%r15b
	addl	$1, %r13d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm6, %xmm10
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	3472(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	vmovd	3648(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3680(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3616(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3600(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3440(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3488(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3424(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3408(%rsp), %xmm4, %xmm3 # 4-byte Folded Reload
	leal	-4(%r12,%rbx,8), %eax
	movl	%r9d, %r12d
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm4
	vpinsrd	$1, %r13d, %xmm4, %xmm4
	vpinsrd	$2, %esi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm0, %xmm14, %xmm6
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm6, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm2, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpaddd	%xmm4, %xmm7, %xmm0
	vmovq	%xmm0, %rdi
	movq	%rdi, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	orl	%r8d, %eax
	testb	$1, %al
	sete	2496(%rsp)              # 1-byte Folded Spill
	andb	%r10b, %r15b
	movb	%r15b, 2784(%rsp)       # 1-byte Spill
	andb	%r14b, %r12b
	testl	%ecx, %r11d
	vmovd	%ecx, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm11
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm6
	vblendvps	%xmm7, %xmm1, %xmm6, %xmm1
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpxor	%xmm0, %xmm6, %xmm6
	vmovdqa	4832(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	3680(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm5, %xmm6
	vpand	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm5, %xmm14, %xmm6
	vpsubd	%xmm5, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vmovdqa	5104(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	vpxor	%xmm0, %xmm6, %xmm6
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm2
	vpor	%xmm6, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vpbroadcastd	4256(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm5, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm1, %xmm1
	vpmulld	%xmm12, %xmm2, %xmm2
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm5
	setne	%dl
	vmovq	%xmm5, %r9
	movq	%r9, %rbx
	sarq	$32, %rbx
	vpextrq	$1, %xmm5, %r8
	movq	%r8, %rax
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm3, %xmm5
	vmovq	%xmm5, %r11
	movq	%r11, 2440(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm5, %r13
	movq	%r13, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vmovdqa	3712(%rsp), %xmm0       # 16-byte Reload
	vpaddd	%xmm0, %xmm3, %xmm5
	vmovq	%xmm5, %r15
	movq	%r15, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm5, %r14
	movq	%r14, 5248(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vmovdqa	5488(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm1, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2544(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2528(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	movq	2320(%rsp), %rcx        # 8-byte Reload
	movq	5280(%rsp), %rsi        # 8-byte Reload
	leal	(%rcx,%rsi,8), %esi
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movl	%ecx, 2416(%rsp)        # 4-byte Spill
	movq	4936(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movl	%ecx, 2464(%rsp)        # 4-byte Spill
	movslq	%esi, %rsi
	movq	%rsi, %r10
	orq	$4, %r10
	vpaddd	%xmm4, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2576(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2608(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2592(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2624(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2688(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	movq	%rsi, %rcx
	orq	$6, %rcx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	vmovdqa	3744(%rsp), %xmm6       # 16-byte Reload
	vpaddd	%xmm6, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2848(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2912(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2880(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2896(%rsp)        # 8-byte Spill
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3184(%rsp)        # 8-byte Spill
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm11, %xmm11
	vmovaps	%xmm11, 2656(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	je	.LBB147_654
# BB#653:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_654:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	movzbl	3552(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	movzbl	3520(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_656
# BB#655:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_656:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_658
# BB#657:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_658:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm0, 2336(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_660
# BB#659:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_660:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm0, 2352(%rsp)       # 16-byte Spill
	movzbl	2784(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_662
# BB#661:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_662:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm1, 2368(%rsp)       # 16-byte Spill
	movzbl	2496(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	je	.LBB147_664
# BB#663:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_664:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm2, 2384(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5216(%rsp)       # 16-byte Spill
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_666
# BB#665:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_666:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm3, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2400(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_668
# BB#667:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_668:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],mem[0]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movslq	2416(%rsp), %rcx        # 4-byte Folded Reload
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	12312(%rdi,%rcx,4), %xmm5
	vmovups	12328(%rdi,%rcx,4), %xmm6
	vmovups	12304(%rdi,%rcx,4), %xmm7
	vmovups	12320(%rdi,%rcx,4), %xmm0
	vshufps	$136, 12336(%rdi,%rcx,4), %xmm0, %xmm15 # xmm15 = xmm0[0,2],mem[0,2]
	vmovaps	3872(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm6[1,3]
	vmovaps	5472(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	5504(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm1, %xmm6
	vmulps	%xmm4, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm3, %xmm7
	vsubps	%xmm2, %xmm15, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_670
# BB#669:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm11, 4224(%rsp)      # 16-byte Spill
.LBB147_670:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3872(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_672
# BB#671:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	%xmm8, 4192(%rsp)       # 16-byte Spill
.LBB147_672:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	movslq	%r9d, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%r8d, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2440(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2480(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	5248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r14,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2464(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24592(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vmovups	24608(%rdi,%rax,4), %xmm13
	vmovups	24624(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 3248(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovdqa	2656(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_674
# BB#673:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	2336(%rsp), %xmm2       # 16-byte Reload
.LBB147_674:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	movq	2512(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rdx, %rcx
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2544(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2528(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	vmovups	(%rdi,%r10,4), %xmm6
	vmovaps	%xmm6, 3376(%rsp)       # 16-byte Spill
	movq	2576(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2608(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2592(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2640(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2688(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2672(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2704(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2720(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3328(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm10
	vmovups	48(%rdi,%rsi,4), %xmm9
	vmovaps	%xmm9, 3280(%rsp)       # 16-byte Spill
	vmovups	40(%rdi,%rsi,4), %xmm14
	vmovaps	%xmm14, 3312(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3808(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	3232(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	movq	4872(%rsp), %rsi        # 8-byte Reload
	vmovdqa	2784(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_676
# BB#675:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	2352(%rsp), %xmm6       # 16-byte Reload
.LBB147_676:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_678
# BB#677:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	2368(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5216(%rsp)       # 16-byte Spill
.LBB147_678:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	4192(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_680
# BB#679:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovaps	2384(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
.LBB147_680:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2800(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2816(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3264(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	2864(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2880(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3296(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2912(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2896(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3072(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3360(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5248(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4224(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3552(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_682
# BB#681:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	2400(%rsp), %xmm13      # 16-byte Reload
.LBB147_682:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3280(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3472(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3328(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3680(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3648(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3376(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3520(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_684
# BB#683:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vmovdqa	2496(%rsp), %xmm1       # 16-byte Reload
.LBB147_684:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_652 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2288(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rsi,%rax,4)
	movq	5280(%rsp), %r8         # 8-byte Reload
	addq	$1, %r8
	cmpl	2236(%rsp), %r8d        # 4-byte Folded Reload
	jne	.LBB147_652
.LBB147_685:                            # %end for gH.s0.v10.v10330
                                        #   in Loop: Header=BB147_650 Depth=3
	movl	2236(%rsp), %eax        # 4-byte Reload
	cmpl	1336(%rsp), %eax        # 4-byte Folded Reload
	movq	4664(%rsp), %r8         # 8-byte Reload
	movq	4744(%rsp), %r12        # 8-byte Reload
	movq	4888(%rsp), %r14        # 8-byte Reload
	jge	.LBB147_704
# BB#686:                               # %for gH.s0.v10.v10333.preheader
                                        #   in Loop: Header=BB147_650 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3344(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1776(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 3312(%rsp)        # 8-byte Spill
	movl	1116(%rsp), %ecx        # 4-byte Reload
	movl	1172(%rsp), %r15d       # 4-byte Reload
	movl	1360(%rsp), %eax        # 4-byte Reload
	movl	%eax, %esi
	.align	16, 0x90
.LBB147_687:                            # %for gH.s0.v10.v10333
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_650 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rsi, 4192(%rsp)        # 8-byte Spill
	movl	%ecx, 4224(%rsp)        # 4-byte Spill
	movl	3344(%rsp), %eax        # 4-byte Reload
	testl	%eax, %eax
	setne	%dil
	sete	%r13b
	movslq	%r15d, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	andl	$1, %r15d
	sete	%dl
	leaq	-1(%rcx), %r11
	imulq	%r8, %r11
	movq	%r11, 3872(%rsp)        # 8-byte Spill
	leaq	-3(%rcx), %r9
	imulq	%r8, %r9
	movq	%r9, 3904(%rsp)         # 8-byte Spill
	movl	%ecx, %ebx
	movq	%r14, %r10
	movq	%rsi, %r14
	movq	5312(%rsp), %rsi        # 8-byte Reload
	orl	%esi, %ebx
	testb	$1, %bl
	sete	5248(%rsp)              # 1-byte Folded Spill
	leaq	-2(%rcx), %rbx
	imulq	%r8, %rbx
	andb	%dil, %dl
	movq	4312(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%rbx), %rsi
	movq	%rsi, 3520(%rsp)        # 8-byte Spill
	andb	%r13b, %r15b
	testl	%ecx, %eax
	movq	%r8, %rsi
	setne	%r8b
	movq	%rcx, %r13
	imulq	%rsi, %r13
	leaq	(%rdi,%r13), %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	leaq	-4(%rcx), %rax
	imulq	%rsi, %rax
	leaq	(%r12,%rax), %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	leaq	(%r12,%r13), %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	leaq	(%r12,%rbx), %rcx
	movq	%rcx, 5280(%rsp)        # 8-byte Spill
	leaq	(%rax,%r10), %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	movslq	%r14d, %rax
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	movq	%rax, %rcx
	orq	$4, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	leaq	(%r13,%r10), %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	leaq	(%rbx,%r10), %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	movq	%rax, %r10
	orq	$6, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm14
	vxorps	%xmm12, %xmm12, %xmm12
	vmovdqa	%xmm14, %xmm9
	cmpl	$1, 104(%rbp)
	leaq	(%rdi,%r11), %r11
	movq	4928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r13d
	leaq	(%rdi,%r9), %r12
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3712(%rsp)        # 4-byte Spill
	je	.LBB147_689
# BB#688:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vpxor	%xmm9, %xmm9, %xmm9
.LBB147_689:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	movzbl	5248(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	movzbl	%r8b, %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %xmm10
	vmovdqa	%xmm10, 5248(%rsp)      # 16-byte Spill
	je	.LBB147_691
# BB#690:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vpxor	%xmm10, %xmm10, %xmm10
.LBB147_691:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vpbroadcastd	%xmm0, %xmm11
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm0
	vmovdqa	%xmm11, %xmm1
	movq	%rsi, %r8
	je	.LBB147_693
# BB#692:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_693:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	%xmm1, 3424(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm13
	vmovdqa	%xmm13, %xmm0
	je	.LBB147_695
# BB#694:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_695:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11,4), %rax
	movq	4736(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx,4), %rbx
	leaq	(%rbx,%rcx,4), %rdx
	vmovss	(%rdi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3328(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm5
	movslq	%r13d, %r9
	movq	5672(%rsp), %rax        # 8-byte Reload
	vmovups	12312(%rax,%r9,4), %xmm15
	vmovups	12328(%rax,%r9,4), %xmm0
	vshufps	$221, %xmm0, %xmm15, %xmm6 # xmm6 = xmm15[1,3],xmm0[1,3]
	vmovaps	5472(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm6, %xmm6
	vmovaps	5504(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm5, %xmm1
	leaq	(%rdi,%r12,4), %rdx
	leaq	(%rdx,%rcx,4), %rbx
	leaq	(%rbx,%rcx,4), %rsi
	vmovss	(%rdi,%r12,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rbx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm7, %xmm5
	vmovups	12304(%rax,%r9,4), %xmm6
	vmovups	12320(%rax,%r9,4), %xmm3
	vshufps	$221, %xmm3, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm3[1,3]
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm6
	movq	3520(%rsp), %rbx        # 8-byte Reload
	leaq	(%rdi,%rbx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %r11
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm7, %xmm1
	vshufps	$136, %xmm0, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm0[0,2]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	movq	3808(%rsp), %rbx        # 8-byte Reload
	leaq	(%rdi,%rbx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %r11
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%rcx, %rdx
	vmulps	%xmm1, %xmm7, %xmm1
	vshufps	$136, 12336(%rax,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm1, %xmm7
	cmpl	$0, 104(%rbp)
	je	.LBB147_697
# BB#696:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	%xmm9, %xmm11
.LBB147_697:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	%xmm14, %xmm9
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm7, %xmm7
	vaddps	%xmm5, %xmm6, %xmm5
	movq	4744(%rsp), %r12        # 8-byte Reload
	movq	4888(%rsp), %r14        # 8-byte Reload
	je	.LBB147_699
# BB#698:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	%xmm10, %xmm13
.LBB147_699:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovaps	%xmm6, 3808(%rsp)       # 16-byte Spill
	vmaxps	%xmm12, %xmm0, %xmm15
	vmaxps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3440(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm0
	vmovdqa	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3744(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	movq	%rdx, %rbx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	5216(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	movslq	3712(%rsp), %r11        # 4-byte Folded Reload
	vmovups	24592(%rax,%r11,4), %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovups	24608(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 3712(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm1
	movq	3600(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %r9
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm14, %xmm0
	vmovups	24624(%rax,%r11,4), %xmm6
	vmovaps	%xmm6, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm6[0,2]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm2
	movq	5280(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %r9
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm14, %xmm0
	vmovups	24600(%rax,%r11,4), %xmm6
	vmovaps	%xmm6, 3472(%rsp)       # 16-byte Spill
	vmovups	24616(%rax,%r11,4), %xmm7
	vmovaps	%xmm7, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm4, %xmm7, %xmm7
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm7
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	vfmsub213ps	%xmm2, %xmm0, %xmm7
	vsubps	%xmm1, %xmm7, %xmm0
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vbroadcastss	.LCPI147_20(%rip), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm5, %xmm10
	vmovdqa	%xmm9, %xmm13
	je	.LBB147_701
# BB#700:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	3424(%rsp), %xmm13      # 16-byte Reload
.LBB147_701:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	movq	3616(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	4256(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	32(%rax,%r10,4), %xmm11
	vshufps	$136, %xmm11, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm11[0,2]
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	movq	3648(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm4, %xmm5
	vmovups	48(%rax,%r10,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm11, %xmm3 # xmm3 = xmm11[0,2],xmm3[0,2]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	movq	3680(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%rbx, %rsi
	vmulps	%xmm5, %xmm4, %xmm4
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm7
	vmovaps	%xmm7, 3376(%rsp)       # 16-byte Spill
	vmovups	40(%rax,%r10,4), %xmm5
	vshufps	$136, %xmm5, %xmm7, %xmm7 # xmm7 = xmm7[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm4
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vsubps	%xmm2, %xmm4, %xmm9
	vfmadd213ps	%xmm10, %xmm14, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm10, %xmm14, %xmm9
	vmovaps	%xmm14, 3648(%rsp)      # 16-byte Spill
	vaddps	3520(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 3776(%rsp)      # 16-byte Spill
	vmovdqa	3440(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovdqa	3408(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm2
	vpsrad	$31, %xmm2, %xmm0
	vmovdqa	%xmm0, 3440(%rsp)       # 16-byte Spill
	je	.LBB147_703
# BB#702:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	vmovdqa	3360(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5248(%rsp)       # 16-byte Spill
.LBB147_703:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_687 Depth=4
	movq	3840(%rsp), %r10        # 8-byte Reload
	leaq	1(%r10), %r9
	imulq	%r8, %r9
	leaq	(%r12,%r9), %rcx
	leaq	(%rdi,%rcx,4), %rdx
	movq	%rsi, %rbx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %rax
	vmovss	(%rdi,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	5216(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3712(%rsp), %xmm13      # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm13, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm13[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm10      # 16-byte Reload
	vsubps	%xmm10, %xmm4, %xmm4
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3472(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	movq	3872(%rsp), %r11        # 8-byte Reload
	leaq	(%r12,%r11), %rcx
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	vmovss	(%rdi,%rcx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rsi,%rbx,4), %rcx
	vinsertps	$32, (%rsi,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm15, %xmm7
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm7
	vminps	%xmm8, %xmm4, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vfmsub213ps	%xmm7, %xmm0, %xmm2
	leaq	(%r9,%r14), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4256(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm4, %xmm14, %xmm4
	vshufps	$221, 3392(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm11[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3376(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm7, %xmm5 # xmm5 = xmm7[1,3],xmm5[1,3]
	leaq	(%r11,%r14), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm0, %xmm5
	vmovaps	3744(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm13[1,3]
	movq	3904(%rsp), %rdx        # 8-byte Reload
	leaq	(%r12,%rdx), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm15, %xmm4
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vmulps	3600(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	leaq	(%rdx,%r14), %rax
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm11[1,3]
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3648(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm2, %xmm1, %xmm0
	vfmadd213ps	%xmm2, %xmm1, %xmm4
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3776(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm1, %xmm6, %xmm12, %xmm2
	vmovaps	3440(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm9, %xmm2, %xmm2
	vmovaps	3808(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm5, %xmm3, %xmm12, %xmm5
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm1
	vmovaps	3520(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3552(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3312(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	4872(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	4192(%rsp), %rsi        # 8-byte Reload
	addl	$8, %esi
	addl	$8, %r10d
	movl	%r10d, %r15d
	movl	4224(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_687
.LBB147_704:                            # %end for gH.s0.v10.v10334
                                        #   in Loop: Header=BB147_650 Depth=3
	movl	1336(%rsp), %eax        # 4-byte Reload
	cmpl	2248(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_739
# BB#705:                               # %for gH.s0.v10.v10337.preheader
                                        #   in Loop: Header=BB147_650 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3904(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2176(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1776(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2168(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1168(%rsp), %ecx        # 4-byte Reload
	movl	1364(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_706:                            # %for gH.s0.v10.v10337
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_650 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r13, 3600(%rsp)        # 8-byte Spill
	movq	%rax, 5280(%rsp)        # 8-byte Spill
	movl	%ecx, 3776(%rsp)        # 4-byte Spill
	cmpl	$0, 3904(%rsp)          # 4-byte Folded Reload
	setne	4256(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	2152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r11d
	movl	%r11d, 3744(%rsp)       # 4-byte Spill
	movl	%r11d, %r14d
	andl	$1, %r14d
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	1944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3424(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	movq	1952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3520(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3472(%rsp)        # 4-byte Spill
	movq	1960(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	movq	2128(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	4192(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 4224(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$2, 3712(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3680(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	movq	2056(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm1
	vpinsrd	$1, 3488(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	movq	1968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$2, 3520(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3472(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	vmovd	%r8d, %xmm4
	vpinsrd	$1, %r9d, %xmm4, %xmm4
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, 3440(%rsp)        # 4-byte Spill
	vpinsrd	$2, %r12d, %xmm4, %xmm4
	vpinsrd	$3, %r10d, %xmm4, %xmm4
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	movq	2064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	movq	2072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vmovd	%r11d, %xmm2
	vpbroadcastd	%xmm2, %xmm13
	vpsrad	$31, %xmm1, %xmm7
	vmovdqa	2176(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm7, %xmm7
	vpaddd	%xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm4, %xmm7
	vpand	%xmm10, %xmm7, %xmm7
	vpaddd	%xmm4, %xmm7, %xmm4
	vmovdqa	5184(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm0, %xmm7
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm5, %xmm5
	vmovdqa	5408(%rsp), %xmm2       # 16-byte Reload
	vpmaxsd	%xmm2, %xmm5, %xmm5
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm14, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpsubd	%xmm1, %xmm9, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm5, %xmm0, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4224(%rsp)       # 16-byte Spill
	vmovdqa	5168(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	vmovdqa	5024(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm4, %xmm14, %xmm3
	vpsubd	%xmm4, %xmm9, %xmm5
	vblendvps	%xmm3, %xmm4, %xmm5, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	movq	5312(%rsp), %r10        # 8-byte Reload
	orl	%r10d, %eax
	testb	$1, %al
	movq	1976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vmovdqa	3712(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm4
	vpbroadcastd	3392(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm2, %xmm5, %xmm5
	vpcmpgtd	%xmm3, %xmm14, %xmm6
	vpsubd	%xmm3, %xmm9, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm4, %xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm3, %xmm1
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	sete	3520(%rsp)              # 1-byte Folded Spill
	movb	5248(%rsp), %r12b       # 1-byte Reload
	movb	4256(%rsp), %r9b        # 1-byte Reload
	andb	%r9b, %r12b
	vpaddd	%xmm1, %xmm8, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	movb	5216(%rsp), %r15b       # 1-byte Reload
	andb	%r15b, %r14b
	movl	%r14d, 5248(%rsp)       # 4-byte Spill
	movl	3904(%rsp), %r8d        # 4-byte Reload
	testl	%r11d, %r8d
	setne	3472(%rsp)              # 1-byte Folded Spill
	movq	2088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %ecx
	movl	%ecx, %r11d
	andl	$1, %r11d
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	sete	%r14b
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3424(%rsp)              # 4-byte Folded Reload
	vmovd	3616(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3648(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3552(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3680(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3456(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3488(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3440(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3408(%rsp), %xmm4, %xmm7 # 4-byte Folded Reload
	movq	2080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm4
	vpinsrd	$1, %esi, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm4
	vpaddd	%xmm11, %xmm13, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm2, %xmm6, %xmm6
	vpcmpgtd	%xmm0, %xmm14, %xmm1
	vpsubd	%xmm0, %xmm9, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm6, %xmm0, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpaddd	%xmm4, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	orl	%r10d, %eax
	testb	$1, %al
	sete	2608(%rsp)              # 1-byte Folded Spill
	andb	%r9b, %r14b
	movb	%r14b, 4256(%rsp)       # 1-byte Spill
	andb	%r15b, %r11b
	testl	%ecx, %r8d
	vmovd	%ecx, %xmm0
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm6
	vpsrad	$31, %xmm7, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm3
	vpsubd	%xmm1, %xmm9, %xmm7
	vblendvps	%xmm3, %xmm1, %xmm7, %xmm1
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm3, %xmm3
	vpbroadcastd	3680(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm2, %xmm7, %xmm7
	vpaddd	%xmm2, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm2, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm7, %xmm1, %xmm1
	vpsrad	$31, %xmm5, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm9, %xmm7
	vblendvps	%xmm5, %xmm3, %xmm7, %xmm3
	vmovdqa	5104(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm5, %xmm5
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm1, %xmm1
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm2
	setne	%cl
	vmovq	%xmm2, %r14
	movq	%r14, 2368(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 2384(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm2, %rsi
	movq	%rsi, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vmovdqa	3712(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm2
	vmovq	%xmm2, %r8
	movq	%r8, 2440(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm2, %r13
	movq	%r13, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vmovdqa	5488(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2496(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2528(%rsp)        # 8-byte Spill
	movq	2112(%rsp), %rdx        # 8-byte Reload
	movq	5280(%rsp), %rbx        # 8-byte Reload
	leal	(%rdx,%rbx), %edi
	movslq	%edi, %rdi
	movq	%rdi, %rdx
	orq	$4, %rdx
	movq	%rdx, 2512(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2544(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2576(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2592(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2688(%rsp)        # 8-byte Spill
	movq	%rdi, %rdx
	orq	$6, %rdx
	movq	%rdx, 2704(%rsp)        # 8-byte Spill
	vmovdqa	4192(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2880(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2896(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	vmovdqa	4224(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3680(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm11
	vmovaps	%xmm11, 2640(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	movq	2096(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rbx), %r10d
	movq	2104(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rbx), %edx
	movl	%edx, 2352(%rsp)        # 4-byte Spill
	je	.LBB147_708
# BB#707:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_708:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	movzbl	3520(%rsp), %r12d       # 1-byte Folded Reload
	vmovd	%r12d, %xmm1
	movzbl	3472(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_710
# BB#709:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_710:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm1
	movq	4872(%rsp), %r12        # 8-byte Reload
	je	.LBB147_712
# BB#711:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_712:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm0, 2256(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_714
# BB#713:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_714:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm0, 2272(%rsp)       # 16-byte Spill
	movzbl	4256(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_716
# BB#715:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_716:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm1, 2288(%rsp)       # 16-byte Spill
	movzbl	2608(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	je	.LBB147_718
# BB#717:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_718:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm2, 2304(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5216(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_720
# BB#719:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_720:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2320(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_722
# BB#721:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_722:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm0, 2336(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2816(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2752(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	movslq	%r10d, %rcx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm5
	vmovups	12328(%rdx,%rcx,4), %xmm6
	vmovups	12304(%rdx,%rcx,4), %xmm7
	vmovups	12320(%rdx,%rcx,4), %xmm0
	vshufps	$136, 12336(%rdx,%rcx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,2],mem[0,2]
	vmovaps	3872(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm4 # xmm4 = xmm5[1,3],xmm6[1,3]
	vmovaps	5472(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm4, %xmm4
	vmovaps	5504(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm4, %xmm6
	vmulps	%xmm14, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm1, %xmm7
	vsubps	%xmm2, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_724
# BB#723:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm11, 4224(%rsp)      # 16-byte Spill
.LBB147_724:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3872(%rsp), %xmm15, %xmm14 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_726
# BB#725:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	%xmm8, 4192(%rsp)       # 16-byte Spill
.LBB147_726:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	movq	2368(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2384(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2400(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rsi,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2440(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r13,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2352(%rsp), %rax        # 4-byte Folded Reload
	movq	%rdx, %rsi
	vmovups	24592(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vmovups	24608(%rsi,%rax,4), %xmm13
	vmovups	24624(%rsi,%rax,4), %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rax,4), %xmm2
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 2816(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovdqa	2640(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_728
# BB#727:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	2256(%rsp), %xmm2       # 16-byte Reload
.LBB147_728:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	movq	2464(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rcx
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2496(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2480(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2528(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2512(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	movq	2544(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2576(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2592(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2672(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2688(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%rcx, %rdx
	movq	2704(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3312(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rdi,4), %xmm10
	vmovups	48(%rsi,%rdi,4), %xmm9
	vmovaps	%xmm9, 3264(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rdi,4), %xmm14
	vmovaps	%xmm14, 3296(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3808(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2816(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	2800(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	vmovdqa	2608(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_730
# BB#729:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	2272(%rsp), %xmm6       # 16-byte Reload
.LBB147_730:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_732
# BB#731:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	2288(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5216(%rsp)       # 16-byte Spill
.LBB147_732:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	4192(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_734
# BB#733:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovaps	2304(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
.LBB147_734:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2864(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2880(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2896(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2832(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	2912(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3072(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3280(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3344(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5248(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4224(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3520(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_736
# BB#735:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	2320(%rsp), %xmm13      # 16-byte Reload
.LBB147_736:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3264(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3456(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3312(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3648(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3616(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3360(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3472(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_738
# BB#737:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vmovdqa	2336(%rsp), %xmm1       # 16-byte Reload
.LBB147_738:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_706 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3744(%rsp), %rax        # 4-byte Folded Reload
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r12,%rax,4)
	movq	5280(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	3600(%rsp), %r13        # 8-byte Reload
	addl	$8, %r13d
	movl	3776(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_706
.LBB147_739:                            # %end for gH.s0.v10.v10338
                                        #   in Loop: Header=BB147_650 Depth=3
	movl	1440(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 1440(%rsp)        # 4-byte Spill
	addq	$1, 5312(%rsp)          # 8-byte Folded Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1360(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1364(%rsp)        # 4-byte Folded Spill
	cmpl	1260(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_650
.LBB147_740:                            # %end for gH.s0.v11328
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	2240(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1428(%rsp)        # 4-byte Folded Reload
	jle	.LBB147_778
# BB#741:                               #   in Loop: Header=BB147_467 Depth=2
	movq	1680(%rsp), %rax        # 8-byte Reload
	movq	928(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rax), %eax
	imull	1832(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 2168(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_742:                            # %for gH.s0.v11341
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_744 Depth 4
	cmpl	$0, 2248(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_777
# BB#743:                               # %for gH.s0.v10.v10344.preheader
                                        #   in Loop: Header=BB147_742 Depth=3
	movl	2240(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %edi
	movl	%r8d, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	andl	$1, %edi
	movl	%edi, 4192(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1804(%rsp), %ebx        # 4-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1848(%rsp), %rdi        # 8-byte Reload
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%esi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movq	1808(%rsp), %rax        # 8-byte Reload
	cmpl	%r8d, %eax
	cmovgl	%ecx, %edx
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2256(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	imulq	1776(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2176(%rsp)        # 8-byte Spill
	movl	2248(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %r10        # 8-byte Reload
	movl	2168(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_744:                            # %for gH.s0.v10.v10344
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_742 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 4192(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r10d, %r15d
	andl	$1, %r15d
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	3144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4224(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3472(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	movq	3128(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3136(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vmovd	4224(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4256(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3776(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3744(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3520(%rsp)       # 16-byte Spill
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vmovd	%r14d, %xmm0
	vpinsrd	$1, 3616(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3600(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r8d, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r13d, %xmm2, %xmm2
	vpinsrd	$2, %r11d, %xmm2, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm2, %xmm5
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	leal	-3(%r10), %eax
	vmovd	%eax, %xmm11
	vmovd	%r10d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2256(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm3, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5408(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm1
	vmovdqa	%xmm1, 4256(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm0
	vmovdqa	5168(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm1, %xmm7, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovdqa	5024(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm11, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4224(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	movl	%r10d, %eax
	movl	2240(%rsp), %r14d       # 4-byte Reload
	orl	%r14d, %eax
	testb	$1, %al
	movq	3152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vmovdqa	3520(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm9, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vpbroadcastd	3776(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm12, %xmm3, %xmm1
	vmovdqa	%xmm1, 3776(%rsp)       # 16-byte Spill
	sete	3600(%rsp)              # 1-byte Folded Spill
	movb	5280(%rsp), %r13b       # 1-byte Reload
	movb	5216(%rsp), %r9b        # 1-byte Reload
	andb	%r9b, %r13b
	vpaddd	%xmm1, %xmm7, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movb	5248(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r15b
	movl	%r15d, 5280(%rsp)       # 4-byte Spill
	movl	4192(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3520(%rsp)              # 1-byte Folded Spill
	leal	1(%r10), %ebx
	movl	%ebx, %r12d
	andl	$1, %r12d
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	sete	%r15b
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3472(%rsp)              # 4-byte Folded Reload
	vmovd	3680(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3712(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3648(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3744(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3552(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3488(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3456(%rsp), %xmm4, %xmm3 # 4-byte Folded Reload
	leal	-4(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %esi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm0, %xmm14, %xmm6
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm6, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm2, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm6
	vpaddd	%xmm6, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	movl	%ebx, %eax
	orl	%r14d, %eax
	testb	$1, %al
	sete	2640(%rsp)              # 1-byte Folded Spill
	andb	%r9b, %r15b
	movb	%r15b, 5216(%rsp)       # 1-byte Spill
	andb	%r11b, %r12b
	testl	%ebx, %r8d
	vmovd	%ebx, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm11
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm0, %xmm0
	vmovdqa	5008(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	4832(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm7
	vpor	%xmm1, %xmm7, %xmm1
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	3744(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm1
	vpsrad	$31, %xmm5, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm0
	vmovdqa	5104(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm2
	vpor	%xmm5, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	5248(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm2, %xmm0, %xmm5, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm2
	setne	%r11b
	vmovq	%xmm2, %r15
	movq	%r15, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r13
	movq	%r13, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vpaddd	%xmm6, %xmm3, %xmm2
	vmovq	%xmm2, %r8
	movq	%r8, 2416(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 2440(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vmovdqa	3776(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovq	%xmm2, %rdx
	movq	%rdx, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rax
	movq	%rax, %rcx
	orq	$4, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	vpaddd	%xmm6, %xmm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2576(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2608(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2592(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2624(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	movq	%rax, %rcx
	orq	$6, %rcx
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	vmovdqa	4224(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2880(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2912(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3072(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3200(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm11, %xmm11
	vmovaps	%xmm11, 2672(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movq	4936(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rdi), %esi
	movl	%esi, 2368(%rsp)        # 4-byte Spill
	je	.LBB147_746
# BB#745:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_746:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	movzbl	3600(%rsp), %r14d       # 1-byte Folded Reload
	vmovd	%r14d, %xmm1
	movzbl	3520(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_748
# BB#747:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_748:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	movl	5280(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movq	5672(%rsp), %rsi        # 8-byte Reload
	je	.LBB147_750
# BB#749:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_750:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm0, 2272(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	je	.LBB147_752
# BB#751:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_752:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_754
# BB#753:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_754:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm1, 2304(%rsp)       # 16-byte Spill
	movzbl	2640(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	movzbl	%r11b, %ebx
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	je	.LBB147_756
# BB#755:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_756:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm2, 2320(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	movzbl	%r12b, %ebx
	vmovd	%ebx, %xmm1
	je	.LBB147_758
# BB#757:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_758:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2336(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	4872(%rsp), %r11        # 8-byte Reload
	je	.LBB147_760
# BB#759:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_760:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm0, 2352(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	movq	5528(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3376(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3392(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdi,%rbx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3296(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3312(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3328(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdi,%rbx,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	2816(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2832(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2848(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdi,%rbx,4), %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],mem[0]
	movq	2752(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2768(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2784(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdi,%rbx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movq	%rdi, %rbx
	movslq	%ecx, %rcx
	vmovups	12312(%rsi,%rcx,4), %xmm5
	vmovups	12328(%rsi,%rcx,4), %xmm6
	vmovups	12304(%rsi,%rcx,4), %xmm7
	vmovups	12320(%rsi,%rcx,4), %xmm0
	vshufps	$136, 12336(%rsi,%rcx,4), %xmm0, %xmm15 # xmm15 = xmm0[0,2],mem[0,2]
	vmovaps	3904(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm6[1,3]
	vmovaps	5472(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	5504(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm1, %xmm6
	vmulps	%xmm4, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm3, %xmm7
	vsubps	%xmm2, %xmm15, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_762
# BB#761:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm11, 4256(%rsp)      # 16-byte Spill
.LBB147_762:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3904(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_764
# BB#763:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	%xmm8, 4224(%rsp)       # 16-byte Spill
.LBB147_764:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	movq	2384(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2400(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%r13,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2416(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2448(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2464(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2480(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2368(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24592(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	vmovups	24608(%rsi,%rcx,4), %xmm13
	vmovups	24624(%rsi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 2848(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vmovaps	3872(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	vmovdqa	2672(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_766
# BB#765:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	2272(%rsp), %xmm2       # 16-byte Reload
.LBB147_766:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	movq	2496(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rbx, %rdx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2528(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2512(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2544(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2560(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 3392(%rsp)       # 16-byte Spill
	movq	2576(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2608(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2592(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2624(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2656(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2704(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2688(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm8
	vmovaps	%xmm8, 3344(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm10
	vmovups	48(%rsi,%rax,4), %xmm9
	vmovaps	%xmm9, 3296(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rax,4), %xmm14
	vmovaps	%xmm14, 3328(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3840(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2848(%rsp), %xmm3       # 16-byte Reload
	vmulps	5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	2816(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	2832(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	vmovdqa	2640(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_768
# BB#767:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	2288(%rsp), %xmm6       # 16-byte Reload
.LBB147_768:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_770
# BB#769:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	2304(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5248(%rsp)       # 16-byte Spill
.LBB147_770:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	4224(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_772
# BB#771:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovaps	2320(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
.LBB147_772:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2880(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2896(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2912(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3072(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3872(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2864(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3312(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3376(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5280(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3600(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_774
# BB#773:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	2336(%rsp), %xmm13      # 16-byte Reload
.LBB147_774:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3472(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3296(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3344(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3680(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3392(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3776(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3520(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_776
# BB#775:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vmovdqa	2352(%rsp), %xmm1       # 16-byte Reload
.LBB147_776:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_744 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r10d, %rax
	movq	2176(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r11,%rax,4)
	movq	5312(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r10d
	movl	3808(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_744
.LBB147_777:                            # %end for gH.s0.v10.v10345
                                        #   in Loop: Header=BB147_742 Depth=3
	movl	2240(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2240(%rsp)        # 4-byte Spill
	movl	2168(%rsp), %ecx        # 4-byte Reload
	addl	1832(%rsp), %ecx        # 4-byte Folded Reload
	movl	%ecx, 2168(%rsp)        # 4-byte Spill
	cmpl	1428(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_742
.LBB147_778:                            # %produce gV349
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1816(%rsp), %rax        # 8-byte Reload
	movq	1072(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %edx
	cmovgel	%eax, %edx
	addl	$2, %edx
	movl	1428(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	movl	%edx, 2640(%rsp)        # 4-byte Spill
	leal	3(%rcx), %ecx
	movl	%ecx, 2384(%rsp)        # 4-byte Spill
	movl	1044(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	%rcx, 2864(%rsp)        # 8-byte Spill
	movl	1436(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	%edx, %eax
	movl	2248(%rsp), %ebx        # 4-byte Reload
	jl	.LBB147_779
	jmp	.LBB147_781
.LBB147_780:                            # %for gV.s0.v11351.end for gV.s0.v10.v10354_crit_edge
                                        #   in Loop: Header=BB147_779 Depth=3
	addl	$1, %r8d
	movl	%r8d, %eax
	jmp	.LBB147_842
	.align	16, 0x90
.LBB147_779:                            # %for gV.s0.v11351
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_824 Depth 4
	testl	%ebx, %ebx
	jle	.LBB147_780
# BB#823:                               # %for gV.s0.v10.v10353.preheader
                                        #   in Loop: Header=BB147_779 Depth=3
	movq	%r8, 2880(%rsp)         # 8-byte Spill
	movl	%r8d, %ebx
	movq	1816(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ebx
	leal	-1(%rbx), %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edi
	movl	%ecx, %r11d
	subl	%eax, %edi
	movq	1848(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %esi
	cmovgl	%r8d, %esi
	addl	$-1, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ecx
	cmovll	%edi, %esi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r8d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %ecx
	cmovlel	%edx, %ebx
	movl	%r8d, %ecx
	subl	%r9d, %ecx
	cmovll	%edx, %ebx
	cmovlel	%edi, %esi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r12d, %edi
	addl	%edx, %edi
	movl	%r11d, %eax
	subl	%edi, %eax
	cmpl	%edi, %r10d
	cmovgl	%edi, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	leal	1(%r8), %edi
	movl	%edi, 2624(%rsp)        # 4-byte Spill
	cmpl	%edi, %r13d
	movl	%r13d, %edx
	cmovgl	%edi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%eax, %edx
	movl	%r8d, %edi
	andl	$1, %edi
	movl	%edi, 5248(%rsp)        # 4-byte Spill
	movslq	%ebx, %r11
	movq	1880(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2848(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rbx
	movq	%rbx, 5312(%rsp)        # 8-byte Spill
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5312(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r11), %r12
	movl	1860(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %edx
	subl	%esi, %edx
	cmpl	%esi, %r10d
	cmovgl	%esi, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	leal	2(%r8), %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %esi
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rax,%rdx), %r15
	subl	%ecx, %ebx
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	leal	-2(%r8), %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ebx, %ecx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %ecx
	movslq	%ecx, %rcx
	imulq	%r14, %rcx
	leaq	(%rax,%rcx), %rbx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r15,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1776(%rsp), %rcx        # 8-byte Folded Reload
	movq	1576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2784(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2768(%rsp)        # 8-byte Spill
	subq	4760(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	movq	1584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1680(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2704(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	xorl	%r8d, %r8d
	movl	2248(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_824:                            # %for gV.s0.v10.v10353
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_779 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5248(%rsp)          # 4-byte Folded Reload
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %r9d
	movl	%r9d, 3776(%rsp)        # 4-byte Spill
	movl	%r9d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r15d
	cltd
	idivl	%r15d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vmovd	%esi, %xmm0
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	vpinsrd	$1, %r10d, %xmm0, %xmm0
	vpinsrd	$2, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r14d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2848(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r9d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5392(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	movq	2880(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	sete	%r10b
	andb	5280(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5312(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5248(%rsp), %r9d        # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r12
	movq	%r12, %r13
	sarq	$32, %r13
	vpextrq	$1, %xmm2, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm0, %r9
	movq	%r9, 3248(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	movq	2784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	movq	2768(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %r14d
	movq	2656(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3184(%rsp)        # 4-byte Spill
	movq	2752(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	movq	2736(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3280(%rsp)        # 4-byte Spill
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3296(%rsp)        # 4-byte Spill
	je	.LBB147_826
# BB#825:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_826:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	movzbl	%r10b, %r10d
	vmovd	%r10d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_828
# BB#827:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_828:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5312(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_830
# BB#829:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_830:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	%xmm3, 5312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3072(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_832
# BB#831:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_832:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r14d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movslq	3184(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r12d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r11d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5472(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2816(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5312(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_834
# BB#833:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovdqa	2912(%rsp), %xmm9       # 16-byte Reload
.LBB147_834:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5472(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3312(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_836
# BB#835:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovdqa	2896(%rsp), %xmm7       # 16-byte Reload
.LBB147_836:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4224(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5312(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5216(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3424(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_838
# BB#837:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovdqa	3072(%rsp), %xmm6       # 16-byte Reload
.LBB147_838:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	3472(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3472(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	je	.LBB147_840
# BB#839:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5280(%rsp)       # 16-byte Spill
.LBB147_840:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_824 Depth=4
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3488(%rsp), %xmm11      # 16-byte Reload
	vmulps	4224(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4192(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vmulps	3872(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3472(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3840(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3408(%rsp), %xmm2       # 16-byte Reload
	vmulps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3600(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5312(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2800(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_824
# BB#841:                               #   in Loop: Header=BB147_779 Depth=3
	movl	2624(%rsp), %eax        # 4-byte Reload
.LBB147_842:                            # %end for gV.s0.v10.v10354
                                        #   in Loop: Header=BB147_779 Depth=3
	movl	%eax, %r8d
	cmpl	2640(%rsp), %eax        # 4-byte Folded Reload
	movl	2248(%rsp), %ebx        # 4-byte Reload
	jne	.LBB147_779
.LBB147_781:                            # %end for gV.s0.v11352
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	2864(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, 2640(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_863
# BB#782:                               #   in Loop: Header=BB147_467 Depth=2
	movl	940(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	jmp	.LBB147_783
.LBB147_822:                            # %end for gV.s0.v10.v10364.end for gV.s0.v10.v10368_crit_edge
                                        #   in Loop: Header=BB147_783 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	jmp	.LBB147_862
	.align	16, 0x90
.LBB147_783:                            # %for gV.s0.v11357
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_785 Depth 4
                                        #         Child Loop BB147_804 Depth 4
                                        #         Child Loop BB147_844 Depth 4
	cmpl	$0, 1320(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_802
# BB#784:                               # %for gV.s0.v10.v10359.preheader
                                        #   in Loop: Header=BB147_783 Depth=3
	movq	5248(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2848(%rsp)       # 16-byte Spill
	movq	%r10, %rax
	movq	1880(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rbx
	imulq	%rbx, %rax
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rdi
	leaq	1(%r10), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rcx), %rdx
	leaq	-1(%r10), %rsi
	imulq	%rbx, %rsi
	leaq	(%rsi,%rcx), %rsi
	movq	1888(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rcx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rax), %r8
	leaq	2(%r10), %rdx
	imulq	%rbx, %rdx
	leaq	(%rsi,%rdx), %r9
	leaq	-2(%r10), %rdi
	imulq	%rbx, %rdi
	leaq	(%rsi,%rdi), %rbx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rdi,%rsi), %rdi
	vbroadcastss	(%rcx,%rbx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r9,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r8,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdi,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movl	%r10d, %ecx
	andl	$63, %ecx
	imulq	1776(%rsp), %rcx        # 8-byte Folded Reload
	movq	1568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %edi
	movq	1752(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %edi
	movq	%rdi, 2784(%rsp)        # 8-byte Spill
	movq	1560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %ebx
	imull	%esi, %ebx
	movq	%rbx, 2768(%rsp)        # 8-byte Spill
	subq	4760(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdx
	leal	(%rdx,%rdi), %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	1552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	imull	%esi, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	movq	1544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	imull	%esi, %eax
	movq	1536(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movq	%rdi, 2704(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	xorl	%r8d, %r8d
	movq	1192(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_785:                            # %for gV.s0.v10.v10359
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_783 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	cmpl	$0, 5216(%rsp)          # 4-byte Folded Reload
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %r14d
	movl	%r14d, 3744(%rsp)       # 4-byte Spill
	movl	%r14d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r15d
	cltd
	idivl	%r15d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r11d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2848(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5392(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	sete	%r11b
	andb	5280(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5312(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5216(%rsp), %r14d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r15
	movq	%r15, %r9
	sarq	$32, %r9
	vpextrq	$1, %xmm2, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpextrq	$1, %xmm0, %r14
	movq	%r14, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	movq	2784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	movq	2768(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %r13d
	movq	2656(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3168(%rsp)        # 4-byte Spill
	movq	2752(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3248(%rsp)        # 4-byte Spill
	movq	2736(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3280(%rsp)        # 4-byte Spill
	je	.LBB147_787
# BB#786:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_787:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	movzbl	%r11b, %r11d
	vmovd	%r11d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_789
# BB#788:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_789:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	%xmm1, 2880(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5312(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_791
# BB#790:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_791:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	%xmm3, 5312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2912(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3712(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	je	.LBB147_793
# BB#792:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_793:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3456(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r13d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3472(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	movslq	3168(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r15d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r12d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5472(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2816(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	4256(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5312(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_795
# BB#794:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovdqa	2896(%rsp), %xmm9       # 16-byte Reload
.LBB147_795:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5472(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovaps	4224(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3712(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3296(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_797
# BB#796:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovdqa	2880(%rsp), %xmm7       # 16-byte Reload
.LBB147_797:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	3408(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3248(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4192(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5312(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	4256(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_799
# BB#798:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovdqa	2912(%rsp), %xmm6       # 16-byte Reload
.LBB147_799:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm5       # 16-byte Reload
	vmulps	3872(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3312(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3808(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3456(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	je	.LBB147_801
# BB#800:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovdqa	3072(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5280(%rsp)       # 16-byte Spill
.LBB147_801:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_785 Depth=4
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3472(%rsp), %xmm11      # 16-byte Reload
	vmulps	4192(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	3904(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3680(%rsp), %xmm7       # 16-byte Reload
	vmulps	3840(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3808(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4224(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vmulps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3520(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3872(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3600(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3552(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3712(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5312(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3744(%rsp), %rax        # 4-byte Folded Reload
	movq	2800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	movl	3776(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	cmpl	$-1, %eax
	jne	.LBB147_785
.LBB147_802:                            # %end for gV.s0.v10.v10360
                                        #   in Loop: Header=BB147_783 Depth=3
	movl	1316(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1320(%rsp)        # 4-byte Folded Reload
	movq	4664(%rsp), %r8         # 8-byte Reload
	movq	4888(%rsp), %rdx        # 8-byte Reload
	jge	.LBB147_821
# BB#803:                               # %for gV.s0.v10.v10363.preheader
                                        #   in Loop: Header=BB147_783 Depth=3
	movq	5248(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %eax
	movq	%r14, %r11
	movq	1880(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rbx
	imulq	%rbx, %r11
	andl	$1, %eax
	movl	%eax, 3280(%rsp)        # 4-byte Spill
	leaq	1(%r14), %rcx
	imulq	%rbx, %rcx
	movq	1840(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r9
	leaq	-1(%r14), %rsi
	imulq	%rbx, %rsi
	leaq	(%rcx,%rdi), %rcx
	leaq	(%rsi,%rdi), %rsi
	movq	1888(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r9,4), %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	leaq	2(%r14), %rcx
	imulq	%rbx, %rcx
	movq	1872(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r9
	leaq	-2(%r14), %rsi
	imulq	%rbx, %rsi
	leaq	(%rdi,%rcx), %r10
	leaq	(%rdi,%rsi), %rbx
	movq	1864(%rsp), %rdi        # 8-byte Reload
	leaq	(%r11,%rdi), %r11
	leaq	(%rcx,%rdi), %rcx
	leaq	(%rsi,%rdi), %rsi
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r9,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r14d, %eax
	andl	$63, %eax
	imulq	1776(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movq	1568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %ecx
	movq	1752(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %ecx
	movq	2216(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rcx), %eax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	movq	1560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	addl	$-8, %ecx
	movq	%rcx, 3200(%rsp)        # 8-byte Spill
	leal	(%rsi,%rax), %ecx
	movq	%rcx, 3072(%rsp)        # 8-byte Spill
	addl	$-8, %eax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	movq	1552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	movq	2224(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	movq	1544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	movq	1536(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r14), %ebx
	imull	%edi, %ebx
	movq	%rdx, %rdi
	leal	(%rax,%rcx), %eax
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	addl	$-8, %ebx
	movq	%rbx, 2896(%rsp)        # 8-byte Spill
	movl	1112(%rsp), %ecx        # 4-byte Reload
	movl	1164(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r13d
	.align	16, 0x90
.LBB147_804:                            # %for gV.s0.v10.v10363
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_783 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	movl	3280(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	setne	%r9b
	sete	%r10b
	movq	3064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r15d
	movslq	%r15d, %r14
	movq	%r14, 3776(%rsp)        # 8-byte Spill
	andl	$1, %r15d
	sete	%al
	leaq	-2(%r14), %rsi
	imulq	%r8, %rsi
	leaq	(%rdi,%rsi), %r11
	movq	4312(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rsi), %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	movq	4744(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx), %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	movl	%r14d, %esi
	movq	%rdi, %rdx
	movq	5248(%rsp), %rdi        # 8-byte Reload
	orl	%edi, %esi
	testb	$1, %sil
	movq	%r8, %rsi
	sete	%r8b
	andb	%r9b, %al
	andb	%r10b, %r15b
	testl	%r14d, %r12d
	setne	%r10b
	leaq	-1(%r14), %rdi
	imulq	%rsi, %rdi
	leaq	(%rdx,%rdi), %r12
	leaq	(%rbx,%rdi), %r14
	leaq	(%rdi,%rcx), %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	movq	3200(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ebx
	movslq	%ebx, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	movq	3168(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ebx
	movslq	%ebx, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movq	2896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movq	2880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movq	2832(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movq	3184(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3440(%rsp)        # 4-byte Spill
	movq	3072(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	movq	2848(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3472(%rsp)        # 4-byte Spill
	je	.LBB147_806
# BB#805:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_806:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movzbl	%r8b, %ebx
	vmovd	%ebx, %xmm0
	movzbl	%r10b, %ebx
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_808
# BB#807:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_808:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movzbl	%r15b, %ebx
	vmovd	%ebx, %xmm0
	vmovaps	%xmm3, %xmm1
	movq	%rsi, %r8
	je	.LBB147_810
# BB#809:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_810:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	%xmm3, 5312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_812
# BB#811:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_812:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r11,4), %rbx
	movq	4736(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rdx,4), %rbx
	vinsertps	$16, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rbx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rbx,4), %xmm15
	vmovups	12328(%rdx,%rbx,4), %xmm7
	cltq
	vmovups	12312(%rdx,%rax,4), %xmm4
	vmovups	12328(%rdx,%rax,4), %xmm9
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movslq	%ecx, %rax
	vmovups	12312(%rdx,%rax,4), %xmm1
	vmovups	12328(%rdx,%rax,4), %xmm3
	vmovss	(%rdi,%r12,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r12,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm5, %xmm0
	vshufps	$136, %xmm7, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm7[0,2]
	vmovaps	5472(%rsp), %xmm10      # 16-byte Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmovaps	5504(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm5, %xmm0
	vshufps	$136, %xmm9, %xmm4, %xmm6 # xmm6 = xmm4[0,2],xmm9[0,2]
	vmovaps	%xmm4, %xmm2
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm8
	vmovss	(%rdi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r14,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vshufps	$221, %xmm7, %xmm15, %xmm4 # xmm4 = xmm15[1,3],xmm7[1,3]
	vmulps	%xmm13, %xmm12, %xmm6
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm6, %xmm4, %xmm15
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm9[1,3]
	vmulps	%xmm13, %xmm14, %xmm6
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vshufps	$136, %xmm3, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm14, %xmm0, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm3, %xmm3
	vminps	%xmm14, %xmm8, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm8
	vmovaps	3232(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm5, %xmm5
	vsubps	%xmm10, %xmm7, %xmm0
	vmulps	%xmm0, %xmm11, %xmm7
	cmpl	$0, 104(%rbp)
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_814
# BB#813:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovdqa	3392(%rsp), %xmm6       # 16-byte Reload
.LBB147_814:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmulps	%xmm7, %xmm5, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm15, %xmm10
	vminps	%xmm14, %xmm2, %xmm9
	vsubps	5472(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm8, %xmm3, %xmm8
	movq	4888(%rsp), %rdi        # 8-byte Reload
	je	.LBB147_816
# BB#815:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_816:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	3376(%rsp), %xmm4       # 16-byte Reload
	vmulps	4256(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3440(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmulps	4224(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	movslq	3456(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	4192(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	movslq	3472(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm15
	vfmsub213ps	%xmm1, %xmm15, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm10, %xmm4
	vmaxps	%xmm5, %xmm9, %xmm3
	vmulps	%xmm13, %xmm12, %xmm1
	vmulps	5504(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm13
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovdqa	5216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm9
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm8, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm6
	vmovdqa	3744(%rsp), %xmm12      # 16-byte Reload
	je	.LBB147_818
# BB#817:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovdqa	3296(%rsp), %xmm12      # 16-byte Reload
.LBB147_818:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vaddps	%xmm4, %xmm3, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	3520(%rsp), %xmm4       # 16-byte Reload
	vmulps	3904(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3648(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm2[0,2]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	3872(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm11
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm11, %xmm5 # xmm5 = xmm11[0,2],xmm5[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	3840(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm10
	vshufps	$136, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm10[0,2]
	vsubps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm3, %xmm3
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm0, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm15, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm0, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm8
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm6, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm7, %xmm6, %xmm8
	vpsrad	$31, %xmm13, %xmm1
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm9, %xmm0
	vmovdqa	%xmm0, 3680(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm12, %xmm1
	vpsrad	$31, %xmm1, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	je	.LBB147_820
# BB#819:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5280(%rsp)       # 16-byte Spill
.LBB147_820:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_804 Depth=4
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm1, %xmm9, %xmm1
	vmovaps	3488(%rsp), %xmm2       # 16-byte Reload
	vmulps	4224(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	4192(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vpxor	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm1, %xmm1
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vfmsub213ps	%xmm1, %xmm15, %xmm4
	vshufps	$221, 3408(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm1
	vmovaps	3552(%rsp), %xmm11      # 16-byte Reload
	vmulps	3872(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3344(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm10[1,3]
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	3840(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vfmsub213ps	%xmm1, %xmm15, %xmm5
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vmovaps	3744(%rsp), %xmm2       # 16-byte Reload
	vmulps	3472(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3600(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3648(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vsubps	%xmm4, %xmm5, %xmm4
	vfmadd213ps	%xmm2, %xmm6, %xmm1
	vfmadd213ps	%xmm2, %xmm6, %xmm4
	vmovdqa	5280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm0, %xmm7, %xmm12, %xmm2
	vmovaps	3520(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm8, %xmm2, %xmm2
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm5, %xmm12, %xmm3
	vblendvps	%xmm0, %xmm4, %xmm3, %xmm0
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, 5312(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm7, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3216(%rsp), %rax        # 8-byte Reload
	movq	3776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	3808(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_804
.LBB147_821:                            # %end for gV.s0.v10.v10364
                                        #   in Loop: Header=BB147_783 Depth=3
	movl	2248(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, 1316(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_822
# BB#843:                               # %for gV.s0.v10.v10367.preheader
                                        #   in Loop: Header=BB147_783 Depth=3
	movq	5248(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %edx
	movl	%r9d, %ecx
	movq	1688(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	leal	8(%rcx), %eax
	movq	1712(%rsp), %r11        # 8-byte Reload
	imull	%r11d, %eax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	andl	$1, %edx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2816(%rsp)       # 16-byte Spill
	movq	%r9, %r8
	movq	1880(%rsp), %rbx        # 8-byte Reload
	imulq	%rbx, %r8
	leal	9(%rcx), %r14d
	imull	%r11d, %r14d
	movq	1840(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r8), %r10
	leaq	1(%r9), %rdx
	movq	%rdx, 2704(%rsp)        # 8-byte Spill
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rdx
	leaq	-1(%r9), %rsi
	imulq	%rbx, %rsi
	leaq	(%rsi,%rdi), %rsi
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r10,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	leaq	2(%r9), %rax
	imulq	%rbx, %rax
	movq	%r9, %rdx
	addq	$-2, %rdx
	imulq	%rbx, %rdx
	movq	1872(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdx), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	leaq	(%rbx,%rax), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	(%rbx,%r8), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	leal	7(%rcx), %ebx
	imull	%r11d, %ebx
	movq	4928(%rsp), %rsi        # 8-byte Reload
	addl	%esi, %r14d
	movq	%r14, 2800(%rsp)        # 8-byte Spill
	addl	%esi, %ebx
	movq	%rbx, 2752(%rsp)        # 8-byte Spill
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	leal	10(%rcx), %edx
	imull	%r11d, %edx
	movq	%rdx, 2736(%rsp)        # 8-byte Spill
	addl	$6, %ecx
	imull	%r11d, %ecx
	movq	%rcx, 2848(%rsp)        # 8-byte Spill
	leaq	(%r8,%rsi), %rdx
	leaq	(%rax,%rsi), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	andl	$63, %r9d
	imulq	1776(%rsp), %r9         # 8-byte Folded Reload
	subq	4760(%rsp), %r9         # 8-byte Folded Reload
	movq	%r9, 2720(%rsp)         # 8-byte Spill
	movq	1128(%rsp), %rcx        # 8-byte Reload
	movq	4728(%rsp), %rdi        # 8-byte Reload
	.align	16, 0x90
.LBB147_844:                            # %for gV.s0.v10.v10367
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_783 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rcx, 5312(%rsp)        # 8-byte Spill
	cmpl	$0, 4192(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %r13        # 8-byte Reload
	leal	(%r13,%rcx,8), %r8d
	movl	%r8d, 3680(%rsp)        # 4-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	sete	4256(%rsp)              # 1-byte Folded Spill
	movl	%r8d, %ecx
	subl	%edi, %ecx
	leal	-1(%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	addl	$-2, %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r12d
	vpinsrd	$3, %r11d, %xmm0, %xmm15
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2816(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm8
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm3
	movq	5312(%rsp), %rax        # 8-byte Reload
	leal	-2(%r13,%rax,8), %eax
	vmovdqa	5392(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm13, %xmm4
	vmovdqa	5360(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm1, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm4, %xmm4
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm4, %xmm1, %xmm1
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm1, %xmm12, %xmm3
	vpextrq	$1, %xmm3, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vmovq	%xmm3, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm1, %xmm10, %xmm3
	vpextrq	$1, %xmm3, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	vmovq	%xmm3, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movl	%r8d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	5312(%rsp), %rax        # 8-byte Reload
	leal	-1(%r13,%rax,8), %eax
	vmovd	%eax, %xmm1
	sete	4224(%rsp)              # 1-byte Folded Spill
	movb	4256(%rsp), %al         # 1-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm3
	andb	5280(%rsp), %r15b       # 1-byte Folded Reload
	vpsrad	$31, %xmm15, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm13, %xmm4
	vpsubd	%xmm2, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5184(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	testl	4192(%rsp), %r8d        # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm1
	setne	%r9b
	vmovq	%xmm1, %r13
	movq	%r13, %rsi
	sarq	$32, %rsi
	vpextrq	$1, %xmm1, %r11
	movq	%r11, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm10, %xmm1
	vmovq	%xmm1, %r12
	movq	%r12, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vpextrq	$1, %xmm1, %rdi
	movq	%rdi, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %r8
	movq	%r8, 3248(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	movq	2848(%rsp), %rax        # 8-byte Reload
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx,8), %r10d
	movq	4936(%rsp), %rbx        # 8-byte Reload
	leal	(%r10,%rbx), %eax
	movl	%eax, 3264(%rsp)        # 4-byte Spill
	movslq	%r10d, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	2736(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	leal	(%rdx,%rbx), %eax
	movl	%eax, 3392(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movq	2832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movq	4928(%rsp), %rax        # 8-byte Reload
	leal	(%rdx,%rax), %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	leal	(%rdx,%rbx), %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movslq	%edx, %r10
	movq	%r10, %rax
	orq	$6, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_846
# BB#845:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_846:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	movzbl	4224(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%r9b, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_848
# BB#847:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_848:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4224(%rsp)       # 16-byte Spill
	movzbl	%r15b, %edx
	vmovd	%edx, %xmm0
	je	.LBB147_850
# BB#849:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_850:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	%xmm1, 2912(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_852
# BB#851:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_852:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3472(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	movq	2752(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movslq	%edx, %rdx
	movq	5672(%rsp), %rax        # 8-byte Reload
	vmovups	12312(%rax,%rdx,4), %xmm1
	vmovups	12328(%rax,%rdx,4), %xmm6
	movq	2800(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rcx,8), %edx
	movslq	%edx, %rdx
	vmovups	12312(%rax,%rdx,4), %xmm8
	vmovups	12328(%rax,%rdx,4), %xmm10
	movq	3424(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3440(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3456(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movslq	3168(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	12312(%rax,%rdx,4), %xmm13
	vmovups	12328(%rax,%rdx,4), %xmm3
	movq	%rax, %r9
	movslq	%r13d, %rdx
	vmovss	(%rbx,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm11, %xmm4
	vshufps	$136, %xmm6, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm6[0,2]
	vmovaps	5472(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm4, %xmm9
	vmovaps	2768(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm11, %xmm4
	vshufps	$136, %xmm10, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vmulps	%xmm15, %xmm0, %xmm6
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm1, %xmm1
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r8,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm8, %xmm0 # xmm0 = xmm8[1,3],xmm10[1,3]
	vmulps	%xmm15, %xmm5, %xmm6
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vshufps	$136, %xmm3, %xmm13, %xmm6 # xmm6 = xmm13[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm13, %xmm10 # xmm10 = xmm13[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm9, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm3, %xmm8
	vminps	%xmm13, %xmm12, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm9
	vmulps	3904(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	cmpl	$0, 104(%rbp)
	je	.LBB147_854
# BB#853:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	2880(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 4224(%rsp)       # 16-byte Spill
.LBB147_854:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmulps	%xmm5, %xmm3, %xmm2
	vmovaps	%xmm2, 3312(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm1, %xmm11
	vminps	%xmm13, %xmm0, %xmm12
	vsubps	5472(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
	vaddps	%xmm9, %xmm8, %xmm9
	movq	4728(%rsp), %rdi        # 8-byte Reload
	movl	2248(%rsp), %ebx        # 4-byte Reload
	vmovdqa	3408(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_856
# BB#855:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovdqa	2896(%rsp), %xmm8       # 16-byte Reload
.LBB147_856:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	3344(%rsp), %xmm4       # 16-byte Reload
	vmulps	3872(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	3840(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm6[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	3808(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	5280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm6
	vmovaps	%xmm6, 3344(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm6
	vminps	%xmm13, %xmm5, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm5
	vbroadcastss	.LCPI147_18(%rip), %xmm14
	vfmsub213ps	%xmm6, %xmm14, %xmm5
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	vmaxps	%xmm7, %xmm11, %xmm2
	vmaxps	%xmm7, %xmm12, %xmm5
	vmulps	3904(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
	vmovdqa	4224(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm8, %xmm0
	vmovdqa	%xmm0, 3280(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm9, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm10
	je	.LBB147_858
# BB#857:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	2912(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_858:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vaddps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vmovaps	3472(%rsp), %xmm5       # 16-byte Reload
	vmulps	3776(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	40(%r9,%rax,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm2[0,2]
	vmovaps	5680(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm1, %xmm8
	vmulps	3744(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm11
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	40(%r9,%rax,4), %xmm12
	vshufps	$136, %xmm12, %xmm11, %xmm4 # xmm4 = xmm11[0,2],xmm12[0,2]
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3712(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm5
	vmovaps	%xmm5, 3472(%rsp)       # 16-byte Spill
	vmovups	40(%r9,%r10,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm14, %xmm4
	vminps	%xmm13, %xmm8, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm9
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm10, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm10, %xmm9
	vpsrad	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3600(%rsp)       # 16-byte Spill
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_860
# BB#859:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovdqa	3072(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_860:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_844 Depth=4
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3456(%rsp), %xmm8       # 16-byte Reload
	vmulps	3840(%rsp), %xmm8, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	3808(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm3, %xmm3
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm14, %xmm3
	vshufps	$221, %xmm12, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm12[1,3]
	vxorps	%xmm12, %xmm12, %xmm12
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3648(%rsp), %xmm11      # 16-byte Reload
	vmulps	3744(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3472(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	3712(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm14, %xmm4
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	3872(%rsp), %xmm8, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vmulps	3408(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3520(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vmulps	3776(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vsubps	%xmm3, %xmm4, %xmm3
	vfmadd213ps	%xmm2, %xmm10, %xmm0
	vfmadd213ps	%xmm2, %xmm10, %xmm3
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm2, %xmm7, %xmm12, %xmm4
	vmovaps	4256(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm9, %xmm4, %xmm4
	vmovaps	4224(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm5, %xmm12, %xmm6
	vblendvps	%xmm2, %xmm3, %xmm6, %xmm2
	vmovaps	3600(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 5280(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	3616(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, %xmm7, %xmm3, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3680(%rsp), %rax        # 4-byte Folded Reload
	movq	2720(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	4880(%rsp), %rsi        # 8-byte Reload
	vmovups	%ymm0, (%rsi,%rax,4)
	addq	$1, %rcx
	cmpl	%ebx, %ecx
	jne	.LBB147_844
# BB#861:                               #   in Loop: Header=BB147_783 Depth=3
	movq	2704(%rsp), %rax        # 8-byte Reload
.LBB147_862:                            # %end for gV.s0.v10.v10368
                                        #   in Loop: Header=BB147_783 Depth=3
	movl	2640(%rsp), %esi        # 4-byte Reload
	addl	$1, %esi
	movl	%esi, 2640(%rsp)        # 4-byte Spill
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movq	2864(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %esi
	jne	.LBB147_783
.LBB147_863:                            # %end for gV.s0.v11358
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	2864(%rsp), %rax        # 8-byte Reload
	cmpl	1428(%rsp), %eax        # 4-byte Folded Reload
	movl	1284(%rsp), %eax        # 4-byte Reload
	movl	1280(%rsp), %edx        # 4-byte Reload
	movl	1276(%rsp), %esi        # 4-byte Reload
	jl	.LBB147_864
	jmp	.LBB147_886
.LBB147_865:                            # %for gV.s0.v11371.end for gV.s0.v10.v10375_crit_edge
                                        #   in Loop: Header=BB147_864 Depth=3
	movq	2864(%rsp), %rcx        # 8-byte Reload
	addl	$1, %ecx
	movl	%ecx, %edi
	jmp	.LBB147_885
	.align	16, 0x90
.LBB147_864:                            # %for gV.s0.v11371
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_867 Depth 4
	testl	%ebx, %ebx
	jle	.LBB147_865
# BB#866:                               # %for gV.s0.v10.v10374.preheader
                                        #   in Loop: Header=BB147_864 Depth=3
	movq	2864(%rsp), %r8         # 8-byte Reload
	movl	%r8d, %ecx
	movq	1816(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ecx
	leal	-1(%rcx), %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1808(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %edx
	cmovgl	%r8d, %edx
	addl	$-1, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	1860(%rsp), %edi        # 4-byte Reload
	movl	%edi, %esi
	movl	%edi, %r11d
	subl	%eax, %esi
	movq	1848(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %esi
	addl	%r9d, %esi
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ebx
	cmovgel	%edx, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	cmpl	%r8d, %r13d
	movl	%r13d, %edx
	cmovgl	%r8d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	subl	%eax, %edi
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	cmpl	%r8d, %ebx
	cmovgl	%edx, %edi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	leal	1(%r8), %ebx
	movl	%ebx, 2656(%rsp)        # 4-byte Spill
	cmpl	%ebx, %r13d
	movl	%r13d, %edx
	cmovgl	%ebx, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%r11d, %ebx
	subl	%eax, %ebx
	cmpl	%eax, %r10d
	cmovgl	%eax, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %r13d
	cmovgl	%edx, %ebx
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2880(%rsp)       # 16-byte Spill
	movslq	%edi, %r11
	movq	1880(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	movq	1840(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movslq	%ebx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5312(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%r11), %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	leal	2(%r8), %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	1860(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ebx
	subl	%esi, %ebx
	cmpl	%esi, %r10d
	cmovgl	%esi, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %ebx
	movslq	%ebx, %rdx
	imulq	%r14, %rdx
	leaq	(%r15,%rdx), %r12
	leal	-2(%r8), %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	subl	%ecx, %eax
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %eax
	cltq
	imulq	%r14, %rax
	leaq	(%r15,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rbx
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movq	5312(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1776(%rsp), %rcx        # 8-byte Folded Reload
	movq	1576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2800(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2784(%rsp)        # 8-byte Spill
	subq	4760(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	1584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1680(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	movl	2248(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_867:                            # %for gV.s0.v10.v10374
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_864 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5248(%rsp)          # 4-byte Folded Reload
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r11d
	movl	%r11d, 3776(%rsp)       # 4-byte Spill
	movl	%r11d, %r13d
	andl	$1, %r13d
	sete	%r15b
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vmovd	%esi, %xmm0
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r9d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r10d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2880(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5200(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5392(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	movq	2864(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	sete	%r9b
	andb	5280(%rsp), %r15b       # 1-byte Folded Reload
	movzbl	%r15b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5312(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5248(%rsp), %r11d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r8
	movq	%r8, %r14
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r13
	movq	%r13, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm0, %r11
	movq	%r11, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	2800(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	movq	2784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movq	2704(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %r10d
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3184(%rsp)        # 4-byte Spill
	movq	2768(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	movq	2752(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3280(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3296(%rsp)        # 4-byte Spill
	je	.LBB147_869
# BB#868:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_869:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	movzbl	%r9b, %r9d
	vmovd	%r9d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_871
# BB#870:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_871:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5312(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_873
# BB#872:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_873:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	%xmm3, 5312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3072(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_875
# BB#874:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_875:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3472(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5672(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r10d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3440(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movslq	3184(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r8d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r13d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5472(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2832(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5312(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_877
# BB#876:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovdqa	2912(%rsp), %xmm9       # 16-byte Reload
.LBB147_877:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5472(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4880(%rsp), %rcx        # 8-byte Reload
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3312(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_879
# BB#878:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovdqa	2896(%rsp), %xmm7       # 16-byte Reload
.LBB147_879:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4224(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5312(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5216(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5504(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3424(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_881
# BB#880:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovdqa	3072(%rsp), %xmm6       # 16-byte Reload
.LBB147_881:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	3472(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3472(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	je	.LBB147_883
# BB#882:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5280(%rsp)       # 16-byte Spill
.LBB147_883:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_867 Depth=4
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3488(%rsp), %xmm11      # 16-byte Reload
	vmulps	4224(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4192(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vmulps	3872(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3472(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3840(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3408(%rsp), %xmm2       # 16-byte Reload
	vmulps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3600(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5312(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2816(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r12d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_867
# BB#884:                               #   in Loop: Header=BB147_864 Depth=3
	movl	1284(%rsp), %eax        # 4-byte Reload
	movl	1280(%rsp), %edx        # 4-byte Reload
	movl	1276(%rsp), %esi        # 4-byte Reload
	movl	2656(%rsp), %edi        # 4-byte Reload
.LBB147_885:                            # %end for gV.s0.v10.v10375
                                        #   in Loop: Header=BB147_864 Depth=3
	movl	%edi, %ecx
	movq	%rcx, 2864(%rsp)        # 8-byte Spill
	cmpl	1428(%rsp), %edi        # 4-byte Folded Reload
	movl	2248(%rsp), %ebx        # 4-byte Reload
	jne	.LBB147_864
.LBB147_886:                            # %produce dV379
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	%esi, 1276(%rsp)        # 4-byte Spill
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movl	%eax, 1284(%rsp)        # 4-byte Spill
	movq	1072(%rsp), %rcx        # 8-byte Reload
	leal	6(%rcx), %edx
	movl	%edx, 2592(%rsp)        # 4-byte Spill
	movl	1708(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	movl	1432(%rsp), %eax        # 4-byte Reload
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	movl	%esi, 2416(%rsp)        # 4-byte Spill
	leal	7(%rcx), %ecx
	movl	1044(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %esi
	cmovgel	%esi, %ecx
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	movl	%edx, %r9d
	cmpl	%esi, %edx
	movl	2252(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_887
	jmp	.LBB147_889
.LBB147_888:                            # %for dV.s0.v11381.end for dV.s0.v10.v10384_crit_edge
                                        #   in Loop: Header=BB147_887 Depth=3
	addl	$1, %r9d
	movl	%r9d, %eax
	jmp	.LBB147_950
	.align	16, 0x90
.LBB147_887:                            # %for dV.s0.v11381
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_932 Depth 4
	testl	%eax, %eax
	jle	.LBB147_888
# BB#931:                               # %for dV.s0.v10.v10383.preheader
                                        #   in Loop: Header=BB147_887 Depth=3
	movq	%r9, 2752(%rsp)         # 8-byte Spill
	movl	%r9d, %edi
	movq	1816(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1824(%rsp), %r13        # 8-byte Reload
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %r15d
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r11d
	subl	%eax, %esi
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1804(%rsp), %r8d        # 4-byte Reload
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %r10d
	cmovgl	%r9d, %r10d
	addl	$-1, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	cmpl	%r9d, %ecx
	cmovll	%esi, %r10d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, %r8d
	movl	%r8d, %edi
	cmovgl	%r9d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r9d, %ecx
	cmovlel	%edx, %edi
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r10d
	leal	1(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	leal	1(%r9), %eax
	movl	%eax, 2400(%rsp)        # 4-byte Spill
	cmpl	%eax, %r8d
	movl	%r8d, %r14d
	cmovgl	%eax, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r9d, %r8d
	cmovlel	%esi, %r14d
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movslq	%edi, %rdi
	movq	1880(%rsp), %r12        # 8-byte Reload
	imulq	%r12, %rdi
	movq	%rdi, 5312(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %r11d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2720(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%rdi), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%r11d, %edi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r13d
	sarl	$31, %edi
	movl	1836(%rsp), %eax        # 4-byte Reload
	andl	%eax, %edi
	addl	%r11d, %edi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	addl	%edx, %ecx
	movl	1860(%rsp), %eax        # 4-byte Reload
	subl	%edi, %eax
	movq	1848(%rsp), %r13        # 8-byte Reload
	cmpl	%edi, %r13d
	cmovgl	%edi, %eax
	addl	%ebx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	2(%r9), %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%eax, %edx
	cmpl	%r9d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r11
	movq	1888(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	imulq	%r12, %r11
	leaq	(%r15,%r11), %rdx
	movl	1860(%rsp), %edi        # 4-byte Reload
	subl	%ecx, %edi
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %edi
	addl	%ebx, %edi
	cmpl	%edi, %r8d
	cmovlel	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	leal	-2(%r9), %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%edi, %ecx
	cmpl	%r9d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%edi, %ecx
	movslq	%ecx, %rcx
	imulq	%r12, %rcx
	leaq	(%r15,%rcx), %rdi
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	cmpl	%r9d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r14d
	movslq	%r14d, %rdx
	imulq	%r12, %rdx
	leaq	(%rdx,%r15), %rdx
	movslq	%r10d, %rsi
	imulq	%r12, %rsi
	leaq	(%rsi,%r15), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r10
	leaq	(%rdi,%rcx), %rsi
	movq	5312(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1864(%rsp), %rbx        # 8-byte Reload
	leaq	(%r11,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	movq	1584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	1576(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2576(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2544(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2528(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2512(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2496(%rsp)        # 8-byte Spill
	movq	1680(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2480(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2440(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	2252(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_932:                            # %for dV.s0.v10.v10383
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_887 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	cmpl	$0, 5280(%rsp)          # 4-byte Folded Reload
	setne	3744(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movl	%r12d, 3840(%rsp)       # 4-byte Spill
	movl	%r12d, %r15d
	andl	$1, %r15d
	sete	%r13b
	movq	3936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	movq	3944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r11d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r15d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2720(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5360(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4184(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	movl	%r12d, %eax
	movq	2752(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	3744(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_54(%rip), %xmm3, %xmm3
	vmovdqa	4800(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5280(%rsp), %r12d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r11
	movq	%r11, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r13
	movq	%r13, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3216(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2440(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r15d
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r10d
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r9d
	movq	2608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movq	2560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3280(%rsp)        # 4-byte Spill
	movq	2544(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	je	.LBB147_934
# BB#933:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_934:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	je	.LBB147_936
# BB#935:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_936:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_938
# BB#937:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_938:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	%xmm4, 3376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	je	.LBB147_940
# BB#939:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_940:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r15d, %rcx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2912(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2896(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2880(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3072(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movslq	%r12d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5504(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2688(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2864(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2640(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2832(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2912(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	2896(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3776(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3072(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3312(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3376(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_942
# BB#941:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovdqa	2784(%rsp), %xmm10      # 16-byte Reload
.LBB147_942:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vandps	3424(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3344(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3328(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4752(%rsp), %rdx        # 8-byte Reload
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3392(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_944
# BB#943:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovdqa	2768(%rsp), %xmm11      # 16-byte Reload
.LBB147_944:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3616(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5216(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3360(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3600(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4256(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3408(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	movq	%rcx, %rax
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3328(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_946
# BB#945:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovdqa	2800(%rsp), %xmm5       # 16-byte Reload
.LBB147_946:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vmulps	4224(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm4
	vmovaps	%xmm4, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4192(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm7
	vmovaps	%xmm7, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3904(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm14
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3376(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovdqa	3328(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmulps	3424(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	vmulps	3296(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	je	.LBB147_948
# BB#947:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
.LBB147_948:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_932 Depth=4
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	5248(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3600(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5216(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	4224(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4192(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5312(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3744(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3520(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3456(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3840(%rsp), %rax        # 4-byte Folded Reload
	movq	2624(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r14d
	movl	3872(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_932
# BB#949:                               #   in Loop: Header=BB147_887 Depth=3
	movl	2400(%rsp), %eax        # 4-byte Reload
.LBB147_950:                            # %end for dV.s0.v10.v10384
                                        #   in Loop: Header=BB147_887 Depth=3
	movl	%eax, %r9d
	cmpl	2416(%rsp), %eax        # 4-byte Folded Reload
	movl	2252(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_887
.LBB147_889:                            # %end for dV.s0.v11382
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	2736(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, 2416(%rsp)        # 4-byte Folded Reload
	movq	4752(%rsp), %r13        # 8-byte Reload
	jge	.LBB147_971
# BB#890:                               #   in Loop: Header=BB147_467 Depth=2
	movl	944(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	jmp	.LBB147_891
.LBB147_930:                            # %end for dV.s0.v10.v10394.end for dV.s0.v10.v10398_crit_edge
                                        #   in Loop: Header=BB147_891 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	jmp	.LBB147_970
	.align	16, 0x90
.LBB147_891:                            # %for dV.s0.v11387
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_893 Depth 4
                                        #         Child Loop BB147_912 Depth 4
                                        #         Child Loop BB147_952 Depth 4
	cmpl	$0, 1312(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_910
# BB#892:                               # %for dV.s0.v10.v10389.preheader
                                        #   in Loop: Header=BB147_891 Depth=3
	movq	5312(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movq	%r11, %r9
	movq	1880(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	imulq	%rax, %r9
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2720(%rsp)       # 16-byte Spill
	leaq	2(%r11), %r10
	imulq	%rax, %r10
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r9), %rdx
	leaq	(%rcx,%r10), %rsi
	movq	1888(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rdx
	imulq	%rax, %rdx
	leaq	(%rcx,%rdx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	leaq	-1(%r11), %rdi
	imulq	%rax, %rdi
	leaq	(%rdi,%rcx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %r8
	leaq	(%rax,%rdx), %rdi
	leaq	(%rax,%r9), %rsi
	movq	1864(%rsp), %rax        # 8-byte Reload
	leaq	(%r10,%rax), %rcx
	leaq	(%rdx,%rax), %rdx
	leaq	(%r9,%rax), %rax
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movl	%r11d, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	1552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	movq	1752(%rsp), %rdx        # 8-byte Reload
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	movq	1544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	imull	%edx, %eax
	movq	1568(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%edx, %esi
	movq	%rsi, 2576(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2544(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2528(%rsp)        # 8-byte Spill
	movq	1560(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %edi
	imull	%edx, %edi
	movq	%rdi, 2512(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2496(%rsp)        # 8-byte Spill
	movq	1536(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%edx, %esi
	movq	%rsi, 2480(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2440(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	movl	1160(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_893:                            # %for dV.s0.v10.v10389
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_891 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	cmpl	$0, 5248(%rsp)          # 4-byte Folded Reload
	setne	3712(%rsp)              # 1-byte Folded Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r14d
	movl	%r14d, 3808(%rsp)       # 4-byte Spill
	movl	%r14d, %r11d
	andl	$1, %r11d
	sete	%r13b
	movq	3936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	movq	3944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r15d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r11d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2720(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5360(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4184(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	sete	3376(%rsp)              # 1-byte Folded Spill
	andb	3712(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5280(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_54(%rip), %xmm3, %xmm3
	vmovdqa	4800(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5248(%rsp), %r14d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r15
	movq	%r15, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r11
	movq	%r11, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3200(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3472(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2440(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r13d
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r10d
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r9d
	movq	2608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r14d
	movq	2560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3264(%rsp)        # 4-byte Spill
	movq	2544(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3392(%rsp)        # 4-byte Spill
	je	.LBB147_895
# BB#894:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_895:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	movzbl	3376(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	je	.LBB147_897
# BB#896:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_897:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_899
# BB#898:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_899:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3712(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	je	.LBB147_901
# BB#900:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_901:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r13d, %rcx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2896(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2880(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2864(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 2912(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3296(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5504(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2688(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2832(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2848(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2640(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2816(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	movq	3072(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2896(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vmovaps	2880(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3776(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2912(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3296(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3360(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_903
# BB#902:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovdqa	2768(%rsp), %xmm10      # 16-byte Reload
.LBB147_903:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vandps	3408(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3328(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3312(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4752(%rsp), %r13        # 8-byte Reload
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3376(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_905
# BB#904:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovdqa	2752(%rsp), %xmm11      # 16-byte Reload
.LBB147_905:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3600(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	4256(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3344(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4224(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3712(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_907
# BB#906:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovdqa	2784(%rsp), %xmm5       # 16-byte Reload
.LBB147_907:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vmulps	4192(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	3904(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm6
	vmovaps	%xmm6, 3344(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm7
	vmovaps	%xmm7, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3872(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm14
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3360(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmulps	3408(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmulps	3280(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_909
# BB#908:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
.LBB147_909:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_893 Depth=4
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3744(%rsp), %xmm2       # 16-byte Reload
	vmulps	5216(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3520(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	4256(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4224(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3776(%rsp), %xmm7       # 16-byte Reload
	vmulps	4192(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	3904(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3872(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5280(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3472(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3712(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3488(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3440(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2624(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addl	$8, %r12d
	movl	3840(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_893
.LBB147_910:                            # %end for dV.s0.v10.v10390
                                        #   in Loop: Header=BB147_891 Depth=3
	movl	1312(%rsp), %eax        # 4-byte Reload
	cmpl	1332(%rsp), %eax        # 4-byte Folded Reload
	movq	4888(%rsp), %r15        # 8-byte Reload
	jge	.LBB147_929
# BB#911:                               # %for dV.s0.v10.v10393.preheader
                                        #   in Loop: Header=BB147_891 Depth=3
	movq	5312(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	movq	%r11, %r9
	movq	1880(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	imulq	%rax, %r9
	movq	1840(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r9), %rcx
	leaq	2(%r11), %r10
	imulq	%rax, %r10
	leaq	(%rdx,%r10), %rsi
	movq	1888(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rcx
	imulq	%rax, %rcx
	leaq	(%rdx,%rcx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rdx), %rsi
	leaq	-1(%r11), %rdi
	imulq	%rax, %rdi
	leaq	(%rdi,%rdx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %r8
	leaq	(%rax,%rcx), %rdi
	leaq	(%rax,%r9), %rsi
	movq	1864(%rsp), %rax        # 8-byte Reload
	leaq	(%r10,%rax), %rdx
	leaq	(%rcx,%rax), %rcx
	leaq	(%r9,%rax), %rax
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r11d, %edx
	andl	$63, %edx
	imulq	1784(%rsp), %rdx        # 8-byte Folded Reload
	movq	1552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %eax
	movq	1544(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%esi, %ecx
	subq	4760(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2832(%rsp)        # 8-byte Spill
	movq	2224(%rsp), %rdx        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdx), %eax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	movq	1568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %edi
	imull	%esi, %edi
	movq	2216(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdi), %ecx
	movq	%rcx, 2768(%rsp)        # 8-byte Spill
	movq	1560(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ebx
	imull	%esi, %ebx
	leal	(%rdx,%rdi), %ecx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	addl	$-8, %edi
	movq	%rdi, 2784(%rsp)        # 8-byte Spill
	leal	(%rax,%rbx), %ecx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %ecx
	movq	%rcx, 2688(%rsp)        # 8-byte Spill
	movq	1536(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%esi, %ecx
	addl	$-8, %ebx
	movq	%rbx, 2752(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	leal	(%rdx,%rcx), %eax
	movq	%rax, 2640(%rsp)        # 8-byte Spill
	addl	$-8, %ecx
	movq	%rcx, 2672(%rsp)        # 8-byte Spill
	movl	1152(%rsp), %ecx        # 4-byte Reload
	movl	1156(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r9d
	.align	16, 0x90
.LBB147_912:                            # %for dV.s0.v10.v10393
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_891 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	movl	3168(%rsp), %r13d       # 4-byte Reload
	testl	%r13d, %r13d
	setne	%r14b
	sete	%r10b
	movq	3064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %r8d
	movslq	%r8d, %rdx
	movq	%rdx, 3776(%rsp)        # 8-byte Spill
	andl	$1, %r8d
	sete	%r12b
	leaq	-6(%rdx), %rdi
	movq	4664(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %rdi
	movq	4312(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdi), %r11
	leaq	(%r15,%rdi), %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	andb	%r14b, %r12b
	andb	%r10b, %r8b
	testl	%edx, %r13d
	setne	%r14b
	leaq	-5(%rdx), %rcx
	imulq	%rsi, %rcx
	movq	4744(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi), %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rcx), %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	leaq	(%r15,%rcx), %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	leaq	(%rcx,%rsi), %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	movq	2672(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	movq	2784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	movzbl	%r12b, %edx
	vmovd	%edx, %xmm0
	movq	2752(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %edx
	movslq	%edx, %r12
	movq	%r12, %rcx
	orq	$2, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ebx
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r13d
	movq	2688(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r15d
	movq	2816(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %edx
	movq	2800(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r10d
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3424(%rsp)        # 4-byte Spill
	movq	2768(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3520(%rsp)        # 4-byte Spill
	je	.LBB147_914
# BB#913:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_914:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_916
# BB#915:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_916:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm4, %xmm1
	movq	5672(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_918
# BB#917:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_918:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_920
# BB#919:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_920:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r11,4), %rax
	movq	4736(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm0, %xmm8 # xmm8 = xmm0[0,1,2],mem[0]
	movslq	%ebx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	movslq	%r13d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	movslq	%r15d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3296(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm13
	vmovaps	%xmm13, 3328(%rsp)      # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm10
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	movslq	%r10d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm11
	vmovups	12312(%rcx,%rax,4), %xmm12
	vmovaps	3072(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm7
	vshufps	$136, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	5504(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vbroadcastss	.LCPI147_17(%rip), %xmm7
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	2912(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm8, %xmm6
	vmovaps	5280(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, %xmm1, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm1[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovaps	2896(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm8, %xmm4
	vmovaps	5248(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vmovaps	2880(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm8, %xmm3
	vshufps	$136, %xmm10, %xmm13, %xmm4 # xmm4 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2864(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm10, %xmm8, %xmm4
	vshufps	$136, %xmm12, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm12[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm3, %xmm6 # xmm6 = xmm3[0,1,2],mem[0]
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	%xmm6, %xmm0, %xmm4
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm4, %xmm3, %xmm8
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmulps	%xmm6, %xmm14, %xmm4
	vxorps	%xmm14, %xmm14, %xmm14
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm6, %xmm2, %xmm2
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm1, %xmm1
	movq	3440(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm12[1,3]
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm6, %xmm10, %xmm4
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm4, %xmm2, %xmm6
	vbroadcastss	.LCPI147_21(%rip), %xmm11
	vminps	%xmm7, %xmm8, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm4
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm5
	vminps	%xmm7, %xmm1, %xmm3
	vminps	%xmm7, %xmm6, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3488(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_922
# BB#921:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovdqa	3200(%rsp), %xmm8       # 16-byte Reload
.LBB147_922:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vandps	3376(%rsp), %xmm11, %xmm6 # 16-byte Folded Reload
	vandps	3360(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm4, %xmm9
	vsubps	%xmm2, %xmm5, %xmm15
	vmaxps	%xmm14, %xmm3, %xmm2
	vmaxps	%xmm14, %xmm1, %xmm5
	vandps	3344(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	4752(%rsp), %r13        # 8-byte Reload
	vmovaps	2848(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_924
# BB#923:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	3184(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
.LBB147_924:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vaddps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmulps	%xmm10, %xmm13, %xmm0
	movslq	3424(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm1
	vmulps	4224(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	movslq	3456(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm3
	vmulps	4192(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	movslq	3520(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm5
	vandps	%xmm11, %xmm9, %xmm2
	vandps	%xmm11, %xmm15, %xmm0
	vpslld	$31, %xmm8, %xmm4
	vmovdqa	%xmm4, 3344(%rsp)       # 16-byte Spill
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm6
	vminps	%xmm7, %xmm3, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vminps	%xmm7, %xmm5, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm9
	vfnmadd213ps	%xmm3, %xmm9, %xmm6
	vbroadcastss	.LCPI147_20(%rip), %xmm15
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3328(%rsp)       # 16-byte Spill
	vandps	%xmm11, %xmm6, %xmm3
	vaddps	5280(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovdqa	3744(%rsp), %xmm12      # 16-byte Reload
	je	.LBB147_926
# BB#925:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovdqa	3216(%rsp), %xmm12      # 16-byte Reload
.LBB147_926:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm6       # 16-byte Reload
	vmulps	3904(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5680(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vmulps	3872(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3648(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3392(%rsp)       # 16-byte Spill
	vmovups	(%rcx,%r12,4), %xmm8
	vshufps	$136, %xmm8, %xmm4, %xmm13 # xmm13 = xmm4[0,2],xmm8[0,2]
	vsubps	%xmm1, %xmm13, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vfnmadd213ps	%xmm2, %xmm9, %xmm0
	vandps	%xmm11, %xmm0, %xmm0
	vaddps	5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vandps	3376(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 5280(%rsp)       # 16-byte Spill
	vmulps	3408(%rsp), %xmm15, %xmm13 # 16-byte Folded Reload
	vmovdqa	3328(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 3712(%rsp)       # 16-byte Spill
	vmulps	3312(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3600(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm12, %xmm6
	vpsrad	$31, %xmm6, %xmm14
	vmulps	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_928
# BB#927:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_928:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_912 Depth=4
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3552(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3440(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	4224(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3520(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm4, %xmm12 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vminps	%xmm7, %xmm2, %xmm2
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm2, %xmm2
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm12
	vfnmadd213ps	%xmm2, %xmm9, %xmm12
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3680(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm1, %xmm4
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	3904(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm10
	vmovaps	3648(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	3872(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm2, %xmm5 # xmm5 = xmm2[1,3],xmm8[1,3]
	vmulps	3840(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vminps	%xmm7, %xmm10, %xmm2
	vminps	%xmm7, %xmm3, %xmm3
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm2, %xmm2
	vfnmadd213ps	%xmm3, %xmm9, %xmm2
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vblendvps	%xmm1, %xmm13, %xmm6, %xmm3
	vblendvps	%xmm14, 3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	4256(%rsp), %xmm15, %xmm5 # 16-byte Folded Reload
	vblendvps	%xmm14, %xmm5, %xmm6, %xmm6
	vandps	%xmm11, %xmm2, %xmm2
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm2, %xmm6, %xmm1
	vmovaps	3712(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3600(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm13, %xmm2, %xmm2
	vandps	%xmm11, %xmm12, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2832(%rsp), %rax        # 8-byte Reload
	movq	3776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addl	$8, %r9d
	movl	3808(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	movq	4888(%rsp), %r15        # 8-byte Reload
	jne	.LBB147_912
.LBB147_929:                            # %end for dV.s0.v10.v10394
                                        #   in Loop: Header=BB147_891 Depth=3
	movl	1332(%rsp), %eax        # 4-byte Reload
	cmpl	2252(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_930
# BB#951:                               # %for dV.s0.v10.v10397.preheader
                                        #   in Loop: Header=BB147_891 Depth=3
	movq	5312(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %r14d
	movq	1688(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r14d
	leal	8(%r14), %eax
	movq	1712(%rsp), %r10        # 8-byte Reload
	imull	%r10d, %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movq	%rdx, %r8
	movq	1880(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %r8
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rdi
	leal	10(%r14), %esi
	imull	%r10d, %esi
	movq	%rsi, 2720(%rsp)        # 8-byte Spill
	leaq	2(%rdx), %r9
	imulq	%rax, %r9
	leaq	(%rcx,%r9), %rsi
	movq	1888(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	leal	6(%r14), %edi
	imull	%r10d, %edi
	movq	%rdi, 2688(%rsp)        # 8-byte Spill
	leaq	-2(%rdx), %r11
	imulq	%rax, %r11
	leaq	(%rcx,%r11), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	leaq	1(%rdx), %rdi
	movq	%rdi, 2544(%rsp)        # 8-byte Spill
	movq	%rdx, %rsi
	addq	$-1, %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	%rdi, %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r8), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	leaq	(%rax,%r11), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	(%rax,%r9), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movl	%edx, %esi
	movq	1864(%rsp), %rcx        # 8-byte Reload
	leaq	(%r8,%rcx), %rax
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leal	9(%r14), %edi
	imull	%r10d, %edi
	andl	$1, %esi
	movl	%esi, 3872(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	4928(%rsp), %rax        # 8-byte Reload
	addl	%eax, %edi
	movq	%rdi, 2608(%rsp)        # 8-byte Spill
	addl	$7, %r14d
	imull	%r10d, %r14d
	addl	%eax, %r14d
	movq	%r14, 2768(%rsp)        # 8-byte Spill
	leaq	(%r9,%rcx), %rax
	leaq	(%r11,%rcx), %rcx
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	andl	$63, %edx
	imulq	1784(%rsp), %rdx        # 8-byte Folded Reload
	subq	4760(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	movq	1120(%rsp), %r11        # 8-byte Reload
	.align	16, 0x90
.LBB147_952:                            # %for dV.s0.v10.v10397
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_891 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	cmpl	$0, 3872(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r11,8), %r15d
	movl	%r15d, 3744(%rsp)       # 4-byte Spill
	movl	%r15d, %eax
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	sete	4256(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %ecx
	movq	4728(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	leal	-5(%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r14d
	addl	$-6, %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r14d, %xmm1, %xmm8
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r13d
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2576(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm13
	vmovdqa	4992(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm2, %xmm2
	leal	-6(%r12,%r11,8), %eax
	vmovdqa	5392(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm3
	vmovdqa	5360(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm1, %xmm0, %xmm5
	vblendvps	%xmm3, %xmm1, %xmm5, %xmm1
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm3, %xmm3
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm3, %xmm1, %xmm1
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm1, %xmm10, %xmm2
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 3264(%rsp)         # 8-byte Spill
	vmovq	%xmm2, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm1, %xmm12, %xmm2
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vmovq	%xmm2, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	leal	-5(%r12,%r11,8), %eax
	vmovd	%eax, %xmm1
	sete	3360(%rsp)              # 1-byte Folded Spill
	movb	4256(%rsp), %al         # 1-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm2
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	5280(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, %r12d
	vpsrad	$31, %xmm8, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpsubd	%xmm3, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vmovdqa	4976(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm4, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm6, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	testl	3872(%rsp), %r15d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm10, %xmm1
	setne	%r13b
	vmovq	%xmm1, %r15
	movq	%r15, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpaddd	%xmm0, %xmm12, %xmm1
	vmovq	%xmm1, %r8
	movq	%r8, 3168(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm1, %r10
	movq	%r10, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movq	2752(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %edx
	movq	4928(%rsp), %rbx        # 8-byte Reload
	leal	(%rdx,%rbx), %r14d
	movq	4936(%rsp), %rcx        # 8-byte Reload
	leal	(%rdx,%rcx), %eax
	movl	%eax, 3312(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	2688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %edi
	leal	(%rdi,%rbx), %edx
	leal	(%rdi,%rcx), %eax
	movl	%eax, 3328(%rsp)        # 4-byte Spill
	movslq	%edi, %rax
	movq	%rax, %rdi
	orq	$2, %rdi
	movq	%rdi, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	leal	(%rax,%rbx), %edi
	leal	(%rax,%rcx), %ecx
	movl	%ecx, 3392(%rsp)        # 4-byte Spill
	cltq
	movq	%rax, %rcx
	orq	$2, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	je	.LBB147_954
# BB#953:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_954:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movzbl	3360(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_956
# BB#955:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_956:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm3, %xmm1
	movq	4752(%rsp), %r13        # 8-byte Reload
	je	.LBB147_958
# BB#957:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_958:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	je	.LBB147_960
# BB#959:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_960:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %rax
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	12296(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 2864(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 2880(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 2848(%rsp)       # 16-byte Spill
	movslq	%edi, %rax
	vmovups	12296(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm14
	vmovaps	%xmm14, 2896(%rsp)      # 16-byte Spill
	movq	2768(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	vmovups	12296(%rcx,%rax,4), %xmm10
	vmovaps	%xmm10, 3216(%rsp)      # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm9
	vmovaps	%xmm9, 3248(%rsp)       # 16-byte Spill
	movq	2608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	vmovups	12296(%rcx,%rax,4), %xmm6
	vmovups	12312(%rcx,%rax,4), %xmm12
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2704(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm4
	vshufps	$136, %xmm3, %xmm2, %xmm7 # xmm7 = xmm2[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm7, %xmm7
	vmovaps	5504(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm4, %xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm4, %xmm4
	vmovaps	2672(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm0, %xmm7
	vmovaps	5280(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm5[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm2
	vmovaps	5248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm14, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm14[0,2]
	vxorps	%xmm5, %xmm5, %xmm5
	vsubps	%xmm15, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm2, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm2
	vshufps	$136, %xmm9, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm9[0,2]
	vxorps	%xmm7, %xmm7, %xmm7
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	2624(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm0, %xmm0
	vshufps	$136, %xmm12, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm12[0,2]
	vmovaps	%xmm6, %xmm9
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	2912(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3072(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	vmovaps	2864(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm10, %xmm1, %xmm2
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm11, %xmm7
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm7, %xmm2, %xmm7
	movq	3680(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2896(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm3, %xmm4
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm4, %xmm2, %xmm4
	vmovss	(%rbx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3472(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3440(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm5, %xmm5
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm5, %xmm1, %xmm6
	vmovss	(%rbx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm12[1,3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmulps	%xmm10, %xmm14, %xmm3
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm8, %xmm7, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm5
	vminps	%xmm8, %xmm6, %xmm6
	vminps	%xmm8, %xmm3, %xmm4
	cmpl	$0, 104(%rbp)
	vmovdqa	3344(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_962
# BB#961:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovdqa	2784(%rsp), %xmm11      # 16-byte Reload
.LBB147_962:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vandps	3376(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3264(%rsp), %xmm12, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm1, %xmm10
	vsubps	%xmm0, %xmm5, %xmm13
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm6, %xmm0
	vmaxps	%xmm1, %xmm4, %xmm6
	vandps	3408(%rsp), %xmm12, %xmm14 # 16-byte Folded Reload
	movl	2252(%rsp), %edx        # 4-byte Reload
	vmovdqa	3360(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_964
# BB#963:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovdqa	2800(%rsp), %xmm1       # 16-byte Reload
.LBB147_964:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vaddps	%xmm3, %xmm7, %xmm3
	vmovaps	%xmm3, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmulps	4224(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	movslq	3312(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3472(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3328(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3440(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	3904(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3392(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vandps	%xmm12, %xmm10, %xmm7
	vandps	%xmm12, %xmm13, %xmm6
	vpslld	$31, %xmm11, %xmm9
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm4
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm4
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm1, %xmm0
	vmovdqa	%xmm0, 3328(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm4, %xmm3
	vaddps	%xmm3, %xmm14, %xmm2
	je	.LBB147_966
# BB#965:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovdqa	2816(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_966:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vaddps	%xmm7, %xmm6, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vmulps	3840(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	3808(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm5[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm4, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	3776(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm15
	vshufps	$136, %xmm15, %xmm5, %xmm0 # xmm0 = xmm5[0,2],xmm15[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm8, %xmm6, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vminps	%xmm8, %xmm3, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vandps	%xmm12, %xmm1, %xmm0
	vaddps	%xmm0, %xmm14, %xmm1
	vandps	3360(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm9, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmulps	3408(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vmovdqa	3328(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm2, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm14
	vmulps	%xmm11, %xmm1, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_968
# BB#967:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_968:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_952 Depth=4
	vmovaps	3472(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	5248(%rsp), %xmm2       # 16-byte Reload
	vmulps	4224(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm1
	vfnmadd213ps	%xmm3, %xmm10, %xmm1
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	5280(%rsp), %xmm7       # 16-byte Reload
	vmulps	3840(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	3808(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$221, %xmm15, %xmm5, %xmm4 # xmm4 = xmm5[1,3],xmm15[1,3]
	vmulps	3776(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm0, %xmm0
	vfnmadd213ps	%xmm3, %xmm10, %xmm0
	vmovdqa	5216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3600(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm2, %xmm5, %xmm9, %xmm3
	vblendvps	%xmm14, 4256(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vblendvps	%xmm14, %xmm4, %xmm9, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3680(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm11, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm0, %xmm7, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3408(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm5, %xmm2, %xmm2
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm11, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3744(%rsp), %rax        # 4-byte Folded Reload
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addq	$1, %r11
	cmpl	%edx, %r11d
	jne	.LBB147_952
# BB#969:                               #   in Loop: Header=BB147_891 Depth=3
	movq	2544(%rsp), %rax        # 8-byte Reload
.LBB147_970:                            # %end for dV.s0.v10.v10398
                                        #   in Loop: Header=BB147_891 Depth=3
	movl	2416(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2416(%rsp)        # 4-byte Spill
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movq	2736(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	jne	.LBB147_891
.LBB147_971:                            # %end for dV.s0.v11388
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	%r13, 4752(%rsp)        # 8-byte Spill
	movq	2736(%rsp), %rax        # 8-byte Reload
	cmpl	1432(%rsp), %eax        # 4-byte Folded Reload
	movl	2252(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_972
	jmp	.LBB147_994
.LBB147_973:                            # %for dV.s0.v11401.end for dV.s0.v10.v10405_crit_edge
                                        #   in Loop: Header=BB147_972 Depth=3
	movq	2736(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movl	%eax, %ecx
	jmp	.LBB147_993
	.align	16, 0x90
.LBB147_972:                            # %for dV.s0.v11401
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_975 Depth 4
	testl	%eax, %eax
	jle	.LBB147_973
# BB#974:                               # %for dV.s0.v10.v10404.preheader
                                        #   in Loop: Header=BB147_972 Depth=3
	movq	2736(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %r10d
	movq	1816(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r10d
	leal	-1(%r10), %eax
	cltd
	movq	1824(%rsp), %r12        # 8-byte Reload
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r13d       # 4-byte Reload
	andl	%r13d, %eax
	addl	%edx, %eax
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %edx
	cmovgl	%r9d, %edx
	addl	$-1, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1860(%rsp), %esi        # 4-byte Reload
	movl	%esi, %r11d
	movl	%esi, %ebx
	subl	%eax, %r11d
	movq	1848(%rsp), %rsi        # 8-byte Reload
	cmpl	%eax, %esi
	movq	%rsi, %r14
	cmovgl	%eax, %r11d
	addl	%edi, %r11d
	movl	1804(%rsp), %r8d        # 4-byte Reload
	cmpl	%r11d, %r8d
	cmovlel	%r8d, %r11d
	cmpl	%edi, %r11d
	cmovll	%edi, %r11d
	cmpl	%r9d, %ecx
	cmovgel	%edx, %r11d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	cmpl	%r9d, %r8d
	movl	%r8d, %edx
	cmovgl	%r9d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%ebx, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%edi, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r9d, %ecx
	cmovgl	%edx, %esi
	leal	1(%r10), %eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	leal	1(%r9), %ecx
	movl	%ecx, 2440(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r8d
	movl	%r8d, %edx
	cmovgl	%ecx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	subl	%eax, %ebx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ebx
	addl	%edi, %ebx
	cmpl	%ebx, %r8d
	cmovlel	%r8d, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	%r9d, %r8d
	cmovgl	%edx, %ebx
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movslq	%esi, %rax
	movq	1880(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2752(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %r14        # 8-byte Reload
	leaq	(%r14,%rax), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	leal	2(%r10), %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andl	%r13d, %esi
	addl	$-2, %r10d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	addl	%ecx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r13d, %ecx
	addl	%edx, %ecx
	leal	2(%r9), %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	1860(%rsp), %r12d       # 4-byte Reload
	movl	%r12d, %edx
	subl	%esi, %edx
	movq	1848(%rsp), %r13        # 8-byte Reload
	cmpl	%esi, %r13d
	cmovgl	%esi, %edx
	addl	%edi, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r9d, 1772(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r10
	movq	1888(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	imulq	%r15, %r10
	leaq	(%r14,%r10), %rdx
	movq	%rdx, 5248(%rsp)        # 8-byte Spill
	leal	-2(%r9), %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	subl	%ecx, %r12d
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r12d
	addl	%edi, %r12d
	cmpl	%r12d, %r8d
	cmovlel	%r8d, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	%r9d, 1768(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r12d
	movslq	%r12d, %rcx
	imulq	%r15, %rcx
	leaq	(%r14,%rcx), %rdx
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	movq	5248(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movslq	%ebx, %rdx
	imulq	%r15, %rdx
	leaq	(%rdx,%r14), %rdx
	movslq	%r11d, %rsi
	imulq	%r15, %rsi
	leaq	(%rsi,%r14), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r10), %r11
	leaq	(%rdi,%rcx), %rsi
	movq	5312(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1864(%rsp), %rbx        # 8-byte Reload
	leaq	(%r10,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2640(%rsp)        # 8-byte Spill
	movq	1672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1832(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	1584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	1576(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2608(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2560(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2544(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2528(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2512(%rsp)        # 8-byte Spill
	movq	1680(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2496(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	2252(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_975:                            # %for dV.s0.v10.v10404
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_972 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	cmpl	$0, 5280(%rsp)          # 4-byte Folded Reload
	setne	3744(%rsp)              # 1-byte Folded Spill
	sete	5312(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movl	%r12d, 3840(%rsp)       # 4-byte Spill
	movl	%r12d, %r14d
	andl	$1, %r14d
	sete	%r13b
	movq	3936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	movq	3944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r11d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r14d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2752(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5392(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5360(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4184(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	movl	%r12d, %eax
	movq	2736(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	3744(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5312(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_54(%rip), %xmm3, %xmm3
	vmovdqa	4800(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5280(%rsp), %r12d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r11
	movq	%r11, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r14
	movq	%r14, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3216(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	movq	2608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r10d
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r9d
	movq	2624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r13d
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movq	2464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3280(%rsp)        # 4-byte Spill
	movq	2560(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	movq	2512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	je	.LBB147_977
# BB#976:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_977:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	je	.LBB147_979
# BB#978:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_979:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_981
# BB#980:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_981:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	%xmm4, 3376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	je	.LBB147_983
# BB#982:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_983:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r12d, %rcx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2912(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2896(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2880(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3072(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movslq	%r13d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5472(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5504(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2704(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2864(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2656(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2832(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2912(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	2896(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3776(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3072(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3312(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3216(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3376(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_985
# BB#984:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovdqa	2784(%rsp), %xmm10      # 16-byte Reload
.LBB147_985:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vandps	3424(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3344(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3328(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4752(%rsp), %rdx        # 8-byte Reload
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3392(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_987
# BB#986:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovdqa	2768(%rsp), %xmm11      # 16-byte Reload
.LBB147_987:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3616(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5216(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3360(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3600(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4256(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3408(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3328(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_989
# BB#988:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovdqa	2800(%rsp), %xmm5       # 16-byte Reload
.LBB147_989:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vmulps	4224(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3440(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4192(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm7
	vmovaps	%xmm7, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3904(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm14
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3376(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovdqa	3328(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmulps	3424(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	vmulps	3296(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	je	.LBB147_991
# BB#990:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
.LBB147_991:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_975 Depth=4
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	5248(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3600(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5216(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5680(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	4224(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4192(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3904(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5312(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3744(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3520(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3456(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3840(%rsp), %rax        # 4-byte Folded Reload
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r15d
	movl	3872(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_975
# BB#992:                               #   in Loop: Header=BB147_972 Depth=3
	movl	2440(%rsp), %ecx        # 4-byte Reload
.LBB147_993:                            # %end for dV.s0.v10.v10405
                                        #   in Loop: Header=BB147_972 Depth=3
	movl	%ecx, %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	cmpl	1432(%rsp), %ecx        # 4-byte Folded Reload
	movl	2252(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_972
.LBB147_994:                            # %produce dh409
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1816(%rsp), %rax        # 8-byte Reload
	movl	2592(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	movl	1432(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	movl	%ecx, %edi
	cmovgl	%esi, %edi
	movl	%edi, 2464(%rsp)        # 4-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 2576(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	%ecx, 2440(%rsp)        # 4-byte Spill
	movl	996(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2512(%rsp)        # 4-byte Spill
	movl	992(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2496(%rsp)        # 4-byte Spill
	movl	988(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2480(%rsp)        # 4-byte Spill
	cmpl	%edi, %edx
	jge	.LBB147_1015
	.align	16, 0x90
.LBB147_995:                            # %for dh.s0.v11411
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_997 Depth 4
	cmpl	$0, 2252(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1014
# BB#996:                               # %for dh.s0.v10.v10413.preheader
                                        #   in Loop: Header=BB147_995 Depth=3
	movl	2592(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1804(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1808(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 2560(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5312(%rsp)       # 16-byte Spill
	cltq
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	movl	2252(%rsp), %eax        # 4-byte Reload
	movq	5352(%rsp), %r15        # 8-byte Reload
	movl	2512(%rsp), %r10d       # 4-byte Reload
	movl	2496(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, 3904(%rsp)        # 4-byte Spill
	movl	2480(%rsp), %r9d        # 4-byte Reload
	.align	16, 0x90
.LBB147_997:                            # %for dh.s0.v10.v10413
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_995 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%r9d, 3808(%rsp)        # 4-byte Spill
	movl	%r10d, 3840(%rsp)       # 4-byte Spill
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	movl	2560(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5280(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r11d
	andl	$1, %r11d
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	4144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5312(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r13d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4992(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5408(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3600(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r13d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5008(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4832(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	4168(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3680(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	leal	-8(%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r13d
	sarq	$32, %rbx
	movq	%rbx, 3440(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3472(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	leal	-7(%r15), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4336(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3968(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3776(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3744(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	vmovdqa	4320(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3952(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	2592(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	2736(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %bl         # 1-byte Reload
	andb	5280(%rsp), %bl         # 1-byte Folded Reload
	andb	5248(%rsp), %r11b       # 1-byte Folded Reload
	movl	%r11d, 5248(%rsp)       # 4-byte Spill
	testl	%r15d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5280(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r13d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r15), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	5024(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, %r9
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r8
	movq	%r8, %r13
	sarq	$32, %r13
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3600(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3488(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3552(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movslq	%r10d, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_999
# BB#998:                               # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_999:                            # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movzbl	2736(%rsp), %r10d       # 1-byte Folded Reload
	vmovd	%r10d, %xmm0
	movzbl	5280(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_1001
# BB#1000:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1001:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	%xmm1, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1003
# BB#1002:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1003:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	je	.LBB147_1005
# BB#1004:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1005:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3440(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3472(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3520(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2832(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2816(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2880(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2864(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2912(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	3904(%rsp), %rbx        # 4-byte Folded Reload
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	%rsi, %r10
	movq	2896(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3168(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3200(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	movq	3072(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3248(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r11d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r8d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2544(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5248(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1007
# BB#1006:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	2624(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5280(%rsp)       # 16-byte Spill
.LBB147_1007:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4720(%rsp), %r8         # 8-byte Reload
	movl	3808(%rsp), %r9d        # 4-byte Reload
	vmovdqa	2736(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1009
# BB#1008:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovdqa	2608(%rsp), %xmm15      # 16-byte Reload
.LBB147_1009:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	movq	2672(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2688(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r12,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2704(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	%r9d, %rcx
	vmovups	24584(%r10,%rcx,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rcx,4), %xmm5
	vmovaps	%xmm5, 3216(%rsp)       # 16-byte Spill
	vmovups	24576(%r10,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	vmovups	24592(%r10,%rcx,4), %xmm9
	vmovups	24608(%r10,%rcx,4), %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5760(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3680(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3680(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5280(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3200(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3184(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_1011
# BB#1010:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_1011:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm5
	vmovaps	%xmm5, 3440(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm7
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm11
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r10,%rax,4), %xmm15
	vmovaps	%xmm15, 3424(%rsp)      # 16-byte Spill
	vmovups	32(%r10,%rax,4), %xmm8
	vmovaps	%xmm8, 3520(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vaddps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3200(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmulps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovdqa	3184(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	3168(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovdqa	4256(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4256(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	je	.LBB147_1013
# BB#1012:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_1013:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_997 Depth=4
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3776(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3712(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3472(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5280(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5488(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3520(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	5216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3600(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4256(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3488(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3552(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	2528(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r8,%rax,4)
	addl	$8, %r9d
	addl	$8, 3904(%rsp)          # 4-byte Folded Spill
	movl	3840(%rsp), %r10d       # 4-byte Reload
	addl	$8, %r10d
	addl	$8, %r15d
	movl	3872(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_997
.LBB147_1014:                           # %end for dh.s0.v10.v10414
                                        #   in Loop: Header=BB147_995 Depth=3
	movl	2592(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2592(%rsp)        # 4-byte Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	addl	%eax, 2480(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2496(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2512(%rsp)        # 4-byte Folded Spill
	cmpl	2464(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_995
.LBB147_1015:                           # %end for dh.s0.v11412
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	2440(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 2464(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_1075
# BB#1016:                              #   in Loop: Header=BB147_467 Depth=2
	movq	1672(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	movl	948(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %edx
	imull	1832(%rsp), %edx        # 4-byte Folded Reload
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	notl	%ecx
	movq	712(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movl	%eax, 2448(%rsp)        # 4-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1017:                           # %for dh.s0.v11417
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1019 Depth 4
                                        #         Child Loop BB147_1038 Depth 4
                                        #         Child Loop BB147_1057 Depth 4
	cmpl	$0, 1308(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1036
# BB#1018:                              # %for dh.s0.v10.v10419.preheader
                                        #   in Loop: Header=BB147_1017 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 2624(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5280(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1784(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2560(%rsp)        # 8-byte Spill
	movl	1144(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %r13        # 8-byte Reload
	movq	2480(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1019:                           # %for dh.s0.v10.v10419
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1017 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 4192(%rsp)        # 4-byte Spill
	movl	2624(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	4256(%rsp)              # 1-byte Folded Spill
	movl	%r13d, %r14d
	andl	$1, %r14d
	sete	%r15b
	movq	4144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5280(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 3904(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r12d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4992(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5408(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3776(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r12d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5008(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4832(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	movq	4168(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3808(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	leal	-8(%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r12d
	sarq	$32, %rbx
	movq	%rbx, 3520(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3616(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3872(%rsp)       # 16-byte Spill
	leal	-7(%r13), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2880(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4336(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3968(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3600(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	3904(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3872(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3872(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	vmovdqa	4320(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3952(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3904(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	andb	5216(%rsp), %r15b       # 1-byte Folded Reload
	andb	4256(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 5216(%rsp)       # 4-byte Spill
	testl	%r13d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	4256(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r12d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%r15b, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	5024(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3840(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, %r11
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r14
	movq	%r14, %r12
	sarq	$32, %r12
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3776(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2704(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	3600(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3648(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3472(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	4928(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %r8d
	movq	4936(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %edx
	movl	%edx, 2752(%rsp)        # 4-byte Spill
	je	.LBB147_1021
# BB#1020:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1021:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movzbl	%bl, %ebx
	vmovd	%ebx, %xmm0
	movzbl	4256(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_1023
# BB#1022:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1023:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1025
# BB#1024:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1025:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2672(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3776(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	je	.LBB147_1027
# BB#1026:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1027:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3520(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3616(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2896(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2880(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2864(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3072(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2912(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3184(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r8d, %rbx
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	%rsi, %r8
	movq	3168(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3216(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3248(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	movq	3200(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3264(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3312(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r15d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r14d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r12,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2608(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm2
	vminps	%xmm2, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm2, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm13
	vmulps	3808(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm2, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	vmovdqa	5216(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1029
# BB#1028:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovdqa	2656(%rsp), %xmm15      # 16-byte Reload
.LBB147_1029:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vandps	%xmm13, %xmm1, %xmm12
	vandps	%xmm13, %xmm4, %xmm1
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovdqa	2784(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_1031
# BB#1030:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovdqa	2640(%rsp), %xmm8       # 16-byte Reload
.LBB147_1031:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	movq	2704(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2720(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2736(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2816(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2848(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2752(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3248(%rsp)       # 16-byte Spill
	vmovups	24576(%r8,%rcx,4), %xmm11
	vmovaps	%xmm11, 3296(%rsp)      # 16-byte Spill
	vmovups	24592(%r8,%rcx,4), %xmm9
	vmovups	24608(%r8,%rcx,4), %xmm10
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5760(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm10, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3808(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3808(%rsp)      # 16-byte Spill
	vandps	%xmm13, %xmm14, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vandps	3744(%rsp), %xmm13, %xmm10 # 16-byte Folded Reload
	vpslld	$31, %xmm15, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vminps	%xmm2, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm5
	vfnmadd213ps	%xmm0, %xmm5, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm8, %xmm0
	vmovdqa	%xmm0, 3520(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm3, %xmm0
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovdqa	3776(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_1033
# BB#1032:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovdqa	2672(%rsp), %xmm11      # 16-byte Reload
.LBB147_1033:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm6
	vmovaps	%xmm6, 3392(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm7
	vmovaps	%xmm7, 3376(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3472(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm8
	vmovaps	%xmm8, 3744(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r8,%rax,4), %xmm14
	vmovaps	%xmm14, 3488(%rsp)      # 16-byte Spill
	vmovups	32(%r8,%rax,4), %xmm15
	vmovaps	%xmm15, 3600(%rsp)      # 16-byte Spill
	vaddps	3712(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm4
	vshufps	$136, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm7[0,2]
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm10, %xmm0
	vshufps	$136, %xmm8, %xmm14, %xmm6 # xmm6 = xmm14[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm10, %xmm3
	vshufps	$136, %xmm15, %xmm8, %xmm6 # xmm6 = xmm8[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm5, %xmm3
	vmovaps	%xmm5, 3776(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm3, %xmm0
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3616(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmulps	3680(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovdqa	3520(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmulps	3232(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm11, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	je	.LBB147_1035
# BB#1034:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_1035:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1019 Depth=4
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3872(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm14, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3904(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3840(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm11, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3312(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm4
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm9, %xmm4
	vmovdqa	5488(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm14, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm10, %xmm1
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm8
	vpaddd	%xmm15, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	3744(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm11, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3600(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm10, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm2, %xmm8, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm9, %xmm1
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm5, %xmm2
	vmovaps	3552(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3520(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3712(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm6, %xmm8, %xmm5, %xmm6
	vandps	%xmm13, %xmm1, %xmm1
	vmovaps	3808(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3680(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3616(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm13, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4720(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	5248(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r13d
	movl	4192(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1019
.LBB147_1036:                           # %end for dh.s0.v10.v10420
                                        #   in Loop: Header=BB147_1017 Depth=3
	movl	1308(%rsp), %eax        # 4-byte Reload
	cmpl	1340(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1055
# BB#1037:                              # %for dh.s0.v10.v10423.preheader
                                        #   in Loop: Header=BB147_1017 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3344(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1784(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 3312(%rsp)        # 8-byte Spill
	movl	1108(%rsp), %ecx        # 4-byte Reload
	movl	1148(%rsp), %eax        # 4-byte Reload
	movl	%eax, %edx
	movl	2448(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r13d
	.align	16, 0x90
.LBB147_1038:                           # %for dh.s0.v10.v10423
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1017 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rdx, 3904(%rsp)        # 8-byte Spill
	movl	%ecx, 4192(%rsp)        # 4-byte Spill
	movl	3344(%rsp), %r10d       # 4-byte Reload
	testl	%r10d, %r10d
	setne	%r15b
	sete	5248(%rsp)              # 1-byte Folded Spill
	leal	-8(%rdx), %eax
	movslq	%eax, %r12
	movq	%r12, 3872(%rsp)        # 8-byte Spill
	andl	$1, %eax
	sete	%r14b
	leaq	-6(%r12), %rdx
	movq	4664(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rdx
	leaq	-4(%r12), %r8
	imulq	%r11, %r8
	leaq	-8(%r12), %rdi
	imulq	%r11, %rdi
	leaq	-5(%r12), %rcx
	imulq	%r11, %rcx
	movq	%rcx, 5280(%rsp)        # 8-byte Spill
	movq	4312(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdx), %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	leaq	-7(%r12), %r9
	imulq	%r11, %r9
	movq	%r9, 3840(%rsp)         # 8-byte Spill
	movl	%r12d, %esi
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %esi
	testb	$1, %sil
	sete	3456(%rsp)              # 1-byte Folded Spill
	andb	%r15b, %r14b
	andb	5248(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	testl	%r12d, %r10d
	setne	3808(%rsp)              # 1-byte Folded Spill
	leaq	-3(%r12), %r15
	imulq	%r11, %r15
	movq	%r15, 3408(%rsp)        # 8-byte Spill
	movq	4744(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rdx), %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	movq	4888(%rsp), %r10        # 8-byte Reload
	leaq	(%rdx,%r10), %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	leal	-8(%r13), %edx
	movslq	%edx, %r11
	movq	%r11, %rcx
	orq	$2, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	movzbl	%r14b, %ecx
	vmovd	%ecx, %xmm0
	movq	%r11, %rcx
	orq	$6, %rcx
	movq	%rcx, 3600(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rdi), %r12
	leaq	(%rsi,%rdi), %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	movq	%r10, %rdx
	leaq	(%rdi,%rdx), %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	movq	%r11, %rcx
	orq	$4, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	leaq	(%rbx,%r8), %r10
	leaq	(%rsi,%r8), %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	leaq	(%r8,%rdx), %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vxorps	%xmm13, %xmm13, %xmm13
	cmpl	$1, 104(%rbp)
	movq	2224(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %ecx
	movq	5280(%rsp), %rax        # 8-byte Reload
	leaq	(%rbx,%rax), %r8
	leaq	(%rbx,%r9), %r9
	leaq	(%rbx,%r15), %r15
	movq	2216(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	je	.LBB147_1040
# BB#1039:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1040:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movzbl	3456(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	3808(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1042
# BB#1041:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1042:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5248(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %edi
	vmovd	%edi, %xmm0
	vmovaps	%xmm3, %xmm2
	je	.LBB147_1044
# BB#1043:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1044:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3440(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	movq	%r13, 3424(%rsp)        # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm10
	vmovdqa	%xmm10, %xmm0
	movq	5672(%rsp), %r14        # 8-byte Reload
	je	.LBB147_1046
# BB#1045:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_1046:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovdqa	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rdx        # 8-byte Reload
	movq	3472(%rsp), %r13        # 8-byte Reload
	leaq	(%rdx,%r13,4), %rdi
	movq	4736(%rsp), %rax        # 8-byte Reload
	leaq	(%rdi,%rax,4), %rbx
	leaq	(%rbx,%rax,4), %rsi
	vmovss	(%rdx,%r13,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3328(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm0, %xmm11, %xmm1
	movslq	%ecx, %rbx
	vmovups	12296(%r14,%rbx,4), %xmm0
	vmovups	12312(%r14,%rbx,4), %xmm7
	vshufps	$136, %xmm7, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm7[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm5
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm2
	leaq	(%rdx,%r10,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdx,%r10,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm11, %xmm3
	vmovups	12304(%r14,%rbx,4), %xmm1
	vmovups	12320(%r14,%rbx,4), %xmm14
	vshufps	$136, %xmm14, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm12
	vbroadcastss	.LCPI147_21(%rip), %xmm15
	leaq	(%rdx,%r12,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	vmovss	(%rdx,%r12,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm11, %xmm3
	vmovups	12288(%r14,%rbx,4), %xmm6
	vshufps	$136, %xmm1, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm1[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	leaq	(%rdx,%r8,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r8,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm11, %xmm3
	vshufps	$221, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm3, %xmm0, %xmm3
	leaq	(%rdx,%r9,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r9,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm11, %xmm0
	vshufps	$221, %xmm1, %xmm6, %xmm4 # xmm4 = xmm6[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm7
	vsubps	%xmm0, %xmm7, %xmm3
	leaq	(%rdx,%r15,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r15,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%rax, %rcx
	vmulps	%xmm4, %xmm11, %xmm4
	vshufps	$221, %xmm14, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm14      # 16-byte Reload
	je	.LBB147_1048
# BB#1047:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovdqa	3392(%rsp), %xmm14      # 16-byte Reload
.LBB147_1048:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vandps	%xmm15, %xmm12, %xmm11
	vandps	%xmm15, %xmm2, %xmm8
	vsubps	%xmm7, %xmm0, %xmm9
	vsubps	%xmm7, %xmm1, %xmm12
	vandps	%xmm15, %xmm3, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movq	4744(%rsp), %r10        # 8-byte Reload
	movq	4888(%rsp), %r9         # 8-byte Reload
	je	.LBB147_1050
# BB#1049:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovdqa	3456(%rsp), %xmm10      # 16-byte Reload
.LBB147_1050:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	movq	%rdx, %rdi
	movq	3552(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rax
	movq	%rcx, %rbx
	leaq	(%rax,%rbx,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4256(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	movslq	3488(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovups	24600(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm2[0,2]
	vmovaps	5728(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	movq	3520(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r8
	vmovss	(%rdi,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r8,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm6, %xmm1
	vmovups	24576(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 3520(%rsp)       # 16-byte Spill
	vmovups	24592(%r14,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm7[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	movq	3776(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r8
	vmovss	(%rdi,%rsi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r8,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vmovups	24608(%r14,%rax,4), %xmm6
	vmovaps	%xmm6, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm7, %xmm6 # xmm6 = xmm7[0,2],xmm6[0,2]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vaddps	%xmm8, %xmm11, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 3776(%rsp)      # 16-byte Spill
	vandps	%xmm15, %xmm9, %xmm3
	vandps	%xmm15, %xmm12, %xmm4
	vpslld	$31, %xmm14, %xmm0
	vmovdqa	%xmm0, 3376(%rsp)       # 16-byte Spill
	vminps	%xmm5, %xmm2, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm2
	vminps	%xmm5, %xmm1, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm5, %xmm6, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vfnmadd213ps	%xmm0, %xmm12, %xmm2
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm10, %xmm14
	vandps	%xmm15, %xmm2, %xmm2
	vaddps	5248(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	je	.LBB147_1052
# BB#1051:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_1052:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vaddps	%xmm4, %xmm3, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	movq	%rdi, %rsi
	movq	3616(%rsp), %rdx        # 8-byte Reload
	leaq	(%rsi,%rdx,4), %rax
	movq	%rbx, %rdi
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rbx
	vmovss	(%rsi,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	4224(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm2, %xmm0, %xmm2
	movq	3680(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 3600(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm3[0,2]
	vmovaps	5680(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm2, %xmm8
	movq	3648(%rsp), %rbx        # 8-byte Reload
	leaq	(%rsi,%rbx,4), %rax
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rdx
	vmovss	(%rsi,%rbx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm0, %xmm4
	vmovups	(%r14,%r11,4), %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm2
	vshufps	$136, %xmm2, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm2[0,2]
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	movq	3744(%rsp), %rbx        # 8-byte Reload
	leaq	(%rsi,%rbx,4), %rax
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rdx
	vmovss	(%rsi,%rbx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	%rdi, %rdx
	vmulps	%xmm6, %xmm0, %xmm6
	vmovups	32(%r14,%r11,4), %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm0[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vaddps	%xmm3, %xmm4, %xmm3
	vminps	%xmm5, %xmm8, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vfnmadd213ps	%xmm3, %xmm12, %xmm4
	vandps	%xmm15, %xmm4, %xmm3
	vaddps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovdqa	3376(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmulps	3392(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm14, %xmm0
	vmovdqa	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm10, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovdqa	5216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpsrad	$31, %xmm4, %xmm10
	vmulps	%xmm11, %xmm3, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovdqa	3808(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1054
# BB#1053:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovdqa	3360(%rsp), %xmm9       # 16-byte Reload
.LBB147_1054:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1038 Depth=4
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	movq	5280(%rsp), %r8         # 8-byte Reload
	leaq	(%r10,%r8), %rax
	movq	%rsi, %rdi
	leaq	(%rdi,%rax,4), %rcx
	movq	%rdx, %rbx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	5728(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm3, %xmm3
	vmovaps	5760(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm3, %xmm14, %xmm3
	vmulps	%xmm4, %xmm3, %xmm4
	movq	3840(%rsp), %rsi        # 8-byte Reload
	leaq	(%r10,%rsi), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm0, %xmm3
	vmovaps	3520(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm6 # xmm6 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm3, %xmm6, %xmm3
	vshufps	$221, 3456(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm7[1,3],mem[1,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	leaq	(%r10,%rdx), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm0, %xmm7
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vminps	%xmm5, %xmm6, %xmm6
	vmaxps	%xmm13, %xmm6, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm8
	vfnmadd213ps	%xmm3, %xmm12, %xmm8
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	leaq	(%r8,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4224(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	5680(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	leaq	(%rsi,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, %xmm2, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm2[1,3]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm4, %xmm6, %xmm4
	vshufps	$221, 3440(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	leaq	(%rdx,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm7, %xmm6
	vsubps	%xmm0, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vminps	%xmm5, %xmm3, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm2, %xmm2
	vaddps	%xmm2, %xmm4, %xmm2
	vmaxps	%xmm13, %xmm3, %xmm3
	vfnmadd213ps	%xmm2, %xmm12, %xmm3
	vpslld	$31, %xmm9, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm1, %xmm7, %xmm13, %xmm2
	vblendvps	%xmm10, 5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3552(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vblendvps	%xmm10, %xmm4, %xmm13, %xmm5
	vandps	%xmm15, %xmm3, %xmm3
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vaddps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm11, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm3, %xmm5, %xmm1
	vmovaps	3744(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm7, %xmm2, %xmm2
	vandps	%xmm15, %xmm8, %xmm3
	vaddps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm11, %xmm3, %xmm0
	vblendvps	%xmm5, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3312(%rsp), %rax        # 8-byte Reload
	movq	3872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	4720(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	3424(%rsp), %r13        # 8-byte Reload
	addl	$8, %r13d
	movq	3904(%rsp), %rdx        # 8-byte Reload
	addl	$8, %edx
	movl	4192(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1038
.LBB147_1055:                           # %end for dh.s0.v10.v10424
                                        #   in Loop: Header=BB147_1017 Depth=3
	movl	1340(%rsp), %eax        # 4-byte Reload
	cmpl	2252(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1074
# BB#1056:                              # %for dh.s0.v10.v10427.preheader
                                        #   in Loop: Header=BB147_1017 Depth=3
	movq	5312(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 4224(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5280(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1784(%rsp), %rdi        # 8-byte Folded Reload
	subq	4760(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2544(%rsp)        # 8-byte Spill
	movq	1088(%rsp), %rax        # 8-byte Reload
	movq	2480(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	movq	1080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	movq	1096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1140(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1057:                           # %for dh.s0.v10.v10427
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1017 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3904(%rsp)        # 4-byte Spill
	cmpl	$0, 4224(%rsp)          # 4-byte Folded Reload
	setne	5248(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	2048(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movl	%r9d, 3872(%rsp)        # 4-byte Spill
	movl	%r9d, %r14d
	andl	$1, %r14d
	sete	4256(%rsp)              # 1-byte Folded Spill
	movq	1896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movq	1904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vmovdqa	5280(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm8
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 3840(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovd	%r9d, %xmm0
	vpbroadcastd	%xmm0, %xmm2
	vmovdqa	%xmm2, 3776(%rsp)       # 16-byte Spill
	vmovdqa	4992(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm0, %xmm1
	vmovdqa	%xmm2, %xmm7
	movq	2000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vmovdqa	5376(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm4, %xmm4
	vmovdqa	5408(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vmovdqa	5392(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm13, %xmm5
	vmovdqa	5360(%rsp), %xmm10      # 16-byte Reload
	vpsubd	%xmm3, %xmm10, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	movq	1912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vmovd	%xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovdqa	5424(%rsp), %xmm11      # 16-byte Reload
	vpmulld	%xmm11, %xmm1, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	1920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpand	%xmm8, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm8, %xmm4, %xmm4
	vmovdqa	5008(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm2, %xmm5
	movq	2008(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm9, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpcmpgtd	%xmm1, %xmm13, %xmm6
	vpsubd	%xmm1, %xmm10, %xmm2
	vblendvps	%xmm6, %xmm1, %xmm2, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm5, %xmm7, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	movq	1928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5168(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm0, %xmm15, %xmm4
	vmovd	%xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrq	$1, %xmm4, %rbx
	movq	%rbx, 3744(%rsp)        # 8-byte Spill
	vmovq	%xmm4, %r15
	movq	%r15, 3648(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	movq	2016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vmovd	%esi, %xmm5
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r10d
	sarq	$32, %r15
	movq	%r15, 3488(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm5, %xmm2
	sarq	$32, %rbx
	movq	%rbx, 3472(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm15, %xmm6
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vmovq	%xmm6, %rcx
	movq	%rcx, 3520(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm2, %xmm5
	movq	2024(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	movq	2032(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm8
	sarq	$32, %rcx
	movq	%rcx, 2880(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm6, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm13, %xmm6
	vpsubd	%xmm3, %xmm10, %xmm1
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vmovdqa	4336(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3776(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm4, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm3
	vpaddd	%xmm3, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm5, %xmm1
	vpand	5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm1, %xmm1
	vmovdqa	4976(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm4, %xmm4
	vpbroadcastd	%xmm7, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vmovdqa	3840(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm13, %xmm6
	vpsubd	%xmm0, %xmm10, %xmm7
	vblendvps	%xmm6, %xmm0, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm5, %xmm6, %xmm4
	vpmulld	%xmm11, %xmm4, %xmm4
	vmovdqa	%xmm4, 3808(%rsp)       # 16-byte Spill
	vpaddd	%xmm4, %xmm15, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	4320(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm4, %xmm4
	vmovdqa	%xmm2, %xmm7
	vpbroadcastd	%xmm8, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm13, %xmm5
	vpsubd	%xmm1, %xmm10, %xmm6
	vblendvps	%xmm5, %xmm1, %xmm6, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm2, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	%xmm1, 3840(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	1936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	2752(%rsp)              # 1-byte Folded Spill
	movb	4256(%rsp), %bl         # 1-byte Reload
	andb	5248(%rsp), %bl         # 1-byte Folded Reload
	andb	5216(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 5216(%rsp)       # 4-byte Spill
	testl	4224(%rsp), %r9d        # 4-byte Folded Reload
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	setne	5248(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r10d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	movq	2040(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm2
	vpsubd	%xmm1, %xmm10, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5024(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm0, %xmm0
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm0, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, %r12
	sarq	$32, %r12
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, %r11
	sarq	$32, %r11
	vmovdqa	3712(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r8
	movq	%r8, 2672(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %r15
	movq	%r15, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vmovdqa	3680(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3680(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %r14d
	movq	2528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 2720(%rsp)        # 4-byte Spill
	je	.LBB147_1059
# BB#1058:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1059:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movzbl	2752(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	5248(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_1061
# BB#1060:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1061:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	%xmm1, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1063
# BB#1062:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1063:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3712(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	je	.LBB147_1065
# BB#1064:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1065:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3472(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3520(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2880(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2848(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2864(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movq	2832(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2912(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2896(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %rbx
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	3184(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3296(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3328(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rbx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3344(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3376(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rbx,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r9d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%edi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2592(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm13, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm4, %xmm4
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm4, %xmm4
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm13, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5216(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm13, %xmm5, %xmm5
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1067
# BB#1066:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	2624(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5248(%rsp)       # 16-byte Spill
.LBB147_1067:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vandps	%xmm2, %xmm1, %xmm13
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4720(%rsp), %r9         # 8-byte Reload
	vmovdqa	2752(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1069
# BB#1068:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovdqa	2608(%rsp), %xmm15      # 16-byte Reload
.LBB147_1069:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	movq	2672(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2688(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2704(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2816(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2720(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vmovups	24576(%rsi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3344(%rsp)      # 16-byte Spill
	vmovups	24592(%rsi,%rcx,4), %xmm9
	vmovups	24608(%rsi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5760(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3744(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 3744(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3520(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3488(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovdqa	3712(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_1071
# BB#1070:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovdqa	2640(%rsp), %xmm13      # 16-byte Reload
.LBB147_1071:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	movq	3072(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 3360(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm7
	vmovaps	%xmm7, 3712(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3600(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%rsi,%rax,4), %xmm15
	vmovaps	%xmm15, 3440(%rsp)      # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3552(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm4
	vshufps	$136, %xmm6, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm6[0,2]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm10, %xmm0
	vshufps	$136, %xmm7, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm10, %xmm3
	vshufps	$136, %xmm8, %xmm7, %xmm6 # xmm6 = xmm7[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3520(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmulps	3648(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovdqa	3488(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmulps	3296(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3520(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	je	.LBB147_1073
# BB#1072:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_1073:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1057 Depth=4
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3808(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3840(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3776(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm14, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3472(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm5, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm8
	vmovaps	5248(%rsp), %xmm13      # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm13, %xmm8
	vmovdqa	5488(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm15, %xmm4, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm10, %xmm1
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm7, %xmm4, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm14, %xmm4, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3552(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm7[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm10, %xmm7
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm12, %xmm3, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm13, %xmm1
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm0, %xmm7, %xmm9, %xmm2
	vmovaps	3520(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, 3488(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmulps	3680(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm4, %xmm3, %xmm9, %xmm6
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm11, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3600(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm7, %xmm1, %xmm1
	vandps	%xmm2, %xmm8, %xmm2
	vaddps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm11, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm6, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3872(%rsp), %rax        # 4-byte Folded Reload
	movq	2544(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	addl	$8, %r13d
	movl	3904(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1057
.LBB147_1074:                           # %end for dh.s0.v10.v10428
                                        #   in Loop: Header=BB147_1017 Depth=3
	movl	2464(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2464(%rsp)        # 4-byte Spill
	addq	$1, 5312(%rsp)          # 8-byte Folded Spill
	movl	1832(%rsp), %eax        # 4-byte Reload
	movq	2480(%rsp), %rdx        # 8-byte Reload
	addl	%eax, %edx
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	addl	%eax, 2448(%rsp)        # 4-byte Folded Spill
	cmpl	2440(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1017
.LBB147_1075:                           # %end for dh.s0.v11418
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	2576(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1432(%rsp)        # 4-byte Folded Reload
	jle	.LBB147_1097
# BB#1076:                              #   in Loop: Header=BB147_467 Depth=2
	movq	912(%rsp), %rax         # 8-byte Reload
	movq	952(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rax), %eax
	imull	1832(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 2544(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_1077:                           # %for dh.s0.v11431
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1079 Depth 4
	cmpl	$0, 2252(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1096
# BB#1078:                              # %for dh.s0.v10.v10434.preheader
                                        #   in Loop: Header=BB147_1077 Depth=3
	movl	2576(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %edi
	movl	%r8d, %eax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	andl	$1, %edi
	movl	%edi, 2608(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1804(%rsp), %ebx        # 4-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1848(%rsp), %rdi        # 8-byte Reload
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%esi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movq	1808(%rsp), %rax        # 8-byte Reload
	cmpl	%r8d, %eax
	cmovgl	%ecx, %edx
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5312(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	imulq	1880(%rsp), %rax        # 8-byte Folded Reload
	movq	1840(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	1872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1864(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	imulq	1784(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movl	2252(%rsp), %ecx        # 4-byte Reload
	movq	5352(%rsp), %r15        # 8-byte Reload
	movl	2544(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1079:                           # %for dh.s0.v10.v10434
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1077 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5280(%rsp)        # 8-byte Spill
	movl	%ecx, 3872(%rsp)        # 4-byte Spill
	movl	2608(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5248(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r12d
	andl	$1, %r12d
	sete	%r13b
	movq	4144(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5312(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4256(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4992(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4816(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5360(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5408(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4696(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 4224(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5008(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4832(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	4168(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovdqa	5168(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3744(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	leal	-8(%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3488(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3600(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3840(%rsp)       # 16-byte Spill
	leal	-7(%r15), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2880(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2864(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4336(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3968(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4256(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4976(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3840(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3808(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vmovdqa	4320(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3952(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3840(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	2576(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	andb	5248(%rsp), %r13b       # 1-byte Folded Reload
	andb	5216(%rsp), %r12b       # 1-byte Folded Reload
	movl	%r12d, 5216(%rsp)       # 4-byte Spill
	testl	%r15d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5248(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r15), %eax
	vmovd	%eax, %xmm4
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	5024(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4848(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r13
	movq	%r13, %r11
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r12
	movq	%r12, %r14
	sarq	$32, %r14
	vmovdqa	5440(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	4224(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2688(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	3552(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3616(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	vmovdqa	5488(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	movq	5280(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3440(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3520(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	movq	4928(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %r8d
	movq	4936(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %edx
	movl	%edx, 2736(%rsp)        # 4-byte Spill
	je	.LBB147_1081
# BB#1080:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1081:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movzbl	%bl, %ebx
	vmovd	%ebx, %xmm0
	movzbl	5248(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4256(%rsp)       # 16-byte Spill
	je	.LBB147_1083
# BB#1082:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1083:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	%xmm1, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1085
# BB#1084:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1085:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	je	.LBB147_1087
# BB#1086:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1087:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3488(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3600(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2880(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3648(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2864(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2848(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2912(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2896(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r8d, %rbx
	movq	5672(%rsp), %r8         # 8-byte Reload
	vmovups	12296(%r8,%rbx,4), %xmm7
	vmovups	12312(%r8,%rbx,4), %xmm0
	vmovups	12304(%r8,%rbx,4), %xmm3
	vmovups	12320(%r8,%rbx,4), %xmm14
	vmovups	12288(%r8,%rbx,4), %xmm5
	movq	3072(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3200(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3232(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3184(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3248(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3296(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r13d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r12d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r14,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2592(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5472(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5504(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5216(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1089
# BB#1088:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	2640(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5248(%rsp)       # 16-byte Spill
.LBB147_1089:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4720(%rsp), %r11        # 8-byte Reload
	vmovdqa	2768(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1091
# BB#1090:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovdqa	2624(%rsp), %xmm15      # 16-byte Reload
.LBB147_1091:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	movq	2688(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2704(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2720(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2832(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2736(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3248(%rsp)       # 16-byte Spill
	vmovups	24576(%r8,%rcx,4), %xmm10
	vmovaps	%xmm10, 3296(%rsp)      # 16-byte Spill
	vmovups	24592(%r8,%rcx,4), %xmm9
	vmovups	24608(%r8,%rcx,4), %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5728(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5760(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3744(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_1093
# BB#1092:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
.LBB147_1093:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm7
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm11
	movq	3472(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3520(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r8,%rax,4), %xmm15
	vmovaps	%xmm15, 3472(%rsp)      # 16-byte Spill
	vmovups	32(%r8,%rax,4), %xmm8
	vmovaps	%xmm8, 3600(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	3904(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5680(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3648(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmulps	3680(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovdqa	3232(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmulps	3216(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovdqa	4224(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4224(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	je	.LBB147_1095
# BB#1094:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
.LBB147_1095:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1079 Depth=4
	vmovdqa	5440(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3808(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3840(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3776(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3488(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5248(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5488(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	3904(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5680(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3472(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3600(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	4256(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4224(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3552(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3712(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3520(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3744(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3680(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3616(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r11,%rax,4)
	movq	5280(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r15d
	movl	3872(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1079
.LBB147_1096:                           # %end for dh.s0.v10.v10435
                                        #   in Loop: Header=BB147_1077 Depth=3
	movl	2576(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2576(%rsp)        # 4-byte Spill
	movl	2544(%rsp), %ecx        # 4-byte Reload
	addl	1832(%rsp), %ecx        # 4-byte Folded Reload
	movl	%ecx, 2544(%rsp)        # 4-byte Spill
	cmpl	1432(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1077
.LBB147_1097:                           # %for f4.s0.v11441.preheader
                                        #   in Loop: Header=BB147_467 Depth=2
	movl	1012(%rsp), %r9d        # 4-byte Reload
	movl	1436(%rsp), %eax        # 4-byte Reload
	movl	2248(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	movq	864(%rsp), %r15         # 8-byte Reload
	movq	4720(%rsp), %rbx        # 8-byte Reload
	vmovaps	768(%rsp), %ymm2        # 32-byte Reload
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vmovdqu	.LCPI147_22(%rip), %xmm3
	jle	.LBB147_1098
	.align	16, 0x90
.LBB147_1105:                           # %for f4.s0.v10.v10444.preheader.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1101 Depth 4
                                        #           Child Loop BB147_1102 Depth 5
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	movl	%eax, %r11d
	andl	$63, %r11d
	movl	%r11d, %r10d
	movq	1232(%rsp), %rax        # 8-byte Reload
	imull	%eax, %r10d
	imulq	1264(%rsp), %r11        # 8-byte Folded Reload
	subq	4760(%rsp), %r11        # 8-byte Folded Reload
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB147_1101:                           # %for f4.s0.v10.v10444.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1105 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB147_1102 Depth 5
	leal	(,%r14,8), %r13d
	vxorps	%ymm0, %ymm0, %ymm0
	movl	$9, %ecx
	movl	%r9d, %eax
	movq	4752(%rsp), %r8         # 8-byte Reload
	.align	16, 0x90
.LBB147_1102:                           # %for sum.s1.r4$y449.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1105 Depth=3
                                        #         Parent Loop BB147_1101 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	movl	%eax, %esi
	andl	$63, %esi
	imull	%r15d, %esi
	leal	(%r13,%rsi), %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$1, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$2, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$3, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$4, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$5, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$6, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	orl	$7, %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	leal	8(%r13,%rsi), %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	addl	$1, %eax
	addl	$-1, %ecx
	jne	.LBB147_1102
# BB#1103:                              # %consume sum455.us
                                        #   in Loop: Header=BB147_1101 Depth=4
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%r13,%rax), %eax
	addl	%r10d, %r13d
	vpbroadcastd	%xmm3, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm0
	movslq	%r13d, %rcx
	movq	4872(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm1
	movq	4880(%rsp), %rdx        # 8-byte Reload
	vblendvps	%ymm0, (%rdx,%rcx,4), %ymm1, %ymm0
	cltq
	leaq	(%rax,%r11), %rax
	movq	5096(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r14
	cmpl	%r12d, %r14d
	jne	.LBB147_1101
# BB#1104:                              # %end for f4.s0.v10.v10445.us
                                        #   in Loop: Header=BB147_1105 Depth=3
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %eax
	addl	$1, %r9d
	cmpl	2384(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1105
.LBB147_1098:                           # %produce f7457
                                        #   in Loop: Header=BB147_467 Depth=2
	notl	1028(%rsp)              # 4-byte Folded Spill
	movl	1708(%rsp), %eax        # 4-byte Reload
	movq	1072(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %esi
	cmovgel	%eax, %esi
	movl	1436(%rsp), %eax        # 4-byte Reload
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	movq	%rsi, 2176(%rsp)        # 8-byte Spill
	leal	1(%rcx), %edx
	movl	1044(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%eax, %edx
	addl	$1, %edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movq	%rdx, 2440(%rsp)        # 8-byte Spill
	movl	%ecx, %r8d
	cmpl	%esi, %ecx
	movl	1800(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_1099
	jmp	.LBB147_1142
.LBB147_1100:                           # %for f7.s0.v11459.end for f7.s0.v10.v10462_crit_edge
                                        #   in Loop: Header=BB147_1099 Depth=3
	addl	$1, %r8d
	movl	%r8d, %ecx
	jmp	.LBB147_1141
	.align	16, 0x90
.LBB147_1099:                           # %for f7.s0.v11459
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1107 Depth 4
	testl	%eax, %eax
	jle	.LBB147_1100
# BB#1106:                              # %for f7.s0.v10.v10461.preheader
                                        #   in Loop: Header=BB147_1099 Depth=3
	movq	%r8, 2464(%rsp)         # 8-byte Spill
	movl	%r8d, %edi
	movq	1816(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r10d
	subl	%eax, %esi
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r9
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1808(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %r14d
	cmovgl	%r8d, %r14d
	addl	$-1, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %ecx
	cmovll	%esi, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r9d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %edi
	cmovgl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r8d, %ecx
	cmovlel	%edx, %edi
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r14d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	leal	1(%r8), %eax
	movl	%eax, 2256(%rsp)        # 4-byte Spill
	cmpl	%eax, %r13d
	movl	%r13d, %r9d
	cmovgl	%eax, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	cmpl	%r8d, %r13d
	cmovlel	%r11d, %r9d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2448(%rsp)       # 16-byte Spill
	movslq	%edi, %r10
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%edi, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1880(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %r10
	movq	1864(%rsp), %r12        # 8-byte Reload
	leaq	(%r10,%r12), %rcx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%esi, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%esi, %edx
	cmovgl	%esi, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	2(%r8), %r10d
	cmpl	%r10d, %r13d
	movl	%r13d, %edx
	cmovgl	%r10d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r15, %rcx
	leaq	(%rcx,%r12), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1848(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	-2(%r8), %esi
	cmpl	%esi, %r13d
	movl	%r13d, %eax
	cmovgl	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %eax
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %eax
	cltq
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movslq	%r14d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%r11d, %r9d
	movslq	%r9d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movq	2480(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1792(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	movl	2256(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2368(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1832(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2352(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2336(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2320(%rsp)        # 8-byte Spill
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 2400(%rsp)        # 8-byte Spill
	movq	1464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2304(%rsp)        # 8-byte Spill
	andl	$63, %r10d
	imull	%eax, %r10d
	movq	%r10, 2416(%rsp)        # 8-byte Spill
	movq	1456(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2288(%rsp)        # 8-byte Spill
	movq	1592(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1107:                           # %for f7.s0.v10.v10461
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1099 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3072(%rsp)        # 4-byte Spill
	cmpl	$0, 3168(%rsp)          # 4-byte Folded Reload
	sete	3840(%rsp)              # 1-byte Folded Spill
	setne	3744(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4256(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r15d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2448(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5216(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5120(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5056(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5360(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5376(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5424(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3808(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r14d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3776(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4960(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4784(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r14d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r15d
	movl	%r15d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2800(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2848(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r14d
	vmovq	%xmm1, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	movb	3744(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5280(%rsp)       # 1-byte Folded Spill
	movl	5216(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2464(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r14
	sete	3264(%rsp)              # 1-byte Folded Spill
	movl	3168(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3232(%rsp)              # 1-byte Folded Spill
	movb	3840(%rsp), %r10b       # 1-byte Reload
	movl	5312(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r12d
	movzbl	3264(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r15b
	movq	2288(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movslq	%ecx, %r10
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movslq	%edi, %rcx
	movq	%r10, %rsi
	orq	$6, %rsi
	movq	%rsi, 3344(%rsp)        # 8-byte Spill
	movq	%rcx, %r12
	movq	%rcx, %rsi
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2480(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2832(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2736(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	movq	2368(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	je	.LBB147_1109
# BB#1108:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1109:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movzbl	5280(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5312(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1111
# BB#1110:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1111:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3232(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_1113
# BB#1112:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1113:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1115
# BB#1114:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1115:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1117
# BB#1116:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1117:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	je	.LBB147_1119
# BB#1118:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1119:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, %r9
	je	.LBB147_1121
# BB#1120:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1121:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2880(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2912(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_1123
# BB#1122:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1123:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3776(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2688(%rsp)       # 16-byte Spill
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2816(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rcx,%r14,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovups	48(%rcx,%r14,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5096(%rsp), %rdi        # 8-byte Reload
	vmovups	8(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r8,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4224(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rcx,%r10,4), %xmm14
	vmovups	48(%rcx,%r10,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rcx,%r9,4), %xmm6
	vmovups	48(%rcx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	vmovups	40(%rcx,%r14,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovups	56(%rcx,%r14,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	movq	%rcx, %rbx
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3472(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2800(%rsp)      # 16-byte Spill
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	movslq	2736(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	48(%rbx,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3904(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm14
	vmovaps	%xmm14, 3776(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3840(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	3872(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovups	48(%rbx,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm3
	vmovaps	%xmm3, 3712(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	movslq	2832(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5248(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3248(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	movslq	3264(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4256(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3328(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1125
# BB#1124:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	2608(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
.LBB147_1125:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vsubps	3472(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2720(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vsubps	3488(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3312(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3280(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3456(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2752(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3552(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2832(%rsp)       # 16-byte Spill
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3392(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3808(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2608(%rsp)       # 16-byte Spill
	movq	4712(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_1127
# BB#1126:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2496(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3184(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1127:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3424(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2640(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2800(%rsp), %xmm2       # 16-byte Reload
	vmulps	4224(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rbx,%r12,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4256(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vaddps	3440(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3648(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3360(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1129
# BB#1128:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovdqa	2512(%rsp), %xmm6       # 16-byte Reload
.LBB147_1129:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2656(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vsubps	2640(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vmulps	3872(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3520(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3424(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3472(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2880(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1131
# BB#1130:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovdqa	2528(%rsp), %xmm4       # 16-byte Reload
.LBB147_1131:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3328(%rsp), %xmm4       # 16-byte Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2912(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1133
# BB#1132:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	2544(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
.LBB147_1133:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4256(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vmulps	5248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3280(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3200(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1135
# BB#1134:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	2560(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3216(%rsp)       # 16-byte Spill
.LBB147_1135:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3840(%rsp), %xmm4       # 16-byte Reload
	vmulps	4224(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	4192(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2800(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2720(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2832(%rsp), %xmm4       # 16-byte Reload
	vaddps	3440(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3808(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3424(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5488(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1137
# BB#1136:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovdqa	2576(%rsp), %xmm15      # 16-byte Reload
.LBB147_1137:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vaddps	3552(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3680(%rsp), %xmm6       # 16-byte Reload
	vmulps	3872(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3904(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vaddps	3328(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	vmovdqa	2896(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1139
# BB#1138:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovdqa	2592(%rsp), %xmm0       # 16-byte Reload
.LBB147_1139:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1107 Depth=4
	vmovaps	3456(%rsp), %xmm7       # 16-byte Reload
	vaddps	3488(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3472(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3520(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5216(%rsp), %rax        # 4-byte Folded Reload
	movq	2384(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	3072(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1107
# BB#1140:                              #   in Loop: Header=BB147_1099 Depth=3
	movl	2256(%rsp), %ecx        # 4-byte Reload
.LBB147_1141:                           # %end for f7.s0.v10.v10462
                                        #   in Loop: Header=BB147_1099 Depth=3
	movl	%ecx, %r8d
	movq	2176(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	movl	1800(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_1099
.LBB147_1142:                           # %end for f7.s0.v11460
                                        #   in Loop: Header=BB147_467 Depth=2
	movslq	1028(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 4224(%rsp)        # 8-byte Spill
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	movq	2176(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	movq	%rax, 2168(%rsp)        # 8-byte Spill
	movq	2440(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	jl	.LBB147_1143
	jmp	.LBB147_1235
.LBB147_1198:                           # %end for f7.s0.v10.v10472.end for f7.s0.v10.v10476_crit_edge
                                        #   in Loop: Header=BB147_1143 Depth=3
	movq	2168(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2168(%rsp)        # 8-byte Spill
	movq	4256(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	vmovaps	5488(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB147_1234
	.align	16, 0x90
.LBB147_1143:                           # %for f7.s0.v11465
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1145 Depth 4
                                        #         Child Loop BB147_1180 Depth 4
                                        #         Child Loop BB147_1200 Depth 4
	cmpl	$0, 3164(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1178
# BB#1144:                              # %for f7.s0.v10.v10467.preheader
                                        #   in Loop: Header=BB147_1143 Depth=3
	movq	4256(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %eax
	andl	$1, %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	movl	%edx, %r10d
	andl	$63, %r10d
	movl	%r10d, %eax
	movq	1344(%rsp), %r8         # 8-byte Reload
	imull	%r8d, %eax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	movl	%edx, %ebx
	movq	1688(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ebx
	leal	8(%rbx), %eax
	movq	1712(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2416(%rsp)       # 16-byte Spill
	movq	%rdx, %rax
	movq	1880(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %rax
	movq	1864(%rsp), %r14        # 8-byte Reload
	leaq	(%rax,%r14), %rax
	leal	10(%rbx), %ecx
	imull	%edi, %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	movq	1888(%rsp), %r9         # 8-byte Reload
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leaq	2(%rdx), %rcx
	movq	%rcx, %rax
	imulq	%rsi, %rax
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	leal	6(%rbx), %ecx
	imull	%edi, %ecx
	movq	%rcx, 2368(%rsp)        # 8-byte Spill
	leaq	(%rax,%r14), %rax
	leaq	-2(%rdx), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	leal	62(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	leal	7(%rbx), %eax
	imull	%edi, %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	leaq	-1(%rdx), %rax
	imulq	%rsi, %rax
	leal	63(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	addl	$9, %ebx
	imull	%edi, %ebx
	movq	%rbx, 2464(%rsp)        # 8-byte Spill
	leaq	(%rax,%r14), %rax
	leaq	1(%rdx), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	imulq	1792(%rsp), %r10        # 8-byte Folded Reload
	subq	4760(%rsp), %r10        # 8-byte Folded Reload
	movq	%r10, 2496(%rsp)        # 8-byte Spill
	xorl	%r10d, %r10d
	.align	16, 0x90
.LBB147_1145:                           # %for f7.s0.v10.v10467
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1143 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	cmpl	$0, 3168(%rsp)          # 4-byte Folded Reload
	sete	3776(%rsp)              # 1-byte Folded Spill
	setne	3744(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r10,8), %ecx
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movl	%ecx, %r8d
	movq	4728(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r8d
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3440(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3424(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	leal	2(%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%esi, %ecx
	idivl	%ecx
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %esi
	movl	%edx, %r12d
	leal	-2(%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	%r9d, %ebx
	idivl	%ebx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	vmovd	%r15d, %xmm0
	leal	-1(%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r13d, %xmm0, %xmm0
	vpinsrd	$2, 5216(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vpinsrd	$3, 3712(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r14d, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpinsrd	$1, 3680(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, 3648(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r13d
	movl	%edx, %esi
	vpinsrd	$3, %r12d, %xmm2, %xmm1
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	5352(%rsp), %r12        # 8-byte Reload
	leal	2(%r12,%r10,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovd	%r9d, %xmm4
	leal	1(%r8), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vpinsrd	$1, 3616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, %r11d, %xmm4, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, 3600(%rsp), %xmm4, %xmm6 # 4-byte Folded Reload
	leal	-2(%r12,%r10,8), %eax
	vmovd	%eax, %xmm5
	vmovd	%r15d, %xmm4
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2416(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3808(%rsp), %r11d       # 4-byte Reload
	vmovd	%r11d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5392(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm13, %xmm1
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm7, %xmm8, %xmm2
	vblendvps	%xmm1, %xmm7, %xmm2, %xmm1
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	5056(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vmovdqa	5408(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm12, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm10, %xmm0, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vmovdqa	5424(%rsp), %xmm11      # 16-byte Reload
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	5488(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm1, %xmm15, %xmm1
	vpinsrd	$1, 3552(%rsp), %xmm4, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r14d, %xmm2, %xmm2
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%edi
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3680(%rsp)        # 8-byte Spill
	vpinsrd	$3, %esi, %xmm2, %xmm2
	leal	-1(%r12,%r10,8), %eax
	vmovd	%eax, %xmm4
	vmovaps	%xmm4, 3648(%rsp)       # 16-byte Spill
	vmovq	%xmm1, %rsi
	movq	%rsi, 3248(%rsp)        # 8-byte Spill
	vmovdqa	5216(%rsp), %xmm4       # 16-byte Reload
	vpsrad	$31, %xmm4, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm7, %xmm1, %xmm4, %xmm1
	vmovdqa	4960(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	4784(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	3712(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm6, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm6, %xmm4, %xmm4
	vpcmpgtd	%xmm4, %xmm13, %xmm6
	vpsubd	%xmm4, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vmovdqa	5200(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vmovdqa	5152(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vmovdqa	%xmm10, %xmm7
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm4, %xmm5, %xmm4
	vpsrad	$31, %xmm2, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm2
	vpcmpgtd	%xmm2, %xmm13, %xmm5
	vpsubd	%xmm2, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5184(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	.LCPI147_54(%rip), %xmm5, %xmm5
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vmovdqa	5136(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %r9d, %xmm6, %xmm3
	sarq	$32, %rsi
	movq	%rsi, 2832(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r13d
	sarq	$32, %rbx
	movq	%rbx, 2816(%rsp)        # 8-byte Spill
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vmovq	%xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpmulld	%xmm11, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm12, %xmm2, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	3648(%rsp), %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm10, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm13, %xmm3
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	leal	1(%r12,%r10,8), %r9d
	vmovd	%r9d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	movb	3744(%rsp), %r14b       # 1-byte Reload
	andb	%r14b, 5280(%rsp)       # 1-byte Folded Spill
	movl	%r11d, %ecx
	movl	%ecx, %eax
	movq	4256(%rsp), %r11        # 8-byte Reload
	orl	%r11d, %eax
	testb	$1, %al
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	sete	3216(%rsp)              # 1-byte Folded Spill
	movl	3168(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3328(%rsp)              # 1-byte Folded Spill
	movb	3776(%rsp), %bl         # 1-byte Reload
	movl	5312(%rsp), %eax        # 4-byte Reload
	andb	%bl, %al
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	movq	%rdx, %rax
	orq	$6, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	cltq
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movl	%r9d, %r13d
	andl	$1, %r13d
	sete	%r15b
	addl	$3, %r8d
	vmovd	%r8d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3440(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3424(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	3(%rax,%r10,8), %eax
	vmovd	%eax, %xmm2
	andb	%r14b, %r15b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm9, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm3
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm10, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovq	%xmm0, %r8
	movq	%r8, 3440(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	orl	%r11d, %eax
	testb	$1, %al
	sete	%al
	testl	%r9d, %r12d
	movzbl	3216(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%bl, %r13b
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %ecx
	movslq	%ecx, %r14
	movq	2368(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %edi
	movslq	%edi, %r9
	movq	%r14, %r11
	orq	$6, %r11
	movq	%r9, %r12
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm5
	vmovaps	%xmm5, %xmm0
	cmpl	$1, 104(%rbp)
	je	.LBB147_1147
# BB#1146:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1147:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movzbl	5280(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movl	5312(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %edi
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm4
	vmovaps	%xmm4, %xmm1
	je	.LBB147_1149
# BB#1148:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1149:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm1, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3328(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	je	.LBB147_1151
# BB#1150:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1151:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1153
# BB#1152:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1153:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1155
# BB#1154:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1155:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	%r15b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm3
	je	.LBB147_1157
# BB#1156:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1157:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm3, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
	movzbl	%dl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1159
# BB#1158:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1159:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm5, 3328(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 2880(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2912(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3072(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	je	.LBB147_1161
# BB#1160:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1161:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	2800(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm13
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm10, %xmm3
	vmovaps	%xmm0, %xmm14
	movq	5672(%rsp), %rsi        # 8-byte Reload
	movq	3264(%rsp), %rdx        # 8-byte Reload
	vmovups	32(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovups	48(%rsi,%rdx,4), %xmm15
	vshufps	$136, %xmm15, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm15[0,2]
	vmovaps	5680(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %r15
	movq	5096(%rsp), %rdi        # 8-byte Reload
	vmovups	8(%rdi,%r15,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r15,4), %xmm2
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm2[0,2]
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm10, %xmm6
	vmovups	32(%rsi,%r14,4), %xmm9
	vmovups	48(%rsi,%r14,4), %xmm11
	vshufps	$136, %xmm11, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm11[0,2]
	vsubps	%xmm0, %xmm7, %xmm7
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm6, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	3904(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm4
	vmovups	32(%rsi,%r9,4), %xmm12
	vmovups	48(%rsi,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm12, %xmm1 # xmm1 = xmm12[0,2],xmm7[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rdx,4), %xmm6
	vmovaps	%xmm6, 3680(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%rdx,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm14, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm13, 2800(%rsp)      # 16-byte Spill
	movq	3280(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm14, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %r13
	vmovups	8(%rdi,%r13,4), %xmm1
	vmovups	24(%rdi,%r13,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm6, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm12, %xmm1 # xmm1 = xmm12[1,3],xmm7[1,3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm12, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm15[1,3]
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm11, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm11[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm3, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rsi,%rax,4), %xmm15
	vmovups	48(%rsi,%rax,4), %xmm14
	vshufps	$136, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3872(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm10, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rax,4), %xmm7
	vmovaps	%xmm7, 3712(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3776(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm6, %xmm11, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm10, %xmm1
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rsi,%rax,4), %xmm2
	vmovups	48(%rsi,%rax,4), %xmm4
	vshufps	$136, %xmm4, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rax,4), %xmm10
	vmovaps	%xmm10, 3648(%rsp)      # 16-byte Spill
	vmovups	56(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm10, %xmm3 # xmm3 = xmm10[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm9, %xmm11, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$221, %xmm4, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm4[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm9, %xmm12, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm6, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm9, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	movq	%rsi, %rdx
	vshufps	$221, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm6, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	5216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movq	3440(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	cltq
	vmovaps	5312(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm2, %xmm6
	vmovaps	5280(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmovaps	%xmm2, 3440(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm10
	vmovaps	3360(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm4
	vmulps	5248(%rsp), %xmm12, %xmm11 # 16-byte Folded Reload
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm14
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2320(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r10,8), %ebx
	movslq	%ebx, %rbx
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rcx,%rsi,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %esi
	movslq	%esi, %rsi
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm3, %xmm15
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm3
	vmovups	16(%rdi,%rax,4), %xmm7
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm9
	vmovaps	%xmm9, 3312(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm0
	vmovups	24(%rdi,%rsi,4), %xmm12
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rbx,4), %xmm2
	vmovups	24(%rdi,%rbx,4), %xmm1
	vmovups	16(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3552(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3248(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm13, %xmm5 # xmm5 = xmm13[0,2],xmm3[0,2]
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[1,3],xmm7[1,3]
	je	.LBB147_1163
# BB#1162:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	2672(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
.LBB147_1163:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vsubps	3408(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm7, %xmm10, %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vsubps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[1,3],xmm3[1,3]
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmulps	%xmm14, %xmm11, %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	3264(%rsp), %xmm4       # 16-byte Reload
	vmulps	5248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	3440(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm3, %xmm4, %xmm13
	vmovaps	2688(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm3, %xmm4, %xmm4
	vmovaps	3472(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 3280(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2672(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2656(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm11
	vmovaps	2624(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm10
	vmovaps	3456(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm5
	vmovaps	%xmm5, 3440(%rsp)       # 16-byte Spill
	vmaxps	%xmm3, %xmm15, %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm7, %xmm5
	vmovaps	%xmm5, 2624(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	3488(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vmovaps	%xmm9, 2816(%rsp)       # 16-byte Spill
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3360(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm3[0,2],mem[0,2]
	vshufps	$136, %xmm1, %xmm2, %xmm6 # xmm6 = xmm2[0,2],xmm1[0,2]
	vmovaps	5312(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm3, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm3[0,2],mem[0,2]
	vshufps	$136, %xmm12, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm12[0,2]
	vmovaps	5280(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3520(%rsp), %xmm3, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm3[0,2],mem[0,2]
	vaddps	3744(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	movq	4712(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_1165
# BB#1164:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3184(%rsp)       # 16-byte Spill
.LBB147_1165:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vsubps	%xmm15, %xmm13, %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vsubps	3424(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm2, %xmm0 # xmm0 = xmm2[1,3],xmm1[1,3]
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm11, %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm14, %xmm10, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	(%rdx,%r11,4), %xmm0
	vmovups	40(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2800(%rsp), %xmm2       # 16-byte Reload
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r15,4), %xmm1
	vmovups	16(%rdi,%r15,4), %xmm5
	vmovaps	%xmm5, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rdx,%r12,4), %xmm2
	vmovups	40(%rdx,%r9,4), %xmm5
	vmovaps	%xmm5, 2800(%rsp)       # 16-byte Spill
	movq	%rdx, %rax
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm14
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%r13,4), %xmm2
	vmovups	16(%rdi,%r13,4), %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	2624(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm4
	vaddps	3408(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmovaps	3280(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm7
	vmovaps	2688(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm12
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm3
	vmovaps	2656(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm2
	vmovaps	3296(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm15 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm15, %xmm10
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm11
	vmovdqa	3328(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1167
# BB#1166:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovdqa	2512(%rsp), %xmm6       # 16-byte Reload
.LBB147_1167:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vsubps	%xmm0, %xmm4, %xmm0
	vsubps	2640(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vsubps	2608(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm2, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm4       # 16-byte Reload
	vmulps	3840(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	3232(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm8, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	3872(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm14, %xmm3
	vmovaps	%xmm14, %xmm12
	vxorps	%xmm14, %xmm14, %xmm14
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3472(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	3392(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm10, %xmm3, %xmm7
	vmovaps	%xmm3, %xmm10
	vpslld	$31, %xmm6, %xmm3
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm11, %xmm1, %xmm6
	vmovdqa	2880(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1169
# BB#1168:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovdqa	2528(%rsp), %xmm4       # 16-byte Reload
.LBB147_1169:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vaddps	%xmm0, %xmm2, %xmm1
	vbroadcastss	.LCPI147_23(%rip), %xmm9
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vmulps	%xmm11, %xmm7, %xmm7
	vpslld	$31, %xmm4, %xmm4
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vaddps	3328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm0, %xmm14, %xmm0
	vblendvps	%xmm3, %xmm6, %xmm0, %xmm3
	je	.LBB147_1171
# BB#1170:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
.LBB147_1171:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vblendvps	%xmm5, %xmm7, %xmm3, %xmm3
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[1,3],mem[1,3]
	vmovaps	3600(%rsp), %xmm2       # 16-byte Reload
	vmulps	5248(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm0, %xmm4, %xmm0
	vaddps	%xmm10, %xmm15, %xmm4
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm4, %xmm4
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vmulps	%xmm9, %xmm1, %xmm1
	vmovdqa	2896(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1173
# BB#1172:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovdqa	2560(%rsp), %xmm15      # 16-byte Reload
.LBB147_1173:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vblendvps	%xmm5, %xmm1, %xmm3, %xmm1
	vaddps	3376(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vmulps	4192(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rax,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmovaps	2800(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rax,%r9,4), %xmm2, %xmm4 # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmovaps	%xmm12, %xmm6
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2704(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r13,4), %xmm2, %xmm4 # xmm4 = xmm2[0,2],mem[0,2]
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vaddps	3408(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vaddps	3744(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm0, %xmm3
	vmovdqa	3072(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_1175
# BB#1174:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovdqa	2576(%rsp), %xmm2       # 16-byte Reload
.LBB147_1175:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vaddps	3488(%rsp), %xmm1, %xmm10 # 16-byte Folded Reload
	vmulps	%xmm9, %xmm5, %xmm9
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm6, %xmm1
	vmulps	%xmm4, %xmm1, %xmm4
	vmovaps	3600(%rsp), %xmm6       # 16-byte Reload
	vmulps	3840(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3872(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vaddps	3296(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3312(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm11, %xmm3, %xmm5
	vmulps	%xmm11, %xmm0, %xmm6
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm3
	vpslld	$31, %xmm15, %xmm4
	vpslld	$31, %xmm2, %xmm7
	vmovdqa	2912(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_1177
# BB#1176:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovdqa	2592(%rsp), %xmm1       # 16-byte Reload
.LBB147_1177:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1145 Depth=4
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vaddps	3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	3264(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpslld	$31, %xmm1, %xmm1
	vblendvps	%xmm1, %xmm0, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm6, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm9, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm10, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r10
	cmpl	3164(%rsp), %r10d       # 4-byte Folded Reload
	jne	.LBB147_1145
.LBB147_1178:                           # %end for f7.s0.v10.v10468
                                        #   in Loop: Header=BB147_1143 Depth=3
	movl	3164(%rsp), %eax        # 4-byte Reload
	cmpl	1452(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1197
# BB#1179:                              # %for f7.s0.v10.v10471.preheader
                                        #   in Loop: Header=BB147_1143 Depth=3
	movq	4256(%rsp), %r12        # 8-byte Reload
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 2480(%rsp)        # 4-byte Spill
	movl	%r12d, %r10d
	andl	$63, %r10d
	movq	%r12, %rax
	movq	1880(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	imulq	%rsi, %rax
	movq	1864(%rsp), %rdi        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	movq	1888(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leaq	2(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	leaq	-2(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	leaq	-1(%r12), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	leaq	(%rcx,%rdi), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	1(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	movq	%r10, %r8
	movq	2168(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %r15d
	andl	$63, %r15d
	movl	1764(%rsp), %r14d       # 4-byte Reload
	imull	%r14d, %r15d
	movq	1528(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movq	1752(%rsp), %r13        # 8-byte Reload
	imull	%r13d, %ecx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	imulq	1792(%rsp), %r8         # 8-byte Folded Reload
	movb	%r12b, %cl
	addb	$63, %cl
	movzbl	%cl, %r9d
	andl	$63, %r9d
	imull	%r14d, %r9d
	movq	1520(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %r11d
	imull	%r13d, %r11d
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movb	%r12b, %al
	addb	$62, %al
	movzbl	%al, %edi
	andl	$63, %edi
	imull	%r14d, %edi
	movq	1512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %ebx
	imull	%r13d, %ebx
	leal	2(%r12), %esi
	andl	$63, %esi
	imull	%r14d, %esi
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %edx
	imull	%r13d, %edx
	movq	1496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	imull	%r13d, %eax
	subq	4760(%rsp), %r8         # 8-byte Folded Reload
	movq	%r8, 2464(%rsp)         # 8-byte Spill
	movq	4192(%rsp), %r12        # 8-byte Reload
	imull	%r14d, %r10d
	movq	%rax, %r14
	movl	1304(%rsp), %r8d        # 4-byte Reload
	movq	5352(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1180:                           # %for f7.s0.v10.v10471
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1143 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r11, 3744(%rsp)        # 8-byte Spill
	movq	%r12, 4192(%rsp)        # 8-byte Spill
	movq	%rsi, 3648(%rsp)        # 8-byte Spill
	movq	%rdi, 3712(%rsp)        # 8-byte Spill
	movq	%r9, 3776(%rsp)         # 8-byte Spill
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	movl	%r8d, 2896(%rsp)        # 4-byte Spill
	movq	%r14, 2912(%rsp)        # 8-byte Spill
	movq	%rdx, 3072(%rsp)        # 8-byte Spill
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	movq	%r15, 3680(%rsp)        # 8-byte Spill
	movq	%r10, 3184(%rsp)        # 8-byte Spill
	movl	2480(%rsp), %r13d       # 4-byte Reload
	testl	%r13d, %r13d
	sete	5280(%rsp)              # 1-byte Folded Spill
	setne	%r12b
	movq	2200(%rsp), %r15        # 8-byte Reload
	movq	%rbx, %r8
	movq	%rdx, %rsi
	leal	(%r15,%rax), %r11d
	movslq	%r11d, %r9
	movq	%r9, 2800(%rsp)         # 8-byte Spill
	andl	$1, %r11d
	sete	%cl
	movq	%r9, %rdx
	movq	4664(%rsp), %rbx        # 8-byte Reload
	imulq	%rbx, %rdx
	movq	4888(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	leaq	2(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	leaq	-2(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	leaq	-1(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	leaq	1(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	andb	%r12b, %cl
	movb	%cl, 3600(%rsp)         # 1-byte Spill
	movl	%r9d, %ecx
	movq	4256(%rsp), %rdx        # 8-byte Reload
	orl	%edx, %ecx
	testb	$1, %cl
	sete	%cl
	leal	(%r15,%r14), %edx
	movslq	%edx, %r14
	leal	(%r15,%rsi), %edx
	movslq	%edx, %rax
	leal	(%r15,%r8), %edx
	movslq	%edx, %rdx
	testl	%r9d, %r13d
	movq	%rax, %r13
	setne	3520(%rsp)              # 1-byte Folded Spill
	andb	5280(%rsp), %r11b       # 1-byte Folded Reload
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movq	3744(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %eax
	movslq	%eax, %r12
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	movq	4192(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %ecx
	movslq	%ecx, %r8
	movq	%r8, %rcx
	orq	$6, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	leaq	3(%r9), %rsi
	imulq	%rbx, %rsi
	movq	%r12, %rcx
	orq	$6, %rcx
	movq	%rcx, 3440(%rsp)        # 8-byte Spill
	leaq	(%rsi,%rdi), %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	%r13, %rcx
	orq	$6, %rcx
	movq	%rcx, 2768(%rsp)        # 8-byte Spill
	movq	%rdx, %rcx
	movq	%rdx, %rsi
	orq	$6, %rcx
	movq	%rcx, 2784(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	leal	(%r15,%r10), %r9d
	movq	3648(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %r10d
	movq	3712(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %eax
	movl	%eax, 3552(%rsp)        # 4-byte Spill
	movq	3776(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %ecx
	movl	%ecx, 5280(%rsp)        # 4-byte Spill
	movq	3680(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movl	%ecx, 3472(%rsp)        # 4-byte Spill
	movq	3680(%rsp), %r15        # 8-byte Reload
	je	.LBB147_1182
# BB#1181:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1182:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	3600(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	%r11b, %edi
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	je	.LBB147_1184
# BB#1183:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1184:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm1, 2528(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	movzbl	3520(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movq	5096(%rsp), %rbx        # 8-byte Reload
	movq	%rsi, %rax
	je	.LBB147_1186
# BB#1185:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1186:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	je	.LBB147_1188
# BB#1187:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1188:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rdi        # 8-byte Reload
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rcx,4), %rdx
	movq	4736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm2, 2720(%rsp)       # 16-byte Spill
	vmovaps	5248(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm2, %xmm0
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	32(%rsi,%r14,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovups	48(%rsi,%r14,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	5680(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movslq	%r10d, %r10
	vmovups	8(%rbx,%r10,4), %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmovaps	3904(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm13, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm1
	movq	%r13, 3392(%rsp)        # 8-byte Spill
	vmovups	32(%rsi,%r13,4), %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovups	48(%rsi,%r13,4), %xmm15
	vshufps	$136, %xmm15, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm15[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vbroadcastss	.LCPI147_17(%rip), %xmm10
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	movslq	3552(%rsp), %r11        # 4-byte Folded Reload
	vmovups	8(%rbx,%r11,4), %xmm9
	vmovups	24(%rbx,%r11,4), %xmm7
	vmovaps	3872(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm5
	vmovups	32(%rsi,%rax,4), %xmm4
	vmovups	48(%rsi,%rax,4), %xmm3
	movq	%rax, %r13
	vshufps	$136, %xmm3, %xmm4, %xmm0 # xmm0 = xmm4[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vshufps	$136, %xmm7, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm7[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	movq	5216(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rdx
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm11, 2832(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%r14,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r14,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm12, %xmm11, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	movslq	%r9d, %r9
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm14
	vmovups	16(%rbx,%r9,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm5[0,2]
	vsubps	%xmm5, %xmm14, %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	leaq	(%rdi,%rax,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm14, 2672(%rsp)      # 16-byte Spill
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm12, %xmm14, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm9, %xmm0 # xmm0 = xmm9[1,3],xmm7[1,3]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm4, %xmm0 # xmm0 = xmm4[1,3],xmm3[1,3]
	movq	3488(%rsp), %r14        # 8-byte Reload
	leaq	(%rdi,%r14,4), %rdx
	vmovss	(%rdi,%r14,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm2, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm15[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm13, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r12,4), %xmm12
	vmovups	48(%rsi,%r12,4), %xmm15
	vshufps	$136, %xmm15, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm15[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3840(%rsp), %xmm4       # 16-byte Reload
	vmovaps	2720(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm4, %xmm2, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%r12,4), %xmm3
	vmovaps	%xmm3, 2752(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r12,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm11, %xmm7
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm2
	vmovups	32(%rsi,%r8,4), %xmm13
	vmovups	48(%rsi,%r8,4), %xmm1
	vshufps	$136, %xmm1, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm1[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%r8,4), %xmm7
	vmovaps	%xmm7, 3488(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r8,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm11, %xmm2
	vmulps	%xmm0, %xmm2, %xmm2
	vshufps	$221, %xmm1, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm15, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm15[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3440(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	5312(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx,4), %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %r14        # 8-byte Reload
	leaq	(%rdi,%r14,4), %rax
	vmovss	(%rdi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vmulps	5248(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm8
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm13
	movslq	5280(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movslq	3472(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm6
	vminps	%xmm10, %xmm2, %xmm2
	cmpl	$0, 104(%rbp)
	vmovups	8(%rbx,%r9,4), %xmm12
	vmovups	24(%rbx,%r9,4), %xmm0
	vmovups	(%rbx,%r9,4), %xmm15
	vmovups	8(%rbx,%rcx,4), %xmm7
	vmovups	24(%rbx,%rcx,4), %xmm3
	vmovups	16(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	8(%rbx,%rax,4), %xmm11
	vmovups	24(%rbx,%rax,4), %xmm14
	vmovups	16(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm1 # xmm1 = xmm12[0,2],xmm0[0,2]
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, 5216(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[1,3],mem[1,3]
	je	.LBB147_1190
# BB#1189:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovaps	2544(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
.LBB147_1190:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vsubps	%xmm1, %xmm5, %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm0[1,3]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm9, %xmm12
	vsubps	3312(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2656(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2608(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vxorps	%xmm4, %xmm4, %xmm4
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm4, %xmm1, %xmm9
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm4, %xmm1, %xmm13
	vmaxps	%xmm4, %xmm6, %xmm8
	vmaxps	%xmm4, %xmm2, %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, 5216(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[0,2],mem[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm5
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3520(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm11, %xmm4 # xmm4 = xmm11[0,2],xmm14[0,2]
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3376(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm3, %xmm7, %xmm0 # xmm0 = xmm7[0,2],xmm3[0,2]
	vmovaps	5280(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, 3360(%rsp), %xmm6, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm6[0,2],mem[0,2]
	movq	4712(%rsp), %rcx        # 8-byte Reload
	movq	4192(%rsp), %rdx        # 8-byte Reload
	movq	2912(%rsp), %r14        # 8-byte Reload
	movl	2896(%rsp), %r8d        # 4-byte Reload
	je	.LBB147_1192
# BB#1191:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	%xmm12, %xmm6
	vmovaps	2528(%rsp), %xmm12      # 16-byte Reload
	vmovaps	%xmm12, 3200(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, %xmm12
.LBB147_1192:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vshufps	$221, %xmm3, %xmm7, %xmm3 # xmm3 = xmm7[1,3],xmm3[1,3]
	vmovaps	%xmm3, 2528(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[1,3],xmm14[1,3]
	vaddps	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm2, 2592(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm9, %xmm2
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm13, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm0, %xmm8, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	2768(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	movq	3392(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm0, %xmm15, %xmm0
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vmulps	3904(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rbx,%r10,4), %xmm1
	vmovups	16(%rbx,%r10,4), %xmm5
	vmovaps	%xmm5, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3872(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	movq	2784(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm2
	vmovups	40(%rsi,%r13,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm15, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rbx,%r11,4), %xmm2
	vmovups	16(%rbx,%r11,4), %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	2544(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm6, %xmm1, %xmm1
	vmovaps	2608(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm3
	vmovaps	2576(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm5
	vmovaps	2560(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm4
	vmovdqa	3280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 2576(%rsp)       # 16-byte Spill
	vmovdqa	3200(%rsp), %xmm7       # 16-byte Reload
	vpslld	$31, %xmm7, %xmm2
	vmovdqa	%xmm2, 2560(%rsp)       # 16-byte Spill
	vminps	%xmm10, %xmm12, %xmm7
	vmaxps	%xmm6, %xmm7, %xmm7
	vsubps	3456(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm6, 3200(%rsp)       # 16-byte Spill
	vaddps	3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vaddps	3408(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3248(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm6, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm6[1,3],mem[1,3]
	vbroadcastss	.LCPI147_23(%rip), %xmm6
	vmovaps	%xmm6, 3280(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm2
	je	.LBB147_1194
# BB#1193:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovaps	2496(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3216(%rsp)       # 16-byte Spill
.LBB147_1194:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vsubps	2528(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm9
	vsubps	%xmm0, %xmm5, %xmm11
	vsubps	%xmm7, %xmm4, %xmm12
	vmovaps	3264(%rsp), %xmm4       # 16-byte Reload
	vmulps	3808(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovaps	2624(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm4       # 16-byte Reload
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm4, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmovaps	%xmm15, %xmm6
	vmulps	%xmm5, %xmm3, %xmm3
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vaddps	3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm0, %xmm5
	vmovdqa	2576(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmulps	3280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovdqa	2560(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3264(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm8
	vaddps	3312(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm2, %xmm5
	vmovdqa	2864(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1196
# BB#1195:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vmovdqa	2512(%rsp), %xmm0       # 16-byte Reload
.LBB147_1196:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1180 Depth=4
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm15
	vaddps	%xmm14, %xmm15, %xmm0
	vaddps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vbroadcastss	.LCPI147_19(%rip), %xmm9
	vmulps	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm11, %xmm7
	vxorps	%xmm11, %xmm11, %xmm11
	vblendvps	%xmm2, %xmm0, %xmm11, %xmm0
	vblendvps	%xmm8, %xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3488(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2704(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	2720(%rsp), %xmm12      # 16-byte Reload
	vmulps	3808(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	5312(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vshufps	$221, 2736(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm4[1,3],mem[1,3]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	3840(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm10, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vsubps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm3, %xmm15, %xmm4
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vaddps	%xmm1, %xmm14, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm9, %xmm1, %xmm1
	vblendvps	%xmm8, %xmm1, %xmm11, %xmm1
	vmovaps	2608(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm8
	vmovaps	3264(%rsp), %xmm9       # 16-byte Reload
	vmovaps	3216(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm9, 2640(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm0, 3232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	movq	3392(%rsp), %rax        # 8-byte Reload
	vmovaps	2768(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rsi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	2832(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rbx,%r10,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rsi,%r13,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vmulps	3872(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rbx,%r11,4), %xmm3, %xmm5 # xmm5 = xmm3[0,2],mem[0,2]
	vminps	%xmm10, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vsubps	%xmm5, %xmm4, %xmm4
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vaddps	3600(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3520(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm2, %xmm8, %xmm0
	vmovaps	5216(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm3, %xmm2
	vaddps	3440(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3280(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vblendvps	%xmm9, %xmm2, %xmm0, %xmm0
	vaddps	3472(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2464(%rsp), %rax        # 8-byte Reload
	movq	2800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	addl	$8, %edx
	movq	3776(%rsp), %r9         # 8-byte Reload
	addl	$8, %r9d
	movq	3744(%rsp), %r11        # 8-byte Reload
	addl	$8, %r11d
	movq	3712(%rsp), %rdi        # 8-byte Reload
	addl	$8, %edi
	movq	3168(%rsp), %rbx        # 8-byte Reload
	addl	$8, %ebx
	movq	3648(%rsp), %rsi        # 8-byte Reload
	addl	$8, %esi
	movq	%rdx, %r12
	movq	3072(%rsp), %rdx        # 8-byte Reload
	addl	$8, %edx
	addl	$8, %r14d
	movq	3184(%rsp), %r10        # 8-byte Reload
	addl	$8, %r10d
	movq	2880(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$-1, %r8d
	jne	.LBB147_1180
.LBB147_1197:                           # %end for f7.s0.v10.v10472
                                        #   in Loop: Header=BB147_1143 Depth=3
	movl	1452(%rsp), %eax        # 4-byte Reload
	cmpl	1800(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1198
# BB#1199:                              # %for f7.s0.v10.v10475.preheader
                                        #   in Loop: Header=BB147_1143 Depth=3
	movq	4256(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 3072(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$63, %r8d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2416(%rsp)       # 16-byte Spill
	movq	%r9, %rcx
	movq	1880(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rdi
	imulq	%rdi, %rcx
	movq	1864(%rsp), %rbx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	1888(%rsp), %rsi        # 8-byte Reload
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leaq	2(%r9), %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	leaq	-2(%r9), %rdx
	imulq	%rdi, %rdx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	leaq	-1(%r9), %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	leaq	1(%r9), %rcx
	movq	%rcx, 1440(%rsp)        # 8-byte Spill
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	movq	2168(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2168(%rsp)        # 8-byte Spill
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	%r8, %rsi
	imulq	1792(%rsp), %rsi        # 8-byte Folded Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	movq	1528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %edx
	movq	1752(%rsp), %rax        # 8-byte Reload
	imull	%eax, %edx
	subq	4760(%rsp), %rsi        # 8-byte Folded Reload
	movq	%rsi, 2400(%rsp)        # 8-byte Spill
	movq	1288(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$63, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1520(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%eax, %esi
	leal	(%rdx,%rbx), %edx
	movq	%rdx, 2368(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%eax, %edx
	leal	(%rsi,%rbx), %esi
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	leal	2(%r9), %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1504(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%eax, %edx
	movq	1496(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%eax, %esi
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2288(%rsp)        # 8-byte Spill
	imull	%edi, %r8d
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2272(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2256(%rsp)        # 8-byte Spill
	leal	(%r8,%rbx), %eax
	movq	%rax, 2240(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1300(%rsp), %eax        # 4-byte Reload
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	.align	16, 0x90
.LBB147_1200:                           # %for f7.s0.v10.v10475
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1143 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 2912(%rsp)        # 4-byte Spill
	cmpl	$0, 3072(%rsp)          # 4-byte Folded Reload
	sete	3808(%rsp)              # 1-byte Folded Spill
	setne	%r10b
	movb	%r10b, 3312(%rsp)       # 1-byte Spill
	movq	3056(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %ebx
	movl	%ebx, 2896(%rsp)        # 4-byte Spill
	movl	%ebx, %r13d
	andl	$1, %r13d
	sete	%r9b
	movq	2992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm2       # 16-byte Reload
	vpextrd	$1, %xmm2, %ecx
	movl	%ecx, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %esi
	movl	%esi, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%esi, %r12d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r8d
	movl	%r8d, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovd	%esi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r11d
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2416(%rsp), %xmm13      # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm2
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %xmm8
	vmovdqa	5120(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm1
	vpaddd	%xmm11, %xmm8, %xmm3
	vmovdqa	5376(%rsp), %xmm9       # 16-byte Reload
	vpminsd	%xmm9, %xmm3, %xmm3
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vmovdqa	5392(%rsp), %xmm10      # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm10, %xmm4
	vmovdqa	5360(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm2, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	movq	3000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpminsd	%xmm9, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vmovd	%xmm4, %eax
	cltd
	idivl	%r12d
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovd	%edx, %xmm2
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r8d
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edx, %xmm2, %xmm2
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	4960(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	movq	3016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm10, %xmm5
	vpsubd	%xmm3, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	movq	2968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm9, %xmm3, %xmm3
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm4, %xmm3, %xmm2
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm3
	movq	3024(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vpcmpgtd	%xmm4, %xmm10, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm9, %xmm5, %xmm5
	movq	2976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpmaxsd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovd	%xmm6, %eax
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vblendvps	%xmm3, %xmm5, %xmm4, %xmm3
	vmovd	%esi, %xmm4
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm10, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5184(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm5, %xmm5
	movq	3032(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm9, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm4, %xmm4
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm6, %xmm4, %xmm4
	movq	2984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r14d
	vmovdqa	5424(%rsp), %xmm6       # 16-byte Reload
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovd	%xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %r12d
	vpaddd	%xmm1, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovq	%xmm1, %rsi
	movq	%rsi, 2816(%rsp)        # 8-byte Spill
	vpextrd	$3, %xmm5, %eax
	vmovd	%r12d, %xmm1
	cltd
	idivl	%r11d
	sarq	$32, %rsi
	movq	%rsi, 2784(%rsp)        # 8-byte Spill
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpmulld	%xmm6, %xmm2, %xmm2
	sarq	$32, %rcx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vmovq	%xmm2, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm1, %xmm1
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm3, %xmm2
	vpaddd	%xmm2, %xmm12, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm4, %xmm3
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm12, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm1, %xmm2
	vpand	%xmm13, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovdqa	5104(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm10, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	movq	3040(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	vmovd	%r12d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpminsd	%xmm9, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm9, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm3, %xmm1, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	andb	%r10b, %r9b
	movb	%r9b, 5280(%rsp)        # 1-byte Spill
	movl	%ebx, %eax
	movq	4256(%rsp), %r9         # 8-byte Reload
	orl	%r9d, %eax
	testb	$1, %al
	movq	3008(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sete	%r14b
	movl	3072(%rsp), %r10d       # 4-byte Reload
	testl	%ebx, %r10d
	setne	3216(%rsp)              # 1-byte Folded Spill
	movb	3808(%rsp), %r8b        # 1-byte Reload
	andb	%r8b, %r13b
	movl	%r13d, 5312(%rsp)       # 4-byte Spill
	orq	$6, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%r12d, %r13d
	andl	$1, %r13d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	sete	%bl
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	3048(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	3312(%rsp), %bl         # 1-byte Folded Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm10, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm9, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm9, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movl	%r12d, %eax
	orl	%r9d, %eax
	testb	$1, %al
	sete	%r11b
	testl	%r12d, %r10d
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm0
	setne	%r14b
	andb	%r8b, %r13b
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r12
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r9
	movq	%r12, %rax
	orq	$6, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movq	%r9, %rax
	orq	$6, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2240(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3232(%rsp)        # 4-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %edi
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r10d
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3264(%rsp)        # 4-byte Spill
	je	.LBB147_1202
# BB#1201:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1202:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	5280(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5312(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1204
# BB#1203:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1204:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm1, 2448(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	movzbl	3216(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1206
# BB#1205:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1206:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_1208
# BB#1207:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1208:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm1, 2464(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	%r11b, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1210
# BB#1209:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1210:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2864(%rsp)       # 16-byte Spill
	je	.LBB147_1212
# BB#1211:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1212:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1214
# BB#1213:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1214:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2848(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2528(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2880(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	movq	5672(%rsp), %rbx        # 8-byte Reload
	je	.LBB147_1216
# BB#1215:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1216:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	2816(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3776(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2800(%rsp)       # 16-byte Spill
	movq	2768(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2832(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	2736(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vmovups	48(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5096(%rsp), %rdi        # 8-byte Reload
	vmovups	8(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r8,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4192(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rbx,%r12,4), %xmm14
	vmovups	48(%rbx,%r12,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vmovaps	3904(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rbx,%r9,4), %xmm6
	vmovups	48(%rbx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3472(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2784(%rsp)      # 16-byte Spill
	movq	2752(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movslq	%r10d, %r10
	vmovups	8(%rdi,%r10,4), %xmm1
	vmovups	24(%rdi,%r10,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm13
	vmovups	48(%rbx,%rax,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3872(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2800(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm14
	vmovaps	%xmm14, 3744(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3392(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovups	48(%rbx,%rax,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	5216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	movslq	3232(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5248(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3248(%rsp), %rdx        # 4-byte Folded Reload
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	movslq	3264(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3296(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rdx,4), %xmm5
	vmovups	24(%rdi,%rdx,4), %xmm6
	vmovups	16(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1218
# BB#1217:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	2560(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
.LBB147_1218:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vsubps	2688(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	3440(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3456(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3440(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3520(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2768(%rsp)       # 16-byte Spill
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3376(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3776(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2560(%rsp)       # 16-byte Spill
	movq	4712(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_1220
# BB#1219:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2448(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3168(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1220:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vsubps	2720(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2608(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2592(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r12,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2784(%rsp), %xmm2       # 16-byte Reload
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%r10,4), %xmm2
	vmovups	16(%rdi,%r10,4), %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2560(%rsp), %xmm0       # 16-byte Reload
	vaddps	3424(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3232(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3344(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1222
# BB#1221:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovdqa	2464(%rsp), %xmm6       # 16-byte Reload
.LBB147_1222:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2608(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	2592(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	3280(%rsp), %xmm3       # 16-byte Reload
	vmulps	3840(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3872(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3472(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3408(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3440(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2848(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1224
# BB#1223:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovdqa	2480(%rsp), %xmm4       # 16-byte Reload
.LBB147_1224:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3312(%rsp), %xmm4       # 16-byte Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2880(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1226
# BB#1225:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	2496(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
.LBB147_1226:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm4       # 16-byte Reload
	vmulps	5248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3264(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3184(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1228
# BB#1227:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	2512(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3200(%rsp)       # 16-byte Spill
.LBB147_1228:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3808(%rsp), %xmm4       # 16-byte Reload
	vmulps	4192(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rax,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	3904(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2688(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rax,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vaddps	3424(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3776(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5488(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1230
# BB#1229:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovdqa	2528(%rsp), %xmm15      # 16-byte Reload
.LBB147_1230:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vaddps	3520(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vmulps	3840(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3872(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vaddps	3312(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	vmovdqa	2864(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1232
# BB#1231:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovdqa	2544(%rsp), %xmm0       # 16-byte Reload
.LBB147_1232:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1200 Depth=4
	vmovaps	3440(%rsp), %xmm7       # 16-byte Reload
	vaddps	3472(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	2896(%rsp), %rax        # 4-byte Folded Reload
	movq	2400(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	2912(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1200
# BB#1233:                              #   in Loop: Header=BB147_1143 Depth=3
	movq	1440(%rsp), %rax        # 8-byte Reload
.LBB147_1234:                           # %end for f7.s0.v10.v10476
                                        #   in Loop: Header=BB147_1143 Depth=3
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	movq	2440(%rsp), %rax        # 8-byte Reload
	movq	2168(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	jne	.LBB147_1143
.LBB147_1235:                           # %end for f7.s0.v11466
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	2440(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r8d
	cmpl	1436(%rsp), %eax        # 4-byte Folded Reload
	movq	4728(%rsp), %r9         # 8-byte Reload
	movq	4720(%rsp), %rax        # 8-byte Reload
	jl	.LBB147_1236
	jmp	.LBB147_1274
.LBB147_1237:                           # %for f7.s0.v11479.end for f7.s0.v10.v10483_crit_edge
                                        #   in Loop: Header=BB147_1236 Depth=3
	addl	$1, %r8d
	movl	%r8d, %ecx
	jmp	.LBB147_1273
	.align	16, 0x90
.LBB147_1236:                           # %for f7.s0.v11479
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1239 Depth 4
	cmpl	$0, 1800(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1237
# BB#1238:                              # %for f7.s0.v10.v10482.preheader
                                        #   in Loop: Header=BB147_1236 Depth=3
	movq	%r8, 2448(%rsp)         # 8-byte Spill
	movl	%r8d, %r11d
	movq	1816(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r11d
	leal	-1(%r11), %eax
	cltd
	movq	1824(%rsp), %r13        # 8-byte Reload
	idivl	%r13d
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	movl	1836(%rsp), %eax        # 4-byte Reload
	andl	%eax, %esi
	movl	%eax, %ebx
	leal	1(%r11), %eax
	cltd
	idivl	%r13d
	addl	%ecx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	addl	%edx, %ecx
	movq	1808(%rsp), %r15        # 8-byte Reload
	cmpl	%r8d, %r15d
	movl	%r15d, %eax
	cmovgl	%r8d, %eax
	addl	$-1, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	1860(%rsp), %r14d       # 4-byte Reload
	movl	%r14d, %r10d
	subl	%esi, %r10d
	movq	1848(%rsp), %r9         # 8-byte Reload
	cmpl	%esi, %r9d
	cmovgl	%esi, %r10d
	addl	%edi, %r10d
	movl	1804(%rsp), %ebx        # 4-byte Reload
	cmpl	%r10d, %ebx
	cmovlel	%ebx, %r10d
	cmpl	%edi, %r10d
	cmovll	%edi, %r10d
	leal	1(%r8), %esi
	movl	%esi, 2240(%rsp)        # 4-byte Spill
	cmpl	%esi, %ebx
	movl	%ebx, %edx
	cmovgl	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r14d, %r12d
	subl	%ecx, %r12d
	cmpl	%ecx, %r9d
	cmovgl	%ecx, %r12d
	addl	%edi, %r12d
	cmpl	%r12d, %ebx
	cmovlel	%ebx, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	%r8d, %ebx
	movl	%ebx, %esi
	cmovgl	%r8d, %esi
	cmovgl	%edx, %r12d
	cmpl	%r8d, %r15d
	cmovgel	%eax, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1836(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	subl	%eax, %r14d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r14d
	addl	%edi, %r14d
	cmpl	%r14d, %ebx
	cmovlel	%ebx, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	cmpl	%r8d, %r15d
	cmovgl	%esi, %r14d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3072(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2416(%rsp)       # 16-byte Spill
	movl	%r8d, %esi
	leal	2(%r11), %eax
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	andl	$63, %esi
	movq	%rsi, 2464(%rsp)        # 8-byte Spill
	movl	%r9d, %esi
	sarl	$31, %esi
	addl	$-2, %r11d
	movl	%r11d, %eax
	cltd
	idivl	%r13d
	movl	1836(%rsp), %r11d       # 4-byte Reload
	andl	%r11d, %esi
	addl	%r9d, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movslq	%r14d, %rcx
	movq	1880(%rsp), %r9         # 8-byte Reload
	imulq	%r9, %rcx
	movq	1864(%rsp), %r14        # 8-byte Reload
	leaq	(%rcx,%r14), %rcx
	movq	1888(%rsp), %r11        # 8-byte Reload
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leal	2(%r8), %r15d
	cmpl	%r15d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%esi, %edx
	movq	1848(%rsp), %r13        # 8-byte Reload
	cmpl	%esi, %r13d
	cmovgl	%esi, %edx
	addl	%edi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r9, %rcx
	leaq	(%rcx,%r14), %rcx
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leal	-2(%r8), %r13d
	cmpl	%r13d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r13d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1848(%rsp), %rsi        # 8-byte Reload
	cmpl	%eax, %esi
	cmovgl	%eax, %edx
	addl	%edi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rax
	imulq	%r9, %rax
	leaq	(%rax,%r14), %rax
	movslq	%r10d, %rcx
	imulq	%r9, %rcx
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movslq	%r12d, %rax
	imulq	%r9, %rax
	leaq	(%rax,%r14), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	2464(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1792(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	movl	2240(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1832(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2336(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2320(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2304(%rsp)        # 8-byte Spill
	andl	$63, %r13d
	imull	%eax, %r13d
	movq	%r13, 2384(%rsp)        # 8-byte Spill
	movq	1464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2288(%rsp)        # 8-byte Spill
	andl	$63, %r15d
	imull	%eax, %r15d
	movq	%r15, 2400(%rsp)        # 8-byte Spill
	movq	1456(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	movq	1592(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2256(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1239:                           # %for f7.s0.v10.v10482
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1236 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 2912(%rsp)        # 4-byte Spill
	cmpl	$0, 3072(%rsp)          # 4-byte Folded Reload
	sete	3808(%rsp)              # 1-byte Folded Spill
	setne	3712(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	sete	5280(%rsp)              # 1-byte Folded Spill
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4256(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4256(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3776(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r15d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2416(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5216(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5120(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5056(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5392(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5360(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5408(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5376(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5424(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3776(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r14d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3744(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4960(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4784(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r14d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5184(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r15d
	movl	%r15d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2784(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2832(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r14d
	vmovq	%xmm1, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4256(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movb	3712(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5280(%rsp)       # 1-byte Folded Spill
	movl	5216(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2448(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r14
	sete	3248(%rsp)              # 1-byte Folded Spill
	movl	3072(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3216(%rsp)              # 1-byte Folded Spill
	movb	3808(%rsp), %r10b       # 1-byte Reload
	movl	5312(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3312(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r12d
	movzbl	3248(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r15b
	movq	2272(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movslq	%ecx, %r10
	movq	2288(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movslq	%edi, %rcx
	movq	%r10, %rsi
	orq	$6, %rsi
	movq	%rsi, 3328(%rsp)        # 8-byte Spill
	movq	%rcx, %r12
	movq	%rcx, %rsi
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2464(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2816(%rsp)        # 4-byte Spill
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movq	2384(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2720(%rsp)        # 4-byte Spill
	movq	2320(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	je	.LBB147_1241
# BB#1240:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1241:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	5280(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5312(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1243
# BB#1242:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1243:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	movzbl	3216(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_1245
# BB#1244:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1245:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_1247
# BB#1246:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1247:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1249
# BB#1248:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1249:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2880(%rsp)       # 16-byte Spill
	je	.LBB147_1251
# BB#1250:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1251:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, %r9
	je	.LBB147_1253
# BB#1252:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1253:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2864(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2896(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	je	.LBB147_1255
# BB#1254:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1255:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	3744(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3776(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2832(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2672(%rsp)       # 16-byte Spill
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5672(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rcx,%r14,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovups	48(%rcx,%r14,4), %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5680(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5096(%rsp), %rdi        # 8-byte Reload
	vmovups	8(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r8,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4192(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rcx,%r10,4), %xmm14
	vmovups	48(%rcx,%r10,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	vmovaps	3904(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rcx,%r9,4), %xmm6
	vmovups	48(%rcx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	vmovups	40(%rcx,%r14,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovups	56(%rcx,%r14,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	movq	%rcx, %rbx
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2784(%rsp)      # 16-byte Spill
	movq	2752(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movslq	2720(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3440(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vmovaps	3744(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	48(%rbx,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3872(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm14
	vmovaps	%xmm14, 3744(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3808(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovups	48(%rbx,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4256(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3600(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	movslq	2816(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3440(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5248(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3232(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movslq	3248(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2656(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3440(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4256(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3312(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5312(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1257
# BB#1256:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	2592(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
.LBB147_1257:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vsubps	3456(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3440(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3440(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2736(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3520(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2816(%rsp)       # 16-byte Spill
	vmovaps	4256(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3376(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3776(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2592(%rsp)       # 16-byte Spill
	movq	4712(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_1259
# BB#1258:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2480(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3168(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1259:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vsubps	3456(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2624(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3472(%rsp)       # 16-byte Spill
	vmovaps	3440(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5680(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2784(%rsp), %xmm2       # 16-byte Reload
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rbx,%r12,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4256(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vaddps	3424(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3232(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3344(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1261
# BB#1260:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovdqa	2496(%rsp), %xmm6       # 16-byte Reload
.LBB147_1261:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2640(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	2624(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	3280(%rsp), %xmm3       # 16-byte Reload
	vmulps	3840(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3872(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3472(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3408(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3440(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2864(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1263
# BB#1262:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovdqa	2512(%rsp), %xmm4       # 16-byte Reload
.LBB147_1263:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3312(%rsp), %xmm4       # 16-byte Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2896(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1265
# BB#1264:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	2528(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
.LBB147_1265:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4256(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm4       # 16-byte Reload
	vmulps	5248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3264(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3184(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1267
# BB#1266:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	2544(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3200(%rsp)       # 16-byte Spill
.LBB147_1267:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3808(%rsp), %xmm4       # 16-byte Reload
	vmulps	4192(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	3904(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2704(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2816(%rsp), %xmm4       # 16-byte Reload
	vaddps	3424(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3776(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5488(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1269
# BB#1268:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovdqa	2560(%rsp), %xmm15      # 16-byte Reload
.LBB147_1269:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vaddps	3520(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vmulps	3840(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3600(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3872(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vaddps	3312(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3328(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	vmovdqa	2880(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1271
# BB#1270:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovdqa	2576(%rsp), %xmm0       # 16-byte Reload
.LBB147_1271:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1239 Depth=4
	vmovaps	3440(%rsp), %xmm7       # 16-byte Reload
	vaddps	3472(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5216(%rsp), %rax        # 4-byte Folded Reload
	movq	2368(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	2912(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1239
# BB#1272:                              #   in Loop: Header=BB147_1236 Depth=3
	movq	4728(%rsp), %r9         # 8-byte Reload
	movq	4720(%rsp), %rax        # 8-byte Reload
	movl	2240(%rsp), %ecx        # 4-byte Reload
.LBB147_1273:                           # %end for f7.s0.v10.v10483
                                        #   in Loop: Header=BB147_1236 Depth=3
	movl	%ecx, %r8d
	cmpl	1436(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1236
.LBB147_1274:                           # %produce f8487
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1072(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	movq	2176(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	jl	.LBB147_1275
	jmp	.LBB147_1313
.LBB147_1276:                           # %for f8.s0.v11489.end for f8.s0.v10.v10492_crit_edge
                                        #   in Loop: Header=BB147_1275 Depth=3
	addl	$1, %r8d
	movl	%r8d, %edx
	jmp	.LBB147_1312
	.align	16, 0x90
.LBB147_1275:                           # %for f8.s0.v11489
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1278 Depth 4
	cmpl	$0, 1800(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1276
# BB#1277:                              # %for f8.s0.v10.v10491.preheader
                                        #   in Loop: Header=BB147_1275 Depth=3
	movq	%r8, 2512(%rsp)         # 8-byte Spill
	movl	%r8d, %eax
	movq	1816(%rsp), %r11        # 8-byte Reload
	subl	%r11d, %eax
	addl	$-1, %eax
	cltd
	movq	1824(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %esi
	addl	%edx, %eax
	movl	1860(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	movl	%ecx, %r10d
	subl	%eax, %edx
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	movl	1804(%rsp), %r13d       # 4-byte Reload
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movq	1808(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %r9d
	cmovgl	%r8d, %r9d
	addl	$-1, %r9d
	cmpl	%r11d, %r9d
	cmovll	%r11d, %r9d
	cmpl	%r8d, %ebx
	cmovll	%edx, %r9d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovlel	%edx, %r9d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r10d, %esi
	subl	%edi, %esi
	cmpl	%edi, %r14d
	cmovgl	%edi, %esi
	addl	%r11d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	leal	1(%r8), %ecx
	movl	%ecx, 2288(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r13d
	movl	%r13d, %edi
	cmovgl	%ecx, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%esi, %edi
	movl	%r13d, %r10d
	cmovgl	%r8d, %r10d
	cmpl	%r11d, %r10d
	cmovll	%r11d, %r10d
	cmpl	%r8d, %ebx
	cmovlel	%edx, %r10d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovll	%edx, %r10d
	movl	%r8d, %r12d
	andl	$1, %r12d
	movl	%r12d, 2304(%rsp)       # 4-byte Spill
	leal	-2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2496(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	movl	%r14d, %ebx
	addl	$2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	sarl	$31, %ebx
	movl	1836(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %ebx
	addl	%r14d, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %edi
	movslq	%edi, %rcx
	movq	1880(%rsp), %rdi        # 8-byte Reload
	imulq	%rdi, %rcx
	movq	1872(%rsp), %rsi        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	1888(%rsp), %r14        # 8-byte Reload
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %ecx        # 4-byte Reload
	subl	%ebx, %ecx
	movq	1848(%rsp), %r15        # 8-byte Reload
	cmpl	%ebx, %r15d
	cmovgl	%ebx, %ecx
	addl	%r11d, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	leal	-2(%r8), %r9d
	cmpl	%r9d, %r13d
	movl	%r13d, %edx
	cmovgl	%r9d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	movslq	%r10d, %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%r14,%rdx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	movl	1860(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	leal	2(%r8), %ebx
	cmpl	%ebx, %r13d
	movl	%r13d, %eax
	cmovgl	%ebx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %eax
	cmpl	%r8d, 1700(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %eax
	cltq
	imulq	%rdi, %rax
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movq	2528(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1792(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1764(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2464(%rsp)        # 8-byte Spill
	movq	1456(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1832(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	subq	4760(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2448(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2416(%rsp)        # 8-byte Spill
	movq	1592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r9d
	imull	%esi, %r9d
	movq	%r9, 2480(%rsp)         # 8-byte Spill
	movq	1464(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2352(%rsp)        # 8-byte Spill
	movl	2288(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1278:                           # %for f8.s0.v10.v10491
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1275 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3216(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	sete	5216(%rsp)              # 1-byte Folded Spill
	setne	3808(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3520(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3776(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3744(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3712(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3680(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3648(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2496(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3840(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5392(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5360(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5104(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5408(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5440(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3776(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5184(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3712(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5200(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5152(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3168(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3072(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3680(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5120(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	5056(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4960(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4784(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movb	3808(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5248(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2512(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	2304(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3328(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r15b       # 1-byte Reload
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	%r15b, %al
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3520(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3600(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3488(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movl	%r11d, %r12d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r15b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2880(%rsp)        # 4-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %edi
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %esi
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r15d
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3312(%rsp)        # 4-byte Spill
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r11d
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3344(%rsp)        # 4-byte Spill
	je	.LBB147_1280
# BB#1279:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1280:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5280(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1282
# BB#1281:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1282:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	movzbl	3328(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1284
# BB#1283:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1284:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	je	.LBB147_1286
# BB#1285:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1286:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	je	.LBB147_1288
# BB#1287:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1288:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1290
# BB#1289:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1290:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm2, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1292
# BB#1291:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1292:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2640(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	je	.LBB147_1294
# BB#1293:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1294:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3776(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3072(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2896(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2912(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3072(%rsp)      # 16-byte Spill
	vmovaps	4256(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5096(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2832(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 2816(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3776(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2832(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2784(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2752(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3472(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2800(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3440(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3168(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2912(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movslq	2880(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movslq	3808(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3808(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movslq	3312(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3904(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	movslq	3344(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3872(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3600(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	movslq	3488(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovaps	3472(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	movslq	%r15d, %rcx
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3520(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2832(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3600(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3472(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_1296
# BB#1295:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	2544(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3248(%rsp)       # 16-byte Spill
.LBB147_1296:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vsubps	3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vsubps	2736(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vsubps	2752(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vsubps	3456(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2720(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3600(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3472(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1298
# BB#1297:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	2560(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3232(%rsp)       # 16-byte Spill
.LBB147_1298:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3456(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5728(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3440(%rsp), %xmm3       # 16-byte Reload
	vmulps	4256(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3600(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4192(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3520(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3312(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vmulps	5312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2720(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2704(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2688(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2672(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3376(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vaddps	3552(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3408(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3600(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
	vmovdqa	3328(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1300
# BB#1299:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovdqa	2576(%rsp), %xmm7       # 16-byte Reload
.LBB147_1300:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3312(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3072(%rsp), %xmm5       # 16-byte Reload
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3456(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3600(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1302
# BB#1301:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovdqa	2592(%rsp), %xmm0       # 16-byte Reload
.LBB147_1302:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2848(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3808(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3328(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3440(%rsp)      # 16-byte Spill
	vaddps	3392(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3312(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5216(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1304
# BB#1303:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovdqa	2624(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3280(%rsp)       # 16-byte Spill
.LBB147_1304:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3360(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1306
# BB#1305:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3264(%rsp)       # 16-byte Spill
.LBB147_1306:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3424(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2864(%rsp), %xmm15      # 16-byte Reload
	vmulps	4256(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3520(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2912(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3312(%rsp), %xmm3       # 16-byte Reload
	vaddps	3488(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3456(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3552(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_1308
# BB#1307:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
.LBB147_1308:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vaddps	3328(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3168(%rsp), %xmm5       # 16-byte Reload
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rax,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3392(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3440(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3200(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1310
# BB#1309:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovdqa	2656(%rsp), %xmm5       # 16-byte Reload
.LBB147_1310:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1278 Depth=4
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2896(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5312(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3072(%rsp), %xmm2       # 16-byte Reload
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3840(%rsp), %rax        # 4-byte Folded Reload
	movq	2448(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3216(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1278
# BB#1311:                              #   in Loop: Header=BB147_1275 Depth=3
	movq	4728(%rsp), %r9         # 8-byte Reload
	movq	4720(%rsp), %rax        # 8-byte Reload
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	movl	2288(%rsp), %edx        # 4-byte Reload
.LBB147_1312:                           # %end for f8.s0.v10.v10492
                                        #   in Loop: Header=BB147_1275 Depth=3
	movl	%edx, %r8d
	movq	2176(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %edx
	jne	.LBB147_1275
.LBB147_1313:                           # %end for f8.s0.v11490
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	movq	2440(%rsp), %rax        # 8-byte Reload
	movq	2176(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	jl	.LBB147_1314
	jmp	.LBB147_1406
.LBB147_1369:                           # %end for f8.s0.v10.v10502.end for f8.s0.v10.v10506_crit_edge
                                        #   in Loop: Header=BB147_1314 Depth=3
	movq	2176(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2176(%rsp)        # 8-byte Spill
	movq	4224(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB147_1405
	.align	16, 0x90
.LBB147_1314:                           # %for f8.s0.v11495
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1316 Depth 4
                                        #         Child Loop BB147_1351 Depth 4
                                        #         Child Loop BB147_1371 Depth 4
	cmpl	$0, 3164(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1349
# BB#1315:                              # %for f8.s0.v10.v10497.preheader
                                        #   in Loop: Header=BB147_1314 Depth=3
	movq	4224(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %r14d
	andl	$63, %r14d
	movl	%r14d, %eax
	movq	1344(%rsp), %r8         # 8-byte Reload
	imull	%r8d, %eax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	andl	$1, %eax
	movl	%eax, 3216(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2480(%rsp)       # 16-byte Spill
	movl	%edx, %ebx
	movq	1688(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ebx
	leal	9(%rbx), %edi
	movq	1712(%rsp), %r12        # 8-byte Reload
	imull	%r12d, %edi
	leaq	1(%rdx), %rax
	movq	1880(%rsp), %rsi        # 8-byte Reload
	movq	%rsi, %r10
	imulq	%r10, %rax
	movq	2176(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rsi        # 8-byte Reload
	addl	%esi, %edi
	movq	%rdi, 2448(%rsp)        # 8-byte Spill
	movq	1872(%rsp), %r15        # 8-byte Reload
	leaq	(%rax,%r15), %rax
	movq	1888(%rsp), %r11        # 8-byte Reload
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	leal	7(%rbx), %eax
	imull	%r12d, %eax
	addl	%esi, %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leaq	-1(%rdx), %rax
	imulq	%r10, %rax
	leal	63(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	leal	6(%rbx), %ecx
	imull	%r12d, %ecx
	leaq	(%rax,%r15), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	addl	%esi, %ecx
	movq	%rcx, 2368(%rsp)        # 8-byte Spill
	leaq	-2(%rdx), %rax
	imulq	%r10, %rax
	leaq	(%rax,%r15), %rax
	leal	62(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	leal	8(%rbx), %ecx
	imull	%r12d, %ecx
	addl	%esi, %ecx
	movq	%rcx, 2336(%rsp)        # 8-byte Spill
	movq	%rdx, %rcx
	imulq	%r10, %rcx
	leaq	(%rcx,%r15), %rcx
	addl	$10, %ebx
	imull	%r12d, %ebx
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	addl	%esi, %ebx
	movq	%rbx, 2464(%rsp)        # 8-byte Spill
	leaq	2(%rdx), %rcx
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movq	%rcx, %rax
	imulq	%r10, %rax
	leaq	(%rax,%r15), %rax
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	imulq	1792(%rsp), %r14        # 8-byte Folded Reload
	subq	4760(%rsp), %r14        # 8-byte Folded Reload
	movq	%r14, 2512(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	.align	16, 0x90
.LBB147_1316:                           # %for f8.s0.v10.v10497
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1314 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r15, 3552(%rsp)        # 8-byte Spill
	cmpl	$0, 3216(%rsp)          # 4-byte Folded Reload
	sete	3872(%rsp)              # 1-byte Folded Spill
	setne	3840(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %r8d
	movl	%r8d, 3200(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movl	%r8d, %r10d
	subl	%r9d, %r10d
	leal	1(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r13d
	movl	%r13d, 3280(%rsp)       # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%esi, %edi
	idivl	%edi
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, 3600(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3520(%rsp)        # 4-byte Spill
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpinsrd	$2, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, %r15d, %xmm0, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	movl	%r13d, %ecx
	idivl	%ecx
	movl	%edx, %r15d
	vmovd	3744(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 3776(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	vpinsrd	$2, 3712(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r12d, %xmm1, %xmm2
	movq	5352(%rsp), %r13        # 8-byte Reload
	movq	3552(%rsp), %r12        # 8-byte Reload
	leal	-1(%r13,%r12,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	leal	2(%r10), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	3616(%rsp), %xmm3       # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 3680(%rsp), %xmm3, %xmm3 # 4-byte Folded Reload
	vmovd	%xmm5, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, 3600(%rsp), %xmm3, %xmm3 # 4-byte Folded Reload
	vpinsrd	$3, 3520(%rsp), %xmm3, %xmm4 # 4-byte Folded Reload
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	leal	-2(%r13,%r12,8), %eax
	vmovd	%eax, %xmm3
	vmovd	%r11d, %xmm6
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %r9d, %xmm6, %xmm5
	vpinsrd	$2, %r15d, %xmm5, %xmm5
	vpsrad	$31, %xmm0, %xmm6
	vmovdqa	2480(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm0, %xmm6, %xmm6
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5392(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm11, %xmm7
	vmovdqa	5360(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm6, %xmm15, %xmm1
	vblendvps	%xmm7, %xmm6, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vmovdqa	5072(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vmovdqa	5408(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	%r12, %rbx
	leal	1(%r13,%rbx,8), %r9d
	movq	%r13, %r12
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vmovdqa	5424(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vpinsrd	$3, %r14d, %xmm5, %xmm5
	vmovdqa	5440(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovd	%edi, %xmm6
	vpextrq	$1, %xmm1, %rdi
	movq	%rdi, 3808(%rsp)        # 8-byte Spill
	vpinsrd	$1, %esi, %xmm6, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vmovdqa	5184(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	.LCPI147_54(%rip), %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3776(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm4, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm4
	vpsubd	%xmm2, %xmm15, %xmm7
	vblendvps	%xmm4, %xmm2, %xmm7, %xmm2
	vmovdqa	5200(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	.LCPI147_54(%rip), %xmm4, %xmm4
	vmovdqa	5152(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpinsrd	$2, %ecx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm4, %xmm2, %xmm3, %xmm2
	vpsrad	$31, %xmm5, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm4
	vpsubd	%xmm3, %xmm15, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5056(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm14, %xmm0, %xmm5
	vpminsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpinsrd	$3, %edx, %xmm6, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	4960(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	4784(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	leal	2(%r12,%rbx,8), %eax
	vmovd	%eax, %xmm6
	sarq	$32, %rdi
	movq	%rdi, 3328(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm3, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm4, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm9, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movb	3840(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5248(%rsp)       # 1-byte Folded Spill
	movl	%r8d, %eax
	movq	4224(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	sete	3232(%rsp)              # 1-byte Folded Spill
	movl	3216(%rsp), %r15d       # 4-byte Reload
	testl	%r8d, %r15d
	setne	3248(%rsp)              # 1-byte Folded Spill
	movb	3872(%rsp), %r11b       # 1-byte Reload
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	%r11b, %al
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	sete	%r14b
	addl	$3, %r10d
	vmovd	%r10d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	leal	3(%r12,%rcx,8), %eax
	movq	%rcx, %rdi
	vmovd	%eax, %xmm2
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	andb	%r13b, %r14b
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpextrq	$1, %xmm0, %r13
	movq	%r13, %r12
	sarq	$32, %r12
	movl	%r9d, %eax
	orl	%ebx, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r15d
	movzbl	3232(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	setne	%al
	andb	%r11b, %r8b
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_1318
# BB#1317:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1318:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	movl	5280(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3072(%rsp)       # 16-byte Spill
	movq	5096(%rsp), %rbx        # 8-byte Reload
	je	.LBB147_1320
# BB#1319:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1320:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm1, 2528(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movzbl	3248(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	movq	4728(%rsp), %r9         # 8-byte Reload
	movq	%rdi, %r15
	je	.LBB147_1322
# BB#1321:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1322:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_1324
# BB#1323:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1324:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1326
# BB#1325:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1326:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	%r14b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r8b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm3
	je	.LBB147_1328
# BB#1327:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1328:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm3, 2576(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movq	5672(%rsp), %rsi        # 8-byte Reload
	je	.LBB147_1330
# BB#1329:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1330:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3168(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	je	.LBB147_1332
# BB#1331:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1332:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 2896(%rsp)      # 16-byte Spill
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm0
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %rcx
	vmovups	24608(%rsi,%rcx,4), %xmm9
	vmovups	24624(%rsi,%rcx,4), %xmm14
	vshufps	$221, %xmm14, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm14[1,3]
	vmovaps	5728(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %r11
	vmovups	8(%rbx,%r11,4), %xmm11
	vmovaps	4256(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm1
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	cltq
	vmovups	24608(%rsi,%rax,4), %xmm12
	vmovups	24624(%rsi,%rax,4), %xmm0
	vshufps	$221, %xmm0, %xmm12, %xmm6 # xmm6 = xmm12[1,3],xmm0[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm1
	vmovups	24600(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 3328(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3840(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm15, %xmm1
	vmovups	24600(%rsi,%rax,4), %xmm7
	vmovaps	%xmm7, 2800(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm6
	vmovaps	%xmm6, 3808(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm6[1,3]
	vsubps	%xmm8, %xmm7, %xmm7
	vmulps	%xmm7, %xmm4, %xmm7
	vmulps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r11,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm7 # xmm7 = xmm11[1,3],xmm1[1,3]
	vmovaps	%xmm7, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vmovss	(%rdx,%rdi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3424(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3440(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm10, 3312(%rsp)      # 16-byte Spill
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3520(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3472(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3488(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm13, 2912(%rsp)      # 16-byte Spill
	vmovups	24632(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %r14
	vmovups	8(%rbx,%r14,4), %xmm0
	vmovups	24(%rbx,%r14,4), %xmm1
	vshufps	$221, %xmm1, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm1[1,3]
	vmovaps	%xmm2, 3616(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm9, %xmm0 # xmm0 = xmm9[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm5, %xmm0 # xmm0 = xmm5[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	cltq
	vmovups	24600(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm12
	vmovaps	%xmm12, 3872(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movq	2368(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15,8), %ecx
	movslq	%ecx, %r8
	vmovups	24608(%rsi,%r8,4), %xmm15
	vmovups	24624(%rsi,%r8,4), %xmm14
	vshufps	$221, %xmm14, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	4192(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm9, %xmm1, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movq	2464(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15,8), %ecx
	movslq	%ecx, %rcx
	vmovups	24608(%rsi,%rcx,4), %xmm7
	vmovups	24624(%rsi,%rcx,4), %xmm6
	vshufps	$221, %xmm6, %xmm7, %xmm3 # xmm3 = xmm7[1,3],xmm6[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	3904(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm5
	vmulps	%xmm3, %xmm5, %xmm5
	vmovups	24608(%rsi,%rax,4), %xmm2
	vmovups	24624(%rsi,%rax,4), %xmm1
	vshufps	$221, %xmm1, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm1[1,3]
	vshufps	$136, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm1[0,2]
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm6[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm9, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm1 # xmm1 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm11, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3776(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r10,4), %xmm1, %xmm2 # xmm2 = xmm1[0],mem[0],xmm1[2,3]
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %rdi
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmovaps	3408(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm15
	vmovaps	3392(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmovaps	%xmm6, 3712(%rsp)       # 16-byte Spill
	vmovaps	3600(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm6
	vmovaps	%xmm6, 3680(%rsp)       # 16-byte Spill
	vmovaps	3520(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm7
	vmovaps	3488(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm14
	vmovaps	3472(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	movslq	%r13d, %rax
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vinsertps	$48, (%rdx,%r12,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	movq	2320(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15,8), %edx
	cltq
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm2
	vsubps	%xmm8, %xmm3, %xmm8
	movslq	%edx, %rdx
	vminps	%xmm4, %xmm5, %xmm0
	cmpl	$0, 104(%rbp)
	vmovups	(%rbx,%r11,4), %xmm3
	vmovaps	%xmm3, 2752(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%r11,4), %xmm5
	vmovaps	%xmm5, 3600(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%r11,4), %xmm6
	vmovaps	%xmm6, 2784(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%r14,4), %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%r14,4), %xmm12
	vmovups	32(%rbx,%r14,4), %xmm3
	vmovaps	%xmm3, 3472(%rsp)       # 16-byte Spill
	vmovups	8(%rbx,%rdi,4), %xmm3
	vmovaps	%xmm3, 3408(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rdi,4), %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rdi,4), %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rdi,4), %xmm3
	vmovaps	%xmm3, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rdi,4), %xmm3
	vmovaps	%xmm3, 3776(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm6[0,2]
	vmovups	8(%rbx,%rax,4), %xmm10
	vmovups	24(%rbx,%rax,4), %xmm9
	vmovups	8(%rbx,%rdx,4), %xmm13
	vmovups	24(%rbx,%rdx,4), %xmm11
	je	.LBB147_1334
# BB#1333:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	2672(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
.LBB147_1334:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vsubps	3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vsubps	3360(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vmovaps	3680(%rsp), %xmm1       # 16-byte Reload
	vsubps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm7, %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vsubps	3456(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm5
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	3744(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm7
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2736(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm6
	vmaxps	%xmm1, %xmm2, %xmm2
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmulps	5760(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3488(%rsp)      # 16-byte Spill
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm12, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm12[1,3]
	vshufps	$136, 3472(%rsp), %xmm12, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm12[0,2],mem[0,2]
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm9, %xmm10, %xmm8 # xmm8 = xmm10[1,3],xmm9[1,3]
	vshufps	$221, %xmm11, %xmm13, %xmm12 # xmm12 = xmm13[1,3],xmm11[1,3]
	je	.LBB147_1336
# BB#1335:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	2528(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
.LBB147_1336:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vsubps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm11, %xmm13, %xmm11 # xmm11 = xmm13[0,2],xmm11[0,2]
	vshufps	$136, %xmm9, %xmm10, %xmm10 # xmm10 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm14, %xmm6, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm2, %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmulps	5248(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3840(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5728(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	3440(%rsp), %xmm5       # 16-byte Reload
	vmulps	5216(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3648(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm8
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3808(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	4256(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vmovaps	3600(%rsp), %xmm7       # 16-byte Reload
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm7, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm7[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm8, %xmm5
	vshufps	$221, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[1,3],xmm7[1,3]
	vmovaps	2672(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm2
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vmulps	5312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2736(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm3, %xmm14
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm8
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm9
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm12
	vaddps	3376(%rsp), %xmm5, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	3520(%rsp), %xmm5       # 16-byte Reload
	vaddps	3616(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vmovaps	3408(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[1,3],mem[1,3]
	vmovaps	%xmm6, 3648(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm6
	vmovaps	%xmm6, 5248(%rsp)       # 16-byte Spill
	je	.LBB147_1338
# BB#1337:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	2544(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3232(%rsp)       # 16-byte Spill
.LBB147_1338:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm2, 3312(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm8, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm9, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm12, %xmm7
	vmovups	(%rbx,%rdx,4), %xmm1
	vmovups	16(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	2896(%rsp), %xmm6       # 16-byte Reload
	vmulps	3904(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%rcx,4), %xmm3
	vmovups	24616(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	%xmm14, %xmm10
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4192(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%r8,4), %xmm3
	vmovups	24616(%rsi,%r8,4), %xmm6
	vmovaps	%xmm6, 2896(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm6[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm14
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rbx,%rax,4), %xmm3
	vmovups	16(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3744(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	3456(%rsp), %xmm13, %xmm12 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vaddps	2688(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm9
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vsubps	3648(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vaddps	3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	5248(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovdqa	3072(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1340
# BB#1339:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovdqa	2560(%rsp), %xmm0       # 16-byte Reload
.LBB147_1340:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3872(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[0,2],mem[0,2]
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vpslld	$31, %xmm0, %xmm5
	vmovaps	3344(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	2704(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm2, %xmm1, %xmm1
	vsubps	3328(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm7, 3440(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm0, %xmm0
	vblendvps	%xmm5, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm6, %xmm3, %xmm0, %xmm3
	vaddps	3312(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5248(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	je	.LBB147_1342
# BB#1341:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovdqa	2592(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
.LBB147_1342:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm5, %xmm2
	vblendvps	%xmm6, %xmm1, %xmm3, %xmm3
	vaddps	3360(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1344
# BB#1343:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	2576(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
.LBB147_1344:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vblendvps	%xmm0, %xmm2, %xmm3, %xmm13
	vaddps	3424(%rsp), %xmm6, %xmm15 # 16-byte Folded Reload
	vmovaps	3488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	2832(%rsp), %xmm9       # 16-byte Reload
	vmulps	5216(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm5, %xmm1, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	3600(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm7, %xmm5, %xmm5
	vsubps	%xmm1, %xmm5, %xmm1
	vmovaps	3312(%rsp), %xmm5       # 16-byte Reload
	vaddps	3520(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vaddps	3456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm8
	vmovdqa	3168(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1346
# BB#1345:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovdqa	2608(%rsp), %xmm6       # 16-byte Reload
.LBB147_1346:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vaddps	3328(%rsp), %xmm13, %xmm13 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm15, %xmm15
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	2912(%rsp), %xmm3       # 16-byte Reload
	vmulps	3904(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2896(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rax,4), %xmm2, %xmm3 # xmm3 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3392(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm8, %xmm10
	vmulps	%xmm1, %xmm0, %xmm11
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm8
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vpslld	$31, %xmm6, %xmm0
	vmovdqa	3184(%rsp), %xmm3       # 16-byte Reload
	je	.LBB147_1348
# BB#1347:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovdqa	2624(%rsp), %xmm3       # 16-byte Reload
.LBB147_1348:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1316 Depth=4
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3776(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	3872(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[1,3],mem[1,3]
	vmulps	5312(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vsubps	5728(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	5760(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	3232(%rsp), %xmm2       # 16-byte Reload
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3712(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpslld	$31, %xmm3, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vblendvps	%xmm0, %xmm11, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm10, %xmm0, %xmm0
	vblendvps	%xmm8, %xmm15, %xmm0, %xmm0
	vaddps	3648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	movq	2512(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r15
	cmpl	3164(%rsp), %r15d       # 4-byte Folded Reload
	jne	.LBB147_1316
.LBB147_1349:                           # %end for f8.s0.v10.v10498
                                        #   in Loop: Header=BB147_1314 Depth=3
	movl	3164(%rsp), %eax        # 4-byte Reload
	cmpl	1452(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1368
# BB#1350:                              # %for f8.s0.v10.v10501.preheader
                                        #   in Loop: Header=BB147_1314 Depth=3
	movq	4224(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 2672(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$63, %r8d
	leaq	1(%r9), %rcx
	movq	1880(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rbx
	imulq	%rbx, %rcx
	movq	1872(%rsp), %rax        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1888(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	leaq	-1(%r9), %rcx
	imulq	%rbx, %rcx
	leaq	(%rcx,%rax), %rcx
	leaq	-2(%r9), %rdx
	imulq	%rbx, %rdx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rax), %rcx
	movq	%r9, %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rax), %rdx
	leaq	2(%r9), %rsi
	imulq	%rbx, %rsi
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	leaq	(%rsi,%rax), %rdx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	movq	%r8, %rax
	imulq	1792(%rsp), %rax        # 8-byte Folded Reload
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	leal	2(%r9), %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	1504(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	movq	1752(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %edx
	movq	2200(%rsp), %rsi        # 8-byte Reload
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rdi        # 8-byte Reload
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2624(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2608(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2592(%rsp)        # 8-byte Spill
	movq	1512(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2576(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$63, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	movq	1520(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2544(%rsp)        # 8-byte Spill
	movq	2176(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	movq	1528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2528(%rsp)        # 8-byte Spill
	imull	%eax, %r8d
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	leal	(%r8,%rsi), %eax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	xorl	%r10d, %r10d
	movl	1304(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1351:                           # %for f8.s0.v10.v10501
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1314 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3328(%rsp)        # 4-byte Spill
	movl	2672(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	sete	%al
	setne	%bl
	movq	2120(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10), %esi
	movslq	%esi, %r8
	andl	$1, %esi
	sete	%cl
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	movl	%edx, 5248(%rsp)        # 4-byte Spill
	leaq	1(%r8), %rdx
	movq	4664(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rdx
	movq	4744(%rsp), %r15        # 8-byte Reload
	leaq	(%rdx,%r15), %r14
	leaq	-1(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %r13
	leaq	-2(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	movq	%r8, %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3680(%rsp)        # 8-byte Spill
	leaq	2(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3712(%rsp)        # 8-byte Spill
	andb	%bl, %cl
	movl	%r8d, %edx
	movq	4224(%rsp), %rdi        # 8-byte Reload
	orl	%edi, %edx
	testb	$1, %dl
	sete	%bl
	movq	2528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %r9d
	movq	2512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edi
	movq	2560(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	movl	%edx, 5280(%rsp)        # 4-byte Spill
	movq	2544(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	testl	%r8d, %r12d
	setne	%r12b
	andb	%al, %sil
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	leaq	3(%r8), %rax
	imulq	%r11, %rax
	leaq	(%rax,%r15), %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	movq	2608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3456(%rsp)        # 4-byte Spill
	movq	2592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3552(%rsp)        # 4-byte Spill
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3472(%rsp)        # 4-byte Spill
	movq	2640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3600(%rsp)        # 4-byte Spill
	movq	2624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	je	.LBB147_1353
# BB#1352:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1353:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	movzbl	%sil, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1355
# BB#1354:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1355:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	%xmm2, 2688(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1357
# BB#1356:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1357:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	5096(%rsp), %r15        # 8-byte Reload
	je	.LBB147_1359
# BB#1358:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1359:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movq	5528(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r14,4), %rax
	movq	4736(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm1
	vmovss	(%rsi,%r13,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r13,4), %rax
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm14, 3264(%rsp)      # 16-byte Spill
	movslq	%r9d, %r9
	vmovups	8(%r15,%r9,4), %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovups	24(%r15,%r9,4), %xmm2
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm2[1,3]
	vmovaps	5216(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm2
	movslq	%edi, %r11
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%r11,4), %xmm3
	vmovaps	%xmm3, 3248(%rsp)       # 16-byte Spill
	vmovups	24624(%rdi,%r11,4), %xmm4
	vmovaps	%xmm4, 3232(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm4[1,3]
	vmovaps	5728(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmovaps	5760(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vbroadcastss	.LCPI147_17(%rip), %xmm11
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movslq	5280(%rsp), %r14        # 4-byte Folded Reload
	vmovups	8(%r15,%r14,4), %xmm12
	vmovups	24(%r15,%r14,4), %xmm10
	vshufps	$221, %xmm10, %xmm12, %xmm3 # xmm3 = xmm12[1,3],xmm10[1,3]
	vmovaps	4256(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm0
	movslq	%edx, %rcx
	vmovups	24608(%rdi,%rcx,4), %xmm15
	vmovups	24624(%rdi,%rcx,4), %xmm9
	vshufps	$221, %xmm9, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%r11,4), %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%r11,4), %xmm8
	vmovaps	%xmm8, 3776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm8, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm8[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm5, %xmm14, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vmovups	(%r15,%r9,4), %xmm0
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%r9,4), %xmm2
	vmovaps	%xmm2, 5280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm3 # xmm3 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm4, %xmm14, %xmm3
	vmulps	%xmm1, %xmm3, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm10[0,2]
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm9, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm9[0,2]
	movq	3680(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax,4), %rdx
	vmovss	(%rsi,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$32, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$48, (%rdx,%rbx,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm10, 2896(%rsp)      # 16-byte Spill
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm4, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax,4), %rdx
	vmovss	(%rsi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$48, (%rdx,%rbx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm3, 3712(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm4, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm5, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%r11,4), %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm8, %xmm1 # xmm1 = xmm8[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm5, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	movslq	3456(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rax,4), %xmm13
	vmovaps	%xmm13, 3680(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm13, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm13[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5312(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movslq	3472(%rsp), %r11        # 4-byte Folded Reload
	vmovups	24608(%rdi,%r11,4), %xmm15
	vmovups	24624(%rdi,%r11,4), %xmm14
	vshufps	$221, %xmm14, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	4192(%rsp), %xmm8       # 16-byte Reload
	vmovaps	3648(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm8, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm0
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	movslq	3488(%rsp), %r13        # 4-byte Folded Reload
	vmovups	24608(%rdi,%r13,4), %xmm0
	vmovups	24624(%rdi,%r13,4), %xmm5
	vshufps	$221, %xmm5, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm5[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	3904(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm2, %xmm1, %xmm4
	vmovups	24608(%rdi,%rax,4), %xmm1
	vmovups	24624(%rdi,%rax,4), %xmm3
	vshufps	$221, %xmm3, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm3[1,3]
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm9, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm8, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	3712(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rax
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3520(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rax
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	movslq	5248(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm3
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmovaps	3216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3200(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovaps	2864(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm8
	movslq	3552(%rsp), %rdx        # 4-byte Folded Reload
	vmovaps	3472(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm7
	vsubps	%xmm6, %xmm2, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movslq	3600(%rsp), %rax        # 4-byte Folded Reload
	vminps	%xmm11, %xmm4, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	(%r15,%r14,4), %xmm5
	vmovaps	%xmm5, 2832(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%r14,4), %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovups	32(%r15,%r14,4), %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vmovups	32(%r15,%r9,4), %xmm2
	vmovaps	%xmm2, 3200(%rsp)       # 16-byte Spill
	vmovups	8(%r15,%rcx,4), %xmm4
	vmovaps	%xmm4, 2864(%rsp)       # 16-byte Spill
	vmovups	24(%r15,%rcx,4), %xmm4
	vmovaps	%xmm4, 3392(%rsp)       # 16-byte Spill
	vmovups	(%r15,%rcx,4), %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%rcx,4), %xmm4
	vmovaps	%xmm4, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%r15,%rcx,4), %xmm4
	vmovaps	%xmm4, 3472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm5, %xmm10 # xmm10 = xmm5[1,3],xmm0[1,3]
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm0, %xmm13 # xmm13 = xmm0[0,2],xmm2[0,2]
	vmovups	8(%r15,%rdx,4), %xmm5
	vmovups	24(%r15,%rdx,4), %xmm12
	vmovups	8(%r15,%rax,4), %xmm15
	vmovups	24(%r15,%rax,4), %xmm0
	je	.LBB147_1361
# BB#1360:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	2784(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
.LBB147_1361:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vsubps	%xmm10, %xmm3, %xmm2
	vmovaps	%xmm2, 3600(%rsp)       # 16-byte Spill
	vsubps	3440(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3552(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm9, %xmm1
	vmovaps	%xmm1, 3520(%rsp)       # 16-byte Spill
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vmovaps	2848(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm10
	vmovaps	2816(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm14
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vsubps	2880(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm1, %xmm9 # 16-byte Folded Reload
                                        # xmm9 = xmm1[1,3],mem[1,3]
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm8, %xmm8
	vmaxps	%xmm2, %xmm7, %xmm3
	vmovaps	3648(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vmulps	5760(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm2, %xmm4, %xmm2
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm4, 3648(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm4       # 16-byte Reload
	vaddps	3872(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3808(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2880(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm13 # xmm13 = xmm5[1,3],xmm12[1,3]
	vshufps	$221, %xmm0, %xmm15, %xmm4 # xmm4 = xmm15[1,3],xmm0[1,3]
	je	.LBB147_1363
# BB#1362:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	%xmm10, 3456(%rsp)      # 16-byte Spill
	vmovaps	%xmm14, %xmm10
	vmovaps	2688(%rsp), %xmm14      # 16-byte Reload
	vmovaps	%xmm14, 3408(%rsp)      # 16-byte Spill
	vmovaps	%xmm10, %xmm14
	vmovaps	3456(%rsp), %xmm10      # 16-byte Reload
.LBB147_1363:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vshufps	$136, %xmm0, %xmm15, %xmm15 # xmm15 = xmm15[0,2],xmm0[0,2]
	vshufps	$136, %xmm12, %xmm5, %xmm12 # xmm12 = xmm5[0,2],xmm12[0,2]
	vsubps	%xmm9, %xmm8, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vsubps	%xmm13, %xmm3, %xmm13
	vmulps	%xmm1, %xmm7, %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm2, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	5760(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm1
	vmovaps	3376(%rsp), %xmm3       # 16-byte Reload
	vmulps	5216(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	3072(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	2912(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4256(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmovaps	2896(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	2784(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm8
	vmaxps	%xmm4, %xmm10, %xmm5
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm4, %xmm0, %xmm9
	vaddps	3552(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3520(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm0
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3472(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmaxps	%xmm4, %xmm14, %xmm2
	vmovdqa	3424(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vmovdqa	%xmm4, 2896(%rsp)       # 16-byte Spill
	vmovaps	2880(%rsp), %xmm4       # 16-byte Reload
	vaddps	3600(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2880(%rsp)       # 16-byte Spill
	vmovdqa	3408(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vmovdqa	%xmm4, 2752(%rsp)       # 16-byte Spill
	vaddps	3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3648(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovaps	2864(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[1,3],mem[1,3]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm14
	movq	%rdi, %rcx
	je	.LBB147_1365
# BB#1364:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
.LBB147_1365:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vshufps	$136, 3392(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,2],mem[0,2]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm5, %xmm15
	vsubps	%xmm12, %xmm9, %xmm0
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	vmovups	(%r15,%rax,4), %xmm0
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vmulps	3904(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rcx,%r13,4), %xmm4
	vmovups	24616(%rcx,%r13,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm1[1,3]
	vmovaps	%xmm6, %xmm12
	vsubps	%xmm12, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovups	16(%r15,%rax,4), %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmulps	4192(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rcx,%r11,4), %xmm4
	vmovups	24616(%rcx,%r11,4), %xmm3
	vmovaps	%xmm3, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm3[1,3]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	%xmm5, %xmm7
	vmulps	%xmm4, %xmm2, %xmm2
	vmovups	(%r15,%rdx,4), %xmm5
	vmovups	16(%r15,%rdx,4), %xmm4
	vmovaps	%xmm4, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm4[1,3]
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovdqa	2896(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 2896(%rsp)       # 16-byte Spill
	vmovaps	2880(%rsp), %xmm2       # 16-byte Reload
	vmulps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 2880(%rsp)       # 16-byte Spill
	vmovdqa	2752(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 3168(%rsp)      # 16-byte Spill
	vmulps	%xmm14, %xmm10, %xmm2
	vmovaps	%xmm2, 2864(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm4
	vmovaps	2848(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	3424(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm13, %xmm0
	vmovaps	%xmm13, 3344(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	3440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm14, %xmm0, %xmm0
	vmovdqa	3312(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_1367
# BB#1366:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovdqa	2720(%rsp), %xmm1       # 16-byte Reload
.LBB147_1367:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1351 Depth=4
	vmovaps	5312(%rsp), %xmm9       # 16-byte Reload
	vmulps	3376(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vmovaps	3184(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	5248(%rsp), %xmm13      # 16-byte Reload
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm13[0,2]
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	3392(%rsp), %xmm3, %xmm10 # 16-byte Folded Reload
	vaddps	%xmm15, %xmm10, %xmm3
	vmovaps	2912(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm3, %xmm6, %xmm3
	vmovaps	3072(%rsp), %xmm8       # 16-byte Reload
	vaddps	%xmm8, %xmm3, %xmm3
	vaddps	%xmm3, %xmm2, %xmm3
	vpslld	$31, %xmm1, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 3360(%rsp)      # 16-byte Spill
	vbroadcastss	.LCPI147_23(%rip), %xmm15
	vmulps	%xmm15, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm3, %xmm14, %xmm3
	vmovdqa	%xmm4, %xmm2
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm3
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2816(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmovaps	2800(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 24632(%rcx,%r11,4), %xmm4, %xmm5 # xmm5 = xmm4[0,2],mem[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmovaps	2784(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vaddps	%xmm10, %xmm6, %xmm4
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm8, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufps	$221, 3472(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm13[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3248(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm8, %xmm9, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm11, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm1, %xmm4, %xmm1
	vmovaps	2848(%rsp), %xmm4       # 16-byte Reload
	vaddps	3440(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	3456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm15, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm1, %xmm14, %xmm1
	vmovaps	3168(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	3376(%rsp), %xmm2       # 16-byte Reload
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm10
	vmovaps	3264(%rsp), %xmm9       # 16-byte Reload
	vblendvps	%xmm9, 2864(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2896(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm0, 2880(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3200(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	5216(%rsp), %xmm8, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	4256(%rsp), %xmm8, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm11, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	3600(%rsp), %xmm4       # 16-byte Reload
	vaddps	3840(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3808(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3872(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm2, %xmm10, %xmm0
	vmovaps	3648(%rsp), %xmm2       # 16-byte Reload
	vaddps	3520(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3552(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vblendvps	%xmm9, %xmm2, %xmm0, %xmm0
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2656(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r10d
	movl	3328(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1351
.LBB147_1368:                           # %end for f8.s0.v10.v10502
                                        #   in Loop: Header=BB147_1314 Depth=3
	movl	1452(%rsp), %eax        # 4-byte Reload
	cmpl	1800(%rsp), %eax        # 4-byte Folded Reload
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	jge	.LBB147_1369
# BB#1370:                              # %for f8.s0.v10.v10505.preheader
                                        #   in Loop: Header=BB147_1314 Depth=3
	movq	4224(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	movl	%r11d, %r8d
	andl	$63, %r8d
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2496(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rcx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	movq	1880(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rdi
	imulq	%rdi, %rcx
	movq	1872(%rsp), %rbx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	2176(%rsp), %r10        # 8-byte Reload
	addl	$1, %r10d
	movq	%r10, 2176(%rsp)        # 8-byte Spill
	leaq	-1(%r11), %rdx
	imulq	%rdi, %rdx
	movq	1888(%rsp), %rsi        # 8-byte Reload
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rcx
	imulq	%rdi, %rcx
	movq	%r11, %rdx
	imulq	%rdi, %rdx
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rdx
	vbroadcastss	(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	leaq	2(%r11), %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rbx), %rdx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3872(%rsp)       # 16-byte Spill
	movq	%r8, %rsi
	imulq	1792(%rsp), %rsi        # 8-byte Folded Reload
	leal	2(%r11), %ecx
	andl	$63, %ecx
	movl	1764(%rsp), %r9d        # 4-byte Reload
	imull	%r9d, %ecx
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %edx
	movl	1832(%rsp), %ebx        # 4-byte Reload
	imull	%ebx, %edx
	subq	4760(%rsp), %rsi        # 8-byte Folded Reload
	movq	%rsi, 2480(%rsp)        # 8-byte Spill
	movq	1288(%rsp), %rdi        # 8-byte Reload
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rax        # 8-byte Reload
	leal	(%rdx,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	movb	%r11b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%r9d, %ecx
	movq	1512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r11), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	leal	(%rdx,%rax), %ecx
	movb	%r11b, %dl
	addb	$63, %dl
	movzbl	%dl, %edx
	andl	$63, %edx
	imull	%r9d, %edx
	movq	1520(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%ebx, %esi
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %ecx
	movq	%rcx, 2368(%rsp)        # 8-byte Spill
	leal	(%rsi,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	movl	%r10d, %ecx
	andl	$63, %ecx
	imull	%r9d, %ecx
	movq	1528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r11), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2336(%rsp)        # 8-byte Spill
	imull	%r9d, %r8d
	leal	(%rdx,%rax), %ecx
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	leal	(%r8,%rdi), %eax
	movq	%rax, 2304(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1300(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1371:                           # %for f8.s0.v10.v10505
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1314 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3232(%rsp)        # 4-byte Spill
	cmpl	$0, 3248(%rsp)          # 4-byte Folded Reload
	sete	5216(%rsp)              # 1-byte Folded Spill
	setne	%r15b
	movb	%r15b, 3376(%rsp)       # 1-byte Spill
	movq	3056(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r10d
	movl	%r10d, 3216(%rsp)       # 4-byte Spill
	movl	%r10d, %r12d
	andl	$1, %r12d
	sete	%r13b
	movq	2984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	movl	%r8d, 3408(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r9d
	movl	%r9d, 3424(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2496(%rsp), %xmm14      # 16-byte Reload
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm9
	vmovdqa	5104(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm1
	movq	3040(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vmovdqa	5376(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vmovdqa	5408(%rsp), %xmm8       # 16-byte Reload
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vmovdqa	5392(%rsp), %xmm12      # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm12, %xmm4
	movq	2976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vmovdqa	5360(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm3, %xmm0, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovd	%xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpaddd	%xmm8, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm2, %xmm3, %xmm1
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm14, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm4
	vmovdqa	5184(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm2, %xmm2
	movq	3032(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vpcmpgtd	%xmm4, %xmm12, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	movq	2968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm13, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpaddd	%xmm8, %xmm4, %xmm4
	vmovd	%xmm7, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vblendvps	%xmm2, %xmm3, %xmm4, %xmm2
	vmovd	%esi, %xmm3
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm14, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	5200(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm3
	movq	3024(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vpcmpgtd	%xmm4, %xmm12, %xmm6
	vpsubd	%xmm4, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	movq	2992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm13, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpaddd	%xmm8, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm5, %xmm4, %xmm3
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm5
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm4, %xmm4
	vpcmpgtd	%xmm5, %xmm12, %xmm6
	vpsubd	%xmm5, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vpaddd	%xmm13, %xmm9, %xmm6
	vpminsd	%xmm10, %xmm6, %xmm6
	movq	3000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm13, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpmaxsd	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm8, %xmm5, %xmm5
	vmovd	%xmm7, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vblendvps	%xmm4, %xmm6, %xmm5, %xmm4
	vmovd	%esi, %xmm5
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpinsrd	$2, %edi, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm6
	vpand	%xmm14, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm5, %xmm12, %xmm6
	vpsubd	%xmm5, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vmovdqa	5424(%rsp), %xmm6       # 16-byte Reload
	vpmulld	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2864(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2896(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	movq	3016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vmovq	%xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm3, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm4, %xmm3
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm3
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm8, %xmm5, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpmulld	%xmm6, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	andb	%r15b, %r13b
	movb	%r13b, 5248(%rsp)       # 1-byte Spill
	movl	%r10d, %eax
	movq	4224(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	3008(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	sete	%r13b
	movl	3248(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3344(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, %r12b
	movl	%r12d, 5280(%rsp)       # 4-byte Spill
	movl	3840(%rsp), %r15d       # 4-byte Reload
	movl	%r15d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	sete	%r12b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3392(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3408(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3424(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	3048(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	3376(%rsp), %r12b       # 1-byte Folded Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm14, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm12, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	%ebx, %eax
	testb	$1, %al
	sete	%dil
	testl	%r15d, %r8d
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm0
	setne	%cl
	andb	%r10b, %r9b
	vbroadcastss	%xmm0, %xmm5
	vmovaps	%xmm5, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3360(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2816(%rsp)        # 4-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movq	2368(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r10d
	movq	2352(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r8d
	movq	2416(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	movq	2400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	movq	2384(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 2880(%rsp)        # 4-byte Spill
	movq	2464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r15d
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3328(%rsp)        # 4-byte Spill
	je	.LBB147_1373
# BB#1372:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1373:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %r13d       # 1-byte Folded Reload
	vmovd	%r13d, %xmm0
	movl	5280(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_1375
# BB#1374:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1375:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm1, 2528(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	movzbl	3344(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	je	.LBB147_1377
# BB#1376:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1377:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	je	.LBB147_1379
# BB#1378:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1379:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	%dil, %esi
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1381
# BB#1380:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1381:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	%r12b, %esi
	vmovd	%esi, %xmm0
	movzbl	%r9b, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm4
	je	.LBB147_1383
# BB#1382:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm4, %xmm4, %xmm4
.LBB147_1383:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	movq	5096(%rsp), %rbx        # 8-byte Reload
	je	.LBB147_1385
# BB#1384:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1385:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm5, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3184(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	je	.LBB147_1387
# BB#1386:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1387:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	2832(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2912(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2896(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2848(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3072(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm13, 2912(%rsp)      # 16-byte Spill
	vmovaps	4256(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	cltq
	movq	5672(%rsp), %rsi        # 8-byte Reload
	vmovups	24608(%rsi,%rax,4), %xmm15
	vmovups	24624(%rsi,%rax,4), %xmm8
	vshufps	$221, %xmm8, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm8[1,3]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	movslq	%r10d, %r12
	vmovups	8(%rbx,%r12,4), %xmm9
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	%r8d, %rcx
	vmovups	24608(%rsi,%rcx,4), %xmm11
	vmovups	24624(%rsi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm13, %xmm2
	vmovups	24600(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 2800(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm12
	vmovaps	%xmm12, 3808(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm13, %xmm2
	vmovups	24600(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2768(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2752(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r12,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2736(%rsp)       # 16-byte Spill
	movq	3600(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3440(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3456(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2784(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	3472(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rdx,%rdi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3488(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3520(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3072(%rsp)      # 16-byte Spill
	vmovups	24632(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2896(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	movslq	2816(%rsp), %rdi        # 4-byte Folded Reload
	vmovups	8(%rbx,%rdi,4), %xmm1
	vmovups	24(%rbx,%rdi,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movslq	3840(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3840(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movslq	2880(%rsp), %r9         # 4-byte Folded Reload
	vmovups	24608(%rsi,%r9,4), %xmm13
	vmovups	24624(%rsi,%r9,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3904(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	movslq	3328(%rsp), %r10        # 4-byte Folded Reload
	vmovups	24608(%rsi,%r10,4), %xmm8
	vmovups	24624(%rsi,%r10,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3872(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rsi,%rcx,4), %xmm6
	vmovups	24624(%rsi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movslq	3360(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm12
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3520(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3440(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	movslq	3376(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	2816(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r15d, %rdx
	vminps	%xmm4, %xmm5, %xmm11
	cmpl	$0, 104(%rbp)
	vmovups	(%rbx,%r12,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%r12,4), %xmm2
	vmovaps	%xmm2, 3552(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%r12,4), %xmm5
	vmovaps	%xmm5, 2816(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	8(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rbx,%rcx,4), %xmm2
	vmovups	24(%rbx,%rcx,4), %xmm8
	vmovups	8(%rbx,%rdx,4), %xmm1
	vmovups	24(%rbx,%rdx,4), %xmm13
	je	.LBB147_1389
# BB#1388:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	2512(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3280(%rsp)       # 16-byte Spill
.LBB147_1389:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vsubps	3600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vsubps	2720(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vsubps	2736(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm12
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm11, %xmm14
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3488(%rsp), %xmm3, %xmm11 # 16-byte Folded Reload
                                        # xmm11 = xmm3[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1391
# BB#1390:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	2528(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3264(%rsp)       # 16-byte Spill
.LBB147_1391:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vsubps	%xmm9, %xmm12, %xmm6
	vmovaps	%xmm6, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm12 # xmm12 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3808(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5728(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmulps	4256(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3616(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2768(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4192(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3328(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vmulps	5312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2704(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2688(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm11
	vmovaps	2672(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2656(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3392(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm1       # 16-byte Reload
	vaddps	3600(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3424(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3616(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1393
# BB#1392:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	2544(%rsp), %xmm7       # 16-byte Reload
.LBB147_1393:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3328(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm11, %xmm11
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rdx,4), %xmm1
	vmovups	16(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	2912(%rsp), %xmm5       # 16-byte Reload
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%r10,4), %xmm3
	vmovups	24616(%rsi,%r10,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm12
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%r9,4), %xmm3
	vmovups	24616(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rbx,%rcx,4), %xmm3
	vmovups	16(%rbx,%rcx,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3712(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3472(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2912(%rsp)       # 16-byte Spill
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3168(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1395
# BB#1394:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	2560(%rsp), %xmm0       # 16-byte Reload
.LBB147_1395:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2832(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3840(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm12, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2704(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3344(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm11, %xmm13, %xmm5
	vmovaps	%xmm11, 3456(%rsp)      # 16-byte Spill
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3328(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm11
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5216(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1397
# BB#1396:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	2592(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
.LBB147_1397:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm11, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3376(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1399
# BB#1398:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
.LBB147_1399:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3440(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2864(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2848(%rsp), %xmm15      # 16-byte Reload
	vmulps	4256(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2896(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmovaps	%xmm10, %xmm1
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vaddps	3520(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3472(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3600(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm10
	vmovdqa	3184(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1401
# BB#1400:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	2608(%rsp), %xmm6       # 16-byte Reload
.LBB147_1401:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vaddps	3344(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm11, %xmm14, %xmm14
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm1, %xmm3
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	%xmm0, %xmm12, %xmm0
	vmovaps	3072(%rsp), %xmm5       # 16-byte Reload
	vmulps	3872(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2800(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3408(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm10, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm13
	vpslld	$31, %xmm6, %xmm0
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3200(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1403
# BB#1402:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovdqa	2624(%rsp), %xmm5       # 16-byte Reload
.LBB147_1403:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1371 Depth=4
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5312(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	2912(%rsp), %xmm2       # 16-byte Reload
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm13, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	movq	2480(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3232(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovdqa	5440(%rsp), %xmm11      # 16-byte Reload
	jne	.LBB147_1371
# BB#1404:                              #   in Loop: Header=BB147_1314 Depth=3
	movq	2288(%rsp), %rax        # 8-byte Reload
.LBB147_1405:                           # %end for f8.s0.v10.v10506
                                        #   in Loop: Header=BB147_1314 Depth=3
	movq	%rax, 4224(%rsp)        # 8-byte Spill
	movq	2440(%rsp), %rax        # 8-byte Reload
	movq	2176(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	4728(%rsp), %r9         # 8-byte Reload
	jne	.LBB147_1314
.LBB147_1406:                           # %end for f8.s0.v11496
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	2440(%rsp), %rax        # 8-byte Reload
	cmpl	1436(%rsp), %eax        # 4-byte Folded Reload
	movl	1800(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_1407
	jmp	.LBB147_1445
.LBB147_1408:                           # %for f8.s0.v11509.end for f8.s0.v10.v10513_crit_edge
                                        #   in Loop: Header=BB147_1407 Depth=3
	movq	2440(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movl	%eax, %ecx
	jmp	.LBB147_1444
	.align	16, 0x90
.LBB147_1407:                           # %for f8.s0.v11509
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1410 Depth 4
	testl	%eax, %eax
	jle	.LBB147_1408
# BB#1409:                              # %for f8.s0.v10.v10512.preheader
                                        #   in Loop: Header=BB147_1407 Depth=3
	movq	2440(%rsp), %r8         # 8-byte Reload
	movl	%r8d, %r11d
	movq	1816(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r11d
	leal	1(%r11), %eax
	cltd
	movq	1824(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movq	%rcx, %rbx
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	leal	-1(%r11), %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movl	1836(%rsp), %r13d       # 4-byte Reload
	andl	%r13d, %esi
	addl	%ecx, %esi
	movl	%r9d, %ecx
	sarl	$31, %ecx
	movl	%r11d, %eax
	cltd
	idivl	%ebx
	andl	%r13d, %ecx
	addl	%r9d, %ecx
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%r13d, %ebx
	addl	%edx, %ebx
	leal	1(%r8), %eax
	movl	%eax, 2320(%rsp)        # 4-byte Spill
	movl	1804(%rsp), %r12d       # 4-byte Reload
	cmpl	%eax, %r12d
	movl	%r12d, %edx
	cmovgl	%eax, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1860(%rsp), %r15d       # 4-byte Reload
	movl	%r15d, %eax
	subl	%esi, %eax
	movq	1848(%rsp), %r9         # 8-byte Reload
	cmpl	%esi, %r9d
	cmovgl	%esi, %eax
	addl	%edi, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r8d, %r12d
	cmovgl	%edx, %eax
	movl	%r12d, %edx
	cmovgl	%r8d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r15d, %r10d
	subl	%ebx, %r10d
	cmpl	%ebx, %r9d
	movq	%r9, %r14
	cmovgl	%ebx, %r10d
	addl	%edi, %r10d
	cmpl	%r10d, %r12d
	cmovlel	%r12d, %r10d
	cmpl	%edi, %r10d
	cmovll	%edi, %r10d
	movq	1808(%rsp), %r9         # 8-byte Reload
	cmpl	%r8d, %r9d
	movl	%r9d, %esi
	cmovgl	%r8d, %esi
	cmovgl	%edx, %r10d
	addl	$-1, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	movl	%r15d, %ebx
	subl	%ecx, %ebx
	cmpl	%ecx, %r14d
	cmovgl	%ecx, %ebx
	addl	%edi, %ebx
	cmpl	%ebx, %r12d
	cmovlel	%r12d, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	%r8d, %r9d
	cmovgel	%esi, %ebx
	movl	%r8d, %ecx
	andl	$1, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	vpabsd	5456(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2512(%rsp)       # 16-byte Spill
	movslq	%eax, %r9
	movl	%r8d, %ecx
	leal	-2(%r11), %eax
	cltd
	movq	1824(%rsp), %rsi        # 8-byte Reload
	idivl	%esi
	movl	%edx, %r14d
	andl	$63, %ecx
	movq	%rcx, 2528(%rsp)        # 8-byte Spill
	movl	%r14d, %ecx
	sarl	$31, %ecx
	addl	$2, %r11d
	movl	%r11d, %eax
	cltd
	idivl	%esi
	andl	%r13d, %ecx
	addl	%r14d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	movq	1880(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r9
	movq	1872(%rsp), %r15        # 8-byte Reload
	leaq	(%r9,%r15), %rdx
	movq	1888(%rsp), %r9         # 8-byte Reload
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 4256(%rsp)       # 16-byte Spill
	movslq	%ebx, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%r15), %rdx
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 4224(%rsp)       # 16-byte Spill
	leal	-2(%r8), %r11d
	cmpl	%r11d, %r12d
	movl	%r12d, %edx
	cmovgl	%r11d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1860(%rsp), %r13d       # 4-byte Reload
	movl	%r13d, %esi
	subl	%ecx, %esi
	movq	1848(%rsp), %rbx        # 8-byte Reload
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %esi
	addl	%edi, %esi
	cmpl	%esi, %r12d
	cmovlel	%r12d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r8d, 1768(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rcx
	imulq	%r14, %rcx
	leaq	(%rcx,%r15), %rcx
	movq	%rcx, 5280(%rsp)        # 8-byte Spill
	movslq	%r10d, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%r15), %rdx
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 5312(%rsp)       # 16-byte Spill
	leal	2(%r8), %ebx
	cmpl	%ebx, %r12d
	movl	%r12d, %edx
	cmovgl	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r13d, %esi
	subl	%eax, %esi
	movq	1848(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %esi
	addl	%edi, %esi
	cmpl	%esi, %r12d
	cmovlel	%r12d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r8d, 1772(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rax
	imulq	%r14, %rax
	leaq	(%rax,%r15), %rax
	movq	5280(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%r9,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movq	2528(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1792(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1764(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2480(%rsp)        # 8-byte Spill
	movq	1456(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1832(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3904(%rsp)       # 16-byte Spill
	subq	4760(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	movq	4936(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	movq	1592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r11d
	imull	%esi, %r11d
	movq	%r11, 2496(%rsp)        # 8-byte Spill
	movq	1464(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	movl	2320(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2352(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1800(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1410:                           # %for f8.s0.v10.v10512
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1407 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3216(%rsp)        # 4-byte Spill
	cmpl	$0, 3232(%rsp)          # 4-byte Folded Reload
	sete	5216(%rsp)              # 1-byte Folded Spill
	setne	3840(%rsp)              # 1-byte Folded Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3872(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	4616(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5456(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3600(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3376(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	movq	4896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	movq	4904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3776(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3744(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3712(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3680(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3648(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2512(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3872(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5392(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5360(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5104(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5408(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5376(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4648(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5440(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3808(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5184(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3744(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5200(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5152(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3712(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5120(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	5056(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4960(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4784(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2912(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2864(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2880(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3472(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3440(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movb	3840(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5248(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2440(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	3232(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3344(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r15b       # 1-byte Reload
	movl	5280(%rsp), %eax        # 4-byte Reload
	andb	%r15b, %al
	movl	%eax, 5280(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3600(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3376(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4768(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3600(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r15b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2896(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %edi
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %esi
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r15d
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3360(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r11d
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3376(%rsp)        # 4-byte Spill
	je	.LBB147_1412
# BB#1411:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1412:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	5248(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5280(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1414
# BB#1413:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1414:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movzbl	3344(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1416
# BB#1415:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1416:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_1418
# BB#1417:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1418:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_1420
# BB#1419:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1420:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1422
# BB#1421:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1422:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm2, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3296(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1424
# BB#1423:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1424:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2640(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	je	.LBB147_1426
# BB#1425:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1426:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	3776(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5528(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3808(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2912(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2864(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3072(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2880(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3072(%rsp)      # 16-byte Spill
	vmovaps	4256(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5728(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5280(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5096(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4224(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2848(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3328(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3808(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2832(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2800(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2784(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3520(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2816(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3472(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3440(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3168(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2912(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	movslq	2896(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2880(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movslq	3840(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3840(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5312(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movslq	3360(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	4192(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	movslq	3376(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3904(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2896(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3744(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3600(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2864(%rsp)       # 16-byte Spill
	movslq	%r12d, %rax
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3440(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	movslq	%r15d, %rcx
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3552(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2832(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5280(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_1428
# BB#1427:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	2544(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
.LBB147_1428:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vsubps	3520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3600(%rsp)       # 16-byte Spill
	vsubps	2752(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3520(%rsp)       # 16-byte Spill
	vsubps	2768(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3440(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	3472(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2784(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2752(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vmulps	5312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3488(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5280(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1430
# BB#1429:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	2560(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3248(%rsp)       # 16-byte Spill
.LBB147_1430:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3472(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vmulps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3808(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5728(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmulps	4256(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3616(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4224(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vmulps	5312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2752(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2704(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2688(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2672(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3392(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3520(%rsp), %xmm1       # 16-byte Reload
	vaddps	3600(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3424(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3616(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1432
# BB#1431:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovdqa	2576(%rsp), %xmm7       # 16-byte Reload
.LBB147_1432:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3328(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3072(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3712(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3472(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3616(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3072(%rsp)       # 16-byte Spill
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1434
# BB#1433:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovdqa	2592(%rsp), %xmm0       # 16-byte Reload
.LBB147_1434:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vmulps	5312(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2848(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3840(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3344(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3456(%rsp)      # 16-byte Spill
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3328(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5216(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1436
# BB#1435:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovdqa	2624(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
.LBB147_1436:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3376(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1438
# BB#1437:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
.LBB147_1438:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3440(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2880(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2864(%rsp), %xmm15      # 16-byte Reload
	vmulps	4256(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2912(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4224(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vaddps	3520(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3472(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3600(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_1440
# BB#1439:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
.LBB147_1440:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vaddps	3344(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3168(%rsp), %xmm5       # 16-byte Reload
	vmulps	3904(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	4192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rax,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3408(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3280(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3200(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1442
# BB#1441:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovdqa	2656(%rsp), %xmm5       # 16-byte Reload
.LBB147_1442:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1410 Depth=4
	vmovaps	5280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3744(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2896(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5312(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5728(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3072(%rsp), %xmm2       # 16-byte Reload
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3872(%rsp), %rax        # 4-byte Folded Reload
	movq	2464(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3216(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1410
# BB#1443:                              #   in Loop: Header=BB147_1407 Depth=3
	vmovdqa	5488(%rsp), %xmm12      # 16-byte Reload
	movl	2320(%rsp), %ecx        # 4-byte Reload
.LBB147_1444:                           # %end for f8.s0.v10.v10513
                                        #   in Loop: Header=BB147_1407 Depth=3
	movl	%ecx, %eax
	movq	%rax, 2440(%rsp)        # 8-byte Spill
	cmpl	1436(%rsp), %ecx        # 4-byte Folded Reload
	movl	1800(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_1407
.LBB147_1445:                           # %consume f8516
                                        #   in Loop: Header=BB147_467 Depth=2
	vmovdqa	%xmm12, 5488(%rsp)      # 16-byte Spill
	movq	880(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_1468
# BB#1446:                              # %for f0.s0.v10.v10517.preheader
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	1072(%rsp), %rax        # 8-byte Reload
	cltq
	movq	896(%rsp), %rcx         # 8-byte Reload
	movq	848(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	8(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3904(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3872(%rsp)       # 32-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3840(%rsp)       # 32-byte Spill
	movq	888(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	8(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3808(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3776(%rsp)       # 32-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3744(%rsp)       # 32-byte Spill
	movq	840(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3712(%rsp)       # 32-byte Spill
	movq	832(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3680(%rsp)       # 32-byte Spill
	movq	1000(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %edi
	andl	$63, %edi
	movq	%rdi, %rbx
	movq	1792(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rbx
	movq	%rbx, 3600(%rsp)        # 8-byte Spill
	leaq	1(%rsi), %r12
	andl	$63, %r12d
	movq	%r12, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, 3552(%rsp)        # 8-byte Spill
	movq	856(%rsp), %rdx         # 8-byte Reload
	vpbroadcastd	(%rcx,%rdx,4), %ymm0
	vmovdqa	%ymm0, 3648(%rsp)       # 32-byte Spill
	negq	%rax
	leaq	1(%rsi,%rax), %r14
	shlq	$5, %r14
	leaq	8(%r14), %rax
	movq	%rax, 3520(%rsp)        # 8-byte Spill
	leaq	16(%r14), %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	1264(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rdi
	movq	%rdi, 3616(%rsp)        # 8-byte Spill
	leaq	24(%r14), %rcx
	movq	%rcx, 3472(%rsp)        # 8-byte Spill
	imulq	%rax, %r12
	movq	5352(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r13d
	xorl	%r11d, %r11d
	.align	16, 0x90
.LBB147_1447:                           # %for f0.s0.v10.v10517
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1448 Depth 4
                                        #           Child Loop BB147_1449 Depth 5
	movl	%r11d, %eax
	shll	$5, %eax
	movq	5352(%rsp), %rcx        # 8-byte Reload
	addl	%ecx, %eax
	cltq
	movq	1224(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rdx
	subq	4760(%rsp), %rax        # 8-byte Folded Reload
	movq	3600(%rsp), %r10        # 8-byte Reload
	leaq	(%rax,%r10), %r8
	movq	4712(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rcx,%r8,4), %ymm0
	vmovaps	%ymm0, 5792(%rsp)
	movq	3616(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdx,%rsi), %rbx
	movq	5096(%rsp), %rdi        # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm0
	leaq	(%rdx,%r12), %rbx
	vmovups	(%rdi,%rbx,4), %ymm1
	leaq	8(%rsi,%rdx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm2
	leaq	8(%r12,%rdx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm3
	leaq	16(%rsi,%rdx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm4
	leaq	16(%r12,%rdx), %rbx
	vmovups	(%rdi,%rbx,4), %ymm5
	leaq	24(%rsi,%rdx), %rbx
	vmovups	(%rdi,%rbx,4), %ymm6
	movq	3552(%rsp), %rbx        # 8-byte Reload
	leaq	(%rax,%rbx), %r9
	vmovups	(%rcx,%r9,4), %ymm7
	vmovaps	%ymm7, 5792(%rsp,%r14,4)
	leaq	24(%r12,%rdx), %rdx
	vmovdqu	(%rdi,%rdx,4), %ymm7
	leaq	8(%r10,%rax), %r15
	vmovups	(%rcx,%r15,4), %ymm8
	vmovaps	%ymm8, 5824(%rsp)
	movq	4704(%rsp), %rdi        # 8-byte Reload
	vmovdqu	(%rdi,%r8,4), %ymm8
	leaq	8(%rax,%rbx), %rsi
	vmovups	(%rcx,%rsi,4), %ymm9
	movq	3520(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm9, 5792(%rsp,%rdx,4)
	vmovups	(%rdi,%r9,4), %ymm9
	leaq	16(%rax,%r10), %r8
	vmovups	(%rcx,%r8,4), %ymm10
	vmovaps	%ymm10, 5856(%rsp)
	vmovups	(%rdi,%r15,4), %ymm10
	leaq	16(%rax,%rbx), %r9
	vmovups	(%rcx,%r9,4), %ymm11
	movq	3488(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm11, 5792(%rsp,%rdx,4)
	vmovups	(%rdi,%rsi,4), %ymm11
	leaq	24(%rax,%r10), %r10
	vmovups	(%rcx,%r10,4), %ymm12
	vmovaps	%ymm12, 5888(%rsp)
	leaq	24(%rax,%rbx), %rdx
	vmovdqu	(%rcx,%rdx,4), %ymm12
	movq	3472(%rsp), %rax        # 8-byte Reload
	vmovdqa	%ymm12, 5792(%rsp,%rax,4)
	vmovaps	%ymm0, 6048(%rsp)
	vmovaps	%ymm1, 6048(%rsp,%r14,4)
	vmovdqa	%ymm2, 6080(%rsp)
	vmovdqa	%ymm3, 6080(%rsp,%r14,4)
	vmovdqa	%ymm4, 6112(%rsp)
	vmovaps	%ymm5, 6112(%rsp,%r14,4)
	vmovaps	%ymm6, 6144(%rsp)
	vmovdqa	%ymm7, 6144(%rsp,%r14,4)
	vmovdqa	%ymm8, 6304(%rsp)
	vmovaps	%ymm9, 6304(%rsp,%r14,4)
	vmovaps	%ymm10, 6336(%rsp)
	vmovaps	%ymm11, 6336(%rsp,%r14,4)
	vmovups	(%rdi,%r8,4), %ymm0
	movslq	%r13d, %rax
	vmovaps	%ymm0, 6368(%rsp)
	vmovups	(%rdi,%r9,4), %ymm0
	movq	1248(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rsi
	movq	1240(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovaps	%ymm0, 6368(%rsp,%r14,4)
	vmovups	(%rdi,%r10,4), %ymm0
	vmovaps	%ymm0, 6400(%rsp)
	vmovdqu	(%rdi,%rdx,4), %ymm0
	vmovdqa	%ymm0, 6400(%rsp,%r14,4)
	movq	1416(%rsp), %r10        # 8-byte Reload
	xorl	%ebx, %ebx
	.align	16, 0x90
.LBB147_1448:                           # %for f0.s0.v11.v13.yii541
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1447 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB147_1449 Depth 5
	movq	%rbx, %rdx
	shlq	$7, %rdx
	vmovaps	5792(%rsp,%rdx), %ymm2
	vmovaps	%ymm2, 4192(%rsp)       # 32-byte Spill
	vmovaps	5824(%rsp,%rdx), %ymm1
	vmovaps	%ymm1, 4224(%rsp)       # 32-byte Spill
	vmovaps	5856(%rsp,%rdx), %ymm0
	vmovaps	%ymm0, 4256(%rsp)       # 32-byte Spill
	vmovaps	5888(%rsp,%rdx), %ymm8
	vmovaps	6048(%rsp,%rdx), %ymm3
	vmovaps	6080(%rsp,%rdx), %ymm7
	vmovaps	6112(%rsp,%rdx), %ymm12
	vmovaps	6144(%rsp,%rdx), %ymm4
	vmovaps	6304(%rsp,%rdx), %ymm9
	vmovaps	6336(%rsp,%rdx), %ymm10
	vmovaps	6368(%rsp,%rdx), %ymm11
	vmovaps	3840(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm0, %ymm0
	vmulps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm5, %ymm2, %ymm2
	vmulps	%ymm5, %ymm8, %ymm5
	vmovaps	3872(%rsp), %ymm13      # 32-byte Reload
	vmovaps	%ymm13, %ymm6
	vfmadd213ps	%ymm5, %ymm4, %ymm6
	vmovaps	%ymm13, %ymm5
	vfmadd213ps	%ymm2, %ymm3, %ymm5
	vmovaps	%ymm13, %ymm2
	vfmadd213ps	%ymm1, %ymm7, %ymm2
	vmovaps	%ymm13, %ymm1
	vfmadd213ps	%ymm0, %ymm12, %ymm1
	vmovaps	3904(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm13
	vfmadd213ps	%ymm1, %ymm11, %ymm13
	vmovaps	%ymm13, 5312(%rsp)      # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm2, %ymm10, %ymm1
	vmovaps	%ymm1, 5280(%rsp)       # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm5, %ymm9, %ymm1
	vmovaps	%ymm1, 5248(%rsp)       # 32-byte Spill
	vmovaps	6400(%rsp,%rdx), %ymm15
	vfmadd213ps	%ymm6, %ymm15, %ymm0
	vmovaps	%ymm0, 5216(%rsp)       # 32-byte Spill
	vmovaps	3744(%rsp), %ymm14      # 32-byte Reload
	vmulps	%ymm14, %ymm8, %ymm0
	vmovaps	3776(%rsp), %ymm5       # 32-byte Reload
	vmovaps	%ymm5, %ymm2
	vfmadd213ps	%ymm0, %ymm4, %ymm2
	vmulps	4192(%rsp), %ymm14, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm5, %ymm6
	vfmadd213ps	%ymm0, %ymm3, %ymm6
	vmulps	4224(%rsp), %ymm14, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm5, %ymm1
	vfmadd213ps	%ymm0, %ymm7, %ymm1
	vmulps	4256(%rsp), %ymm14, %ymm0 # 32-byte Folded Reload
	vfmadd213ps	%ymm0, %ymm12, %ymm5
	vmovaps	3808(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm14
	vfmadd213ps	%ymm5, %ymm11, %ymm14
	vmovaps	%ymm0, %ymm5
	vfmadd213ps	%ymm1, %ymm10, %ymm5
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm6, %ymm9, %ymm1
	vmovaps	%ymm0, %ymm6
	vfmadd213ps	%ymm2, %ymm15, %ymm6
	vmovaps	3648(%rsp), %ymm13      # 32-byte Reload
	vmulps	%ymm13, %ymm8, %ymm2
	vmovaps	3680(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm2, %ymm0, %ymm4
	vmulps	4192(%rsp), %ymm13, %ymm2 # 32-byte Folded Reload
	vfmadd213ps	%ymm2, %ymm0, %ymm3
	vmulps	4224(%rsp), %ymm13, %ymm2 # 32-byte Folded Reload
	vfmadd213ps	%ymm2, %ymm0, %ymm7
	vmulps	4256(%rsp), %ymm13, %ymm2 # 32-byte Folded Reload
	vfmadd213ps	%ymm2, %ymm0, %ymm12
	vmovaps	3712(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm0, %ymm11
	vfmadd213ps	%ymm7, %ymm0, %ymm10
	vfmadd213ps	%ymm3, %ymm0, %ymm9
	vfmadd213ps	%ymm4, %ymm0, %ymm15
	xorl	%r8d, %r8d
	movl	$3, %r9d
	movq	%r10, %rdx
	.align	16, 0x90
.LBB147_1449:                           # %for f0.s0.v12544
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_467 Depth=2
                                        #       Parent Loop BB147_1447 Depth=3
                                        #         Parent Loop BB147_1448 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovaps	%ymm1, %ymm3
	cmpl	$1, %r8d
	je	.LBB147_1451
# BB#1450:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	5248(%rsp), %ymm3       # 32-byte Reload
.LBB147_1451:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm5, %ymm2
	je	.LBB147_1453
# BB#1452:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	5280(%rsp), %ymm2       # 32-byte Reload
.LBB147_1453:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm14, %ymm7
	je	.LBB147_1455
# BB#1454:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	5312(%rsp), %ymm7       # 32-byte Reload
.LBB147_1455:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm6, %ymm8
	je	.LBB147_1457
# BB#1456:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	5216(%rsp), %ymm8       # 32-byte Reload
.LBB147_1457:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm15, %ymm4
	testl	%r8d, %r8d
	je	.LBB147_1459
# BB#1458:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm8, %ymm4
.LBB147_1459:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm11, %ymm8
	je	.LBB147_1461
# BB#1460:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm7, %ymm8
.LBB147_1461:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm10, %ymm7
	je	.LBB147_1463
# BB#1462:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm2, %ymm7
.LBB147_1463:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm9, %ymm2
	je	.LBB147_1465
# BB#1464:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vmovaps	%ymm3, %ymm2
.LBB147_1465:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1449 Depth=5
	vbroadcastss	.LCPI147_25(%rip), %ymm3
	vminps	%ymm3, %ymm2, %ymm2
	vminps	%ymm3, %ymm7, %ymm7
	vminps	%ymm3, %ymm8, %ymm8
	vminps	%ymm3, %ymm4, %ymm3
	vxorps	%ymm0, %ymm0, %ymm0
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm7, %ymm4
	vmaxps	%ymm0, %ymm8, %ymm7
	vmaxps	%ymm0, %ymm3, %ymm3
	vcvttps2dq	%ymm2, %ymm2
	vmovdqa	.LCPI147_7(%rip), %ymm0 # ymm0 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm0, %ymm2, %ymm2
	vpermq	$232, %ymm2, %ymm2      # ymm2 = ymm2[0,2,2,3]
	vcvttps2dq	%ymm4, %ymm4
	vpshufb	%ymm0, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vcvttps2dq	%ymm7, %ymm7
	vpshufb	%ymm0, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vcvttps2dq	%ymm3, %ymm3
	vpshufb	%ymm0, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vmovdqa	5568(%rsp), %ymm0       # 32-byte Reload
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm4, %ymm12
	vpmulld	%ymm0, %ymm7, %ymm4
	vpmulld	%ymm0, %ymm3, %ymm3
	vmovd	%r8d, %xmm7
	vpsubd	5536(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm3, %ymm7, %ymm8
	vpaddd	%ymm4, %ymm7, %ymm4
	vpaddd	%ymm12, %ymm7, %ymm3
	vpaddd	%ymm2, %ymm7, %ymm7
	vmovq	%xmm8, %rcx
	movslq	%ecx, %rdi
	movq	5632(%rsp), %r15        # 8-byte Reload
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm2
	vpextrq	$1, %xmm8, %rdi
	sarq	$32, %rcx
	vpinsrw	$1, (%r15,%rcx,2), %xmm2, %xmm2
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vextracti128	$1, %ymm8, %xmm0
	vpinsrw	$2, (%r15,%rcx,2), %xmm2, %xmm2
	vmovq	%xmm0, %rcx
	vpinsrw	$3, (%r15,%rdi,2), %xmm2, %xmm2
	movslq	%ecx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm2, %xmm2
	vpextrq	$1, %xmm0, %rdi
	sarq	$32, %rcx
	vpinsrw	$5, (%r15,%rcx,2), %xmm2, %xmm0
	movslq	%edi, %rcx
	vpinsrw	$6, (%r15,%rcx,2), %xmm0, %xmm0
	vmovq	%xmm4, %rcx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm0, %xmm2
	movslq	%ecx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm0
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rcx
	vpinsrw	$1, (%r15,%rcx,2), %xmm0, %xmm0
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vextracti128	$1, %ymm4, %xmm4
	vpinsrw	$2, (%r15,%rcx,2), %xmm0, %xmm0
	vmovq	%xmm4, %rcx
	vpinsrw	$3, (%r15,%rdi,2), %xmm0, %xmm0
	movslq	%ecx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm0, %xmm0
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rcx
	vpinsrw	$5, (%r15,%rcx,2), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrw	$6, (%r15,%rcx,2), %xmm0, %xmm0
	vmovq	%xmm3, %rcx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm0, %xmm0
	movslq	%ecx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm4
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rcx
	vpinsrw	$1, (%r15,%rcx,2), %xmm4, %xmm4
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vextracti128	$1, %ymm3, %xmm3
	vpinsrw	$2, (%r15,%rcx,2), %xmm4, %xmm4
	vmovq	%xmm3, %rcx
	vpinsrw	$3, (%r15,%rdi,2), %xmm4, %xmm4
	movslq	%ecx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm4, %xmm4
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rcx
	vpinsrw	$5, (%r15,%rcx,2), %xmm4, %xmm3
	movslq	%edi, %rcx
	vpinsrw	$6, (%r15,%rcx,2), %xmm3, %xmm3
	vmovq	%xmm7, %rcx
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm3, %xmm3
	movslq	%ecx, %rdi
	movzwl	(%r15,%rdi,2), %edi
	vmovd	%edi, %xmm4
	vpextrq	$1, %xmm7, %rdi
	sarq	$32, %rcx
	vpinsrw	$1, (%r15,%rcx,2), %xmm4, %xmm4
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vextracti128	$1, %ymm7, %xmm7
	vpinsrw	$2, (%r15,%rcx,2), %xmm4, %xmm4
	vmovq	%xmm7, %rcx
	vpinsrw	$3, (%r15,%rdi,2), %xmm4, %xmm4
	movslq	%ecx, %rdi
	vpinsrw	$4, (%r15,%rdi,2), %xmm4, %xmm4
	vpextrq	$1, %xmm7, %rdi
	sarq	$32, %rcx
	vpinsrw	$5, (%r15,%rcx,2), %xmm4, %xmm4
	movslq	%edi, %rcx
	vpinsrw	$6, (%r15,%rcx,2), %xmm4, %xmm4
	sarq	$32, %rdi
	vpinsrw	$7, (%r15,%rdi,2), %xmm4, %xmm4
	vinserti128	$1, %xmm2, %ymm0, %ymm0
	vinserti128	$1, %xmm3, %ymm4, %ymm2
	vmovdqu	%ymm2, (%rdx,%rax,2)
	vmovdqu	%ymm0, (%rdx,%rsi,2)
	addq	5624(%rsp), %rdx        # 8-byte Folded Reload
	addl	$1, %r8d
	addq	$-1, %r9
	jne	.LBB147_1449
# BB#1466:                              # %end for f0.s0.v12545
                                        #   in Loop: Header=BB147_1448 Depth=4
	addq	$1, %rbx
	addq	2208(%rsp), %r10        # 8-byte Folded Reload
	cmpq	$2, %rbx
	jne	.LBB147_1448
# BB#1467:                              # %end for f0.s0.v11.v13.yii542
                                        #   in Loop: Header=BB147_1447 Depth=3
	addq	$1, %r11
	addl	$32, %r13d
	cmpl	1220(%rsp), %r11d       # 4-byte Folded Reload
	jne	.LBB147_1447
.LBB147_1468:                           # %end for f0.s0.v10.v10518
                                        #   in Loop: Header=BB147_467 Depth=2
	movq	960(%rsp), %rbx         # 8-byte Reload
	addq	$1, %rbx
	movq	1032(%rsp), %rdx        # 8-byte Reload
	addl	$1, %edx
	movl	1284(%rsp), %r11d       # 4-byte Reload
	addl	$2, %r11d
	movl	1276(%rsp), %r12d       # 4-byte Reload
	addl	$-2, %r12d
	movl	1280(%rsp), %r15d       # 4-byte Reload
	addl	$-2, %r15d
	movl	1052(%rsp), %r10d       # 4-byte Reload
	addl	$2, %r10d
	movl	1064(%rsp), %r9d        # 4-byte Reload
	addl	$2, %r9d
	movl	1048(%rsp), %r14d       # 4-byte Reload
	addl	$-2, %r14d
	movl	1056(%rsp), %r13d       # 4-byte Reload
	addl	$2, %r13d
	movl	1060(%rsp), %r8d        # 4-byte Reload
	addl	$-2, %r8d
	addq	$2, 1000(%rsp)          # 8-byte Folded Spill
	movl	828(%rsp), %eax         # 4-byte Reload
	addl	%eax, 976(%rsp)         # 4-byte Folded Spill
	addl	%eax, 980(%rsp)         # 4-byte Folded Spill
	addl	%eax, 984(%rsp)         # 4-byte Folded Spill
	movl	1068(%rsp), %edi        # 4-byte Reload
	addl	$2, %edi
	addl	%eax, 988(%rsp)         # 4-byte Folded Spill
	addl	%eax, 992(%rsp)         # 4-byte Folded Spill
	addl	%eax, 996(%rsp)         # 4-byte Folded Spill
	movl	1024(%rsp), %ecx        # 4-byte Reload
	addl	$2, %ecx
	addl	$2, 1012(%rsp)          # 4-byte Folded Spill
	movq	2208(%rsp), %rax        # 8-byte Reload
	addq	%rax, 1248(%rsp)        # 8-byte Folded Spill
	addq	%rax, 1240(%rsp)        # 8-byte Folded Spill
	cmpq	$16, %rbx
	movq	5672(%rsp), %rsi        # 8-byte Reload
	jne	.LBB147_467
# BB#1469:                              # %call_destructor.exit1061
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4872(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4880(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4752(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4720(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	5096(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4712(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4704(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	movq	688(%rsp), %rcx         # 8-byte Reload
	leal	1(%rcx), %eax
	movl	684(%rsp), %edx         # 4-byte Reload
	addl	$-32, %edx
	cmpl	648(%rsp), %ecx         # 4-byte Folded Reload
	jne	.LBB147_195
.LBB147_1470:                           # %consume f0
	movq	424(%rsp), %rax         # 8-byte Reload
	leal	-8(%rax), %r14d
	movl	%r14d, 4816(%rsp)       # 4-byte Spill
	movq	1016(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %r14d
	cmovgl	%ecx, %r14d
	movq	456(%rsp), %rax         # 8-byte Reload
	orl	$7, %eax
	movq	%rax, 456(%rsp)         # 8-byte Spill
	leal	(%rax,%rcx), %ecx
	movl	%ecx, 4608(%rsp)        # 4-byte Spill
	movl	420(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	movl	%eax, %r15d
	cmovgl	%ecx, %r15d
	movq	760(%rsp), %rax         # 8-byte Reload
	cmpl	$7, %eax
	movl	$8, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 4616(%rsp)        # 4-byte Spill
	movq	%rax, %r13
	movl	%r15d, %ebx
	subl	%r14d, %ebx
	addl	$1, %ebx
	movl	%ecx, %eax
	leaq	(,%rbx,4), %rdx
	movl	%edx, %ecx
	imulq	%rax, %rdx
	leaq	(%rdx,%rdx,2), %r12
	movl	%edx, %edx
	cmpq	$2147483647, %r12       # imm = 0x7FFFFFFF
	ja	.LBB147_1472
# BB#1471:                              # %consume f0
	imulq	%rax, %rcx
	movq	%rbx, %rsi
	shrq	$30, %rsi
	imulq	%rax, %rsi
	shrq	$32, %rcx
	addq	%rcx, %rsi
	leaq	(%rdx,%rdx,2), %rax
	shrq	$32, %rax
	leaq	(%rsi,%rsi,2), %rcx
	addq	%rax, %rcx
	orq	%rsi, %rcx
	shrq	$32, %rcx
	jne	.LBB147_1472
# BB#1475:                              # %assert succeeded548
	leaq	4(%r12), %rsi
	xorl	%eax, %eax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	addq	$4, %r12
	je	.LBB147_1477
# BB#1476:                              # %assert succeeded548
	testq	%rax, %rax
	je	.LBB147_198
.LBB147_1477:                           # %for transpose.s0.v12.preheader
	movl	%r15d, 5760(%rsp)       # 4-byte Spill
	vmovss	72(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	movq	368(%rsp), %rdx         # 8-byte Reload
	movq	384(%rsp), %rcx         # 8-byte Reload
	imull	%edx, %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	imull	4616(%rsp), %ebx        # 4-byte Folded Reload
	movq	%rbx, 5728(%rsp)        # 8-byte Spill
	movq	%r13, %r10
	movslq	%r10d, %rcx
	movq	%rcx, 4544(%rsp)        # 8-byte Spill
	leaq	-1(%rcx), %rdx
	movq	%rdx, 4512(%rsp)        # 8-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm0, %xmm1
	vmovd	%xmm0, %ecx
	movl	$1065353216, %edi       # imm = 0x3F800000
	cmovbl	%ecx, %edi
	movl	$0, %r9d
	testq	%rdx, %rdx
	cmovnsq	%rdx, %r9
	vucomiss	%xmm0, %xmm1
	seta	%cl
	movzbl	%cl, %ecx
	jae	.LBB147_1478
# BB#1479:                              # %for transpose.s0.v12.preheader
	movq	%rax, 5472(%rsp)        # 8-byte Spill
	movl	%edi, %ecx
	andl	$-2139095041, %ecx      # imm = 0xFFFFFFFF807FFFFF
	movl	%ecx, %edx
	sarl	$22, %edx
	movl	$127, %esi
	subl	%edx, %esi
	sarl	$23, %edi
	subl	%esi, %edi
	shll	$23, %esi
	orl	%ecx, %esi
	vmovd	%esi, %xmm0
	vaddss	.LCPI147_28(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm0, %xmm1
	vcvtsi2ssl	%edi, %xmm0, %xmm2
	vmovss	.LCPI147_29(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_30(%rip), %xmm1, %xmm3
	vfmadd213ss	.LCPI147_31(%rip), %xmm1, %xmm3
	vfmadd213ss	.LCPI147_32(%rip), %xmm1, %xmm3
	vmovss	.LCPI147_33(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_34(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_35(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_36(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_17(%rip), %xmm1, %xmm4
	vmulss	%xmm4, %xmm0, %xmm0
	vfmadd213ss	%xmm0, %xmm1, %xmm3
	vmovss	.LCPI147_37(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm3, %xmm2, %xmm0
	jmp	.LBB147_1480
.LBB147_1478:
	movq	%rax, 5472(%rsp)        # 8-byte Spill
	leaq	.LCPI147_38(%rip), %rax
	vmovss	(%rax,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
.LBB147_1480:                           # %for transpose.s0.v12.preheader
	movq	872(%rsp), %rax         # 8-byte Reload
	sarl	$3, %eax
	movq	%rax, 872(%rsp)         # 8-byte Spill
	movl	%r14d, %r15d
	sarl	$31, %r15d
	andl	%r14d, %r15d
	movq	752(%rsp), %r12         # 8-byte Reload
	movq	424(%rsp), %rcx         # 8-byte Reload
	cmpl	%r12d, %ecx
	cmovll	%r12d, %ecx
	addl	$-1, %ecx
	leal	-2(%r12), %eax
	movl	%eax, 5024(%rsp)        # 4-byte Spill
	movl	404(%rsp), %edx         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	testl	%edx, %edx
	movl	$0, %eax
	cmovnsl	%edx, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	$1, %ebx
	subl	%r15d, %ebx
	addl	%eax, %ebx
	movslq	468(%rsp), %rcx         # 4-byte Folded Reload
	movq	%rcx, %rax
	sarq	$63, %rax
	andq	%rcx, %rax
	movslq	356(%rsp), %rcx         # 4-byte Folded Reload
	movq	%rcx, %rdi
	sarq	$63, %rdi
	andq	%rcx, %rdi
	movq	%rdi, 4656(%rsp)        # 8-byte Spill
	leaq	-7(%r9), %rcx
	movq	%rcx, %rdx
	sarq	$63, %rdx
	andq	%rcx, %rdx
	movq	%rdx, 4480(%rsp)        # 8-byte Spill
	movq	360(%rsp), %rcx         # 8-byte Reload
	movslq	%ecx, %rcx
	movslq	%r14d, %rsi
	movq	%rsi, 4664(%rsp)        # 8-byte Spill
	shlq	$5, %rbx
	movq	%rbx, 4944(%rsp)        # 8-byte Spill
	addq	$1, %rcx
	movq	%rax, %rdx
	imulq	%rcx, %rdx
	addq	%rdi, %rdx
	movq	%rdx, 4648(%rsp)        # 8-byte Spill
	movq	%rsi, %rdx
	sarq	$63, %rdx
	andq	%rsi, %rdx
	leaq	(,%rdx,8), %rsi
	negq	%rsi
	movq	%rsi, 4936(%rsp)        # 8-byte Spill
	movq	472(%rsp), %r13         # 8-byte Reload
	leal	7(%r13), %esi
	sarl	$3, %esi
	movl	%esi, 5488(%rsp)        # 4-byte Spill
	movl	%r12d, %edi
	subl	%r15d, %edi
	movl	$7, %esi
	subl	%r10d, %esi
	cmpl	$1, %r10d
	movl	$6, %ebx
	cmovgl	%esi, %ebx
	movl	%ebx, 4920(%rsp)        # 4-byte Spill
	movl	$1, %esi
	subq	%rax, %rsi
	imulq	%rcx, %rsi
	movq	%rsi, 4720(%rsp)        # 8-byte Spill
	movq	%r9, 5696(%rsp)         # 8-byte Spill
	movslq	%r12d, %r14
	movq	%r14, %rax
	subq	%rdx, %rax
	movl	$1, %ecx
	subq	%rdx, %rcx
	andl	$1, 972(%rsp)           # 4-byte Folded Spill
	shlq	$3, %rcx
	movq	%rcx, 4704(%rsp)        # 8-byte Spill
	addl	$-1, %edi
	movslq	%edi, %rcx
	shlq	$3, %rcx
	movq	%rcx, 4696(%rsp)        # 8-byte Spill
	shlq	$3, %rax
	movq	%rax, 4712(%rsp)        # 8-byte Spill
	testl	%r10d, %r10d
	movq	480(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %esi
	notl	%esi
	movl	%esi, 4928(%rsp)        # 4-byte Spill
	movl	$1, %ecx
	cmovgl	%r10d, %ecx
	movl	$7, %eax
	subl	%ecx, %eax
	movl	%eax, 4912(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	notl	%eax
	cltq
	notq	%rax
	cmpq	$-2, %rax
	movq	$-1, %rsi
	cmovleq	%rsi, %rax
	addq	$1, %rax
	movq	%rax, 4800(%rsp)        # 8-byte Spill
	movq	880(%rsp), %rsi         # 8-byte Reload
	shll	$5, %esi
	movq	%rsi, 880(%rsp)         # 8-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	leal	-1(%rdx,%rax), %eax
	movq	5352(%rsp), %rdx        # 8-byte Reload
	leal	31(%rdx,%rsi), %edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	392(%rsp), %eax         # 4-byte Reload
	notl	%eax
	movl	%eax, 392(%rsp)         # 4-byte Spill
	negl	%ecx
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leal	1(%rcx), %eax
	movl	380(%rsp), %r11d        # 4-byte Reload
	subl	%r11d, %eax
	testl	%r12d, %r12d
	movl	$1, %edx
	cmovgl	%r12d, %edx
	addl	$-1, %edx
	movq	1016(%rsp), %r8         # 8-byte Reload
	leal	(%r8,%r13), %edi
	cmpl	%edi, %r12d
	cmovgel	%r12d, %edi
	testl	%edi, %edi
	movl	$1, %ebx
	cmovlel	%ebx, %edi
	movl	$-32, %esi
	movq	672(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %esi
	movl	648(%rsp), %ebx         # 4-byte Reload
	shll	$5, %ebx
	subl	%ebx, %esi
	negl	%edi
	cmpl	%esi, %edi
	cmovgel	%edi, %esi
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	%r8d, %r12d
	negl	%r12d
	subl	%r13d, %r12d
	movq	448(%rsp), %rdx         # 8-byte Reload
	leal	(%r8,%rdx), %edi
	notl	%edi
	cmpl	%edi, %r12d
	cmovgel	%r12d, %edi
	notl	%edi
	cmpl	%edi, %esi
	cmovgel	%esi, %edi
	addl	$1, %edi
	subl	736(%rsp), %edi         # 4-byte Folded Reload
	imull	%eax, %edi
	subl	%r11d, %ecx
	movl	%r8d, %esi
	notl	%esi
	movl	%esi, 4904(%rsp)        # 4-byte Spill
	movl	$31, %eax
	subl	%r8d, %eax
	subl	%r13d, %eax
	cmpl	%esi, %eax
	movl	376(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmovll	%esi, %eax
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	notl	%eax
	movslq	%eax, %rbx
	notq	%rbx
	cmpq	$-2, %rbx
	movq	$-1, %r9
	cmovleq	%r9, %rbx
	movl	$7, %eax
	subl	%r8d, %eax
	subl	%r13d, %eax
	movl	%eax, 5072(%rsp)        # 4-byte Spill
	cmpl	%esi, %eax
	movl	%esi, %edx
	cmovgel	%eax, %edx
	movl	%edx, %eax
	notl	%eax
	movslq	%eax, %r11
	movq	%r11, %rax
	notq	%rax
	cmpq	$-2, %rax
	cmovleq	%r9, %rax
	movq	$-1, %r9
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	leaq	1(%rbx), %rsi
	movq	%rsi, 4760(%rsp)        # 8-byte Spill
	addq	$2, %rbx
	movq	%rbx, 4768(%rsp)        # 8-byte Spill
	negl	%r15d
	movl	%r15d, 4784(%rsp)       # 4-byte Spill
	leaq	(%r14,%rax), %rax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	shlq	$5, %rax
	movq	%rax, 4744(%rsp)        # 8-byte Spill
	movq	%r11, %rax
	negq	%rax
	movq	%rax, 4976(%rsp)        # 8-byte Spill
	cmpl	$7, %r10d
	movl	$8, %esi
	cmovgl	%r10d, %esi
	addl	$1, %edx
	movq	456(%rsp), %rax         # 8-byte Reload
	leal	(%r8,%rax), %eax
	notl	%eax
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	subl	%r12d, %edx
	imull	%esi, %edx
	movl	$6, %ebx
	subq	5696(%rsp), %rbx        # 8-byte Folded Reload
	notl	%r12d
	movslq	%r12d, %rsi
	addq	$1, %rsi
	subq	%r11, %rsi
	movq	%rsi, 4888(%rsp)        # 8-byte Spill
	cmpq	$-2, %rbx
	cmovleq	%r9, %rbx
	movl	%r13d, %esi
	negl	%esi
	subl	%r8d, %esi
	cmpl	%eax, %esi
	cmovll	%eax, %esi
	notl	%esi
	movslq	%esi, %rsi
	addq	$1, %rsi
	subq	%r11, %rsi
	movq	384(%rsp), %rax         # 8-byte Reload
	cltq
	movq	%rax, 4672(%rsp)        # 8-byte Spill
	movq	5728(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	movslq	5760(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 4384(%rsp)        # 8-byte Spill
	vmulss	.LCPI147_19(%rip), %xmm0, %xmm1
	vmovss	%xmm1, 4448(%rsp)       # 4-byte Spill
	vmulss	.LCPI147_39(%rip), %xmm1, %xmm0
	vmovss	%xmm0, 4416(%rsp)       # 4-byte Spill
	movslq	%edi, %rax
	movq	%rax, 4640(%rsp)        # 8-byte Spill
	movslq	%ecx, %rax
	leaq	4(,%rax,4), %r13
	movq	%r13, 4880(%rsp)        # 8-byte Spill
	leaq	1(%rax), %rax
	movq	%rax, 4736(%rsp)        # 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, 4632(%rsp)        # 8-byte Spill
	shlq	$3, %rsi
	movq	%rsi, 5056(%rsp)        # 8-byte Spill
	leaq	2(%rbx), %rax
	movq	%rax, 4872(%rsp)        # 8-byte Spill
	addq	$1, %rbx
	movq	%rbx, 4896(%rsp)        # 8-byte Spill
	vroundss	$1, %xmm0, %xmm0, %xmm2
	vmovss	%xmm2, 4848(%rsp)       # 4-byte Spill
	vmovss	.LCPI147_40(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm2, %xmm0
	vmovss	.LCPI147_41(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm0, %xmm2, %xmm1
	vmulss	%xmm1, %xmm1, %xmm0
	vmovss	.LCPI147_42(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_43(%rip), %xmm0, %xmm2
	vfmadd213ss	.LCPI147_44(%rip), %xmm0, %xmm2
	vmovss	.LCPI147_17(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI147_45(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_46(%rip), %xmm0, %xmm4
	vfmadd213ss	%xmm3, %xmm0, %xmm2
	vfmadd213ss	.LCPI147_47(%rip), %xmm0, %xmm4
	vfmadd213ss	%xmm3, %xmm0, %xmm4
	vfmadd213ss	%xmm2, %xmm1, %xmm4
	vmovss	%xmm4, 4688(%rsp)       # 4-byte Spill
	movq	4944(%rsp), %rax        # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 4832(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1481:                           # %for transpose.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1483 Depth 2
                                        #       Child Loop BB147_1496 Depth 3
                                        #       Child Loop BB147_1501 Depth 3
                                        #       Child Loop BB147_1504 Depth 3
                                        #         Child Loop BB147_1505 Depth 4
	movq	872(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_1508
# BB#1482:                              # %for transpose.s0.v11.v132.preheader
                                        #   in Loop: Header=BB147_1481 Depth=1
	movq	4680(%rsp), %rax        # 8-byte Reload
	imulq	4672(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, %rcx
	subq	4648(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 4960(%rsp)        # 8-byte Spill
	subq	4656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 4728(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
	movl	$-1, %edx
	.align	16, 0x90
.LBB147_1483:                           # %for transpose.s0.v11.v132
                                        #   Parent Loop BB147_1481 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1496 Depth 3
                                        #       Child Loop BB147_1501 Depth 3
                                        #       Child Loop BB147_1504 Depth 3
                                        #         Child Loop BB147_1505 Depth 4
	movq	%rcx, 4992(%rsp)        # 8-byte Spill
	movl	%edx, 5008(%rsp)        # 4-byte Spill
	movl	4912(%rsp), %r14d       # 4-byte Reload
	cmpl	%r14d, %edx
	movl	%r14d, %r15d
	cmovgel	%edx, %r15d
	cmpl	%r14d, %edx
	cmovgel	%edx, %r14d
	movl	4920(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %edx
	movl	%eax, %ebx
	cmovgel	%edx, %ebx
	leal	(,%rcx,8), %r12d
	movl	704(%rsp), %eax         # 4-byte Reload
	cmpl	%r12d, %eax
	cmovlel	%eax, %r12d
	testq	$-2147483648, 4944(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_1484
# BB#1485:                              # %assert succeeded552
                                        #   in Loop: Header=BB147_1483 Depth=2
	xorl	%edi, %edi
	movq	4832(%rsp), %rsi        # 8-byte Reload
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB147_1565
# BB#1486:                              # %for blur.s1.v10.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	notl	%ebx
	movslq	%ebx, %rcx
	movslq	%r12d, %r8
	movq	4936(%rsp), %rdx        # 8-byte Reload
	subq	%r8, %rdx
	movq	4960(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rcx), %rsi
	leaq	(%rdx,%rcx), %rdi
	movq	1416(%rsp), %r10        # 8-byte Reload
	movzwl	(%r10,%rsi,2), %esi
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	1(%rbx,%rcx), %rsi
	movzwl	(%r10,%rsi,2), %esi
	vcvtsi2ssl	%esi, %xmm0, %xmm1
	vmovss	%xmm0, (%rax,%rdi,4)
	leaq	1(%rdx,%rcx), %rsi
	vmovss	%xmm1, (%rax,%rsi,4)
	leaq	2(%rbx,%rcx), %rsi
	movzwl	(%r10,%rsi,2), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	2(%rdx,%rcx), %rsi
	leaq	3(%rbx,%rcx), %rdi
	movzwl	(%r10,%rdi,2), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%rax,%rsi,4)
	leaq	3(%rdx,%rcx), %rsi
	vmovss	%xmm1, (%rax,%rsi,4)
	leaq	4(%rbx,%rcx), %rsi
	movzwl	(%r10,%rsi,2), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	4(%rdx,%rcx), %rsi
	leaq	5(%rbx,%rcx), %rdi
	movzwl	(%r10,%rdi,2), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%rax,%rsi,4)
	leaq	5(%rdx,%rcx), %rsi
	vmovss	%xmm1, (%rax,%rsi,4)
	leaq	6(%rbx,%rcx), %rsi
	movzwl	(%r10,%rsi,2), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	6(%rdx,%rcx), %rsi
	leaq	7(%rbx,%rcx), %rdi
	movzwl	(%r10,%rdi,2), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%rax,%rsi,4)
	leaq	7(%rdx,%rcx), %rcx
	vmovss	%xmm1, (%rax,%rcx,4)
	movq	752(%rsp), %r9          # 8-byte Reload
	cmpl	$1, %r9d
	movq	%rax, %r12
	jle	.LBB147_1502
# BB#1487:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	vcvttss2si	4848(%rsp), %eax # 4-byte Folded Reload
	leal	127(%rax), %edx
	cmpl	$255, %edx
	jl	.LBB147_1488
# BB#1489:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	vmovss	.LCPI147_48(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB147_1490
	.align	16, 0x90
.LBB147_1488:                           #   in Loop: Header=BB147_1483 Depth=2
	shll	$23, %edx
	vmovd	%edx, %xmm0
	vmulss	4688(%rsp), %xmm0, %xmm1 # 4-byte Folded Reload
.LBB147_1490:                           # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	cmpl	$-127, %eax
	jg	.LBB147_1492
# BB#1491:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1492:                           # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	vmovss	.LCPI147_17(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %esi
	cmpl	$0, 972(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1494
# BB#1493:                              # %for blur.s2.r76$x.prol
                                        #   in Loop: Header=BB147_1483 Depth=2
	movq	4728(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %rax
	addq	4720(%rsp), %rax        # 8-byte Folded Reload
	vpmovzxwd	(%r10,%rax,2), %ymm2 # ymm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	movq	4704(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r12,%rax,4), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%r12,%rax,4)
	movl	$2, %esi
.LBB147_1494:                           # %for blur.s2.r76$x.preheader.split
                                        #   in Loop: Header=BB147_1483 Depth=2
	cmpl	$0, 5024(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1497
# BB#1495:                              # %for blur.s2.r76$x.preheader.split.split
                                        #   in Loop: Header=BB147_1483 Depth=2
	notl	%r14d
	movslq	%r14d, %r8
	movl	%r9d, %eax
	subl	%esi, %eax
	movq	4760(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	4736(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rcx
	movq	4800(%rsp), %rdi        # 8-byte Reload
	leaq	(%rcx,%rdi), %rcx
	addq	%r8, %rcx
	movq	5096(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	shlq	$5, %rdx
	leaq	(%rdx,%r12), %rdx
	movq	4768(%rsp), %rbx        # 8-byte Reload
	leaq	(%rsi,%rbx), %rsi
	imulq	%r11, %rsi
	leaq	(%rsi,%rdi), %rsi
	addq	%r8, %rsi
	movq	%r10, %rdi
	.align	16, 0x90
.LBB147_1496:                           # %for blur.s2.r76$x
                                        #   Parent Loop BB147_1481 Depth=1
                                        #     Parent Loop BB147_1483 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vpmovzxwd	(%rdi,%rcx,2), %ymm2 # ymm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vmovaps	(%rdx), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rdx)
	vpmovzxwd	(%rdi,%rsi,2), %ymm2 # ymm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rdx)
	addq	$64, %rdx
	addq	%r13, %rdi
	addl	$-2, %eax
	jne	.LBB147_1496
.LBB147_1497:                           # %for blur.s3.r76$x.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	movl	$1, %r8d
	cmpl	$0, 972(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1499
# BB#1498:                              # %for blur.s3.r76$x.prol
                                        #   in Loop: Header=BB147_1483 Depth=2
	movq	4696(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r12,%rax,4), %ymm2
	vmulps	(%r12,%rax,4), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	movq	4712(%rsp), %rax        # 8-byte Reload
	vmovaps	%ymm2, -64(%r12,%rax,4)
	movl	$2, %r8d
.LBB147_1499:                           # %for blur.s3.r76$x.preheader.split
                                        #   in Loop: Header=BB147_1483 Depth=2
	cmpl	$0, 5024(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1502
# BB#1500:                              # %for blur.s3.r76$x.preheader.split.split
                                        #   in Loop: Header=BB147_1483 Depth=2
	movl	4784(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	subl	%r8d, %edx
	movq	4744(%rsp), %rax        # 8-byte Reload
	leaq	(%r12,%rax), %rax
	movq	4752(%rsp), %rsi        # 8-byte Reload
	subq	%r8, %rsi
	movq	%r8, %rdi
	shlq	$5, %rdi
	subq	%rdi, %rax
	shlq	$5, %rsi
	leaq	-32(%r12,%rsi), %rsi
	leal	1(%r8), %ebx
	movl	%ecx, %edi
	subl	%ebx, %edi
	movl	%r9d, %ebx
	.align	16, 0x90
.LBB147_1501:                           # %for blur.s3.r76$x
                                        #   Parent Loop BB147_1481 Depth=1
                                        #     Parent Loop BB147_1483 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leal	(%rdx,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r12), %ymm2
	vmulps	(%rcx,%r12), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rax)
	leal	(%rdi,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r12), %ymm2
	vmulps	(%rcx,%r12), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	addl	$-2, %ebx
	addq	$-64, %rax
	addq	$-64, %rsi
	cmpl	%ebx, %r8d
	jne	.LBB147_1501
.LBB147_1502:                           # %consume blur
                                        #   in Loop: Header=BB147_1483 Depth=2
	movq	%r12, 5048(%rsp)        # 8-byte Spill
	cmpl	$0, 5488(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1507
# BB#1503:                              # %for transpose.s0.v10.v130.preheader
                                        #   in Loop: Header=BB147_1483 Depth=2
	notl	%r15d
	movslq	%r15d, %rax
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	4896(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	4888(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rcx
	imulq	%rdx, %rax
	movq	4976(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	%rcx, 5456(%rsp)        # 8-byte Spill
	movq	5048(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	subq	$-128, %rsi
	movq	%rsi, 5440(%rsp)        # 8-byte Spill
	leaq	260(%rcx), %rsi
	movq	%rsi, 5424(%rsp)        # 8-byte Spill
	leaq	228(%rcx), %rsi
	movq	%rsi, 5408(%rsp)        # 8-byte Spill
	leaq	196(%rcx), %rsi
	movq	%rsi, 5392(%rsp)        # 8-byte Spill
	leaq	164(%rcx), %rsi
	movq	%rsi, 5376(%rsp)        # 8-byte Spill
	leaq	132(%rcx), %rsi
	movq	%rsi, 5360(%rsp)        # 8-byte Spill
	leaq	100(%rcx), %rsi
	movq	%rsi, 5312(%rsp)        # 8-byte Spill
	leaq	68(%rcx), %rsi
	movq	%rsi, 5280(%rsp)        # 8-byte Spill
	leaq	36(%rcx), %rsi
	movq	%rsi, 5248(%rsp)        # 8-byte Spill
	leaq	256(%rcx), %rsi
	movq	%rsi, 5216(%rsp)        # 8-byte Spill
	leaq	224(%rcx), %rsi
	movq	%rsi, 5200(%rsp)        # 8-byte Spill
	leaq	192(%rcx), %rsi
	movq	%rsi, 5184(%rsp)        # 8-byte Spill
	leaq	(%rax,%rdx), %rax
	movq	%rax, 5168(%rsp)        # 8-byte Spill
	leaq	160(%rcx), %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, 5136(%rsp)        # 8-byte Spill
	leaq	64(%rcx), %rax
	movq	%rax, 5120(%rsp)        # 8-byte Spill
	leaq	32(%rcx), %rax
	movq	%rax, 5104(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	4904(%rsp), %ecx        # 4-byte Reload
	.align	16, 0x90
.LBB147_1504:                           # %for transpose.s0.v10.v130
                                        #   Parent Loop BB147_1481 Depth=1
                                        #     Parent Loop BB147_1483 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1505 Depth 4
	movl	%ecx, 5504(%rsp)        # 4-byte Spill
	movl	%eax, 5528(%rsp)        # 4-byte Spill
	movl	5072(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	movslq	%eax, %rcx
	movq	5096(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	shlq	$5, %r8
	movq	5456(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rcx), %rax
	movq	%rax, 5760(%rsp)        # 8-byte Spill
	movq	5168(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rax
	movq	%rax, 5728(%rsp)        # 8-byte Spill
	movq	5424(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5696(%rsp)        # 8-byte Spill
	movq	5408(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5680(%rsp)        # 8-byte Spill
	movq	5392(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	movq	5376(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5632(%rsp)        # 8-byte Spill
	movq	5360(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5624(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5568(%rsp)        # 8-byte Spill
	movq	5280(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5536(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r14
	movq	5216(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r15
	movq	5200(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r11
	movq	5184(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	5152(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r8), %rsi
	movq	5440(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r8), %rdx
	movq	5136(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r8), %rdi
	movq	5120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r8), %r12
	movq	5104(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %r13
	movq	5472(%rsp), %r8         # 8-byte Reload
	xorl	%r9d, %r9d
	movq	5056(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1505:                           # %for transpose.s0.v11.v11
                                        #   Parent Loop BB147_1481 Depth=1
                                        #     Parent Loop BB147_1483 Depth=2
                                        #       Parent Loop BB147_1504 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	(%rsi,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r11,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r13,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5728(%rsp), %rbx        # 8-byte Reload
	vmovups	%ymm0, (%r8,%rbx,4)
	movq	5632(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	movq	5672(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	5680(%rsp), %rbx        # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	5696(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r14,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	movq	5536(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	5568(%rsp), %rbx        # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	5624(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5760(%rsp), %r10        # 8-byte Reload
	vmovups	%ymm0, (%r8,%r10,4)
	addq	$8, %r9
	addq	%rax, %r8
	cmpq	$32, %r9
	jne	.LBB147_1505
# BB#1506:                              # %end for transpose.s0.v11.v11
                                        #   in Loop: Header=BB147_1504 Depth=3
	movl	5528(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5504(%rsp), %ecx        # 4-byte Reload
	addl	$-8, %ecx
	cmpl	5488(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1504
.LBB147_1507:                           # %call_destructor.exit774
                                        #   in Loop: Header=BB147_1483 Depth=2
	xorl	%edi, %edi
	movq	5048(%rsp), %rsi        # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	4992(%rsp), %rsi        # 8-byte Reload
	leal	1(%rsi), %eax
	movl	5008(%rsp), %edx        # 4-byte Reload
	addl	$-8, %edx
	movq	872(%rsp), %rcx         # 8-byte Reload
	cmpl	%ecx, %esi
	movl	%eax, %ecx
	movq	4880(%rsp), %r13        # 8-byte Reload
	jne	.LBB147_1483
.LBB147_1508:                           # %end for transpose.s0.v11.v132
                                        #   in Loop: Header=BB147_1481 Depth=1
	movq	4680(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	movq	4640(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4800(%rsp)        # 8-byte Folded Spill
	movq	4632(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4976(%rsp)        # 8-byte Folded Spill
	cmpl	$3, %eax
	jne	.LBB147_1481
# BB#1509:                              # %consume transpose
	movq	1016(%rsp), %rax        # 8-byte Reload
	movl	396(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movq	432(%rsp), %rbx         # 8-byte Reload
	movl	4608(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	movl	420(%rsp), %eax         # 4-byte Reload
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	-8(%rax), %r15d
	movq	480(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %r15d
	cmovgl	%eax, %r15d
	subl	%ecx, %ebx
	addl	$1, %ebx
	movq	1200(%rsp), %rax        # 8-byte Reload
	cmpl	$7, %eax
	movl	$8, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 4576(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	leaq	(,%rax,4), %rdx
	movl	%edx, %ecx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdx,2), %r14
	movl	%edx, %edx
	cmpq	$2147483647, %r14       # imm = 0x7FFFFFFF
	ja	.LBB147_1511
# BB#1510:                              # %consume transpose
	imulq	%rbx, %rcx
	shrq	$30, %rax
	imulq	%rbx, %rax
	shrq	$32, %rcx
	addq	%rcx, %rax
	leaq	(%rdx,%rdx,2), %rcx
	shrq	$32, %rcx
	leaq	(%rax,%rax,2), %rdx
	addq	%rcx, %rdx
	orq	%rax, %rdx
	shrq	$32, %rdx
	jne	.LBB147_1511
# BB#1514:                              # %assert succeeded556
	leaq	4(%r14), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	addq	$4, %r14
	je	.LBB147_1516
# BB#1515:                              # %assert succeeded556
	testq	%rax, %rax
	je	.LBB147_1565
.LBB147_1516:                           # %for transpose$1.s0.v12.preheader
	movq	%rax, 5536(%rsp)        # 8-byte Spill
	imull	4576(%rsp), %ebx        # 4-byte Folded Reload
	movq	%rbx, 432(%rsp)         # 8-byte Spill
	movl	%r15d, %r12d
	sarl	$31, %r12d
	andl	%r15d, %r12d
	movq	760(%rsp), %r8          # 8-byte Reload
	movq	440(%rsp), %rax         # 8-byte Reload
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	addl	$-1, %eax
	leal	-2(%r8), %edx
	movl	%edx, 5008(%rsp)        # 4-byte Spill
	movl	400(%rsp), %ecx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	testl	%ecx, %ecx
	movl	$0, %edx
	cmovsl	%edx, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	$1, %edx
	subl	%r12d, %edx
	addl	%ecx, %edx
	movslq	%r15d, %rax
	movq	%rax, %r15
	sarq	$63, %r15
	andq	%rax, %r15
	shlq	$5, %rdx
	movq	%rdx, 4920(%rsp)        # 8-byte Spill
	movq	4384(%rsp), %r13        # 8-byte Reload
	subq	4664(%rsp), %r13        # 8-byte Folded Reload
	addq	$1, %r13
	movq	1200(%rsp), %rcx        # 8-byte Reload
	leal	7(%rcx), %eax
	movq	%rcx, %rdx
	sarl	$3, %eax
	movl	%eax, 5456(%rsp)        # 4-byte Spill
	movl	%r8d, %edi
	subl	%r12d, %edi
	movq	1016(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r11d
	notl	%r11d
	movl	%r11d, 4960(%rsp)       # 4-byte Spill
	movl	$7, %r9d
	subl	%eax, %r9d
	movq	%rax, %rbx
	movq	472(%rsp), %rax         # 8-byte Reload
	subl	%eax, %r9d
	movq	%rax, %rsi
	movl	%r9d, 4912(%rsp)        # 4-byte Spill
	movl	$7, %ecx
	movq	480(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ecx
	subl	%edx, %ecx
	movl	%ecx, 5056(%rsp)        # 4-byte Spill
	movl	4928(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	movslq	%eax, %r10
	movq	%r10, %rax
	sarq	$63, %rax
	andq	%r10, %rax
	shlq	$5, %rax
	xorq	$-32, %rax
	addq	$32, %rax
	movq	%rax, 4904(%rsp)        # 8-byte Spill
	movl	%ebx, %ecx
	negl	%ecx
	subl	%esi, %ecx
	movl	4608(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	cmpl	%r11d, %r9d
	movl	%r11d, %esi
	cmovgel	%r9d, %esi
	leal	1(%rsi), %r9d
	movl	%r9d, %eax
	subl	%edx, %eax
	movl	4616(%rsp), %ebx        # 4-byte Reload
	imull	%eax, %ebx
	movl	%ebx, 4616(%rsp)        # 4-byte Spill
	notl	%edx
	movslq	%edx, %rdx
	addq	$1, %rdx
	notl	%esi
	movslq	%esi, %rsi
	subq	%rsi, %rdx
	movq	4480(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rdx
	movl	$1, %r14d
	subq	%rax, %r14
	imulq	%r13, %r14
	movq	%r14, 4712(%rsp)        # 8-byte Spill
	movq	4544(%rsp), %r13        # 8-byte Reload
	movq	%r13, %r14
	subq	%r15, %r14
	movl	$1, %eax
	subq	%r15, %rax
	negq	%rdx
	subq	%rsi, %rdx
	movq	%rdx, 4656(%rsp)        # 8-byte Spill
	andl	$1, 968(%rsp)           # 4-byte Folded Spill
	shlq	$3, %rax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	addl	$-1, %edi
	movslq	%edi, %rax
	shlq	$3, %rax
	movq	%rax, 4688(%rsp)        # 8-byte Spill
	shlq	$3, %r14
	movq	%r14, 4704(%rsp)        # 8-byte Spill
	movq	%rsi, %rax
	negq	%rax
	movq	%rax, 4800(%rsp)        # 8-byte Spill
	cmpl	$7, %r8d
	movl	$8, %eax
	cmovgl	%r8d, %eax
	movq	456(%rsp), %rdx         # 8-byte Reload
	movq	1016(%rsp), %r8         # 8-byte Reload
	leal	(%rdx,%r8), %r14d
	movl	%r14d, %ebx
	notl	%ebx
	cmpl	%ebx, %ecx
	movl	%ebx, %edi
	cmovgel	%ecx, %edi
	subl	%edi, %r9d
	imull	%eax, %r9d
	notl	%edi
	movslq	%edi, %rax
	addq	$1, %rax
	subq	%rsi, %rax
	movq	%rax, 4768(%rsp)        # 8-byte Spill
	movq	4512(%rsp), %rax        # 8-byte Reload
	testq	%rax, %rax
	movl	$0, %edx
	cmovsq	%rdx, %rax
	xorl	%edi, %edi
	movq	%rdi, 4680(%rsp)        # 8-byte Spill
	movl	$6, %edx
	subq	%rax, %rdx
	cmpq	$-2, %rdx
	movq	$-1, %rax
	cmovleq	%rax, %rdx
	movq	472(%rsp), %r15         # 8-byte Reload
	movl	%r15d, %edi
	negl	%edi
	subl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	notl	%edi
	movslq	%edi, %rdi
	addq	$1, %rdi
	subq	%rsi, %rdi
	shlq	$3, %rdi
	movq	%rdi, 4896(%rsp)        # 8-byte Spill
	movq	%r10, %rsi
	notq	%rsi
	cmpq	$-2, %rsi
	cmovleq	%rax, %rsi
	movq	%rsi, 5072(%rsp)        # 8-byte Spill
	leaq	1(%rdx), %rax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	addq	$2, %rdx
	movq	%rdx, 4760(%rsp)        # 8-byte Spill
	negl	%r12d
	movl	%r12d, 4784(%rsp)       # 4-byte Spill
	leaq	(%r13,%rsi), %rax
	movq	%rax, 4744(%rsp)        # 8-byte Spill
	shlq	$5, %rax
	movq	%rax, 4736(%rsp)        # 8-byte Spill
	negq	%r10
	movq	%r10, 4944(%rsp)        # 8-byte Spill
	movq	1200(%rsp), %rdx        # 8-byte Reload
	cmpl	$7, %edx
	movl	$8, %eax
	cmovgl	%edx, %eax
	movl	$31, %esi
	movq	%r8, %rdx
	subl	%edx, %esi
	subl	%r15d, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	movq	448(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rdx), %edi
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	notl	%r14d
	cmpl	%r14d, %ecx
	cmovgel	%ecx, %r14d
	negl	%r14d
	leal	1(%rsi,%r14), %ecx
	imull	%eax, %ecx
	movq	344(%rsp), %rax         # 8-byte Reload
	cmpq	$7, %rax
	movl	$8, %edx
	cmovgq	%rax, %rdx
	movq	%rdx, 4888(%rsp)        # 8-byte Spill
	movq	432(%rsp), %rax         # 8-byte Reload
	cltq
	movq	%rax, 4640(%rsp)        # 8-byte Spill
	movslq	4616(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 4648(%rsp)        # 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, 4632(%rsp)        # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 4616(%rsp)        # 8-byte Spill
	notl	%esi
	movslq	%esi, %rax
	movl	$1, %ecx
	subq	%rax, %rcx
	movq	%rcx, 4872(%rsp)        # 8-byte Spill
	negq	%rax
	movq	%rax, 4880(%rsp)        # 8-byte Spill
	vroundss	$1, 4416(%rsp), %xmm0, %xmm2 # 4-byte Folded Reload
	vmovss	%xmm2, 4848(%rsp)       # 4-byte Spill
	vmovss	.LCPI147_40(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, %xmm1
	vfmadd213ss	4448(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vmovss	.LCPI147_41(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, %xmm3
	vfmadd213ss	%xmm1, %xmm2, %xmm3
	vmulss	%xmm3, %xmm3, %xmm0
	vmovss	.LCPI147_42(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovaps	%xmm1, %xmm4
	vfmadd213ss	.LCPI147_43(%rip), %xmm0, %xmm4
	vfmadd213ss	.LCPI147_44(%rip), %xmm0, %xmm4
	vmovss	.LCPI147_45(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovaps	%xmm1, %xmm2
	vfmadd213ss	.LCPI147_46(%rip), %xmm0, %xmm2
	vmovss	.LCPI147_17(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm0, %xmm4
	vfmadd213ss	.LCPI147_47(%rip), %xmm0, %xmm2
	vfmadd213ss	%xmm1, %xmm0, %xmm2
	vfmadd213ss	%xmm4, %xmm3, %xmm2
	vmovss	%xmm2, 4720(%rsp)       # 4-byte Spill
	movq	4920(%rsp), %rax        # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 4832(%rsp)        # 8-byte Spill
	leaq	(,%rdx,8), %rax
	movq	%rax, 5048(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1517:                           # %for transpose$1.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1519 Depth 2
                                        #       Child Loop BB147_1541 Depth 3
                                        #       Child Loop BB147_1546 Depth 3
                                        #       Child Loop BB147_1549 Depth 3
                                        #         Child Loop BB147_1550 Depth 4
	cmpl	$0, 5488(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1553
# BB#1518:                              # %for transpose$1.s0.v11.v167.preheader
                                        #   in Loop: Header=BB147_1517 Depth=1
	movq	4648(%rsp), %rax        # 8-byte Reload
	movq	4680(%rsp), %rcx        # 8-byte Reload
	imulq	%rcx, %rax
	movq	4656(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	%rax, 4936(%rsp)        # 8-byte Spill
	imulq	4624(%rsp), %rcx        # 8-byte Folded Reload
	subq	4664(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 4728(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	movl	4960(%rsp), %esi        # 4-byte Reload
	.align	16, 0x90
.LBB147_1519:                           # %for transpose$1.s0.v11.v167
                                        #   Parent Loop BB147_1517 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1541 Depth 3
                                        #       Child Loop BB147_1546 Depth 3
                                        #       Child Loop BB147_1549 Depth 3
                                        #         Child Loop BB147_1550 Depth 4
	movl	4912(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %esi
	movl	%ecx, %r15d
	cmovgel	%esi, %r15d
	cmpl	%ecx, %esi
	movl	%ecx, %r12d
	cmovgel	%esi, %r12d
	leal	(,%rdx,8), %eax
	movl	4960(%rsp), %ebx        # 4-byte Reload
	subl	%eax, %ebx
	cmpl	%ecx, %ebx
	cmovll	%ecx, %ebx
	movq	1016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx,8), %r14d
	movl	4816(%rsp), %eax        # 4-byte Reload
	cmpl	%r14d, %eax
	cmovlel	%eax, %r14d
	testq	$-2147483648, 4920(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_1520
# BB#1530:                              # %assert succeeded560
                                        #   in Loop: Header=BB147_1519 Depth=2
	movl	%esi, 4976(%rsp)        # 4-byte Spill
	movq	%rdx, 4992(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	movq	4832(%rsp), %rsi        # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, %r13
	testq	%r13, %r13
	je	.LBB147_3
# BB#1531:                              # %for blur$1.s1.v10.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	notl	%ebx
	movslq	%ebx, %rax
	movq	4936(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	5472(%rsp), %r8         # 8-byte Reload
	leaq	(%r8,%rax,4), %rax
	vmovups	(%rax), %ymm0
	movq	4904(%rsp), %rax        # 8-byte Reload
	vmovups	%ymm0, (%r13,%rax)
	movq	760(%rsp), %r9          # 8-byte Reload
	cmpl	$1, %r9d
	jle	.LBB147_1547
# BB#1532:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	vcvttss2si	4848(%rsp), %eax # 4-byte Folded Reload
	leal	127(%rax), %ecx
	cmpl	$255, %ecx
	jl	.LBB147_1533
# BB#1534:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	vmovss	.LCPI147_48(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB147_1535
	.align	16, 0x90
.LBB147_1533:                           #   in Loop: Header=BB147_1519 Depth=2
	shll	$23, %ecx
	vmovd	%ecx, %xmm0
	vmulss	4720(%rsp), %xmm0, %xmm1 # 4-byte Folded Reload
.LBB147_1535:                           # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	cmpl	$-127, %eax
	jg	.LBB147_1537
# BB#1536:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1537:                           # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	vmovss	.LCPI147_17(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %esi
	cmpl	$0, 968(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1539
# BB#1538:                              # %for blur$1.s2.r101$x.prol
                                        #   in Loop: Header=BB147_1519 Depth=2
	movslq	%r14d, %rax
	movq	4728(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	addq	4712(%rsp), %rax        # 8-byte Folded Reload
	vmulps	(%r8,%rax,4), %ymm0, %ymm2
	movq	4696(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r13,%rax,4), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%r13,%rax,4)
	movl	$2, %esi
.LBB147_1539:                           # %for blur$1.s2.r101$x.preheader.split
                                        #   in Loop: Header=BB147_1519 Depth=2
	cmpl	$0, 5008(%rsp)          # 4-byte Folded Reload
	movq	4896(%rsp), %rbx        # 8-byte Reload
	je	.LBB147_1542
# BB#1540:                              # %for blur$1.s2.r101$x.preheader.split.split
                                        #   in Loop: Header=BB147_1519 Depth=2
	notl	%r12d
	movslq	%r12d, %r10
	movl	%r9d, %eax
	subl	%esi, %eax
	movq	4752(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	4768(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rcx
	movq	4800(%rsp), %r14        # 8-byte Reload
	leaq	(%rcx,%r14), %rcx
	addq	%r10, %rcx
	movq	5072(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	shlq	$5, %rdx
	leaq	(%rdx,%r13), %rdx
	movq	4760(%rsp), %rdi        # 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	imulq	%r11, %rsi
	leaq	(%rsi,%r14), %rsi
	addq	%r10, %rsi
	movq	%r8, %rdi
	.align	16, 0x90
.LBB147_1541:                           # %for blur$1.s2.r101$x
                                        #   Parent Loop BB147_1517 Depth=1
                                        #     Parent Loop BB147_1519 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulps	(%rdi,%rcx,4), %ymm0, %ymm2
	vmovaps	(%rdx), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rdx)
	vmulps	(%rdi,%rsi,4), %ymm0, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rdx)
	addq	$64, %rdx
	addq	%rbx, %rdi
	addl	$-2, %eax
	jne	.LBB147_1541
.LBB147_1542:                           # %for blur$1.s3.r101$x.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	movl	$1, %r8d
	cmpl	$0, 968(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1544
# BB#1543:                              # %for blur$1.s3.r101$x.prol
                                        #   in Loop: Header=BB147_1519 Depth=2
	movq	4688(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r13,%rax,4), %ymm2
	vmulps	(%r13,%rax,4), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	movq	4704(%rsp), %rax        # 8-byte Reload
	vmovaps	%ymm2, -64(%r13,%rax,4)
	movl	$2, %r8d
.LBB147_1544:                           # %for blur$1.s3.r101$x.preheader.split
                                        #   in Loop: Header=BB147_1519 Depth=2
	cmpl	$0, 5008(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1547
# BB#1545:                              # %for blur$1.s3.r101$x.preheader.split.split
                                        #   in Loop: Header=BB147_1519 Depth=2
	movl	4784(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	subl	%r8d, %edx
	movq	4736(%rsp), %rax        # 8-byte Reload
	leaq	(%r13,%rax), %rsi
	movq	4744(%rsp), %rax        # 8-byte Reload
	subq	%r8, %rax
	movq	%r8, %rdi
	shlq	$5, %rdi
	subq	%rdi, %rsi
	shlq	$5, %rax
	leaq	-32(%r13,%rax), %rax
	leal	1(%r8), %ebx
	movl	%ecx, %edi
	subl	%ebx, %edi
	movl	%r9d, %ebx
	.align	16, 0x90
.LBB147_1546:                           # %for blur$1.s3.r101$x
                                        #   Parent Loop BB147_1517 Depth=1
                                        #     Parent Loop BB147_1519 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leal	(%rdx,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r13), %ymm2
	vmulps	(%rcx,%r13), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	leal	(%rdi,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r13), %ymm2
	vmulps	(%rcx,%r13), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rax)
	addl	$-2, %ebx
	addq	$-64, %rsi
	addq	$-64, %rax
	cmpl	%ebx, %r8d
	jne	.LBB147_1546
.LBB147_1547:                           # %consume blur$1
                                        #   in Loop: Header=BB147_1519 Depth=2
	movq	%r13, 5024(%rsp)        # 8-byte Spill
	cmpl	$0, 5456(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1552
# BB#1548:                              # %for transpose$1.s0.v10.v165.preheader
                                        #   in Loop: Header=BB147_1519 Depth=2
	notl	%r15d
	movslq	%r15d, %rax
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	4880(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	4888(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rcx
	imulq	%rdx, %rax
	movq	4944(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	%rcx, 5440(%rsp)        # 8-byte Spill
	movq	5024(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	subq	$-128, %rsi
	movq	%rsi, 5424(%rsp)        # 8-byte Spill
	leaq	260(%rcx), %rsi
	movq	%rsi, 5408(%rsp)        # 8-byte Spill
	leaq	228(%rcx), %rsi
	movq	%rsi, 5392(%rsp)        # 8-byte Spill
	leaq	196(%rcx), %rsi
	movq	%rsi, 5376(%rsp)        # 8-byte Spill
	leaq	164(%rcx), %rsi
	movq	%rsi, 5360(%rsp)        # 8-byte Spill
	leaq	132(%rcx), %rsi
	movq	%rsi, 5312(%rsp)        # 8-byte Spill
	leaq	100(%rcx), %rsi
	movq	%rsi, 5280(%rsp)        # 8-byte Spill
	leaq	68(%rcx), %rsi
	movq	%rsi, 5248(%rsp)        # 8-byte Spill
	leaq	36(%rcx), %rsi
	movq	%rsi, 5216(%rsp)        # 8-byte Spill
	leaq	256(%rcx), %rsi
	movq	%rsi, 5200(%rsp)        # 8-byte Spill
	leaq	224(%rcx), %rsi
	movq	%rsi, 5184(%rsp)        # 8-byte Spill
	leaq	192(%rcx), %rsi
	movq	%rsi, 5168(%rsp)        # 8-byte Spill
	leaq	(%rax,%rdx), %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	leaq	160(%rcx), %rax
	movq	%rax, 5136(%rsp)        # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, 5120(%rsp)        # 8-byte Spill
	leaq	64(%rcx), %rax
	movq	%rax, 5104(%rsp)        # 8-byte Spill
	leaq	32(%rcx), %rax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	4928(%rsp), %ecx        # 4-byte Reload
	.align	16, 0x90
.LBB147_1549:                           # %for transpose$1.s0.v10.v165
                                        #   Parent Loop BB147_1517 Depth=1
                                        #     Parent Loop BB147_1519 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1550 Depth 4
	movl	%ecx, 5504(%rsp)        # 4-byte Spill
	movl	%eax, 5528(%rsp)        # 4-byte Spill
	movl	5056(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	cltq
	movq	5072(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r9
	shlq	$5, %r9
	movq	5440(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	%rcx, 5760(%rsp)        # 8-byte Spill
	movq	5152(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 5728(%rsp)        # 8-byte Spill
	movq	5408(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5696(%rsp)        # 8-byte Spill
	movq	5392(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5680(%rsp)        # 8-byte Spill
	movq	5376(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	movq	5360(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5632(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5624(%rsp)        # 8-byte Spill
	movq	5280(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5568(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rcx
	movq	5216(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %r12
	movq	5200(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	5184(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r9), %rdx
	movq	5168(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r15
	movq	5136(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %rbx
	movq	5424(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r13
	movq	5120(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r14
	movq	5104(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r8
	movq	5096(%rsp), %rsi        # 8-byte Reload
	leaq	(%r9,%rsi), %r9
	movq	5536(%rsp), %r10        # 8-byte Reload
	xorl	%r11d, %r11d
	movq	5048(%rsp), %rsi        # 8-byte Reload
	.align	16, 0x90
.LBB147_1550:                           # %for transpose$1.s0.v11.v11
                                        #   Parent Loop BB147_1517 Depth=1
                                        #     Parent Loop BB147_1519 Depth=2
                                        #       Parent Loop BB147_1549 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	(%rbx,%r11), %xmm0      # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r9,%r11), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r14,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r13,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5728(%rsp), %rdi        # 8-byte Reload
	vmovups	%ymm0, (%r10,%rdi,4)
	movq	5632(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%r11), %xmm0      # xmm0 = mem[0],zero,zero,zero
	movq	5672(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	5680(%rsp), %rdi        # 8-byte Reload
	vinsertps	$32, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	5696(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r12,%r11), %xmm1      # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	5568(%rsp), %rdi        # 8-byte Reload
	vinsertps	$32, (%rdi,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	5624(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdi,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5760(%rsp), %rdi        # 8-byte Reload
	vmovups	%ymm0, (%r10,%rdi,4)
	addq	$8, %r11
	addq	%rsi, %r10
	cmpq	$32, %r11
	jne	.LBB147_1550
# BB#1551:                              # %end for transpose$1.s0.v11.v11
                                        #   in Loop: Header=BB147_1549 Depth=3
	movl	5528(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5504(%rsp), %ecx        # 4-byte Reload
	addl	$-8, %ecx
	cmpl	5456(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1549
.LBB147_1552:                           # %call_destructor.exit689
                                        #   in Loop: Header=BB147_1519 Depth=2
	xorl	%edi, %edi
	movq	5024(%rsp), %rsi        # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	4992(%rsp), %rdx        # 8-byte Reload
	addl	$1, %edx
	movl	4976(%rsp), %esi        # 4-byte Reload
	addl	$-8, %esi
	cmpl	5488(%rsp), %edx        # 4-byte Folded Reload
	jne	.LBB147_1519
.LBB147_1553:                           # %end for transpose$1.s0.v11.v167
                                        #   in Loop: Header=BB147_1517 Depth=1
	movq	4680(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	movq	4632(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4800(%rsp)        # 8-byte Folded Spill
	movq	4616(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4944(%rsp)        # 8-byte Folded Spill
	cmpl	$3, %eax
	jne	.LBB147_1517
# BB#1521:                              # %end for transpose$1.s0.v12
	movq	5472(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB147_1523
# BB#1522:                              # %if.then.i.770
	xorl	%edi, %edi
	callq	halide_free@PLT
.LBB147_1523:                           # %for sharpi.s0.v12.preheader
	movq	472(%rsp), %r8          # 8-byte Reload
	leal	31(%r8), %eax
	sarl	$5, %eax
	movl	%eax, 5392(%rsp)        # 4-byte Spill
	movslq	4576(%rsp), %rcx        # 4-byte Folded Reload
	movq	368(%rsp), %rax         # 8-byte Reload
	movslq	%eax, %rdx
	vmovss	80(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vbroadcastss	%xmm0, %ymm6
	movl	$7, %eax
	movq	480(%rsp), %r9          # 8-byte Reload
	subl	%r9d, %eax
	movq	1200(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %eax
	movl	%r9d, %esi
	notl	%esi
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	leal	1(%r9,%rsi), %eax
	movq	%rax, 5376(%rsp)        # 8-byte Spill
	cmpl	$7, %edi
	movl	$8, %r11d
	cmovgl	%edi, %r11d
	movq	%r11, 5360(%rsp)        # 8-byte Spill
	movl	$31, %eax
	movq	1016(%rsp), %r10        # 8-byte Reload
	subl	%r10d, %eax
	subl	%r8d, %eax
	movl	%eax, 5312(%rsp)        # 4-byte Spill
	movl	%r10d, %esi
	notl	%esi
	movl	%esi, 5184(%rsp)        # 4-byte Spill
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 5280(%rsp)        # 4-byte Spill
	movq	5352(%rsp), %rax        # 8-byte Reload
	movq	880(%rsp), %rsi         # 8-byte Reload
	leal	31(%rax,%rsi), %eax
	movl	%r9d, %esi
	movl	380(%rsp), %r8d         # 4-byte Reload
	subl	%r8d, %esi
	movq	%rsi, 5352(%rsp)        # 8-byte Spill
	leal	-1(%r9,%rdi), %esi
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	movq	760(%rsp), %rsi         # 8-byte Reload
	testl	%esi, %esi
	movl	$1, %ebx
	cmovgl	%esi, %ebx
	leal	(,%r11,8), %esi
	movl	%esi, 5472(%rsp)        # 4-byte Spill
	negl	%ebx
	movl	392(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %ebx
	cmovll	%esi, %ebx
	notl	%ebx
	cmpl	%ebx, %eax
	cmovgel	%eax, %ebx
	addl	$1, %ebx
	subl	%r8d, %ebx
	movq	%rbx, 5248(%rsp)        # 8-byte Spill
	notl	736(%rsp)               # 4-byte Folded Spill
	movq	408(%rsp), %rdi         # 8-byte Reload
	imulq	%rdi, %r10
	leal	(,%rbx,8), %eax
	movl	%eax, 5456(%rsp)        # 4-byte Spill
	negq	%r10
	movq	%rdi, %rax
	shlq	$4, %rax
	movq	%rax, 5440(%rsp)        # 8-byte Spill
	leaq	(%rdi,%rdi), %r9
	vbroadcastss	.LCPI147_39(%rip), %ymm11
	vbroadcastss	.LCPI147_49(%rip), %ymm4
	vbroadcastss	.LCPI147_50(%rip), %ymm8
	vpbroadcastd	.LCPI147_51(%rip), %ymm14
	vpbroadcastd	.LCPI147_52(%rip), %ymm7
	vbroadcastss	.LCPI147_42(%rip), %ymm9
	vbroadcastss	.LCPI147_43(%rip), %ymm13
	vbroadcastss	.LCPI147_44(%rip), %ymm5
	vbroadcastss	.LCPI147_17(%rip), %ymm10
	vbroadcastss	.LCPI147_45(%rip), %ymm1
	vmovaps	%ymm1, 5760(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI147_46(%rip), %ymm2
	vmovaps	%ymm2, 5728(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI147_47(%rip), %ymm3
	vmovaps	%ymm3, 5696(%rsp)       # 32-byte Spill
	vmovdqa	.LCPI147_7(%rip), %ymm12 # ymm12 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	xorl	%eax, %eax
	movl	$0, 5200(%rsp)          # 4-byte Folded Spill
	.align	16, 0x90
.LBB147_1524:                           # %for sharpi.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1561 Depth 2
                                        #       Child Loop BB147_1562 Depth 3
                                        #         Child Loop BB147_1563 Depth 4
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	vmovaps	%ymm3, %ymm0
	movq	%r10, 1016(%rsp)        # 8-byte Spill
	cmpl	$0, 5392(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1554
# BB#1525:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1524 Depth=1
	movl	$2, %eax
	subl	5200(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, 96(%rbp)
	movq	5216(%rsp), %r11        # 8-byte Reload
	cmovel	%r11d, %eax
	vmovss	56(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	cmpl	$1, %eax
	je	.LBB147_1527
# BB#1526:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1524 Depth=1
	vmovss	64(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
.LBB147_1527:                           # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1524 Depth=1
	movslq	%eax, %rbx
	vmovss	48(%rbp), %xmm15        # xmm15 = mem[0],zero,zero,zero
	testl	%eax, %eax
	je	.LBB147_1529
# BB#1528:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1524 Depth=1
	vmovaps	%xmm0, %xmm15
.LBB147_1529:                           # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1524 Depth=1
	movq	%r11, %r8
	movq	4640(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %r8
	movq	4672(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	movq	%r11, 5680(%rsp)        # 8-byte Spill
	movq	%rbx, %rax
	imulq	%rsi, %rax
	movq	%rax, 5488(%rsp)        # 8-byte Spill
	vaddss	.LCPI147_17(%rip), %xmm15, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 5632(%rsp)       # 32-byte Spill
	imulq	%r14, %rbx
	movq	%rbx, 5672(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	5184(%rsp), %esi        # 4-byte Reload
	vxorps	%ymm15, %ymm15, %ymm15
	.align	16, 0x90
.LBB147_1561:                           # %for sharpi.s0.v11.v14
                                        #   Parent Loop BB147_1524 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1562 Depth 3
                                        #         Child Loop BB147_1563 Depth 4
	movl	%esi, 5408(%rsp)        # 4-byte Spill
	movl	%eax, 5424(%rsp)        # 4-byte Spill
	movl	5312(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	movl	5280(%rsp), %esi        # 4-byte Reload
	subl	%eax, %esi
	movq	5360(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %esi
	movq	5376(%rsp), %rbx        # 8-byte Reload
	leal	(%rsi,%rbx), %r14d
	movl	736(%rsp), %esi         # 4-byte Reload
	subl	%eax, %esi
	movq	5248(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %esi
	notl	%eax
	cltq
	imulq	%rdi, %rax
	movq	5352(%rsp), %rbx        # 8-byte Reload
	leal	(%rsi,%rbx), %ebx
	leaq	(%rax,%r10), %rax
	movq	744(%rsp), %rsi         # 8-byte Reload
	leaq	(%rsi,%rax,2), %r12
	xorl	%esi, %esi
	.align	16, 0x90
.LBB147_1562:                           # %for sharpi.s0.v11.v13.v13
                                        #   Parent Loop BB147_1524 Depth=1
                                        #     Parent Loop BB147_1561 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1563 Depth 4
	movq	%rsi, 5504(%rsp)        # 8-byte Spill
	movq	%r12, 5528(%rsp)        # 8-byte Spill
	movl	%ebx, 5568(%rsp)        # 4-byte Spill
	movl	%r14d, 5624(%rsp)       # 4-byte Spill
	movq	1200(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %esi
	movl	%ebx, %r13d
	movl	%r14d, %r15d
	testl	%r10d, %r10d
	movq	1416(%rsp), %r14        # 8-byte Reload
	movq	5536(%rsp), %r10        # 8-byte Reload
	movq	5488(%rsp), %r11        # 8-byte Reload
	jle	.LBB147_1564
	.align	16, 0x90
.LBB147_1563:                           # %for sharpi.s0.v10
                                        #   Parent Loop BB147_1524 Depth=1
                                        #     Parent Loop BB147_1561 Depth=2
                                        #       Parent Loop BB147_1562 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movslq	%r15d, %r15
	leaq	(%r15,%r8), %rbx
	leaq	(%r10,%rbx,4), %rax
	vmovss	(%r10,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rax,%rcx,4), %rbx
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rax
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rcx,4), %rbx
	vmovdqa	%ymm12, %ymm1
	vmovdqa	%ymm7, %ymm12
	vmovdqa	%ymm14, %ymm7
	vmovss	(%rbx,%rcx,4), %xmm14   # xmm14 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$16, (%rbx,%rcx,4), %xmm14, %xmm2 # xmm2 = xmm14[0],mem[0],xmm14[2,3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movslq	%r13d, %r13
	movq	5680(%rsp), %rax        # 8-byte Reload
	leaq	(%r13,%rax), %rax
	movzwl	(%r14,%rax,2), %ebx
	leaq	(%r14,%rax,2), %rax
	vmovd	%ebx, %xmm3
	leaq	(%rax,%rdx,2), %rbx
	vpinsrw	$1, (%rax,%rdx,2), %xmm3, %xmm3
	leaq	(%rbx,%rdx,2), %rax
	vpinsrw	$2, (%rbx,%rdx,2), %xmm3, %xmm3
	leaq	(%rax,%rdx,2), %rbx
	vpinsrw	$3, (%rax,%rdx,2), %xmm3, %xmm3
	leaq	(%rbx,%rdx,2), %rax
	vpinsrw	$4, (%rbx,%rdx,2), %xmm3, %xmm3
	leaq	(%rax,%rdx,2), %rbx
	vpinsrw	$5, (%rax,%rdx,2), %xmm3, %xmm3
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	leaq	(%rbx,%rdx,2), %rax
	vpinsrw	$6, (%rbx,%rdx,2), %xmm3, %xmm2
	vpinsrw	$7, (%rax,%rdx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vsubps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm0, %ymm0, %ymm0
	vfnmadd213ps	%ymm15, %ymm6, %ymm0
	vmulps	%ymm11, %ymm0, %ymm2
	vroundps	$1, %ymm2, %ymm2
	vmovaps	%ymm8, %ymm3
	vcvttps2dq	%ymm2, %ymm14
	vfnmadd213ps	%ymm0, %ymm2, %ymm3
	vfnmadd213ps	%ymm3, %ymm4, %ymm2
	vmulps	%ymm2, %ymm2, %ymm0
	vmovaps	%ymm9, %ymm3
	vfmadd213ps	%ymm13, %ymm0, %ymm3
	vfmadd213ps	%ymm5, %ymm0, %ymm3
	vmovaps	%ymm5, %ymm15
	vmovaps	%ymm13, %ymm5
	vmovaps	%ymm9, %ymm13
	vmovaps	%ymm8, %ymm9
	vmovaps	%ymm4, %ymm8
	vmovaps	%ymm11, %ymm4
	vmovaps	5760(%rsp), %ymm11      # 32-byte Reload
	vfmadd213ps	5728(%rsp), %ymm0, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	5696(%rsp), %ymm0, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm10, %ymm0, %ymm3
	vfmadd213ps	%ymm10, %ymm0, %ymm11
	leaq	(%r15,%r11), %rax
	leaq	(%r10,%rax,4), %rbx
	vpaddd	%ymm7, %ymm14, %ymm0
	vmovdqa	%ymm7, %ymm14
	vmovdqa	%ymm12, %ymm7
	vmovdqa	%ymm1, %ymm12
	vfmadd213ps	%ymm3, %ymm2, %ymm11
	vpcmpgtd	%ymm0, %ymm7, %ymm2
	vpslld	$23, %ymm0, %ymm3
	vfnmadd213ps	%ymm10, %ymm3, %ymm11
	vbroadcastss	.LCPI147_53(%rip), %ymm3
	vblendvps	%ymm2, %ymm11, %ymm3, %ymm2
	vmovss	(%r10,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rax
	vinsertps	$16, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rax,%rcx,4), %rbx
	vinsertps	$32, (%rax,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leaq	(%rbx,%rcx,4), %rax
	vmovaps	%ymm6, %ymm11
	vmovss	(%rax,%rcx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm3, %ymm3
	vmovaps	%ymm11, %ymm6
	vmovaps	%ymm4, %ymm11
	vmovaps	%ymm8, %ymm4
	vmovaps	%ymm9, %ymm8
	vmovaps	%ymm13, %ymm9
	vmovaps	%ymm5, %ymm13
	vmovaps	%ymm15, %ymm5
	vxorps	%ymm15, %ymm15, %ymm15
	vpcmpgtd	%ymm15, %ymm0, %ymm0
	movq	5672(%rsp), %rax        # 8-byte Reload
	leaq	(%r13,%rax), %rax
	vblendvps	%ymm0, %ymm2, %ymm10, %ymm0
	movzwl	(%r14,%rax,2), %ebx
	vmovd	%ebx, %xmm2
	leaq	(%r14,%rax,2), %rax
	vpinsrw	$1, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$2, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$3, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$4, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$5, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$6, (%rax,%rdx,2), %xmm2, %xmm2
	leaq	(%rax,%rdx,2), %rax
	vpinsrw	$7, (%rax,%rdx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vsubps	%ymm3, %ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm0
	vmovaps	5632(%rsp), %ymm1       # 32-byte Reload
	vfmadd213ps	%ymm3, %ymm1, %ymm0
	vbroadcastss	.LCPI147_5(%rip), %ymm2
	vminps	%ymm2, %ymm0, %ymm0
	vmaxps	%ymm15, %ymm0, %ymm0
	vcvttps2dq	%ymm0, %ymm0
	vpshufb	%ymm12, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vmovd	%xmm0, %eax
	movw	%ax, (%r12)
	leaq	(%r12,%r9), %rax
	vpextrw	$1, %xmm0, (%r12,%rdi,2)
	vpextrw	$2, %xmm0, (%r12,%rdi,4)
	vpextrw	$3, %xmm0, (%rax,%rdi,4)
	leaq	(%rax,%r9), %rax
	vpextrw	$4, %xmm0, (%rax,%rdi,4)
	leaq	(%rax,%r9), %rax
	vpextrw	$5, %xmm0, (%rax,%rdi,4)
	leaq	(%rax,%r9), %rax
	vpextrw	$6, %xmm0, (%rax,%rdi,4)
	addq	%r9, %rax
	vpextrw	$7, %xmm0, (%rax,%rdi,4)
	addl	$1, %r15d
	addl	$1, %r13d
	addq	$6, %r12
	addl	$-1, %esi
	jne	.LBB147_1563
.LBB147_1564:                           # %end for sharpi.s0.v10
                                        #   in Loop: Header=BB147_1562 Depth=3
	vmovaps	5696(%rsp), %ymm0       # 32-byte Reload
	vmovaps	5728(%rsp), %ymm2       # 32-byte Reload
	vmovaps	5760(%rsp), %ymm1       # 32-byte Reload
	movq	5504(%rsp), %rsi        # 8-byte Reload
	addq	$1, %rsi
	movl	5624(%rsp), %r14d       # 4-byte Reload
	addl	5472(%rsp), %r14d       # 4-byte Folded Reload
	movl	5568(%rsp), %ebx        # 4-byte Reload
	addl	5456(%rsp), %ebx        # 4-byte Folded Reload
	movq	5528(%rsp), %r12        # 8-byte Reload
	addq	5440(%rsp), %r12        # 8-byte Folded Reload
	cmpq	$4, %rsi
	jne	.LBB147_1562
# BB#1560:                              # %end for sharpi.s0.v11.v13.v13
                                        #   in Loop: Header=BB147_1561 Depth=2
	movl	5424(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5408(%rsp), %esi        # 4-byte Reload
	addl	$-32, %esi
	cmpl	5392(%rsp), %eax        # 4-byte Folded Reload
	movq	1016(%rsp), %r10        # 8-byte Reload
	jne	.LBB147_1561
.LBB147_1554:                           # %end for sharpi.s0.v11.v14
                                        #   in Loop: Header=BB147_1524 Depth=1
	movq	5216(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	addl	$1, 5200(%rsp)          # 4-byte Folded Spill
	addq	$1, %r10
	cmpq	$3, %rax
	vmovaps	%ymm0, %ymm3
	jne	.LBB147_1524
# BB#1555:                              # %end for sharpi.s0.v12
	movq	1416(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB147_1557
# BB#1556:                              # %if.then.i.659
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_1557:                           # %call_destructor.exit660
	xorl	%r8d, %r8d
	movl	$0, %ecx
	movl	$0, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	movl	$0, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	movl	$0, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	movl	$0, %edx
	movl	$0, %r12d
	movl	$0, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	movl	$0, %edi
	movl	$0, %ebx
	movq	5536(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB147_207
# BB#1558:                              # %if.then.i.630
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_1559:                           # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	xorl	%ebx, %ebx
.LBB147_207:                            # %call_destructor.exit.thread
	movq	%rdi, 1416(%rsp)        # 8-byte Spill
	movq	%r12, 4872(%rsp)        # 8-byte Spill
	testl	%ebx, %ebx
	sete	%r14b
.LBB147_9:                              # %call_destructor.exit578
	movq	%rdx, 4880(%rsp)        # 8-byte Spill
	testq	%r8, %r8
	movq	4720(%rsp), %r15        # 8-byte Reload
	movq	4752(%rsp), %r13        # 8-byte Reload
	je	.LBB147_12
# BB#10:                                # %call_destructor.exit578
	testb	%r14b, %r14b
	jne	.LBB147_12
# BB#11:                                # %if.then.i.582
	xorl	%edi, %edi
	movq	%r8, %rsi
	movl	%ebx, %r12d
	movq	%rcx, %rbx
	vzeroupper
	callq	halide_free@PLT
	movq	%rbx, %rcx
	movl	%r12d, %ebx
.LBB147_12:                             # %call_destructor.exit588
	movl	%ebx, %r12d
	testq	%rcx, %rcx
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_14
# BB#13:                                # %if.then.i.592
	xorl	%edi, %edi
	movq	%rcx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_14:                             # %call_destructor.exit593
	movq	5096(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_16
# BB#15:                                # %if.then.i.597
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_16:                             # %call_destructor.exit598
	testq	%r15, %r15
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_18
# BB#17:                                # %if.then.i.602
	xorl	%edi, %edi
	movq	%r15, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_18:                             # %call_destructor.exit603
	testq	%r13, %r13
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_20
# BB#19:                                # %if.then.i.607
	xorl	%edi, %edi
	movq	%r13, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_20:                             # %call_destructor.exit608
	movq	4880(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	movq	1416(%rsp), %rbx        # 8-byte Reload
	jne	.LBB147_22
# BB#21:                                # %if.then.i.612
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_22:                             # %call_destructor.exit613
	movq	4872(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_24
# BB#23:                                # %if.then.i.617
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_24:                             # %call_destructor.exit618
	movq	5672(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_26
# BB#25:                                # %if.then.i.622
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_26:                             # %call_destructor.exit623
	testq	%rbx, %rbx
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_28
# BB#27:                                # %if.then.i.627
	xorl	%edi, %edi
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_28:                             # %call_destructor.exit628
	movl	%r12d, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB147_1565:                           # %assert failed553
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	jmp	.LBB147_1513
.LBB147_1484:                           # %assert failed551
	leaq	.Lstr.173(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	4944(%rsp), %rdx        # 8-byte Reload
.LBB147_1512:                           # %call_destructor.exit.thread
	callq	halide_error_buffer_allocation_too_large@PLT
.LBB147_1513:                           # %call_destructor.exit.thread
	xorl	%ecx, %ecx
	movl	%eax, %ebx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	movq	1416(%rsp), %rdi        # 8-byte Reload
	movq	5472(%rsp), %r8         # 8-byte Reload
	jmp	.LBB147_207
.LBB147_1520:                           # %assert failed559
	leaq	.Lstr.175(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	4920(%rsp), %rdx        # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_4
.LBB147_3:                              # %assert failed561
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_4:                              # %destructor_block
	movl	%eax, %ebx
	testl	%ebx, %ebx
	sete	%r14b
	cmpq	$0, 5536(%rsp)          # 8-byte Folded Reload
	je	.LBB147_5
# BB#6:                                 # %destructor_block
	testl	%ebx, %ebx
	je	.LBB147_5
# BB#7:                                 # %if.then.i.572
	xorl	%edi, %edi
	movq	5536(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 4872(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	jmp	.LBB147_8
.LBB147_5:
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 4872(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
.LBB147_8:                              # %call_destructor.exit578
	movq	5472(%rsp), %r8         # 8-byte Reload
	jmp	.LBB147_9
.LBB147_198:                            # %assert failed247
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	jmp	.LBB147_1474
.LBB147_196:                            # %assert failed245
	leaq	.Lstr.164(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	336(%rsp), %rdx         # 8-byte Reload
.LBB147_1473:                           # %call_destructor.exit.thread
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
.LBB147_1474:                           # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	jmp	.LBB147_206
.LBB147_200:                            # %assert failed249
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	leaq	.Lstr.165(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	640(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_203
.LBB147_202:                            # %assert failed251
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_203:                            # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%r12d, %r12d
	jmp	.LBB147_206
.LBB147_205:                            # %assert failed255
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
.LBB147_206:                            # %call_destructor.exit.thread
	movq	1416(%rsp), %rdi        # 8-byte Reload
	jmp	.LBB147_207
.LBB147_209:                            # %assert failed257
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	leaq	.Lstr.167(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	632(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_212
.LBB147_211:                            # %assert failed259
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_212:                            # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	jmp	.LBB147_225
.LBB147_214:                            # %assert failed263
	movq	%r14, 5672(%rsp)        # 8-byte Spill
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	jmp	.LBB147_225
.LBB147_216:                            # %assert failed265
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	movq	%r15, 4720(%rsp)        # 8-byte Spill
	leaq	.Lstr.169(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	624(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_219
.LBB147_218:                            # %assert failed267
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	movq	%r15, 4720(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_219:                            # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	jmp	.LBB147_225
.LBB147_221:                            # %assert failed269
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	movq	%r15, 4720(%rsp)        # 8-byte Spill
	movq	%r14, 5096(%rsp)        # 8-byte Spill
	leaq	.Lstr.170(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	664(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_224
.LBB147_223:                            # %assert failed271
	movq	%r12, 4752(%rsp)        # 8-byte Spill
	movq	%r15, 4720(%rsp)        # 8-byte Spill
	movq	%r14, 5096(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_224:                            # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
.LBB147_225:                            # %call_destructor.exit.thread
	movq	1416(%rsp), %rdi        # 8-byte Reload
.LBB147_226:                            # %call_destructor.exit.thread
	movq	4880(%rsp), %rdx        # 8-byte Reload
	movq	4872(%rsp), %r12        # 8-byte Reload
	jmp	.LBB147_207
.LBB147_228:                            # %assert failed275
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	movq	1416(%rsp), %rdi        # 8-byte Reload
	movq	4712(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_226
.LBB147_1:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_2
.LBB147_30:                             # %assert failed10
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_2
.LBB147_32:                             # %assert failed29
	leaq	.Lstr.139(%rip), %rsi
	jmp	.LBB147_2
.LBB147_34:                             # %assert failed48
	leaq	.Lstr.140(%rip), %rsi
	jmp	.LBB147_2
.LBB147_36:                             # %assert failed67
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_2
.LBB147_38:                             # %assert failed86
	leaq	.Lstr.142(%rip), %rsi
.LBB147_2:                              # %call_destructor.exit.thread
	xorl	%edi, %edi
	callq	halide_error_buffer_argument_is_null@PLT
	jmp	.LBB147_56
.LBB147_169:                            # %assert failed215
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_158
.LBB147_53:                             # %assert failed123
	leaq	.Lstr.143(%rip), %rsi
	jmp	.LBB147_54
.LBB147_58:                             # %assert failed125
	leaq	.Lstr.145(%rip), %rsi
	jmp	.LBB147_59
.LBB147_61:                             # %assert failed127
	leaq	.Lstr.147(%rip), %rsi
	jmp	.LBB147_59
.LBB147_63:                             # %assert failed129
	leaq	.Lstr.148(%rip), %rsi
.LBB147_59:                             # %call_destructor.exit.thread
	leaq	.Lstr.146(%rip), %rdx
	xorl	%edi, %edi
	movl	$2, %r8d
	movl	%ebx, %ecx
	jmp	.LBB147_55
.LBB147_65:                             # %assert failed131
	leaq	.Lstr.149(%rip), %rsi
	jmp	.LBB147_54
.LBB147_67:                             # %assert failed133
	leaq	.Lstr.150(%rip), %rsi
.LBB147_54:                             # %call_destructor.exit.thread
	leaq	.Lstr.144(%rip), %rdx
	xorl	%edi, %edi
	movl	$4, %r8d
.LBB147_55:                             # %call_destructor.exit.thread
	vzeroupper
	callq	halide_error_bad_elem_size@PLT
	jmp	.LBB147_56
.LBB147_70:                             # %assert failed135
	leal	-1(%rdx,%r9), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	jmp	.LBB147_71
.LBB147_74:                             # %assert failed137
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	4848(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_77:                             # %assert failed139
	leal	-1(%r10,%r11), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	jmp	.LBB147_78
.LBB147_80:                             # %assert failed141
	leaq	.Lstr.143(%rip), %rsi
	jmp	.LBB147_81
.LBB147_84:                             # %assert failed143
	movl	4960(%rsp), %r8d        # 4-byte Reload
	addl	$-1, %r8d
	movl	4352(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.145(%rip), %rsi
	movq	%rdi, %r9
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4976(%rsp), %ecx        # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_86:                             # %assert failed145
	movq	%rsi, %rcx
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_89:                             # %assert failed147
	movl	4936(%rsp), %r8d        # 4-byte Reload
	addl	$-1, %r8d
	movl	1396(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	4944(%rsp), %ecx        # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_91:                             # %assert failed149
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r15d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_93:                             # %assert failed151
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1200(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_96:                             # %assert failed153
	movl	420(%rsp), %eax         # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	468(%rsp), %ecx         # 4-byte Reload
	movl	4992(%rsp), %r8d        # 4-byte Reload
	movl	%r14d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_98:                             # %assert failed155
	leaq	.Lstr.147(%rip), %rsi
	jmp	.LBB147_99
.LBB147_103:                            # %assert failed157
	leal	-1(%rdx,%r8), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movq	%r8, %r9
.LBB147_71:                             # %call_destructor.exit.thread
	movl	$2, %r8d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_105:                            # %assert failed159
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movq	%rdx, %rcx
	movl	$2, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_108:                            # %assert failed161
	leal	-1(%rbx,%r11), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
.LBB147_78:                             # %call_destructor.exit.thread
	xorl	%ecx, %ecx
	movl	$2, %r8d
	movl	%r11d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_110:                            # %assert failed163
	leaq	.Lstr.148(%rip), %rsi
	jmp	.LBB147_111
.LBB147_114:                            # %assert failed165
	leal	-1(%r12,%rbx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	movq	%rbx, %r9
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$4095, %r8d             # imm = 0xFFF
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_116:                            # %assert failed167
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r12d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_118:                            # %assert failed169
	movl	5680(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4784(%rsp), %ecx        # 4-byte Reload
	movq	4712(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_120:                            # %assert failed171
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_123:                            # %assert failed173
	movl	4256(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	5056(%rsp), %ecx        # 4-byte Reload
	movl	5072(%rsp), %r8d        # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_125:                            # %assert failed175
	leaq	.Lstr.149(%rip), %rsi
.LBB147_81:                             # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r10d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_127:                            # %assert failed177
	movl	4144(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4768(%rsp), %ecx        # 4-byte Reload
	movq	5472(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_129:                            # %assert failed179
	leaq	.Lstr.150(%rip), %rsi
.LBB147_111:                            # %call_destructor.exit.thread
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_131:                            # %assert failed181
	movl	1804(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r13d, %ecx
	movl	%eax, %r8d
	movq	1816(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_56
.LBB147_133:                            # %assert failed183
	leaq	.Lstr.150(%rip), %rsi
.LBB147_99:                             # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$1, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_56
.LBB147_135:                            # %assert failed185
	leaq	.Lstr.151(%rip), %rsi
	jmp	.LBB147_136
.LBB147_138:                            # %assert failed187
	leaq	.Lstr.153(%rip), %rsi
	jmp	.LBB147_139
.LBB147_143:                            # %assert failed189
	leaq	.Lstr.154(%rip), %rsi
	leaq	.Lstr.155(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	jmp	.LBB147_140
.LBB147_145:                            # %assert failed191
	leaq	.Lstr.156(%rip), %rsi
	jmp	.LBB147_139
.LBB147_147:                            # %assert failed193
	leaq	.Lstr.157(%rip), %rsi
	leaq	.Lstr.158(%rip), %rcx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	movl	%eax, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_56
.LBB147_149:                            # %assert failed195
	leaq	.Lstr.159(%rip), %rsi
	leaq	.Lstr.155(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	movl	%r15d, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_56
.LBB147_151:                            # %assert failed197
	leaq	.Lstr.160(%rip), %rsi
.LBB147_139:                            # %call_destructor.exit.thread
	leaq	.Lstr.152(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
.LBB147_140:                            # %call_destructor.exit.thread
	movl	%eax, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_56
.LBB147_153:                            # %assert failed199
	leaq	.Lstr.161(%rip), %rsi
	jmp	.LBB147_136
.LBB147_155:                            # %assert failed201
	leaq	.Lstr.162(%rip), %rsi
.LBB147_136:                            # %call_destructor.exit.thread
	leaq	.Lstr.152(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_56
.LBB147_157:                            # %assert failed205
	leaq	.Lstr.140(%rip), %rsi
	jmp	.LBB147_158
.LBB147_161:                            # %assert failed207
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r11, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_56
.LBB147_163:                            # %assert failed211
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_158
.LBB147_165:                            # %assert failed213
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_166
.LBB147_172:                            # %assert failed219
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_166
.LBB147_174:                            # %assert failed221
	leaq	(%rdx,%rdx,2), %rdx
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_166
.LBB147_176:                            # %assert failed225
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_158
.LBB147_178:                            # %assert failed227
	leaq	.Lstr.141(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_56
.LBB147_180:                            # %assert failed231
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_158
.LBB147_182:                            # %assert failed233
	leaq	.Lstr.138(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r10, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_56
.LBB147_184:                            # %assert failed237
	leaq	.Lstr.139(%rip), %rsi
.LBB147_158:                            # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_56
.LBB147_186:                            # %assert failed239
	leaq	.Lstr.139(%rip), %rsi
.LBB147_166:                            # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_56
.LBB147_189:                            # %assert failed241
	leaq	.Lstr.163(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbx, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_56
.LBB147_1472:                           # %assert failed547
	leaq	.Lstr.172(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdx
	jmp	.LBB147_1473
.LBB147_1511:                           # %assert failed555
	leaq	.Lstr.174(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r14, %rdx
	jmp	.LBB147_1512
.LBB147_192:                            # %assert failed243
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_56:                             # %call_destructor.exit.thread
	xorl	%r8d, %r8d
	movl	%eax, %ebx
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 5096(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4720(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4752(%rsp)        # 8-byte Spill
	xorl	%edx, %edx
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5672(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	jmp	.LBB147_207
.Lfunc_end147:
	.size	__sharpi, .Lfunc_end147-__sharpi

	.section	.text.sharpi,"ax",@progbits
	.globl	sharpi
	.align	16, 0x90
	.type	sharpi,@function
sharpi:                                 # @sharpi
# BB#0:                                 # %entry
	testq	%rdi, %rdi
	je	.LBB148_7
# BB#1:                                 # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB148_8
# BB#2:                                 # %assert succeeded11
	testq	%r8, %r8
	je	.LBB148_9
# BB#3:                                 # %assert succeeded30
	testq	%r9, %r9
	je	.LBB148_10
# BB#4:                                 # %assert succeeded49
	movq	80(%rsp), %rax
	testq	%rax, %rax
	je	.LBB148_11
# BB#5:                                 # %assert succeeded68
	movq	104(%rsp), %rax
	testq	%rax, %rax
	je	.LBB148_12
# BB#6:                                 # %assert succeeded87
	jmp	__sharpi@PLT            # TAILCALL
.LBB148_7:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_8:                              # %assert failed10
	leaq	.Lstr.138(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_9:                              # %assert failed29
	leaq	.Lstr.139(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_10:                             # %assert failed48
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_11:                             # %assert failed67
	leaq	.Lstr.141(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_12:                             # %assert failed86
	leaq	.Lstr.142(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.Lfunc_end148:
	.size	sharpi, .Lfunc_end148-sharpi

	.section	.text.sharpi_argv,"ax",@progbits
	.globl	sharpi_argv
	.align	16, 0x90
	.type	sharpi_argv,@function
sharpi_argv:                            # @sharpi_argv
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r14
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdi, %r10
	movq	(%r10), %rdi
	movq	8(%r10), %rcx
	movl	(%rcx), %esi
	movq	16(%r10), %rcx
	movl	(%rcx), %edx
	movq	24(%r10), %rcx
	movq	32(%r10), %r8
	movq	40(%r10), %rax
	vmovss	(%rax), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 108(%rsp)        # 4-byte Spill
	movq	48(%r10), %rax
	vmovss	(%rax), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	56(%r10), %rax
	vmovss	(%rax), %xmm2           # xmm2 = mem[0],zero,zero,zero
	movq	64(%r10), %rax
	vmovss	(%rax), %xmm3           # xmm3 = mem[0],zero,zero,zero
	movq	72(%r10), %rax
	vmovss	(%rax), %xmm4           # xmm4 = mem[0],zero,zero,zero
	movq	80(%r10), %rax
	vmovss	(%rax), %xmm5           # xmm5 = mem[0],zero,zero,zero
	movq	88(%r10), %rax
	vmovss	(%rax), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	96(%r10), %rax
	vmovss	(%rax), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	104(%r10), %rax
	vmovss	(%rax), %xmm8           # xmm8 = mem[0],zero,zero,zero
	movq	112(%r10), %rax
	vmovss	(%rax), %xmm9           # xmm9 = mem[0],zero,zero,zero
	movq	120(%r10), %rax
	vmovss	(%rax), %xmm10          # xmm10 = mem[0],zero,zero,zero
	movq	128(%r10), %rax
	vmovss	(%rax), %xmm11          # xmm11 = mem[0],zero,zero,zero
	movq	136(%r10), %rax
	vmovss	(%rax), %xmm12          # xmm12 = mem[0],zero,zero,zero
	movq	144(%r10), %rax
	vmovss	(%rax), %xmm13          # xmm13 = mem[0],zero,zero,zero
	movq	152(%r10), %rax
	vmovss	(%rax), %xmm14          # xmm14 = mem[0],zero,zero,zero
	movq	160(%r10), %rax
	vmovss	(%rax), %xmm15          # xmm15 = mem[0],zero,zero,zero
	movq	168(%r10), %r14
	movq	176(%r10), %r9
	movq	184(%r10), %r11
	movq	192(%r10), %rbx
	movq	200(%r10), %rax
	movl	(%rax), %ebp
	movq	208(%r10), %rax
	movzbl	(%rbx), %ebx
	vmovss	(%r14), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	%rax, 96(%rsp)
	movl	%ebp, 88(%rsp)
	movl	%ebx, 80(%rsp)
	movq	%r11, 72(%rsp)
	vmovss	%xmm0, 64(%rsp)
	vmovss	%xmm15, 56(%rsp)
	vmovss	%xmm14, 48(%rsp)
	vmovss	%xmm13, 40(%rsp)
	vmovss	%xmm12, 32(%rsp)
	vmovss	%xmm11, 24(%rsp)
	vmovss	%xmm10, 16(%rsp)
	vmovss	%xmm9, 8(%rsp)
	vmovss	%xmm8, (%rsp)
	vmovss	108(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	callq	sharpi@PLT
	addq	$112, %rsp
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end149:
	.size	sharpi_argv, .Lfunc_end149-sharpi_argv

	.section	.text.sharpi_metadata,"ax",@progbits
	.globl	sharpi_metadata
	.align	16, 0x90
	.type	sharpi_metadata,@function
sharpi_metadata:                        # @sharpi_metadata
# BB#0:                                 # %entry
	leaq	.Lsharpi_metadata_storage(%rip), %rax
	retq
.Lfunc_end150:
	.size	sharpi_metadata, .Lfunc_end150-sharpi_metadata

	.type	_ZN6Halide7Runtime8Internal13custom_mallocE,@object # @_ZN6Halide7Runtime8Internal13custom_mallocE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal13custom_mallocE
	.align	8
_ZN6Halide7Runtime8Internal13custom_mallocE:
	.quad	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.size	_ZN6Halide7Runtime8Internal13custom_mallocE, 8

	.type	_ZN6Halide7Runtime8Internal11custom_freeE,@object # @_ZN6Halide7Runtime8Internal11custom_freeE
	.weak	_ZN6Halide7Runtime8Internal11custom_freeE
	.align	8
_ZN6Halide7Runtime8Internal11custom_freeE:
	.quad	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.size	_ZN6Halide7Runtime8Internal11custom_freeE, 8

	.type	_ZN6Halide7Runtime8Internal13error_handlerE,@object # @_ZN6Halide7Runtime8Internal13error_handlerE
	.weak	_ZN6Halide7Runtime8Internal13error_handlerE
	.align	8
_ZN6Halide7Runtime8Internal13error_handlerE:
	.quad	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.size	_ZN6Halide7Runtime8Internal13error_handlerE, 8

	.type	.L.str,@object          # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"Error: "
	.size	.L.str, 8

	.type	_ZN6Halide7Runtime8Internal12custom_printE,@object # @_ZN6Halide7Runtime8Internal12custom_printE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal12custom_printE
	.align	8
_ZN6Halide7Runtime8Internal12custom_printE:
	.quad	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal12custom_printE, 8

	.type	halide_reference_clock_inited,@object # @halide_reference_clock_inited
	.bss
	.weak	halide_reference_clock_inited
halide_reference_clock_inited:
	.byte	0                       # 0x0
	.size	halide_reference_clock_inited, 1

	.type	halide_reference_clock,@object # @halide_reference_clock
	.weak	halide_reference_clock
	.align	8
halide_reference_clock:
	.zero	16
	.size	halide_reference_clock, 16

	.type	.L.str.7,@object        # @.str.7
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.7:
	.asciz	"/tmp/"
	.size	.L.str.7, 6

	.type	.L.str.1,@object        # @.str.1
.L.str.1:
	.asciz	"XXXXXX"
	.size	.L.str.1, 7

	.type	_ZN6Halide7Runtime8Internal10work_queueE,@object # @_ZN6Halide7Runtime8Internal10work_queueE
	.bss
	.weak	_ZN6Halide7Runtime8Internal10work_queueE
	.align	8
_ZN6Halide7Runtime8Internal10work_queueE:
	.zero	800
	.size	_ZN6Halide7Runtime8Internal10work_queueE, 800

	.type	custom_do_task,@object  # @custom_do_task
	.section	.data.rel,"aw",@progbits
	.weak	custom_do_task
	.align	8
custom_do_task:
	.quad	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.size	custom_do_task, 8

	.type	custom_do_par_for,@object # @custom_do_par_for
	.weak	custom_do_par_for
	.align	8
custom_do_par_for:
	.quad	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.size	custom_do_par_for, 8

	.section	.dtors,"aw",@progbits
	.align	8
	.quad	halide_thread_pool_cleanup
	.quad	halide_trace_cleanup
	.quad	halide_cache_cleanup
	.quad	halide_profiler_shutdown
	.type	.L.str.8,@object        # @.str.8
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.8:
	.asciz	"HL_NUM_THREADS"
	.size	.L.str.8, 15

	.type	.L.str.1.9,@object      # @.str.1.9
.L.str.1.9:
	.asciz	"HL_NUMTHREADS"
	.size	.L.str.1.9, 14

	.type	.L.str.2,@object        # @.str.2
.L.str.2:
	.asciz	"halide_set_num_threads: must be >= 0."
	.size	.L.str.2, 38

	.type	_ZN6Halide7Runtime8Internal17custom_get_symbolE,@object # @_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.align	8
_ZN6Halide7Runtime8Internal17custom_get_symbolE:
	.quad	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.size	_ZN6Halide7Runtime8Internal17custom_get_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal19custom_load_libraryE,@object # @_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.weak	_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.align	8
_ZN6Halide7Runtime8Internal19custom_load_libraryE:
	.quad	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.size	_ZN6Halide7Runtime8Internal19custom_load_libraryE, 8

	.type	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE,@object # @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.weak	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.align	8
_ZN6Halide7Runtime8Internal25custom_get_library_symbolE:
	.quad	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE,@object # @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.align	4
_ZN6Halide7Runtime8Internal17halide_gpu_deviceE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, 1

	.type	.L.str.10,@object       # @.str.10
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.10:
	.asciz	"HL_GPU_DEVICE"
	.size	.L.str.10, 14

	.type	_ZN6Halide7Runtime8Internal17halide_trace_fileE,@object # @_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.align	4
_ZN6Halide7Runtime8Internal17halide_trace_fileE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_trace_fileE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_trace_file_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE,@object # @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
	.weak	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, 1

	.type	_ZN6Halide7Runtime8Internal19halide_custom_traceE,@object # @_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.align	8
_ZN6Halide7Runtime8Internal19halide_custom_traceE:
	.quad	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.size	_ZN6Halide7Runtime8Internal19halide_custom_traceE, 8

	.type	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids
	.data
	.align	4
_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids:
	.long	1                       # 0x1
	.size	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids, 4

	.type	.L.str.14,@object       # @.str.14
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.14:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:59 Assert failed: written == total_size && \"Can't write to trace file\"\n"
	.size	.L.str.14, 112

	.type	.L.str.1.15,@object     # @.str.1.15
.L.str.1.15:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:68 Assert failed: print_bits <= 64 && \"Tracing bad type\"\n"
	.size	.L.str.1.15, 98

	.type	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types
	.section	.data.rel.ro.local,"aw",@progbits
	.align	8
.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types:
	.quad	.L.str.2.17
	.quad	.L.str.3
	.quad	.L.str.4
	.quad	.L.str.5
	.quad	.L.str.6
	.quad	.L.str.7.18
	.quad	.L.str.8.19
	.quad	.L.str.9
	.quad	.L.str.10.20
	.quad	.L.str.11
	.size	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types, 80

	.type	.L.str.15,@object       # @.str.15
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.15:
	.asciz	"<"
	.size	.L.str.15, 2

	.type	.L.str.16,@object       # @.str.16
.L.str.16:
	.asciz	">, <"
	.size	.L.str.16, 5

	.type	.L.str.17,@object       # @.str.17
.L.str.17:
	.asciz	", "
	.size	.L.str.17, 3

	.type	.L.str.18,@object       # @.str.18
.L.str.18:
	.asciz	">)"
	.size	.L.str.18, 3

	.type	.L.str.20,@object       # @.str.20
.L.str.20:
	.asciz	" = <"
	.size	.L.str.20, 5

	.type	.L.str.21,@object       # @.str.21
.L.str.21:
	.asciz	" = "
	.size	.L.str.21, 4

	.type	.L.str.22,@object       # @.str.22
.L.str.22:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:136 Assert failed: print_bits >= 16 && \"Tracing a bad type\"\n"
	.size	.L.str.22, 101

	.type	.L.str.23,@object       # @.str.23
.L.str.23:
	.asciz	">"
	.size	.L.str.23, 2

	.type	.L.str.25,@object       # @.str.25
.L.str.25:
	.asciz	"HL_TRACE_FILE"
	.size	.L.str.25, 14

	.type	.L.str.26,@object       # @.str.26
.L.str.26:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:194 Assert failed: (fd > 0) && \"Failed to open trace file\\n\"\n"
	.size	.L.str.26, 102

	.type	.L.str.2.17,@object     # @.str.2.17
.L.str.2.17:
	.asciz	"Load"
	.size	.L.str.2.17, 5

	.type	.L.str.3,@object        # @.str.3
.L.str.3:
	.asciz	"Store"
	.size	.L.str.3, 6

	.type	.L.str.4,@object        # @.str.4
.L.str.4:
	.asciz	"Begin realization"
	.size	.L.str.4, 18

	.type	.L.str.5,@object        # @.str.5
.L.str.5:
	.asciz	"End realization"
	.size	.L.str.5, 16

	.type	.L.str.6,@object        # @.str.6
.L.str.6:
	.asciz	"Produce"
	.size	.L.str.6, 8

	.type	.L.str.7.18,@object     # @.str.7.18
.L.str.7.18:
	.asciz	"End produce"
	.size	.L.str.7.18, 12

	.type	.L.str.8.19,@object     # @.str.8.19
.L.str.8.19:
	.asciz	"Consume"
	.size	.L.str.8.19, 8

	.type	.L.str.9,@object        # @.str.9
.L.str.9:
	.asciz	"End consume"
	.size	.L.str.9, 12

	.type	.L.str.10.20,@object    # @.str.10.20
.L.str.10.20:
	.asciz	"Begin pipeline"
	.size	.L.str.10.20, 15

	.type	.L.str.11,@object       # @.str.11
.L.str.11:
	.asciz	"End pipeline"
	.size	.L.str.11, 13

	.type	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE,@object # @_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.data
	.weak	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.align	2
_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE:
	.short	3                       # 0x3
	.short	3                       # 0x3
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.size	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE, 20

	.type	.L.str.27,@object       # @.str.27
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.27:
	.asciz	"wb"
	.size	.L.str.27, 3

	.type	_ZN6Halide7Runtime8Internal16memoization_lockE,@object # @_ZN6Halide7Runtime8Internal16memoization_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal16memoization_lockE
	.align	8
_ZN6Halide7Runtime8Internal16memoization_lockE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal16memoization_lockE, 64

	.type	_ZN6Halide7Runtime8Internal13cache_entriesE,@object # @_ZN6Halide7Runtime8Internal13cache_entriesE
	.weak	_ZN6Halide7Runtime8Internal13cache_entriesE
	.align	8
_ZN6Halide7Runtime8Internal13cache_entriesE:
	.zero	2048
	.size	_ZN6Halide7Runtime8Internal13cache_entriesE, 2048

	.type	_ZN6Halide7Runtime8Internal18most_recently_usedE,@object # @_ZN6Halide7Runtime8Internal18most_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal18most_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal18most_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal18most_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal19least_recently_usedE,@object # @_ZN6Halide7Runtime8Internal19least_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal19least_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal19least_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal19least_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal14max_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.data
	.weak	_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal14max_cache_sizeE:
	.quad	1048576                 # 0x100000
	.size	_ZN6Halide7Runtime8Internal14max_cache_sizeE, 8

	.type	_ZN6Halide7Runtime8Internal18current_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.bss
	.weak	_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal18current_cache_sizeE:
	.quad	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal18current_cache_sizeE, 8

	.type	.L.str.3.29,@object     # @.str.3.29
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.3.29:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:245 Assert failed: prev_hash_entry != NULL\n"
	.size	.L.str.3.29, 82

	.type	.L.str.4.30,@object     # @.str.4.30
.L.str.4.30:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:335 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.4.30, 85

	.type	.L.str.5.31,@object     # @.str.5.31
.L.str.5.31:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:339 Assert failed: least_recently_used == entry\n"
	.size	.L.str.5.31, 87

	.type	.L.str.6.32,@object     # @.str.6.32
.L.str.6.32:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:342 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.6.32, 85

	.type	.L.str.8.33,@object     # @.str.8.33
.L.str.8.33:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:433 Assert failed: no_host_pointers_equal\n"
	.size	.L.str.8.33, 81

	.type	.L.str.11.34,@object    # @.str.11.34
.L.str.11.34:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:518 Assert failed: entry->in_use_count > 0\n"
	.size	.L.str.11.34, 82

	.type	.L.str.45,@object       # @.str.45
.L.str.45:
	.asciz	"-nan"
	.size	.L.str.45, 5

	.type	.L.str.1.46,@object     # @.str.1.46
.L.str.1.46:
	.asciz	"nan"
	.size	.L.str.1.46, 4

	.type	.L.str.2.47,@object     # @.str.2.47
.L.str.2.47:
	.asciz	"-inf"
	.size	.L.str.2.47, 5

	.type	.L.str.3.48,@object     # @.str.3.48
.L.str.3.48:
	.asciz	"inf"
	.size	.L.str.3.48, 4

	.type	.L.str.4.49,@object     # @.str.4.49
.L.str.4.49:
	.asciz	"-0.000000e+00"
	.size	.L.str.4.49, 14

	.type	.L.str.5.50,@object     # @.str.5.50
.L.str.5.50:
	.asciz	"0.000000e+00"
	.size	.L.str.5.50, 13

	.type	.L.str.6.51,@object     # @.str.6.51
.L.str.6.51:
	.asciz	"-0.000000"
	.size	.L.str.6.51, 10

	.type	.L.str.7.52,@object     # @.str.7.52
.L.str.7.52:
	.asciz	"0.000000"
	.size	.L.str.7.52, 9

	.type	.L.str.8.53,@object     # @.str.8.53
.L.str.8.53:
	.asciz	"-"
	.size	.L.str.8.53, 2

	.type	.L.str.10.55,@object    # @.str.10.55
.L.str.10.55:
	.asciz	"e+"
	.size	.L.str.10.55, 3

	.type	.L.str.11.56,@object    # @.str.11.56
.L.str.11.56:
	.asciz	"e-"
	.size	.L.str.11.56, 3

	.type	.L.str.12.57,@object    # @.str.12.57
.L.str.12.57:
	.asciz	"0123456789abcdef"
	.size	.L.str.12.57, 17

	.type	_ZN6Halide7Runtime8Internal17device_copy_mutexE,@object # @_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.align	8
_ZN6Halide7Runtime8Internal17device_copy_mutexE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal17device_copy_mutexE, 64

	.type	.L.str.25.64,@object    # @.str.25.64
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.25.64:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:138 Assert failed: !buf->host_dirty\n"
	.size	.L.str.25.64, 86

	.type	.L.str.40,@object       # @.str.40
.L.str.40:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:248 Assert failed: buf->dev == 0\n"
	.size	.L.str.40, 83

	.type	.L.str.37,@object       # @.str.37
.L.str.37:
	.asciz	"halide_device_malloc doesn't support switching interfaces\n"
	.size	.L.str.37, 59

	.type	.L.str.42,@object       # @.str.42
.L.str.42:
	.asciz	"halide_device_and_host_malloc doesn't support switching interfaces\n"
	.size	.L.str.42, 68

	.type	.L.str.43,@object       # @.str.43
.L.str.43:
	.asciz	"allocating host and device memory failed\n"
	.size	.L.str.43, 42

	.type	.L.str.45.65,@object    # @.str.45.65
.L.str.45.65:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:322 Assert failed: buf->dev == 0\n"
	.size	.L.str.45.65, 83

	.type	.L.str.68,@object       # @.str.68
.L.str.68:
	.asciz	"Bounds inference call to external stage "
	.size	.L.str.68, 41

	.type	.L.str.1.69,@object     # @.str.1.69
.L.str.1.69:
	.asciz	" returned non-zero value: "
	.size	.L.str.1.69, 27

	.type	.L.str.53,@object       # @.str.53
.L.str.53:
	.asciz	"Printer buffer allocation failed.\n"
	.size	.L.str.53, 35

	.type	.L.str.2.70,@object     # @.str.2.70
.L.str.2.70:
	.asciz	"Call to external stage "
	.size	.L.str.2.70, 24

	.type	.L.str.3.71,@object     # @.str.3.71
.L.str.3.71:
	.asciz	"Bounds given for "
	.size	.L.str.3.71, 18

	.type	.L.str.4.72,@object     # @.str.4.72
.L.str.4.72:
	.asciz	" in "
	.size	.L.str.4.72, 5

	.type	.L.str.5.73,@object     # @.str.5.73
.L.str.5.73:
	.asciz	" (from "
	.size	.L.str.5.73, 8

	.type	.L.str.6.74,@object     # @.str.6.74
.L.str.6.74:
	.asciz	" to "
	.size	.L.str.6.74, 5

	.type	.L.str.7.75,@object     # @.str.7.75
.L.str.7.75:
	.asciz	") do not cover required region (from "
	.size	.L.str.7.75, 38

	.type	.L.str.8.76,@object     # @.str.8.76
.L.str.8.76:
	.asciz	")"
	.size	.L.str.8.76, 2

	.type	.L.str.9.77,@object     # @.str.9.77
.L.str.9.77:
	.asciz	" has type "
	.size	.L.str.9.77, 11

	.type	.L.str.10.78,@object    # @.str.10.78
.L.str.10.78:
	.asciz	" but elem_size of the buffer passed in is "
	.size	.L.str.10.78, 43

	.type	.L.str.11.79,@object    # @.str.11.79
.L.str.11.79:
	.asciz	" instead of "
	.size	.L.str.11.79, 13

	.type	.L.str.12.80,@object    # @.str.12.80
.L.str.12.80:
	.asciz	" is accessed at "
	.size	.L.str.12.80, 17

	.type	.L.str.13.81,@object    # @.str.13.81
.L.str.13.81:
	.asciz	", which is before the min ("
	.size	.L.str.13.81, 28

	.type	.L.str.14.82,@object    # @.str.14.82
.L.str.14.82:
	.asciz	") in dimension "
	.size	.L.str.14.82, 16

	.type	.L.str.15.83,@object    # @.str.15.83
.L.str.15.83:
	.asciz	", which is beyond the max ("
	.size	.L.str.15.83, 28

	.type	.L.str.16.84,@object    # @.str.16.84
.L.str.16.84:
	.asciz	"Total allocation for buffer "
	.size	.L.str.16.84, 29

	.type	.L.str.17.85,@object    # @.str.17.85
.L.str.17.85:
	.asciz	" is "
	.size	.L.str.17.85, 5

	.type	.L.str.18.86,@object    # @.str.18.86
.L.str.18.86:
	.asciz	", which exceeds the maximum size of "
	.size	.L.str.18.86, 37

	.type	.L.str.19.87,@object    # @.str.19.87
.L.str.19.87:
	.asciz	"The extents for buffer "
	.size	.L.str.19.87, 24

	.type	.L.str.20.88,@object    # @.str.20.88
.L.str.20.88:
	.asciz	" dimension "
	.size	.L.str.20.88, 12

	.type	.L.str.21.89,@object    # @.str.21.89
.L.str.21.89:
	.asciz	" is negative ("
	.size	.L.str.21.89, 15

	.type	.L.str.22.90,@object    # @.str.22.90
.L.str.22.90:
	.asciz	"Product of extents for buffer "
	.size	.L.str.22.90, 31

	.type	.L.str.23.91,@object    # @.str.23.91
.L.str.23.91:
	.asciz	"Applying the constraints on "
	.size	.L.str.23.91, 29

	.type	.L.str.24.92,@object    # @.str.24.92
.L.str.24.92:
	.asciz	" to the required region made it smaller. "
	.size	.L.str.24.92, 42

	.type	.L.str.25.93,@object    # @.str.25.93
.L.str.25.93:
	.asciz	"Required size: "
	.size	.L.str.25.93, 16

	.type	.L.str.26.94,@object    # @.str.26.94
.L.str.26.94:
	.asciz	". "
	.size	.L.str.26.94, 3

	.type	.L.str.27.95,@object    # @.str.27.95
.L.str.27.95:
	.asciz	"Constrained size: "
	.size	.L.str.27.95, 19

	.type	.L.str.28,@object       # @.str.28
.L.str.28:
	.asciz	"."
	.size	.L.str.28, 2

	.type	.L.str.29,@object       # @.str.29
.L.str.29:
	.asciz	"Constraint violated: "
	.size	.L.str.29, 22

	.type	.L.str.30,@object       # @.str.30
.L.str.30:
	.asciz	" ("
	.size	.L.str.30, 3

	.type	.L.str.31,@object       # @.str.31
.L.str.31:
	.asciz	") == "
	.size	.L.str.31, 6

	.type	.L.str.32,@object       # @.str.32
.L.str.32:
	.asciz	"Parameter "
	.size	.L.str.32, 11

	.type	.L.str.33,@object       # @.str.33
.L.str.33:
	.asciz	" but must be at least "
	.size	.L.str.33, 23

	.type	.L.str.34,@object       # @.str.34
.L.str.34:
	.asciz	" but must be at most "
	.size	.L.str.34, 22

	.type	.L.str.35,@object       # @.str.35
.L.str.35:
	.asciz	"Out of memory (halide_malloc returned NULL)"
	.size	.L.str.35, 44

	.type	.L.str.36,@object       # @.str.36
.L.str.36:
	.asciz	"Buffer argument "
	.size	.L.str.36, 17

	.type	.L.str.37.96,@object    # @.str.37.96
.L.str.37.96:
	.asciz	" is NULL"
	.size	.L.str.37.96, 9

	.type	.L.str.38,@object       # @.str.38
.L.str.38:
	.asciz	"Failed to dump function "
	.size	.L.str.38, 25

	.type	.L.str.39,@object       # @.str.39
.L.str.39:
	.asciz	" to file "
	.size	.L.str.39, 10

	.type	.L.str.40.97,@object    # @.str.40.97
.L.str.40.97:
	.asciz	" with error "
	.size	.L.str.40.97, 13

	.type	.L.str.41,@object       # @.str.41
.L.str.41:
	.asciz	"The host pointer of "
	.size	.L.str.41, 21

	.type	.L.str.42.98,@object    # @.str.42.98
.L.str.42.98:
	.asciz	" is not aligned to a "
	.size	.L.str.42.98, 22

	.type	.L.str.43.99,@object    # @.str.43.99
.L.str.43.99:
	.asciz	" bytes boundary."
	.size	.L.str.43.99, 17

	.type	.L.str.44,@object       # @.str.44
.L.str.44:
	.asciz	"The folded storage dimension "
	.size	.L.str.44, 30

	.type	.L.str.45.100,@object   # @.str.45.100
.L.str.45.100:
	.asciz	" of "
	.size	.L.str.45.100, 5

	.type	.L.str.46.101,@object   # @.str.46.101
.L.str.46.101:
	.asciz	" was accessed out of order by loop "
	.size	.L.str.46.101, 36

	.type	.L.str.47,@object       # @.str.47
.L.str.47:
	.asciz	"The fold factor ("
	.size	.L.str.47, 18

	.type	.L.str.48,@object       # @.str.48
.L.str.48:
	.asciz	") of dimension "
	.size	.L.str.48, 16

	.type	.L.str.49,@object       # @.str.49
.L.str.49:
	.asciz	" is too small to store the required region accessed by loop "
	.size	.L.str.49, 61

	.type	.L.str.50,@object       # @.str.50
.L.str.50:
	.asciz	")."
	.size	.L.str.50, 3

	.type	.L.str.51,@object       # @.str.51
.L.str.51:
	.asciz	"Requirement Failed: ("
	.size	.L.str.51, 22

	.type	.L.str.52,@object       # @.str.52
.L.str.52:
	.asciz	") "
	.size	.L.str.52, 3

	.type	_ZZ25halide_profiler_get_stateE1s,@object # @_ZZ25halide_profiler_get_stateE1s
	.data
	.align	8
_ZZ25halide_profiler_get_stateE1s:
	.zero	64
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	0                       # 0x0
	.long	0                       # 0x0
	.quad	0
	.quad	0
	.byte	0                       # 0x0
	.zero	7
	.size	_ZZ25halide_profiler_get_stateE1s, 104

	.type	.L.str.103,@object      # @.str.103
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.103:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:204 Assert failed: p_stats != NULL\n"
	.size	.L.str.103, 77

	.type	.L.str.1.104,@object    # @.str.1.104
.L.str.1.104:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:231 Assert failed: p_stats != NULL\n"
	.size	.L.str.1.104, 77

	.type	.L.str.2.105,@object    # @.str.2.105
.L.str.2.105:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:232 Assert failed: func_id >= 0\n"
	.size	.L.str.2.105, 74

	.type	.L.str.3.106,@object    # @.str.3.106
.L.str.3.106:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:233 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.3.106, 90

	.type	.L.str.4.107,@object    # @.str.4.107
.L.str.4.107:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:267 Assert failed: p_stats != NULL\n"
	.size	.L.str.4.107, 77

	.type	.L.str.5.108,@object    # @.str.5.108
.L.str.5.108:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:268 Assert failed: func_id >= 0\n"
	.size	.L.str.5.108, 74

	.type	.L.str.6.109,@object    # @.str.6.109
.L.str.6.109:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:269 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.6.109, 90

	.type	.L.str.7.110,@object    # @.str.7.110
.L.str.7.110:
	.asciz	"\n"
	.size	.L.str.7.110, 2

	.type	.L.str.8.111,@object    # @.str.8.111
.L.str.8.111:
	.asciz	" total time: "
	.size	.L.str.8.111, 14

	.type	.L.str.9.112,@object    # @.str.9.112
.L.str.9.112:
	.asciz	" ms"
	.size	.L.str.9.112, 4

	.type	.L.str.10.113,@object   # @.str.10.113
.L.str.10.113:
	.asciz	"  samples: "
	.size	.L.str.10.113, 12

	.type	.L.str.11.114,@object   # @.str.11.114
.L.str.11.114:
	.asciz	"  runs: "
	.size	.L.str.11.114, 9

	.type	.L.str.12.115,@object   # @.str.12.115
.L.str.12.115:
	.asciz	"  time/run: "
	.size	.L.str.12.115, 13

	.type	.L.str.13.116,@object   # @.str.13.116
.L.str.13.116:
	.asciz	" ms\n"
	.size	.L.str.13.116, 5

	.type	.L.str.14.117,@object   # @.str.14.117
.L.str.14.117:
	.asciz	" average threads used: "
	.size	.L.str.14.117, 24

	.type	.L.str.15.118,@object   # @.str.15.118
.L.str.15.118:
	.asciz	" heap allocations: "
	.size	.L.str.15.118, 20

	.type	.L.str.16.119,@object   # @.str.16.119
.L.str.16.119:
	.asciz	"  peak heap usage: "
	.size	.L.str.16.119, 20

	.type	.L.str.17.120,@object   # @.str.17.120
.L.str.17.120:
	.asciz	" bytes\n"
	.size	.L.str.17.120, 8

	.type	.L.str.18.121,@object   # @.str.18.121
.L.str.18.121:
	.asciz	"  "
	.size	.L.str.18.121, 3

	.type	.L.str.19.122,@object   # @.str.19.122
.L.str.19.122:
	.asciz	": "
	.size	.L.str.19.122, 3

	.type	.L.str.20.123,@object   # @.str.20.123
.L.str.20.123:
	.asciz	" "
	.size	.L.str.20.123, 2

	.type	.L.str.21.124,@object   # @.str.21.124
.L.str.21.124:
	.asciz	"ms"
	.size	.L.str.21.124, 3

	.type	.L.str.22.125,@object   # @.str.22.125
.L.str.22.125:
	.asciz	"("
	.size	.L.str.22.125, 2

	.type	.L.str.23.126,@object   # @.str.23.126
.L.str.23.126:
	.asciz	"%)"
	.size	.L.str.23.126, 3

	.type	.L.str.24.127,@object   # @.str.24.127
.L.str.24.127:
	.asciz	"threads: "
	.size	.L.str.24.127, 10

	.type	.L.str.25.128,@object   # @.str.25.128
.L.str.25.128:
	.asciz	" peak: "
	.size	.L.str.25.128, 8

	.type	.L.str.26.129,@object   # @.str.26.129
.L.str.26.129:
	.asciz	" num: "
	.size	.L.str.26.129, 7

	.type	.L.str.27.130,@object   # @.str.27.130
.L.str.27.130:
	.asciz	" avg: "
	.size	.L.str.27.130, 7

	.type	.L.str.28.131,@object   # @.str.28.131
.L.str.28.131:
	.asciz	" stack: "
	.size	.L.str.28.131, 9

	.type	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE,@object # @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.align	8
_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE:
	.quad	halide_default_can_use_target_features
	.size	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, 8

	.type	_ZZ38halide_default_can_use_target_featuresE11initialized,@object # @_ZZ38halide_default_can_use_target_featuresE11initialized
	.local	_ZZ38halide_default_can_use_target_featuresE11initialized
	.comm	_ZZ38halide_default_can_use_target_featuresE11initialized,1,1
	.type	_ZZ38halide_default_can_use_target_featuresE12cpu_features,@object # @_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.local	_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.comm	_ZZ38halide_default_can_use_target_featuresE12cpu_features,16,8
	.type	.Lstr,@object           # @str
	.section	.rodata,"a",@progbits
	.align	32
.Lstr:
	.asciz	"p0"
	.size	.Lstr, 3

	.type	.Lstr.138,@object       # @str.138
	.align	32
.Lstr.138:
	.asciz	"vignetteH"
	.size	.Lstr.138, 10

	.type	.Lstr.139,@object       # @str.139
	.align	32
.Lstr.139:
	.asciz	"vignetteV"
	.size	.Lstr.139, 10

	.type	.Lstr.140,@object       # @str.140
	.align	32
.Lstr.140:
	.asciz	"ccm"
	.size	.Lstr.140, 4

	.type	.Lstr.141,@object       # @str.141
	.align	32
.Lstr.141:
	.asciz	"toneTable"
	.size	.Lstr.141, 10

	.type	.Lstr.142,@object       # @str.142
	.align	32
.Lstr.142:
	.asciz	"sharpi"
	.size	.Lstr.142, 7

	.type	.Lstr.143,@object       # @str.143
	.align	32
.Lstr.143:
	.asciz	"Input buffer ccm"
	.size	.Lstr.143, 17

	.type	.Lstr.144,@object       # @str.144
	.align	32
.Lstr.144:
	.asciz	"float32"
	.size	.Lstr.144, 8

	.type	.Lstr.145,@object       # @str.145
	.align	32
.Lstr.145:
	.asciz	"Input buffer p0"
	.size	.Lstr.145, 16

	.type	.Lstr.146,@object       # @str.146
	.align	32
.Lstr.146:
	.asciz	"uint16"
	.size	.Lstr.146, 7

	.type	.Lstr.147,@object       # @str.147
	.align	32
.Lstr.147:
	.asciz	"Output buffer sharpi"
	.size	.Lstr.147, 21

	.type	.Lstr.148,@object       # @str.148
	.align	32
.Lstr.148:
	.asciz	"Input buffer toneTable"
	.size	.Lstr.148, 23

	.type	.Lstr.149,@object       # @str.149
	.align	32
.Lstr.149:
	.asciz	"Input buffer vignetteH"
	.size	.Lstr.149, 23

	.type	.Lstr.150,@object       # @str.150
	.align	32
.Lstr.150:
	.asciz	"Input buffer vignetteV"
	.size	.Lstr.150, 23

	.type	.Lstr.151,@object       # @str.151
	.align	32
.Lstr.151:
	.asciz	"ccm.stride.0"
	.size	.Lstr.151, 13

	.type	.Lstr.152,@object       # @str.152
	.align	32
.Lstr.152:
	.asciz	"1"
	.size	.Lstr.152, 2

	.type	.Lstr.153,@object       # @str.153
	.align	32
.Lstr.153:
	.asciz	"p0.stride.0"
	.size	.Lstr.153, 12

	.type	.Lstr.154,@object       # @str.154
	.align	32
.Lstr.154:
	.asciz	"sharpi.stride.0"
	.size	.Lstr.154, 16

	.type	.Lstr.155,@object       # @str.155
	.align	32
.Lstr.155:
	.asciz	"3"
	.size	.Lstr.155, 2

	.type	.Lstr.156,@object       # @str.156
	.align	32
.Lstr.156:
	.asciz	"sharpi.stride.2"
	.size	.Lstr.156, 16

	.type	.Lstr.157,@object       # @str.157
	.align	32
.Lstr.157:
	.asciz	"sharpi.min.2"
	.size	.Lstr.157, 13

	.type	.Lstr.158,@object       # @str.158
	.align	32
.Lstr.158:
	.asciz	"0"
	.size	.Lstr.158, 2

	.type	.Lstr.159,@object       # @str.159
	.align	32
.Lstr.159:
	.asciz	"sharpi.extent.2"
	.size	.Lstr.159, 16

	.type	.Lstr.160,@object       # @str.160
	.align	32
.Lstr.160:
	.asciz	"toneTable.stride.0"
	.size	.Lstr.160, 19

	.type	.Lstr.161,@object       # @str.161
	.align	32
.Lstr.161:
	.asciz	"vignetteH.stride.0"
	.size	.Lstr.161, 19

	.type	.Lstr.162,@object       # @str.162
	.align	32
.Lstr.162:
	.asciz	"vignetteV.stride.0"
	.size	.Lstr.162, 19

	.type	.Lstr.163,@object       # @str.163
	.align	32
.Lstr.163:
	.asciz	"f0"
	.size	.Lstr.163, 3

	.type	.Lstr.164,@object       # @str.164
	.align	32
.Lstr.164:
	.asciz	"deinterleaved$1"
	.size	.Lstr.164, 16

	.type	.Lstr.165,@object       # @str.165
	.align	32
.Lstr.165:
	.asciz	"gH"
	.size	.Lstr.165, 3

	.type	.Lstr.167,@object       # @str.167
	.align	32
.Lstr.167:
	.asciz	"dV"
	.size	.Lstr.167, 3

	.type	.Lstr.169,@object       # @str.169
	.align	32
.Lstr.169:
	.asciz	"f4"
	.size	.Lstr.169, 3

	.type	.Lstr.170,@object       # @str.170
	.align	32
.Lstr.170:
	.asciz	"f7"
	.size	.Lstr.170, 3

	.type	.Lstr.172,@object       # @str.172
	.align	32
.Lstr.172:
	.asciz	"transpose"
	.size	.Lstr.172, 10

	.type	.Lstr.173,@object       # @str.173
	.align	32
.Lstr.173:
	.asciz	"blur"
	.size	.Lstr.173, 5

	.type	.Lstr.174,@object       # @str.174
	.align	32
.Lstr.174:
	.asciz	"transpose$1"
	.size	.Lstr.174, 12

	.type	.Lstr.175,@object       # @str.175
	.align	32
.Lstr.175:
	.asciz	"blur$1"
	.size	.Lstr.175, 7

	.type	.Lstr.176,@object       # @str.176
	.align	32
.Lstr.176:
	.asciz	"p1"
	.size	.Lstr.176, 3

	.type	.L__unnamed_1,@object   # @0
	.align	4
.L__unnamed_1:
	.long	0                       # 0x0
	.size	.L__unnamed_1, 4

	.type	.Lstr.177,@object       # @str.177
	.align	32
.Lstr.177:
	.asciz	"p2"
	.size	.Lstr.177, 3

	.type	.L__unnamed_2,@object   # @1
	.align	4
.L__unnamed_2:
	.long	0                       # 0x0
	.size	.L__unnamed_2, 4

	.type	.Lstr.178,@object       # @str.178
	.align	32
.Lstr.178:
	.asciz	"blackLevelR"
	.size	.Lstr.178, 12

	.type	.L__unnamed_3,@object   # @2
	.align	4
.L__unnamed_3:
	.long	0                       # float 0
	.size	.L__unnamed_3, 4

	.type	.Lstr.179,@object       # @str.179
	.align	32
.Lstr.179:
	.asciz	"blackLevelG"
	.size	.Lstr.179, 12

	.type	.L__unnamed_4,@object   # @3
	.align	4
.L__unnamed_4:
	.long	0                       # float 0
	.size	.L__unnamed_4, 4

	.type	.Lstr.180,@object       # @str.180
	.align	32
.Lstr.180:
	.asciz	"blackLevelB"
	.size	.Lstr.180, 12

	.type	.L__unnamed_5,@object   # @4
	.align	4
.L__unnamed_5:
	.long	0                       # float 0
	.size	.L__unnamed_5, 4

	.type	.Lstr.181,@object       # @str.181
	.align	32
.Lstr.181:
	.asciz	"whiteBalanceGainR"
	.size	.Lstr.181, 18

	.type	.L__unnamed_6,@object   # @5
	.align	4
.L__unnamed_6:
	.long	0                       # float 0
	.size	.L__unnamed_6, 4

	.type	.Lstr.182,@object       # @str.182
	.align	32
.Lstr.182:
	.asciz	"whiteBalanceGainG"
	.size	.Lstr.182, 18

	.type	.L__unnamed_7,@object   # @6
	.align	4
.L__unnamed_7:
	.long	0                       # float 0
	.size	.L__unnamed_7, 4

	.type	.Lstr.183,@object       # @str.183
	.align	32
.Lstr.183:
	.asciz	"whiteBalanceGainB"
	.size	.Lstr.183, 18

	.type	.L__unnamed_8,@object   # @7
	.align	4
.L__unnamed_8:
	.long	0                       # float 0
	.size	.L__unnamed_8, 4

	.type	.Lstr.184,@object       # @str.184
	.align	32
.Lstr.184:
	.asciz	"clampMinR"
	.size	.Lstr.184, 10

	.type	.L__unnamed_9,@object   # @8
	.align	4
.L__unnamed_9:
	.long	0                       # float 0
	.size	.L__unnamed_9, 4

	.type	.Lstr.185,@object       # @str.185
	.align	32
.Lstr.185:
	.asciz	"clampMinG"
	.size	.Lstr.185, 10

	.type	.L__unnamed_10,@object  # @9
	.align	4
.L__unnamed_10:
	.long	0                       # float 0
	.size	.L__unnamed_10, 4

	.type	.Lstr.186,@object       # @str.186
	.align	32
.Lstr.186:
	.asciz	"clampMinB"
	.size	.Lstr.186, 10

	.type	.L__unnamed_11,@object  # @10
	.align	4
.L__unnamed_11:
	.long	0                       # float 0
	.size	.L__unnamed_11, 4

	.type	.Lstr.187,@object       # @str.187
	.align	32
.Lstr.187:
	.asciz	"clampMaxR"
	.size	.Lstr.187, 10

	.type	.L__unnamed_12,@object  # @11
	.align	4
.L__unnamed_12:
	.long	0                       # float 0
	.size	.L__unnamed_12, 4

	.type	.Lstr.188,@object       # @str.188
	.align	32
.Lstr.188:
	.asciz	"clampMaxG"
	.size	.Lstr.188, 10

	.type	.L__unnamed_13,@object  # @12
	.align	4
.L__unnamed_13:
	.long	0                       # float 0
	.size	.L__unnamed_13, 4

	.type	.Lstr.189,@object       # @str.189
	.align	32
.Lstr.189:
	.asciz	"clampMaxB"
	.size	.Lstr.189, 10

	.type	.L__unnamed_14,@object  # @13
	.align	4
.L__unnamed_14:
	.long	0                       # float 0
	.size	.L__unnamed_14, 4

	.type	.Lstr.190,@object       # @str.190
	.align	32
.Lstr.190:
	.asciz	"sharpenningR"
	.size	.Lstr.190, 13

	.type	.L__unnamed_15,@object  # @14
	.align	4
.L__unnamed_15:
	.long	0                       # float 0
	.size	.L__unnamed_15, 4

	.type	.Lstr.191,@object       # @str.191
	.align	32
.Lstr.191:
	.asciz	"sharpenningG"
	.size	.Lstr.191, 13

	.type	.L__unnamed_16,@object  # @15
	.align	4
.L__unnamed_16:
	.long	0                       # float 0
	.size	.L__unnamed_16, 4

	.type	.Lstr.192,@object       # @str.192
	.align	32
.Lstr.192:
	.asciz	"sharpenninngB"
	.size	.Lstr.192, 14

	.type	.L__unnamed_17,@object  # @16
	.align	4
.L__unnamed_17:
	.long	0                       # float 0
	.size	.L__unnamed_17, 4

	.type	.Lstr.193,@object       # @str.193
	.align	32
.Lstr.193:
	.asciz	"sharpeningSupport"
	.size	.Lstr.193, 18

	.type	.L__unnamed_18,@object  # @17
	.align	4
.L__unnamed_18:
	.long	0                       # float 0
	.size	.L__unnamed_18, 4

	.type	.Lstr.194,@object       # @str.194
	.align	32
.Lstr.194:
	.asciz	"noiseCore"
	.size	.Lstr.194, 10

	.type	.L__unnamed_19,@object  # @18
	.align	4
.L__unnamed_19:
	.long	0                       # float 0
	.size	.L__unnamed_19, 4

	.type	.Lstr.195,@object       # @str.195
	.align	32
.Lstr.195:
	.asciz	"p3"
	.size	.Lstr.195, 3

	.type	.L__unnamed_20,@object  # @19
.L__unnamed_20:
	.byte	0                       # 0x0
	.size	.L__unnamed_20, 1

	.type	.Lstr.196,@object       # @str.196
	.align	32
.Lstr.196:
	.asciz	"bayerPattern"
	.size	.Lstr.196, 13

	.type	.L__unnamed_21,@object  # @20
	.align	4
.L__unnamed_21:
	.long	0                       # 0x0
	.size	.L__unnamed_21, 4

	.type	.L__unnamed_22,@object  # @21
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.L__unnamed_22:
	.quad	.Lstr
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	16                      # 0x10
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.176
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_1
	.quad	0
	.quad	0
	.quad	.Lstr.177
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_2
	.quad	0
	.quad	0
	.quad	.Lstr.138
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.139
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.178
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_3
	.quad	0
	.quad	0
	.quad	.Lstr.179
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_4
	.quad	0
	.quad	0
	.quad	.Lstr.180
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_5
	.quad	0
	.quad	0
	.quad	.Lstr.181
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_6
	.quad	0
	.quad	0
	.quad	.Lstr.182
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_7
	.quad	0
	.quad	0
	.quad	.Lstr.183
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_8
	.quad	0
	.quad	0
	.quad	.Lstr.184
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_9
	.quad	0
	.quad	0
	.quad	.Lstr.185
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_10
	.quad	0
	.quad	0
	.quad	.Lstr.186
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_11
	.quad	0
	.quad	0
	.quad	.Lstr.187
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_12
	.quad	0
	.quad	0
	.quad	.Lstr.188
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_13
	.quad	0
	.quad	0
	.quad	.Lstr.189
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_14
	.quad	0
	.quad	0
	.quad	.Lstr.190
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_15
	.quad	0
	.quad	0
	.quad	.Lstr.191
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_16
	.quad	0
	.quad	0
	.quad	.Lstr.192
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_17
	.quad	0
	.quad	0
	.quad	.Lstr.193
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_18
	.quad	0
	.quad	0
	.quad	.Lstr.194
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_19
	.quad	0
	.quad	0
	.quad	.Lstr.140
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.141
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	16                      # 0x10
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.195
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	1                       # 0x1
	.byte	1                       # 0x1
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_20
	.quad	0
	.quad	0
	.quad	.Lstr.196
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_21
	.quad	0
	.quad	0
	.quad	.Lstr.142
	.long	2                       # 0x2
	.long	3                       # 0x3
	.byte	1                       # 0x1
	.byte	16                      # 0x10
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.size	.L__unnamed_22, 1296

	.type	.Lstr.197,@object       # @str.197
	.section	.rodata,"a",@progbits
	.align	32
.Lstr.197:
	.asciz	"x86-64-linux-avx-avx2-f16c-fma-sse41"
	.size	.Lstr.197, 37

	.type	.Lsharpi_metadata_storage,@object # @sharpi_metadata_storage
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.Lsharpi_metadata_storage:
	.long	0                       # 0x0
	.long	27                      # 0x1b
	.quad	.L__unnamed_22
	.quad	.Lstr.197
	.quad	.Lstr.142
	.size	.Lsharpi_metadata_storage, 32


	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.section	".note.GNU-stack","",@progbits
