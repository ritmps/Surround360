	.text
	.file	"sharpi"
	.section	.text._ZN6Halide7Runtime8Internal14default_mallocEPvm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal14default_mallocEPvm,@function
_ZN6Halide7Runtime8Internal14default_mallocEPvm: # @_ZN6Halide7Runtime8Internal14default_mallocEPvm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$-128, %rsi
	movq	%rsi, %rdi
	callq	malloc@PLT
	movq	%rax, %rcx
	xorl	%eax, %eax
	testq	%rcx, %rcx
	je	.LBB0_2
# BB#1:                                 # %if.end
	movq	%rcx, %rax
	addq	$135, %rax
	andq	$-128, %rax
	movq	%rcx, -8(%rax)
.LBB0_2:                                # %cleanup
	popq	%rbp
	retq
.Lfunc_end0:
	.size	_ZN6Halide7Runtime8Internal14default_mallocEPvm, .Lfunc_end0-_ZN6Halide7Runtime8Internal14default_mallocEPvm

	.section	.text._ZN6Halide7Runtime8Internal12default_freeEPvS2_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12default_freeEPvS2_,@function
_ZN6Halide7Runtime8Internal12default_freeEPvS2_: # @_ZN6Halide7Runtime8Internal12default_freeEPvS2_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	-8(%rsi), %rdi
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end1:
	.size	_ZN6Halide7Runtime8Internal12default_freeEPvS2_, .Lfunc_end1-_ZN6Halide7Runtime8Internal12default_freeEPvS2_

	.section	.text.halide_set_custom_malloc,"ax",@progbits
	.weak	halide_set_custom_malloc
	.align	16, 0x90
	.type	halide_set_custom_malloc,@function
halide_set_custom_malloc:               # @halide_set_custom_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end2:
	.size	halide_set_custom_malloc, .Lfunc_end2-halide_set_custom_malloc

	.section	.text.halide_set_custom_free,"ax",@progbits
	.weak	halide_set_custom_free
	.align	16, 0x90
	.type	halide_set_custom_free,@function
halide_set_custom_free:                 # @halide_set_custom_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end3:
	.size	halide_set_custom_free, .Lfunc_end3-halide_set_custom_free

	.section	.text.halide_malloc,"ax",@progbits
	.weak	halide_malloc
	.align	16, 0x90
	.type	halide_malloc,@function
halide_malloc:                          # @halide_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end4:
	.size	halide_malloc, .Lfunc_end4-halide_malloc

	.section	.text.halide_free,"ax",@progbits
	.weak	halide_free
	.align	16, 0x90
	.type	halide_free,@function
halide_free:                            # @halide_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end5:
	.size	halide_free, .Lfunc_end5-halide_free

	.section	.text._ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,@function
_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc: # @_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$4096, %rsp             # imm = 0x1000
	movq	%rsi, %rbx
	movq	%rdi, %r15
	leaq	-34(%rbp), %r12
	leaq	.L.str(%rip), %rdx
	leaq	-4128(%rbp), %r14
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movzbl	-1(%rax), %ecx
	cmpl	$10, %ecx
	je	.LBB6_2
# BB#1:                                 # %if.then
	movw	$10, (%rax)
	addq	$1, %rax
.LBB6_2:                                # %if.end
	movl	$1, %edx
	subq	%r14, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	addq	$4096, %rsp             # imm = 0x1000
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end6:
	.size	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc, .Lfunc_end6-_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc

	.section	.text.halide_error,"ax",@progbits
	.weak	halide_error
	.align	16, 0x90
	.type	halide_error,@function
halide_error:                           # @halide_error
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end7:
	.size	halide_error, .Lfunc_end7-halide_error

	.section	.text.halide_set_error_handler,"ax",@progbits
	.weak	halide_set_error_handler
	.align	16, 0x90
	.type	halide_set_error_handler,@function
halide_set_error_handler:               # @halide_set_error_handler
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end8:
	.size	halide_set_error_handler, .Lfunc_end8-halide_set_error_handler

	.section	.text.halide_print,"ax",@progbits
	.weak	halide_print
	.align	16, 0x90
	.type	halide_print,@function
halide_print:                           # @halide_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end9:
	.size	halide_print, .Lfunc_end9-halide_print

	.section	.text.halide_set_custom_print,"ax",@progbits
	.weak	halide_set_custom_print
	.align	16, 0x90
	.type	halide_set_custom_print,@function
halide_set_custom_print:                # @halide_set_custom_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end10:
	.size	halide_set_custom_print, .Lfunc_end10-halide_set_custom_print

	.section	.text.halide_start_clock,"ax",@progbits
	.weak	halide_start_clock
	.align	16, 0x90
	.type	halide_start_clock,@function
halide_start_clock:                     # @halide_start_clock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	halide_reference_clock_inited@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	jne	.LBB11_2
# BB#1:                                 # %if.then
	movq	halide_reference_clock@GOTPCREL(%rip), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movb	$1, (%rbx)
.LBB11_2:                               # %if.end
	xorl	%eax, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end11:
	.size	halide_start_clock, .Lfunc_end11-halide_start_clock

	.section	.text.halide_current_time_ns,"ax",@progbits
	.weak	halide_current_time_ns
	.align	16, 0x90
	.type	halide_current_time_ns,@function
halide_current_time_ns:                 # @halide_current_time_ns
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	leaq	-16(%rbp), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movq	halide_reference_clock@GOTPCREL(%rip), %rcx
	movq	-16(%rbp), %rdx
	movq	-8(%rbp), %rax
	subq	(%rcx), %rdx
	imulq	$1000000000, %rdx, %rdx # imm = 0x3B9ACA00
	subq	8(%rcx), %rax
	addq	%rdx, %rax
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end12:
	.size	halide_current_time_ns, .Lfunc_end12-halide_current_time_ns

	.section	.text.halide_sleep_ms,"ax",@progbits
	.weak	halide_sleep_ms
	.align	16, 0x90
	.type	halide_sleep_ms,@function
halide_sleep_ms:                        # @halide_sleep_ms
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	imull	$1000, %esi, %edi       # imm = 0x3E8
	popq	%rbp
	jmp	usleep@PLT              # TAILCALL
.Lfunc_end13:
	.size	halide_sleep_ms, .Lfunc_end13-halide_sleep_ms

	.section	.text._ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,@function
_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc: # @_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rbx, %rdi
	callq	strlen@PLT
	movl	$2, %edi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	write@PLT               # TAILCALL
.Lfunc_end14:
	.size	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc, .Lfunc_end14-_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc

	.section	.text.halide_create_temp_file,"ax",@progbits
	.weak	halide_create_temp_file
	.align	16, 0x90
	.type	halide_create_temp_file,@function
halide_create_temp_file:                # @halide_create_temp_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%r8, %r13
	movq	%rdx, %r12
	movq	%rsi, %rbx
	movl	$-22, %edx
	testq	%rbx, %rbx
	je	.LBB15_7
# BB#1:                                 # %entry
	testq	%r12, %r12
	je	.LBB15_7
# BB#2:                                 # %entry
	testq	%rcx, %rcx
	je	.LBB15_7
# BB#3:                                 # %if.end
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	leaq	.L.str.7(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	%rbx, %rdi
	callq	strlen@PLT
	movq	%rax, %r14
	leaq	.L.str.1(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, %r15
	movq	%r12, %rdi
	callq	strlen@PLT
	addq	-48(%rbp), %r14         # 8-byte Folded Reload
	addq	%r15, %r14
	leaq	1(%rax,%r14), %rax
	cmpq	%r13, %rax
	jbe	.LBB15_5
# BB#4:
	movl	$-22, %edx
	jmp	.LBB15_7
.LBB15_5:                               # %if.end.11
	movq	-56(%rbp), %r15         # 8-byte Reload
	leaq	-1(%r15,%r13), %r14
	leaq	.L.str.7(%rip), %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movb	$0, (%rax)
	movq	%r12, %rdi
	callq	strlen@PLT
	movq	%r15, %rdi
	movl	%eax, %esi
	callq	mkstemps@PLT
	cmpl	$-1, %eax
	movl	$-22, %edx
	je	.LBB15_7
# BB#6:                                 # %if.end.21
	movl	%eax, %edi
	callq	close@PLT
	xorl	%edx, %edx
.LBB15_7:                               # %return
	movl	%edx, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end15:
	.size	halide_create_temp_file, .Lfunc_end15-halide_create_temp_file

	.section	.text.halide_host_cpu_count,"ax",@progbits
	.weak	halide_host_cpu_count
	.align	16, 0x90
	.type	halide_host_cpu_count,@function
halide_host_cpu_count:                  # @halide_host_cpu_count
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$84, %edi
	popq	%rbp
	jmp	sysconf@PLT             # TAILCALL
.Lfunc_end16:
	.size	halide_host_cpu_count, .Lfunc_end16-halide_host_cpu_count

	.section	.text._ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,@function
_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv: # @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	movq	8(%rax), %rdi
	callq	*(%rax)
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end17:
	.size	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv, .Lfunc_end17-_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv

	.section	.text.halide_spawn_thread,"ax",@progbits
	.weak	halide_spawn_thread
	.align	16, 0x90
	.type	halide_spawn_thread,@function
halide_spawn_thread:                    # @halide_spawn_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %r15
	movl	$24, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	movq	%r15, (%rbx)
	movq	%r14, 8(%rbx)
	leaq	16(%rbx), %rdi
	movq	$0, 16(%rbx)
	movq	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv@GOTPCREL(%rip), %rdx
	xorl	%esi, %esi
	movq	%rbx, %rcx
	callq	pthread_create@PLT
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end18:
	.size	halide_spawn_thread, .Lfunc_end18-halide_spawn_thread

	.section	.text.halide_join_thread,"ax",@progbits
	.weak	halide_join_thread
	.align	16, 0x90
	.type	halide_join_thread,@function
halide_join_thread:                     # @halide_join_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	$0, -16(%rbp)
	movq	16(%rbx), %rdi
	leaq	-16(%rbp), %rsi
	callq	pthread_join@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end19:
	.size	halide_join_thread, .Lfunc_end19-halide_join_thread

	.section	.text.halide_mutex_lock,"ax",@progbits
	.weak	halide_mutex_lock
	.align	16, 0x90
	.type	halide_mutex_lock,@function
halide_mutex_lock:                      # @halide_mutex_lock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_lock@PLT  # TAILCALL
.Lfunc_end20:
	.size	halide_mutex_lock, .Lfunc_end20-halide_mutex_lock

	.section	.text.halide_mutex_unlock,"ax",@progbits
	.weak	halide_mutex_unlock
	.align	16, 0x90
	.type	halide_mutex_unlock,@function
halide_mutex_unlock:                    # @halide_mutex_unlock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_unlock@PLT # TAILCALL
.Lfunc_end21:
	.size	halide_mutex_unlock, .Lfunc_end21-halide_mutex_unlock

	.section	.text.halide_mutex_destroy,"ax",@progbits
	.weak	halide_mutex_destroy
	.align	16, 0x90
	.type	halide_mutex_destroy,@function
halide_mutex_destroy:                   # @halide_mutex_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	pthread_mutex_destroy@PLT
	xorl	%esi, %esi
	movl	$64, %edx
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	memset@PLT              # TAILCALL
.Lfunc_end22:
	.size	halide_mutex_destroy, .Lfunc_end22-halide_mutex_destroy

	.section	.text.halide_cond_init,"ax",@progbits
	.weak	halide_cond_init
	.align	16, 0x90
	.type	halide_cond_init,@function
halide_cond_init:                       # @halide_cond_init
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%esi, %esi
	popq	%rbp
	jmp	pthread_cond_init@PLT   # TAILCALL
.Lfunc_end23:
	.size	halide_cond_init, .Lfunc_end23-halide_cond_init

	.section	.text.halide_cond_destroy,"ax",@progbits
	.weak	halide_cond_destroy
	.align	16, 0x90
	.type	halide_cond_destroy,@function
halide_cond_destroy:                    # @halide_cond_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_destroy@PLT # TAILCALL
.Lfunc_end24:
	.size	halide_cond_destroy, .Lfunc_end24-halide_cond_destroy

	.section	.text.halide_cond_broadcast,"ax",@progbits
	.weak	halide_cond_broadcast
	.align	16, 0x90
	.type	halide_cond_broadcast,@function
halide_cond_broadcast:                  # @halide_cond_broadcast
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_broadcast@PLT # TAILCALL
.Lfunc_end25:
	.size	halide_cond_broadcast, .Lfunc_end25-halide_cond_broadcast

	.section	.text.halide_cond_wait,"ax",@progbits
	.weak	halide_cond_wait
	.align	16, 0x90
	.type	halide_cond_wait,@function
halide_cond_wait:                       # @halide_cond_wait
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_wait@PLT   # TAILCALL
.Lfunc_end26:
	.size	halide_cond_wait, .Lfunc_end26-halide_cond_wait

	.section	.text._ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,@function
_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_: # @_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rsi, %rax
	movl	%edx, %esi
	movq	%rcx, %rdx
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end27:
	.size	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_, .Lfunc_end27-_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_

	.section	.text._ZN6Halide7Runtime8Internal17clamp_num_threadsEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi,@function
_ZN6Halide7Runtime8Internal17clamp_num_threadsEi: # @_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpl	$64, %edi
	jle	.LBB28_1
# BB#2:                                 # %if.end.3
	movl	$64, %eax
	popq	%rbp
	retq
.LBB28_1:                               # %if.else
	testl	%edi, %edi
	movl	$1, %eax
	cmovgl	%edi, %eax
	popq	%rbp
	retq
.Lfunc_end28:
	.size	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi, .Lfunc_end28-_ZN6Halide7Runtime8Internal17clamp_num_threadsEi

	.section	.text._ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,@function
_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv: # @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.8(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	jne	.LBB29_2
# BB#1:                                 # %if.end
	leaq	.L.str.1.9(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB29_3
.LBB29_2:                               # %if.then.3
	movq	%rax, %rdi
	popq	%rbp
	jmp	atoi@PLT                # TAILCALL
.LBB29_3:                               # %if.else
	popq	%rbp
	jmp	halide_host_cpu_count@PLT # TAILCALL
.Lfunc_end29:
	.size	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv, .Lfunc_end29-_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv

	.section	.text._ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,@function
_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE: # @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdi, %rbx
	movq	%rbx, -56(%rbp)         # 8-byte Spill
	testq	%rbx, %rbx
	je	.LBB30_13
# BB#1:
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	leaq	80(%r13), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	jmp	.LBB30_2
	.align	16, 0x90
.LBB30_28:                              # %if.then.3.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
.LBB30_2:                               # %cond.true.us
                                        # =>This Inner Loop Header: Depth=1
	movl	24(%rbx), %eax
	cmpl	28(%rbx), %eax
	jl	.LBB30_4
# BB#3:                                 # %cond.end.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$0, 40(%rbx)
	jle	.LBB30_20
.LBB30_4:                               # %while.body.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	64(%r13), %r12
	testq	%r12, %r12
	je	.LBB30_28
# BB#5:                                 # %if.else.8.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	8(%r12), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r12), %r15
	movq	24(%r12), %rbx
	movq	32(%r12), %r14
	leal	1(%rbx), %eax
	movl	%eax, 24(%r12)
	movq	%rbx, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_7
# BB#6:                                 # %if.then.12.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	(%r12), %rax
	movq	%rax, 64(%r13)
.LBB30_7:                               # %if.end.13.us
                                        #   in Loop: Header=BB30_2 Depth=1
	incl	40(%r12)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%ebx, %edx
	movq	%r14, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %ebx
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	je	.LBB30_9
# BB#8:                                 # %if.then.18.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	%ebx, 44(%r12)
.LBB30_9:                               # %if.end.19.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	40(%r12), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r12)
	movl	24(%r12), %ecx
	cmpl	28(%r12), %ecx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	jl	.LBB30_2
# BB#10:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpq	%rbx, %r12
	je	.LBB30_2
# BB#11:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$1, %eax
	jg	.LBB30_2
# BB#12:                                # %if.then.24.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_2
.LBB30_13:                              # %cond.false.preheader
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	cmpb	$0, 792(%rbx)
	jne	.LBB30_20
# BB#14:
	leaq	208(%rbx), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	leaq	144(%rbx), %rax
	movq	%rax, -72(%rbp)         # 8-byte Spill
	leaq	80(%rbx), %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB30_15:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	64(%rbx), %r13
	testq	%r13, %r13
	je	.LBB30_16
# BB#21:                                # %if.else.8
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	8(%r13), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r13), %r15
	movq	24(%r13), %r14
	movq	32(%r13), %r12
	leal	1(%r14), %eax
	movl	%eax, 24(%r13)
	movq	%r14, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_23
# BB#22:                                # %if.then.12
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	(%r13), %rax
	movq	%rax, 64(%rbx)
.LBB30_23:                              # %if.end.13
                                        #   in Loop: Header=BB30_15 Depth=1
	incl	40(%r13)
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%r14d, %edx
	movq	%r12, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %r14d
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	testl	%r14d, %r14d
	je	.LBB30_25
# BB#24:                                # %if.then.18
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	%r14d, 44(%r13)
.LBB30_25:                              # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	40(%r13), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r13)
	cmpl	$1, %eax
	jg	.LBB30_19
# BB#26:                                # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	28(%r13), %eax
	cmpl	%eax, 24(%r13)
	jl	.LBB30_19
# BB#27:                                # %if.then.24
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-56(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_19
	.align	16, 0x90
.LBB30_16:                              # %if.else
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	72(%rbx), %rax
	movq	%rax, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jle	.LBB30_17
# BB#18:                                # %if.else.6
                                        #   in Loop: Header=BB30_15 Depth=1
	leal	-1(%rax), %eax
	movl	%eax, 72(%rbx)
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	incl	72(%rbx)
	jmp	.LBB30_19
.LBB30_17:                              # %if.then.5
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-72(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	.align	16, 0x90
.LBB30_19:                              # %cond.false.backedge
                                        #   in Loop: Header=BB30_15 Depth=1
	cmpb	$0, 792(%rbx)
	je	.LBB30_15
.LBB30_20:                              # %while.end
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end30:
	.size	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE, .Lfunc_end30-_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE

	.section	.text.halide_do_task,"ax",@progbits
	.weak	halide_do_task
	.align	16, 0x90
	.type	halide_do_task,@function
halide_do_task:                         # @halide_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end31:
	.size	halide_do_task, .Lfunc_end31-halide_do_task

	.section	.text._ZN6Halide7Runtime8Internal13worker_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13worker_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13worker_threadEPv,@function
_ZN6Halide7Runtime8Internal13worker_threadEPv: # @_ZN6Halide7Runtime8Internal13worker_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	xorl	%edi, %edi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end32:
	.size	_ZN6Halide7Runtime8Internal13worker_threadEPv, .Lfunc_end32-_ZN6Halide7Runtime8Internal13worker_threadEPv

	.section	.text._ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,@function
_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_: # @_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r8, -96(%rbp)          # 8-byte Spill
	movl	%ecx, %r14d
	movl	%edx, %r12d
	movq	%rsi, -104(%rbp)        # 8-byte Spill
	movq	%rdi, %r13
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 793(%rbx)
	je	.LBB33_2
# BB#1:                                 # %entry.while.cond.preheader_crit_edge
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	jmp	.LBB33_5
.LBB33_2:                               # %if.then
	movb	$0, 792(%rbx)
	leaq	80(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	144(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	208(%rbx), %rdi
	callq	halide_cond_init@PLT
	movq	$0, 64(%rbx)
	movl	788(%rbx), %edi
	testl	%edi, %edi
	jne	.LBB33_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %edi
	movl	%edi, 788(%rbx)
.LBB33_4:                               # %if.end
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%rbx)
	movl	$0, 784(%rbx)
	movl	%eax, 72(%rbx)
	movb	$1, 793(%rbx)
	xorl	%ecx, %ecx
.LBB33_5:                               # %while.cond.preheader
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jge	.LBB33_8
# BB#6:
	movq	_ZN6Halide7Runtime8Internal13worker_threadEPv@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB33_7:                               # %while.body
                                        # =>This Inner Loop Header: Depth=1
	xorl	%esi, %esi
	movq	%r15, %rdi
	callq	halide_spawn_thread@PLT
	movslq	784(%rbx), %rcx
	leal	1(%rcx), %edx
	movl	%edx, 784(%rbx)
	movq	%rax, 272(%rbx,%rcx,8)
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jl	.LBB33_7
.LBB33_8:                               # %while.end
	movq	-104(%rbp), %rcx        # 8-byte Reload
	movq	%rcx, -80(%rbp)
	movq	%r13, -72(%rbp)
	movl	%r12d, -64(%rbp)
	leal	(%r12,%r14), %ecx
	movl	%ecx, -60(%rbp)
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	%rcx, -56(%rbp)
	movq	$0, -48(%rbp)
	movq	64(%rbx), %rcx
	testq	%rcx, %rcx
	movl	%eax, %edx
	cmovel	%r14d, %edx
	cmpl	%r14d, %eax
	cmovlel	%eax, %edx
	movl	%edx, 76(%rbx)
	movq	%rcx, -88(%rbp)
	leaq	-88(%rbp), %rax
	movq	%rax, 64(%rbx)
	leaq	144(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
	movl	76(%rbx), %eax
	cmpl	72(%rbx), %eax
	jle	.LBB33_10
# BB#9:                                 # %if.then.14
	movl	$208, %edi
	addq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_cond_broadcast@PLT
.LBB33_10:                              # %if.end.15
	leaq	-88(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	-44(%rbp), %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end33:
	.size	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_, .Lfunc_end33-_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_

	.section	.text.halide_set_num_threads,"ax",@progbits
	.weak	halide_set_num_threads
	.align	16, 0x90
	.type	halide_set_num_threads,@function
halide_set_num_threads:                 # @halide_set_num_threads
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movl	%edi, %ebx
	testl	%ebx, %ebx
	js	.LBB34_1
# BB#2:                                 # %if.end
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	jne	.LBB34_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %ebx
	jmp	.LBB34_4
.LBB34_1:                               # %if.end.thread
	leaq	.L.str.2(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
.LBB34_4:                               # %if.end.3
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movl	788(%r14), %r15d
	movl	%ebx, %edi
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%r14)
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r15d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end34:
	.size	halide_set_num_threads, .Lfunc_end34-halide_set_num_threads

	.section	.text.halide_shutdown_thread_pool,"ax",@progbits
	.weak	halide_shutdown_thread_pool
	.align	16, 0x90
	.type	halide_shutdown_thread_pool,@function
halide_shutdown_thread_pool:            # @halide_shutdown_thread_pool
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	cmpb	$0, 793(%r13)
	je	.LBB35_5
# BB#1:                                 # %if.end
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movb	$1, 792(%r13)
	leaq	80(%r13), %rdi
	movq	%rdi, -48(%rbp)         # 8-byte Spill
	callq	halide_cond_broadcast@PLT
	leaq	144(%r13), %r15
	movq	%r15, %rdi
	callq	halide_cond_broadcast@PLT
	leaq	208(%r13), %r12
	movq	%r12, %rdi
	callq	halide_cond_broadcast@PLT
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%ebx, %ebx
	cmpl	$0, 784(%r13)
	jle	.LBB35_4
# BB#2:
	leaq	272(%r13), %r14
	.align	16, 0x90
.LBB35_3:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rdi
	callq	halide_join_thread@PLT
	addq	$1, %rbx
	movslq	784(%r13), %rax
	addq	$8, %r14
	cmpq	%rax, %rbx
	jl	.LBB35_3
.LBB35_4:                               # %for.cond.cleanup
	movq	%r13, %rdi
	callq	halide_mutex_destroy@PLT
	movq	-48(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_destroy@PLT
	movq	%r15, %rdi
	callq	halide_cond_destroy@PLT
	movq	%r12, %rdi
	callq	halide_cond_destroy@PLT
	movb	$0, 793(%r13)
.LBB35_5:                               # %return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end35:
	.size	halide_shutdown_thread_pool, .Lfunc_end35-halide_shutdown_thread_pool

	.section	.text.halide_thread_pool_cleanup,"ax",@progbits
	.weak	halide_thread_pool_cleanup
	.align	16, 0x90
	.type	halide_thread_pool_cleanup,@function
halide_thread_pool_cleanup:             # @halide_thread_pool_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_thread_pool@PLT # TAILCALL
.Lfunc_end36:
	.size	halide_thread_pool_cleanup, .Lfunc_end36-halide_thread_pool_cleanup

	.section	.text.halide_set_custom_do_task,"ax",@progbits
	.weak	halide_set_custom_do_task
	.align	16, 0x90
	.type	halide_set_custom_do_task,@function
halide_set_custom_do_task:              # @halide_set_custom_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end37:
	.size	halide_set_custom_do_task, .Lfunc_end37-halide_set_custom_do_task

	.section	.text.halide_set_custom_do_par_for,"ax",@progbits
	.weak	halide_set_custom_do_par_for
	.align	16, 0x90
	.type	halide_set_custom_do_par_for,@function
halide_set_custom_do_par_for:           # @halide_set_custom_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end38:
	.size	halide_set_custom_do_par_for, .Lfunc_end38-halide_set_custom_do_par_for

	.section	.text.halide_do_par_for,"ax",@progbits
	.weak	halide_do_par_for
	.align	16, 0x90
	.type	halide_do_par_for,@function
halide_do_par_for:                      # @halide_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end39:
	.size	halide_do_par_for, .Lfunc_end39-halide_do_par_for

	.section	.text._ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,@function
_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc: # @_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end40:
	.size	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc, .Lfunc_end40-_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,@function
_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc: # @_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movl	$1, %esi
	callq	dlopen@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	jne	.LBB41_2
# BB#1:                                 # %if.then
	callq	dlerror@PLT
.LBB41_2:                               # %if.end
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end41:
	.size	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc, .Lfunc_end41-_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,@function
_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc: # @_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end42:
	.size	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc, .Lfunc_end42-_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc

	.section	.text.halide_set_custom_get_symbol,"ax",@progbits
	.weak	halide_set_custom_get_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_symbol,@function
halide_set_custom_get_symbol:           # @halide_set_custom_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end43:
	.size	halide_set_custom_get_symbol, .Lfunc_end43-halide_set_custom_get_symbol

	.section	.text.halide_set_custom_load_library,"ax",@progbits
	.weak	halide_set_custom_load_library
	.align	16, 0x90
	.type	halide_set_custom_load_library,@function
halide_set_custom_load_library:         # @halide_set_custom_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end44:
	.size	halide_set_custom_load_library, .Lfunc_end44-halide_set_custom_load_library

	.section	.text.halide_set_custom_get_library_symbol,"ax",@progbits
	.weak	halide_set_custom_get_library_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_library_symbol,@function
halide_set_custom_get_library_symbol:   # @halide_set_custom_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end45:
	.size	halide_set_custom_get_library_symbol, .Lfunc_end45-halide_set_custom_get_library_symbol

	.section	.text.halide_get_symbol,"ax",@progbits
	.weak	halide_get_symbol
	.align	16, 0x90
	.type	halide_get_symbol,@function
halide_get_symbol:                      # @halide_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end46:
	.size	halide_get_symbol, .Lfunc_end46-halide_get_symbol

	.section	.text.halide_load_library,"ax",@progbits
	.weak	halide_load_library
	.align	16, 0x90
	.type	halide_load_library,@function
halide_load_library:                    # @halide_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end47:
	.size	halide_load_library, .Lfunc_end47-halide_load_library

	.section	.text.halide_get_library_symbol,"ax",@progbits
	.weak	halide_get_library_symbol
	.align	16, 0x90
	.type	halide_get_library_symbol,@function
halide_get_library_symbol:              # @halide_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end48:
	.size	halide_get_library_symbol, .Lfunc_end48-halide_get_library_symbol

	.section	.text.halide_set_gpu_device,"ax",@progbits
	.weak	halide_set_gpu_device
	.align	16, 0x90
	.type	halide_set_gpu_device,@function
halide_set_gpu_device:                  # @halide_set_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end49:
	.size	halide_set_gpu_device, .Lfunc_end49-halide_set_gpu_device

	.section	.text.halide_get_gpu_device,"ax",@progbits
	.weak	halide_get_gpu_device
	.align	16, 0x90
	.type	halide_get_gpu_device,@function
halide_get_gpu_device:                  # @halide_get_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB50_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB50_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %r14
	cmpb	$0, (%r14)
	je	.LBB50_4
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit.if.end.4_crit_edge
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	jmp	.LBB50_7
.LBB50_4:                               # %if.then
	leaq	.L.str.10(%rip), %rdi
	callq	getenv@PLT
	movq	%rax, %rcx
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB50_6
# BB#5:                                 # %if.then.2
	movq	%rcx, %rdi
	callq	atoi@PLT
.LBB50_6:                               # %if.end
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rcx
	movl	%eax, (%rcx)
	movb	$1, (%r14)
.LBB50_7:                               # %if.end.4
	movl	$0, (%rbx)
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end50:
	.size	halide_get_gpu_device, .Lfunc_end50-halide_get_gpu_device

	.section	.text._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,@function
_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t: # @_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4168, %rsp             # imm = 0x1048
	movq	%rsi, %r12
	movq	%rdi, -4160(%rbp)       # 8-byte Spill
	movl	$1, %r14d
	lock		xaddl	%r14d, _ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids(%rip)
	callq	halide_get_trace_file@PLT
	testl	%eax, %eax
	jle	.LBB51_10
# BB#1:                                 # %if.then
	movl	%eax, %r15d
	movzwl	26(%r12), %eax
	movzbl	25(%r12), %r13d
	addl	$7, %r13d
	shrl	$3, %r13d
	imulq	%rax, %r13
	movl	40(%r12), %ebx
	leal	(,%rbx,4), %eax
	movl	%eax, -4192(%rbp)       # 4-byte Spill
	movq	(%r12), %rdi
	callq	strlen@PLT
	addq	$1, %rax
	movq	%rax, -4168(%rbp)       # 8-byte Spill
	leal	(%r13,%rbx,4), %ecx
	leal	28(%rax,%rcx), %edx
	movl	%edx, -4196(%rbp)       # 4-byte Spill
	leal	31(%rax,%rcx), %eax
	andl	$-4, %eax
	movl	%eax, -4152(%rbp)       # 4-byte Spill
	movl	%eax, -4144(%rbp)
	movl	%r14d, -4140(%rbp)
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	vmovupd	24(%r12), %xmm0
	vmovupd	%xmm0, -4136(%rbp)
	movl	%ebx, -4120(%rbp)
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %r14
	.align	16, 0x90
.LBB51_2:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%r14)
	testl	%eax, %eax
	jne	.LBB51_2
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	leaq	-4144(%rbp), %rsi
	movl	$28, %edx
	movl	%r15d, %ebx
	movl	%ebx, %edi
	callq	write@PLT
	movq	%rax, %r15
	movq	%r12, %rcx
	movq	16(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_5
# BB#4:                                 # %if.then.19
	movl	-4192(%rbp), %edx       # 4-byte Reload
	movq	%rcx, %r12
	movl	%ebx, %edi
	callq	write@PLT
	movq	%r12, %rcx
	addq	%rax, %r15
.LBB51_5:                               # %if.end
	movq	%r15, -4192(%rbp)       # 8-byte Spill
	movl	%ebx, %r15d
	movl	-4152(%rbp), %r12d      # 4-byte Reload
	subl	-4196(%rbp), %r12d      # 4-byte Folded Reload
	movq	8(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_6
# BB#7:                                 # %if.then.25
	movq	%rcx, %rbx
	movl	%r15d, %edi
	movq	%r13, %rdx
	callq	write@PLT
	movq	%rbx, %rcx
	movq	-4192(%rbp), %r13       # 8-byte Reload
	addq	%rax, %r13
	jmp	.LBB51_8
.LBB51_10:                              # %if.else
	leaq	-49(%rbp), %r15
	movb	$0, -49(%rbp)
	movzbl	25(%r12), %eax
	movl	$8, %ecx
	.align	16, 0x90
.LBB51_11:                              # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movl	%ecx, %edx
	leal	(%rdx,%rdx), %ecx
	cmpl	%eax, %edx
	jl	.LBB51_11
# BB#12:                                # %while.end
	cmpl	$65, %edx
	movq	%rdx, -4168(%rbp)       # 8-byte Spill
	jl	.LBB51_14
# BB#13:                                # %if.then.46
	leaq	.L.str.1.15(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB51_14:                              # %if.end.47
	movl	28(%r12), %r13d
	leaq	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types(%rip), %rax
	movq	(%rax,%r13,8), %rdx
	leaq	-4144(%rbp), %rdi
	movq	%r12, %rbx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.123(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	36(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.22.125(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_15
# BB#16:                                # %if.then.63
	movq	%r13, -4152(%rbp)       # 8-byte Spill
	leaq	.L.str.15(%rip), %rdx
	movq	%rsi, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	jmp	.LBB51_17
.LBB51_6:
	movq	-4192(%rbp), %r13       # 8-byte Reload
.LBB51_8:                               # %if.end.30
	movq	(%rcx), %rsi
	movq	-4168(%rbp), %rax       # 8-byte Reload
	movl	%eax, %edx
	movl	%r15d, %edi
	callq	write@PLT
	movq	%rax, %rbx
	addq	%r13, %rbx
	movl	$0, -44(%rbp)
	movl	%r12d, %edx
	leaq	-44(%rbp), %rsi
	movl	%r15d, %edi
	callq	write@PLT
	addq	%rbx, %rax
	movl	$0, (%r14)
	movl	-4152(%rbp), %ecx       # 4-byte Reload
	cmpq	%rcx, %rax
	je	.LBB51_45
# BB#9:                                 # %if.then.40
	leaq	.L.str.14(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB51_45
.LBB51_15:
	movq	%r13, -4152(%rbp)       # 8-byte Spill
.LBB51_17:                              # %for.cond.preheader
	movq	%r15, %r13
	cmpl	$0, 40(%rbx)
	jle	.LBB51_18
# BB#21:                                # %for.body.lr.ph
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	xorl	%r12d, %r12d
	xorl	%r15d, %r15d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_22:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_28
# BB#23:                                # %if.then.69
                                        #   in Loop: Header=BB51_22 Depth=1
	movzwl	26(%rbx), %ecx
	cmpl	$2, %ecx
	jb	.LBB51_26
# BB#24:                                # %land.lhs.true
                                        #   in Loop: Header=BB51_22 Depth=1
	movl	%r12d, %eax
	cltd
	idivl	%ecx
	testl	%edx, %edx
	je	.LBB51_25
.LBB51_26:                              # %if.else.80
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.17(%rip), %rdx
	jmp	.LBB51_27
.LBB51_25:                              # %if.then.78
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.16(%rip), %rdx
	.align	16, 0x90
.LBB51_27:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
.LBB51_28:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	16(%rbx), %rax
	movslq	(%rax,%r15), %rdx
	movl	$1, %ecx
	movq	%rsi, %rdi
	movq	%r13, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rsi
	addq	$1, %r14
	movslq	40(%rbx), %rax
	addq	$4, %r15
	addl	$1, %r12d
	cmpq	%rax, %r14
	jl	.LBB51_22
	jmp	.LBB51_19
.LBB51_18:
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
.LBB51_19:                              # %for.cond.cleanup
	movzwl	26(%rbx), %eax
	cmpl	$1, %eax
	jbe	.LBB51_29
# BB#20:                                # %if.then.92
	leaq	.L.str.18(%rip), %rdx
	jmp	.LBB51_30
.LBB51_29:                              # %if.else.94
	leaq	.L.str.8.76(%rip), %rdx
.LBB51_30:                              # %if.end.96
	movq	%rsi, %rdi
	movq	%r13, %r15
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	-4160(%rbp), %r12       # 8-byte Reload
	movq	-4152(%rbp), %rax       # 8-byte Reload
	cmpl	$1, %eax
	jg	.LBB51_42
# BB#31:                                # %if.then.98
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_33
# BB#32:                                # %if.then.103
	leaq	.L.str.20(%rip), %rdx
	jmp	.LBB51_34
.LBB51_33:                              # %if.else.105
	leaq	.L.str.21(%rip), %rdx
.LBB51_34:                              # %for.cond.109.preheader
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	cmpw	$0, 26(%rbx)
	je	.LBB51_42
# BB#35:                                # %for.body.115.lr.ph
	movq	%r12, -4160(%rbp)       # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -4152(%rbp)       # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_36:                              # %for.body.115
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_38
# BB#37:                                # %if.then.117
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%r15, %rsi
	leaq	.L.str.17(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_38:                              # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	movzbl	24(%rbx), %eax
	cmpq	$3, %rax
	ja	.LBB51_74
# BB#39:                                # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	leaq	.LJTI51_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	movq	-4168(%rbp), %rdx       # 8-byte Reload
	jmpq	*%rax
.LBB51_46:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_47
# BB#50:                                # %if.then.133
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movswq	(%rax,%r12), %rdx
	jmp	.LBB51_49
.LBB51_56:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_57
# BB#59:                                # %if.then.169
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzwl	(%rax,%r12), %edx
	jmp	.LBB51_49
.LBB51_64:                              # %if.then.195
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	movq	%rdi, %rbx
	cmpl	$15, %edx
	jle	.LBB51_65
# BB#67:                                # %if.end.198
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$32, %edx
	movq	-4184(%rbp), %rax       # 8-byte Reload
	jne	.LBB51_70
# BB#68:                                # %if.then.200
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	vcvtss2sd	(%rax,%rcx), %xmm0, %xmm0
	xorl	%edx, %edx
	jmp	.LBB51_69
.LBB51_72:                              # %if.then.224
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	(%rax,%r13), %rdx
	movq	%r15, %rsi
	callq	halide_pointer_to_string@PLT
	jmp	.LBB51_73
.LBB51_47:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_51
# BB#48:                                # %if.then.125
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movsbq	(%rax,%r14), %rdx
	jmp	.LBB51_49
.LBB51_57:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_60
# BB#58:                                # %if.then.161
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzbl	(%rax,%r14), %edx
.LBB51_49:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_73
.LBB51_65:                              # %if.else.205.thread
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	leaq	.L.str.22(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movq	-4184(%rbp), %rax       # 8-byte Reload
	movq	8(%rax), %rax
	jmp	.LBB51_66
.LBB51_70:                              # %if.else.205
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	cmpl	$16, %edx
	jne	.LBB51_66
# BB#71:                                # %if.then.207
                                        #   in Loop: Header=BB51_36 Depth=1
	movzwl	(%rax,%r12), %edi
	callq	halide_float16_bits_to_double@PLT
	movl	$1, %edx
.LBB51_69:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_73:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	jmp	.LBB51_74
.LBB51_66:                              # %if.else.212
                                        #   in Loop: Header=BB51_36 Depth=1
	vmovsd	(%rax,%r13), %xmm0      # xmm0 = mem[0],zero
	movl	$1, %edx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB51_55
.LBB51_51:                              # %if.else.139
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_53
# BB#52:                                # %if.then.141
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movslq	(%rax,%rcx), %rdx
	jmp	.LBB51_54
.LBB51_60:                              # %if.else.175
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_62
# BB#61:                                # %if.then.177
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movl	(%rax,%rcx), %edx
	jmp	.LBB51_63
.LBB51_53:                              # %if.else.146
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_54:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_55
.LBB51_62:                              # %if.else.182
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_63:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
.LBB51_55:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_74:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	addq	$1, %r14
	movzwl	26(%rbx), %eax
	addq	$8, %r13
	addq	$2, %r12
	addq	$4, -4152(%rbp)         # 8-byte Folded Spill
	cmpq	%rax, %r14
	jl	.LBB51_36
# BB#40:                                # %for.cond.cleanup.114
	movzwl	%ax, %eax
	cmpl	$1, %eax
	movq	-4160(%rbp), %r12       # 8-byte Reload
	jbe	.LBB51_42
# BB#41:                                # %if.then.240
	leaq	.L.str.23(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_42:                              # %if.end.243
	leaq	.L.str.7.110(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %r15d
	leaq	-4144(%rbp), %rsi
	subq	%rsi, %r15
	addq	%rax, %r15
	movq	%r12, %rdi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB51_43:                              # %while.cond.i.353
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB51_43
# BB#44:                                # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy4096EED2Ev.exit
	leaq	-4144(%rbp), %r14
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	movl	$0, (%rbx)
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
.LBB51_45:                              # %if.end.247
	movl	-4172(%rbp), %eax       # 4-byte Reload
	addq	$4168, %rsp             # imm = 0x1048
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end51:
	.size	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t, .Lfunc_end51-_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.section	.rodata._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"a",@progbits
	.align	4
.LJTI51_0:
	.long	.LBB51_46-.LJTI51_0
	.long	.LBB51_56-.LJTI51_0
	.long	.LBB51_64-.LJTI51_0
	.long	.LBB51_72-.LJTI51_0

	.section	.text.halide_get_trace_file,"ax",@progbits
	.weak	halide_get_trace_file
	.align	16, 0x90
	.type	halide_get_trace_file,@function
halide_get_trace_file:                  # @halide_get_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB52_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB52_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	cmpb	$0, (%rax)
	jne	.LBB52_8
# BB#3:                                 # %if.then
	leaq	.L.str.25(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB52_7
# BB#4:                                 # %if.then.2
	movl	$1089, %esi             # imm = 0x441
	movl	$420, %edx              # imm = 0x1A4
	movq	%rax, %rdi
	callq	open@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jg	.LBB52_6
# BB#5:                                 # %if.then.4
	leaq	.L.str.26(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB52_6:                               # %if.end
	movl	%r15d, %edi
	callq	halide_set_trace_file@PLT
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	jmp	.LBB52_8
.LBB52_7:                               # %if.else
	xorl	%edi, %edi
	callq	halide_set_trace_file@PLT
.LBB52_8:                               # %if.end.6
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	movl	$0, (%rbx)
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end52:
	.size	halide_get_trace_file, .Lfunc_end52-halide_get_trace_file

	.section	.text.halide_set_custom_trace,"ax",@progbits
	.weak	halide_set_custom_trace
	.align	16, 0x90
	.type	halide_set_custom_trace,@function
halide_set_custom_trace:                # @halide_set_custom_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end53:
	.size	halide_set_custom_trace, .Lfunc_end53-halide_set_custom_trace

	.section	.text.halide_set_trace_file,"ax",@progbits
	.weak	halide_set_trace_file
	.align	16, 0x90
	.type	halide_set_trace_file,@function
halide_set_trace_file:                  # @halide_set_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end54:
	.size	halide_set_trace_file, .Lfunc_end54-halide_set_trace_file

	.section	.text.halide_trace,"ax",@progbits
	.weak	halide_trace
	.align	16, 0x90
	.type	halide_trace,@function
halide_trace:                           # @halide_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end55:
	.size	halide_trace, .Lfunc_end55-halide_trace

	.section	.text.halide_shutdown_trace,"ax",@progbits
	.weak	halide_shutdown_trace
	.align	16, 0x90
	.type	halide_shutdown_trace,@function
halide_shutdown_trace:                  # @halide_shutdown_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	je	.LBB56_2
# BB#1:                                 # %if.then
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %r14
	movl	(%r14), %edi
	callq	close@PLT
	movl	$0, (%r14)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rcx
	movb	$0, (%rcx)
	movb	$0, (%rbx)
	jmp	.LBB56_3
.LBB56_2:                               # %return
	xorl	%eax, %eax
.LBB56_3:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end56:
	.size	halide_shutdown_trace, .Lfunc_end56-halide_shutdown_trace

	.section	.text.halide_trace_cleanup,"ax",@progbits
	.weak	halide_trace_cleanup
	.align	16, 0x90
	.type	halide_trace_cleanup,@function
halide_trace_cleanup:                   # @halide_trace_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_trace@PLT # TAILCALL
.Lfunc_end57:
	.size	halide_trace_cleanup, .Lfunc_end57-halide_trace_cleanup

	.section	.text.halide_trace_helper,"ax",@progbits
	.weak	halide_trace_helper
	.align	16, 0x90
	.type	halide_trace_helper,@function
halide_trace_helper:                    # @halide_trace_helper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$56, %rsp
	movl	40(%rbp), %r10d
	movl	32(%rbp), %r11d
	movl	24(%rbp), %eax
	movl	16(%rbp), %ebx
	movq	%rsi, -56(%rbp)
	movq	%rcx, -40(%rbp)
	movq	%rdx, -48(%rbp)
	movb	%r8b, -32(%rbp)
	movb	%r9b, -31(%rbp)
	movw	%bx, -30(%rbp)
	movl	%eax, -28(%rbp)
	movl	%r11d, -24(%rbp)
	movl	%r10d, -20(%rbp)
	movzwl	%bx, %eax
	cmpl	$1, %eax
	movl	$1, %ecx
	cmoval	%eax, %ecx
	imull	48(%rbp), %ecx
	movl	%ecx, -16(%rbp)
	leaq	-56(%rbp), %rsi
	callq	halide_trace@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end58:
	.size	halide_trace_helper, .Lfunc_end58-halide_trace_helper

	.section	.text._ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,@function
_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc: # @_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	.align	16, 0x90
.LBB59_1:                               # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	leaq	1(%rcx), %rax
	cmpb	$0, (%rcx)
	jne	.LBB59_1
# BB#2:                                 # %while.cond.1.preheader
	xorl	%eax, %eax
	cmpq	%rdi, %rcx
	je	.LBB59_19
	.align	16, 0x90
.LBB59_3:                               # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	jne	.LBB59_4
# BB#7:                                 # %while.body.5
                                        #   in Loop: Header=BB59_3 Depth=1
	movzbl	-1(%rcx), %edx
	addq	$-1, %rcx
	cmpl	$46, %edx
	sete	%al
	cmpq	%rcx, %rdi
	jne	.LBB59_3
# BB#8:                                 # %while.end.7
	movzbl	%dl, %eax
	cmpl	$46, %eax
	je	.LBB59_5
# BB#9:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_4:
	movq	%rcx, %rdi
.LBB59_5:                               # %if.end
	movb	1(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$116, %eax
	jne	.LBB59_6
# BB#10:                                # %if.end.16
	movb	2(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$105, %eax
	jne	.LBB59_11
# BB#12:                                # %if.end.24
	movb	3(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$102, %eax
	jne	.LBB59_13
# BB#14:                                # %if.end.32
	movb	4(%rdi), %cl
	movb	$1, %al
	testb	%cl, %cl
	je	.LBB59_19
# BB#15:                                # %if.end.32
	movzbl	%cl, %eax
	cmpl	$70, %eax
	je	.LBB59_18
# BB#16:                                # %if.end.32
	cmpl	$102, %eax
	jne	.LBB59_17
.LBB59_18:                              # %if.end.44
	cmpb	$0, 5(%rdi)
	sete	%al
.LBB59_19:                              # %cleanup
	popq	%rbp
	retq
.LBB59_6:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_11:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_13:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_17:                              # %if.then.43
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end59:
	.size	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc, .Lfunc_end59-_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc

	.section	.text._ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,@function
_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t: # @_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%r8), %r9
	imull	32(%r8), %edi
	imull	36(%r8), %esi
	addl	%edi, %esi
	imull	40(%r8), %edx
	addl	%esi, %edx
	imull	44(%r8), %ecx
	addl	%edx, %ecx
	movslq	%ecx, %rax
	imulq	%r9, %rax
	addq	8(%r8), %rax
	popq	%rbp
	retq
.Lfunc_end60:
	.size	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t, .Lfunc_end60-_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI61_0:
	.long	1                       # 0x1
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI61_1:
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	1                       # 0x1
	.long	1                       # 0x1
	.section	.text.halide_debug_to_file,"ax",@progbits
	.weak	halide_debug_to_file
	.align	16, 0x90
	.type	halide_debug_to_file,@function
halide_debug_to_file:                   # @halide_debug_to_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4232, %rsp             # imm = 0x1088
	movq	%rcx, %r12
	movq	%r12, -4152(%rbp)       # 8-byte Spill
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%r12, %rsi
	callq	halide_copy_to_host@PLT
	leaq	.L.str.27(%rip), %rsi
	movq	%rbx, %rdi
	callq	fopen@PLT
	movq	%rax, %r14
	movl	$-1, %r13d
	testq	%r14, %r14
	je	.LBB61_44
# BB#1:                                 # %if.end
	vmovdqu	16(%r12), %xmm1
	vmovdqa	%xmm1, -4208(%rbp)      # 16-byte Spill
	vpbroadcastd	.LCPI61_0(%rip), %xmm0
	vpmaxsd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, -4176(%rbp)      # 16-byte Spill
	movslq	64(%r12), %r12
	movq	%r12, -4184(%rbp)       # 8-byte Spill
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc@PLT
	testb	%al, %al
	je	.LBB61_18
# BB#2:                                 # %if.then.19
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %ecx
	cmpl	$2, %ecx
	setb	%bl
	vpextrd	$2, %xmm0, %eax
	cmpl	$5, %eax
	setl	%dl
	andb	%bl, %dl
	movl	$1, %esi
	cmovel	%eax, %esi
	movl	%esi, -4212(%rbp)       # 4-byte Spill
	movl	%ecx, %r13d
	cmovnel	%eax, %r13d
	movw	$18761, -4144(%rbp)     # imm = 0x4949
	movw	$42, -4142(%rbp)
	movl	$8, -4140(%rbp)
	movw	$15, -4136(%rbp)
	movw	$256, -4134(%rbp)       # imm = 0x100
	movw	$4, -4132(%rbp)
	movl	$1, -4130(%rbp)
	vmovd	%xmm0, %r8d
	vmovd	%xmm0, -4126(%rbp)
	movw	$257, -4122(%rbp)       # imm = 0x101
	movw	$4, -4120(%rbp)
	movl	$1, -4118(%rbp)
	vpextrd	$1, %xmm0, -4114(%rbp)
	leal	(,%r12,8), %edi
	movw	$258, -4110(%rbp)       # imm = 0x102
	movw	$3, -4108(%rbp)
	movl	$1, -4106(%rbp)
	movw	%di, -4102(%rbp)
	movw	$259, -4098(%rbp)       # imm = 0x103
	movw	$3, -4096(%rbp)
	movl	$1, -4094(%rbp)
	movw	$1, -4090(%rbp)
	cmpl	$2, %r13d
	setg	%bl
	movzbl	%bl, %edi
	addl	$1, %edi
	movw	$262, -4086(%rbp)       # imm = 0x106
	movw	$3, -4084(%rbp)
	movl	$1, -4082(%rbp)
	movw	%di, -4078(%rbp)
	movw	$273, -4074(%rbp)       # imm = 0x111
	movw	$4, -4072(%rbp)
	movl	%r13d, -4070(%rbp)
	movl	$210, -4066(%rbp)
	testb	%dl, %dl
	movw	%cx, %dx
	cmovnew	%ax, %dx
	movw	$277, -4062(%rbp)       # imm = 0x115
	movw	$3, -4060(%rbp)
	movl	$1, -4058(%rbp)
	movw	%dx, -4054(%rbp)
	movw	$278, -4050(%rbp)       # imm = 0x116
	movw	$4, -4048(%rbp)
	movl	$1, -4046(%rbp)
	vpextrd	$1, %xmm0, -4042(%rbp)
	vpextrd	$1, %xmm0, %ebx
	imull	%r8d, %ebx
	imull	%ebx, %eax
	imull	%r12d, %eax
	imull	%ecx, %eax
	cmpl	$1, %r13d
	leal	210(,%r13,4), %ecx
	cmovel	%eax, %ecx
	movw	$279, -4038(%rbp)       # imm = 0x117
	movw	$4, -4036(%rbp)
	movl	%r13d, -4034(%rbp)
	movl	%ecx, -4030(%rbp)
	movw	$282, -4026(%rbp)       # imm = 0x11A
	movw	$5, -4024(%rbp)
	movl	$1, -4022(%rbp)
	movl	$194, -4018(%rbp)
	movw	$283, -4014(%rbp)       # imm = 0x11B
	movw	$5, -4012(%rbp)
	movl	$1, -4010(%rbp)
	movl	$202, -4006(%rbp)
	movw	$284, -4002(%rbp)       # imm = 0x11C
	movw	$3, -4000(%rbp)
	movl	$1, -3998(%rbp)
	movw	$2, -3994(%rbp)
	movw	$296, -3990(%rbp)       # imm = 0x128
	movw	$3, -3988(%rbp)
	movl	$1, -3986(%rbp)
	movw	$1, -3982(%rbp)
	movslq	%r15d, %rax
	movq	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE@GOTPCREL(%rip), %rcx
	movw	(%rcx,%rax,2), %ax
	movw	$339, -3978(%rbp)       # imm = 0x153
	movw	$3, -3976(%rbp)
	movl	$1, -3974(%rbp)
	movw	%ax, -3970(%rbp)
	movw	$-32539, -3966(%rbp)    # imm = 0xFFFFFFFFFFFF80E5
	movw	$4, -3964(%rbp)
	movl	$1, -3962(%rbp)
	movl	%esi, -3958(%rbp)
	vmovdqa	.LCPI61_1(%rip), %xmm0  # xmm0 = [0,1,1,1]
	vmovdqu	%xmm0, -3954(%rbp)
	movl	$1, -3938(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$210, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#4:                                 # %if.end.67
	cmpl	$2, %r13d
	jl	.LBB61_11
# BB#5:                                 # %for.body.lr.ph
	movq	%r14, %r12
	movl	%ebx, -4216(%rbp)       # 4-byte Spill
	movq	-4184(%rbp), %rax       # 8-byte Reload
	imull	%eax, %ebx
	imull	-4212(%rbp), %ebx       # 4-byte Folded Reload
	leal	210(,%r13,8), %eax
	movl	%eax, -44(%rbp)
	xorl	%r15d, %r15d
	leaq	-44(%rbp), %r14
	.align	16, 0x90
.LBB61_6:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r14, %rdi
	movq	%r12, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_16
# BB#7:                                 # %if.end.80
                                        #   in Loop: Header=BB61_6 Depth=1
	addl	%ebx, -44(%rbp)
	addl	$1, %r15d
	cmpl	%r13d, %r15d
	jl	.LBB61_6
# BB#8:                                 # %for.body.91.lr.ph
	movl	-4212(%rbp), %eax       # 4-byte Reload
	imull	-4216(%rbp), %eax       # 4-byte Folded Reload
	movl	%eax, -48(%rbp)
	xorl	%ebx, %ebx
	leaq	-48(%rbp), %r15
	movq	%r12, %r14
	.align	16, 0x90
.LBB61_10:                              # %for.body.91
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r15, %rdi
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#9:                                 # %for.cond.88
                                        #   in Loop: Header=BB61_10 Depth=1
	addl	$1, %ebx
	cmpl	%r13d, %ebx
	jl	.LBB61_10
.LBB61_11:                              # %cleanup.106.thread
	movq	-4184(%rbp), %r12       # 8-byte Reload
	jmp	.LBB61_12
.LBB61_18:                              # %if.else.116
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vmovdqa	%xmm0, -4144(%rbp)
	movl	%r15d, -4128(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$20, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_19
.LBB61_12:                              # %if.end.130
	movl	$4096, %eax             # imm = 0x1000
	xorl	%edx, %edx
	idivl	%r12d
	xorl	%r13d, %r13d
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %r10d
	testl	%r10d, %r10d
	jle	.LBB61_43
# BB#13:                                # %for.cond.135.preheader.lr.ph
	vmovd	%xmm0, %r9d
	movl	%r9d, -4216(%rbp)       # 4-byte Spill
	movl	%eax, %ecx
	imull	%r12d, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, -4232(%rbp)       # 8-byte Spill
	vmovdqa	-4208(%rbp), %xmm1      # 16-byte Reload
	vpextrd	$2, %xmm1, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovgl	%ecx, %ebx
	movq	%rbx, -4248(%rbp)       # 8-byte Spill
	leal	-128(%rbx), %r8d
	movl	%r8d, -4252(%rbp)       # 4-byte Spill
	movl	%r8d, %edi
	shrl	$7, %edi
	addl	$1, %edi
	vpextrd	$1, %xmm0, -4236(%rbp)  # 4-byte Folded Spill
	vpextrd	$2, %xmm0, %esi
	movl	%esi, -4260(%rbp)       # 4-byte Spill
	movl	%ebx, %r11d
	andl	$-128, %r11d
	movl	%r11d, -4264(%rbp)      # 4-byte Spill
	andl	$7, %edi
	movl	%edi, -4256(%rbp)       # 4-byte Spill
	movl	%edi, %r12d
	negl	%r12d
	movl	%r12d, -4268(%rbp)      # 4-byte Spill
	movl	$0, -4208(%rbp)         # 4-byte Folded Spill
	xorl	%r15d, %r15d
.LBB61_14:                              # %for.cond.143.preheader.lr.ph.us.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB61_23 Depth 2
                                        #     Child Loop BB61_26 Depth 2
                                        #     Child Loop BB61_29 Depth 2
                                        #     Child Loop BB61_40 Depth 2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	testl	%r9d, %r9d
	jle	.LBB61_20
# BB#15:                                #   in Loop: Header=BB61_14 Depth=1
	movl	$0, -4212(%rbp)         # 4-byte Folded Spill
.LBB61_40:                              # %for.body.146.lr.ph.us.us.preheader.us
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	movl	%r10d, -4240(%rbp)      # 4-byte Spill
	movl	%eax, %r12d
	movq	%r14, -4224(%rbp)       # 8-byte Spill
	movl	$0, -4176(%rbp)         # 4-byte Folded Spill
.LBB61_34:                              # %for.body.146.lr.ph.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB61_35 Depth 4
	xorl	%r13d, %r13d
	.align	16, 0x90
.LBB61_35:                              # %for.body.146.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        #       Parent Loop BB61_34 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	leal	1(%r15), %r14d
	movl	%r13d, %edi
	movl	-4176(%rbp), %esi       # 4-byte Reload
	movl	-4212(%rbp), %edx       # 4-byte Reload
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	movq	-4152(%rbp), %r8        # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t@PLT
	movq	-4184(%rbp), %rdx       # 8-byte Reload
	movl	%edx, %ecx
	imull	%r15d, %ecx
	movslq	%ecx, %rcx
	leaq	-4144(%rbp,%rcx), %rdi
	movq	%rax, %rsi
	callq	memcpy@PLT
	movl	%r14d, %r15d
	cmpl	%r12d, %r14d
	jne	.LBB61_37
# BB#36:                                # %if.then.153.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	movl	$1, %edx
	leaq	-4144(%rbp), %rdi
	movq	-4232(%rbp), %rsi       # 8-byte Reload
	movq	-4224(%rbp), %rcx       # 8-byte Reload
	callq	fwrite@PLT
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB61_41
.LBB61_37:                              # %for.inc.167.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	addl	$1, %r13d
	cmpl	-4216(%rbp), %r13d      # 4-byte Folded Reload
	jl	.LBB61_35
# BB#38:                                # %for.inc.172.us.us.us
                                        #   in Loop: Header=BB61_34 Depth=3
	movl	-4176(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4176(%rbp)       # 4-byte Spill
	cmpl	-4236(%rbp), %eax       # 4-byte Folded Reload
	jl	.LBB61_34
# BB#39:                                # %for.inc.177.us.us
                                        #   in Loop: Header=BB61_40 Depth=2
	movl	-4212(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4212(%rbp)       # 4-byte Spill
	movl	-4260(%rbp), %esi       # 4-byte Reload
	cmpl	%esi, %eax
	movq	-4224(%rbp), %r14       # 8-byte Reload
	movl	$0, %r13d
	movl	%r12d, %eax
	movl	-4240(%rbp), %r10d      # 4-byte Reload
	movl	-4216(%rbp), %r9d       # 4-byte Reload
	movq	-4248(%rbp), %rbx       # 8-byte Reload
	movl	-4252(%rbp), %r8d       # 4-byte Reload
	movl	-4256(%rbp), %edi       # 4-byte Reload
	movl	-4264(%rbp), %r11d      # 4-byte Reload
	movl	-4268(%rbp), %r12d      # 4-byte Reload
	jl	.LBB61_40
	jmp	.LBB61_30
.LBB61_20:                              # %overflow.checked.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%ebx, %ebx
	je	.LBB61_29
# BB#21:                                # %overflow.checked61
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%r11d, %r11d
	je	.LBB61_28
# BB#22:                                # %vector.body.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%ecx, %ecx
	movl	%r12d, %edx
	testl	%edi, %edi
	je	.LBB61_24
	.align	16, 0x90
.LBB61_23:                              # %vector.body.prol
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	subl	$-128, %ecx
	addl	$1, %edx
	jne	.LBB61_23
.LBB61_24:                              # %vector.body.preheader.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	cmpl	$896, %r8d              # imm = 0x380
	jb	.LBB61_28
# BB#25:                                # %vector.body.preheader.split.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	subl	%ecx, %edx
	.align	16, 0x90
.LBB61_26:                              # %vector.body
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$-1024, %edx            # imm = 0xFFFFFFFFFFFFFC00
	jne	.LBB61_26
# BB#27:                                #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
.LBB61_28:                              # %middle.block
                                        #   in Loop: Header=BB61_14 Depth=1
	cmpl	%edx, %ebx
	je	.LBB61_30
	.align	16, 0x90
.LBB61_29:                              # %overflow.checked
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$1, %edx
	cmpl	%esi, %edx
	jl	.LBB61_29
.LBB61_30:                              # %for.inc.182
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, -4208(%rbp)       # 4-byte Spill
	cmpl	%r10d, %ecx
	movq	-4184(%rbp), %rcx       # 8-byte Reload
	jl	.LBB61_14
# BB#31:                                # %for.end.186
	testl	%r15d, %r15d
	jle	.LBB61_43
# BB#32:                                # %if.then.188
	imull	%ecx, %r15d
	movslq	%r15d, %rsi
	leaq	-4144(%rbp), %rdi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_33
.LBB61_43:                              # %if.end.197
	movq	%r14, %rdi
	callq	fclose@PLT
	jmp	.LBB61_44
.LBB61_41:                              # %cleanup.184
	movq	-4224(%rbp), %rdi       # 8-byte Reload
.LBB61_42:                              # %cleanup.199
	callq	fclose@PLT
	movl	$-1, %r13d
	jmp	.LBB61_44
.LBB61_3:                               # %if.then.65
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_19:                              # %if.then.124
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_16:                              # %cleanup
	movq	%r12, %rdi
.LBB61_17:                              # %cleanup.106.thread415
	callq	fclose@PLT
	movl	$-2, %r13d
.LBB61_44:                              # %cleanup.209
	movl	%r13d, %eax
	addq	$4232, %rsp             # imm = 0x1088
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB61_33:                              # %if.then.194
	movq	%r14, %rdi
	jmp	.LBB61_42
.Lfunc_end61:
	.size	halide_debug_to_file, .Lfunc_end61-halide_debug_to_file

	.section	.text._ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,@function
_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t: # @_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%rdi), %r8
	movl	32(%rdi), %eax
	movl	36(%rdi), %edx
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	%esi, %rax
	movslq	16(%rdi), %rsi
	imulq	%r8, %rsi
	imulq	%rax, %rsi
	cmpq	%r8, %rsi
	cmovbeq	%r8, %rsi
	movl	%edx, %eax
	negl	%eax
	cmovll	%edx, %eax
	cltq
	movslq	20(%rdi), %rdx
	imulq	%r8, %rdx
	imulq	%rax, %rdx
	cmpq	%rsi, %rdx
	cmovbeq	%rsi, %rdx
	movl	40(%rdi), %eax
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	24(%rdi), %rcx
	imulq	%r8, %rcx
	movslq	%esi, %rax
	imulq	%rax, %rcx
	cmpq	%rdx, %rcx
	cmovbeq	%rdx, %rcx
	movl	44(%rdi), %eax
	movl	%eax, %edx
	negl	%edx
	cmovll	%eax, %edx
	movslq	%edx, %rdx
	movslq	28(%rdi), %rax
	imulq	%r8, %rax
	imulq	%rdx, %rax
	cmpq	%rcx, %rax
	cmovbeq	%rcx, %rax
	popq	%rbp
	retq
.Lfunc_end62:
	.size	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t, .Lfunc_end62-_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t

	.section	.text._ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,@function
_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m: # @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	memcmp@PLT
	testl	%eax, %eax
	sete	%al
	popq	%rbp
	retq
.Lfunc_end63:
	.size	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m, .Lfunc_end63-_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m

	.section	.text._ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,@function
_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_: # @_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	64(%rdi), %eax
	cmpl	64(%rsi), %eax
	jne	.LBB64_13
# BB#1:                                 # %for.cond.preheader
	movl	48(%rdi), %eax
	cmpl	48(%rsi), %eax
	jne	.LBB64_13
# BB#2:                                 # %lor.lhs.false
	movl	16(%rdi), %eax
	cmpl	16(%rsi), %eax
	jne	.LBB64_13
# BB#3:                                 # %lor.lhs.false.10
	movl	32(%rdi), %eax
	cmpl	32(%rsi), %eax
	jne	.LBB64_13
# BB#4:                                 # %for.cond
	movl	52(%rdi), %eax
	cmpl	52(%rsi), %eax
	jne	.LBB64_13
# BB#5:                                 # %lor.lhs.false.1
	movl	20(%rdi), %eax
	cmpl	20(%rsi), %eax
	jne	.LBB64_13
# BB#6:                                 # %lor.lhs.false.10.1
	movl	36(%rdi), %eax
	cmpl	36(%rsi), %eax
	jne	.LBB64_13
# BB#7:                                 # %for.cond.1
	movl	56(%rdi), %eax
	cmpl	56(%rsi), %eax
	jne	.LBB64_13
# BB#8:                                 # %lor.lhs.false.2
	movl	24(%rdi), %eax
	cmpl	24(%rsi), %eax
	jne	.LBB64_13
# BB#9:                                 # %lor.lhs.false.10.2
	movl	40(%rdi), %eax
	cmpl	40(%rsi), %eax
	jne	.LBB64_13
# BB#10:                                # %for.cond.2
	movl	60(%rdi), %eax
	cmpl	60(%rsi), %eax
	jne	.LBB64_13
# BB#11:                                # %lor.lhs.false.3
	movl	28(%rdi), %eax
	cmpl	28(%rsi), %eax
	jne	.LBB64_13
# BB#12:                                # %lor.lhs.false.10.3
	movl	44(%rdi), %eax
	cmpl	44(%rsi), %eax
	sete	%al
	popq	%rbp
	retq
.LBB64_13:                              # %cleanup
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end64:
	.size	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_, .Lfunc_end64-_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_

	.section	.text._ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,@function
_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh: # @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	-16(%rdi), %rax
	popq	%rbp
	retq
.Lfunc_end65:
	.size	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh, .Lfunc_end65-_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI66_0:
	.zero	16
	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,@function
_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_: # @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%r8, %r12
	movq	%rsi, %r14
	movq	%rdi, %r13
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r13)
	movq	$0, 16(%r13)
	movq	%rdx, 24(%r13)
	movl	%ecx, 40(%r13)
	movl	$0, 44(%r13)
	movl	%r9d, 48(%r13)
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 32(%r13)
	testq	%rax, %rax
	je	.LBB66_7
# BB#1:                                 # %if.end
	movq	64(%r12), %rcx
	movq	%rcx, 120(%r13)
	vmovups	(%r12), %ymm0
	vmovups	32(%r12), %ymm1
	vmovups	%ymm1, 88(%r13)
	vmovups	%ymm0, 56(%r13)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 56(%r13)
	movq	24(%r13), %rcx
	testq	%rcx, %rcx
	je	.LBB66_4
# BB#2:                                 # %for.body.preheader
	movb	(%r14), %dl
	movb	%dl, (%rax)
	cmpq	$2, %rcx
	jb	.LBB66_4
# BB#3:                                 # %for.body.for.body_crit_edge.preheader
	movb	1(%r14), %cl
	movb	%cl, 1(%rax)
	movl	$2, %eax
	cmpq	$2, 24(%r13)
	jbe	.LBB66_4
	.align	16, 0x90
.LBB66_8:                               # %for.body.for.body_crit_edge.for.body.for.body_crit_edge_crit_edge
                                        # =>This Inner Loop Header: Depth=1
	movq	32(%r13), %rcx
	movb	(%r14,%rax), %dl
	movb	%dl, (%rcx,%rax)
	addq	$1, %rax
	cmpq	24(%r13), %rax
	jb	.LBB66_8
.LBB66_4:                               # %for.cond.11.preheader
	movb	$1, %r15b
	cmpl	$0, 48(%r13)
	je	.LBB66_7
# BB#5:
	movq	16(%rbp), %r14
	xorl	%ebx, %ebx
	.align	16, 0x90
.LBB66_6:                               # %for.body.15
                                        # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	movl	%ebx, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movl	%ebx, %ecx
	movq	(%r14,%rcx,8), %rcx
	movq	64(%rcx), %rdx
	movq	%rdx, 64(%rax)
	vmovups	(%rcx), %ymm0
	vmovups	32(%rcx), %ymm1
	vmovups	%ymm1, 32(%rax)
	vmovups	%ymm0, (%rax)
	addl	$1, %ebx
	cmpl	48(%r13), %ebx
	jb	.LBB66_6
.LBB66_7:                               # %return
	movb	%r15b, %al
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end66:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_, .Lfunc_end66-_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,@function
_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi: # @_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	leaq	128(%rdi,%rax,8), %rax
	popq	%rbp
	retq
.Lfunc_end67:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi, .Lfunc_end67-_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,@function
_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv: # @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	32(%r14), %rsi
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	callq	halide_free@PLT
	cmpl	$0, 48(%r14)
	je	.LBB68_2
	.align	16, 0x90
.LBB68_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_device_free@PLT
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_free@PLT
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB68_1
.LBB68_2:                               # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end68:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv, .Lfunc_end68-_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv

	.section	.text._ZN6Halide7Runtime8Internal8djb_hashEPKhm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8djb_hashEPKhm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8djb_hashEPKhm,@function
_ZN6Halide7Runtime8Internal8djb_hashEPKhm: # @_ZN6Halide7Runtime8Internal8djb_hashEPKhm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$5381, %eax             # imm = 0x1505
	testq	%rsi, %rsi
	je	.LBB69_8
# BB#1:                                 # %for.body.preheader
	leaq	-1(%rsi), %r8
	xorl	%ecx, %ecx
	testb	$7, %sil
	je	.LBB69_2
# BB#3:                                 # %for.body.prol.preheader
	movl	%esi, %r9d
	andl	$7, %r9d
	movl	$5381, %eax             # imm = 0x1505
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB69_4:                               # %for.body.prol
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %edx
	movzbl	(%rdi,%rcx), %eax
	addl	%edx, %eax
	addq	$1, %rcx
	cmpq	%rcx, %r9
	jne	.LBB69_4
	jmp	.LBB69_5
.LBB69_2:
	movl	$5381, %eax             # imm = 0x1505
.LBB69_5:                               # %for.body.preheader.split
	cmpq	$7, %r8
	jb	.LBB69_8
# BB#6:                                 # %for.body.preheader.split.split
	movq	%rcx, %r9
	subq	%rsi, %r9
	leaq	(%rdi,%rcx), %r8
	xorl	%esi, %esi
	.align	16, 0x90
.LBB69_7:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %eax
	movzbl	(%r8,%rsi), %edx
	addl	%eax, %edx
	imull	$33, %edx, %r10d
	leaq	(%rcx,%rsi), %rdx
	movzbl	1(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	2(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	3(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	4(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	5(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	6(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	7(%rdi,%rdx), %eax
	addl	%r10d, %eax
	addq	$8, %rsi
	movq	%r9, %rdx
	addq	%rsi, %rdx
	jne	.LBB69_7
.LBB69_8:                               # %for.cond.cleanup
	popq	%rbp
	retq
.Lfunc_end69:
	.size	_ZN6Halide7Runtime8Internal8djb_hashEPKhm, .Lfunc_end69-_ZN6Halide7Runtime8Internal8djb_hashEPKhm

	.section	.text._ZN6Halide7Runtime8Internal11prune_cacheEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11prune_cacheEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal11prune_cacheEv,@function
_ZN6Halide7Runtime8Internal11prune_cacheEv: # @_ZN6Halide7Runtime8Internal11prune_cacheEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	movq	(%r15), %r14
	testq	%r14, %r14
	je	.LBB70_21
# BB#1:                                 # %entry
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r12
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
	jmp	.LBB70_2
	.align	16, 0x90
.LBB70_20:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, %r14
.LBB70_2:                               # %entry
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB70_6 Depth 2
                                        #     Child Loop BB70_23 Depth 2
	cmpq	%rcx, %rax
	jle	.LBB70_21
# BB#3:                                 # %while.body
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	8(%r14), %r13
	cmpl	$0, 44(%r14)
	jne	.LBB70_19
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB70_2 Depth=1
	movzbl	40(%r14), %eax
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rcx
	cmpq	%r14, %rcx
	je	.LBB70_5
	.align	16, 0x90
.LBB70_6:                               # %while.cond.9
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rcx, %rax
	testq	%rax, %rax
	je	.LBB70_22
# BB#7:                                 # %land.rhs.11
                                        #   in Loop: Header=BB70_6 Depth=2
	movq	(%rax), %rcx
	cmpq	%r14, %rcx
	jne	.LBB70_6
# BB#8:                                 # %if.end
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	%rcx, (%rax)
	jmp	.LBB70_9
.LBB70_5:                               # %if.then.6
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rdx
	movq	%rcx, (%rdx,%rax,8)
.LBB70_9:                               # %if.end.21
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	%r14, (%r15)
	jne	.LBB70_11
# BB#10:                                # %if.then.23
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, (%r15)
.LBB70_11:                              # %if.end.24
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	je	.LBB70_13
# BB#12:                                # %if.then.26
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	%rax, 16(%r13)
.LBB70_13:                              # %if.end.28
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	%r14, (%rax)
	jne	.LBB70_15
# BB#14:                                # %if.then.30
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rcx
	movq	%rax, (%rcx)
.LBB70_15:                              # %if.end.32
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	$0, 16(%r14)
	je	.LBB70_17
# BB#16:                                # %if.then.35
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, 16(%r14)
.LBB70_17:                              # %for.cond.preheader
                                        #   in Loop: Header=BB70_2 Depth=1
	xorl	%ebx, %ebx
	cmpl	$0, 48(%r14)
	je	.LBB70_18
	.align	16, 0x90
.LBB70_23:                              # %for.body
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	subq	%rax, (%r12)
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB70_23
.LBB70_18:                              # %for.cond.cleanup
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r14, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
.LBB70_19:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	jne	.LBB70_20
.LBB70_21:                              # %while.end.41
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB70_22:                              # %if.then.18
	leaq	.L.str.3.29(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_print@PLT
	callq	abort@PLT
.Lfunc_end70:
	.size	_ZN6Halide7Runtime8Internal11prune_cacheEv, .Lfunc_end70-_ZN6Halide7Runtime8Internal11prune_cacheEv

	.section	.text.halide_memoization_cache_set_size,"ax",@progbits
	.weak	halide_memoization_cache_set_size
	.align	16, 0x90
	.type	halide_memoization_cache_set_size,@function
halide_memoization_cache_set_size:      # @halide_memoization_cache_set_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	testq	%rdi, %rdi
	movl	$1048576, %ebx          # imm = 0x100000
	cmovneq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rax
	movq	%rbx, (%rax)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end71:
	.size	halide_memoization_cache_set_size, .Lfunc_end71-halide_memoization_cache_set_size

	.section	.text.halide_memoization_cache_lookup,"ax",@progbits
	.weak	halide_memoization_cache_lookup
	.align	16, 0x90
	.type	halide_memoization_cache_lookup,@function
halide_memoization_cache_lookup:        # @halide_memoization_cache_lookup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r9, -104(%rbp)         # 8-byte Spill
	movl	%r8d, -84(%rbp)         # 4-byte Spill
	movq	%rcx, -96(%rbp)         # 8-byte Spill
	movq	%rsi, -80(%rbp)         # 8-byte Spill
	movq	%rdi, -72(%rbp)         # 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %rdi
	movq	%rax, %rsi
	callq	_ZN6Halide7Runtime8Internal8djb_hashEPKhm@PLT
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movzbl	%al, %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	(%rax,%rbx,8), %rbx
	testq	%rbx, %rbx
	je	.LBB72_11
# BB#1:                                 # %while.body.lr.ph
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_18
# BB#2:                                 # %while.body.lr.ph.split.us
	cltq
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB72_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB72_8 Depth 2
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_10
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_10
# BB#5:                                 # %land.lhs.true.7.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#6:                                 # %land.lhs.true.10.us
                                        #   in Loop: Header=BB72_3 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#7:                                 # %land.lhs.true.13.us
                                        #   in Loop: Header=BB72_3 Depth=1
	xorl	%r15d, %r15d
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	movq	-104(%rbp), %r14        # 8-byte Reload
	movl	$1, %r13d
	jne	.LBB72_10
	.align	16, 0x90
.LBB72_8:                               # %for.body.us
                                        #   Parent Loop BB72_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%r14), %r12
	movq	%rbx, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	jge	.LBB72_9
# BB#43:                                # %for.body.us
                                        #   in Loop: Header=BB72_8 Depth=2
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %r14
	testb	%al, %al
	jne	.LBB72_8
.LBB72_9:                               # %for.cond.cleanup.us
                                        #   in Loop: Header=BB72_3 Depth=1
	testb	%al, %al
	jne	.LBB72_23
	.align	16, 0x90
.LBB72_10:                              # %if.end.64.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_3
	jmp	.LBB72_11
	.align	16, 0x90
.LBB72_18:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_17
# BB#19:                                # %land.lhs.true
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_17
# BB#20:                                # %land.lhs.true.7
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#21:                                # %land.lhs.true.10
                                        #   in Loop: Header=BB72_18 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#22:                                # %land.lhs.true.13
                                        #   in Loop: Header=BB72_18 Depth=1
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	je	.LBB72_23
	.align	16, 0x90
.LBB72_17:                              # %if.end.64
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_18
.LBB72_11:                              # %for.cond.66.preheader
	movl	$1, %r12d
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_42
# BB#12:                                # %for.body.69.lr.ph
	movslq	%eax, %r12
	xorl	%r15d, %r15d
	movq	-104(%rbp), %r13        # 8-byte Reload
	movq	%r13, %r14
	.align	16, 0x90
.LBB72_13:                              # %for.body.69
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rbx
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	leaq	16(%rax), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, 8(%rbx)
	testq	%rax, %rax
	je	.LBB72_14
# BB#40:                                # %for.inc.103
                                        #   in Loop: Header=BB72_13 Depth=1
	addq	$16, %rax
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	-44(%rbp), %ecx         # 4-byte Reload
	movl	%ecx, 8(%rax)
	movq	$0, (%rax)
	addq	$1, %r15
	addq	$8, %r14
	cmpq	%r12, %r15
	jl	.LBB72_13
# BB#41:
	movl	$1, %r12d
	jmp	.LBB72_42
.LBB72_14:                              # %for.cond.79.preheader
	movl	$-1, %r12d
	testl	%r15d, %r15d
	jle	.LBB72_42
# BB#15:                                # %for.body.82.lr.ph
	movslq	%r15d, %r14
	leaq	-8(%r13,%r14,8), %rbx
	addq	$1, %r14
	movq	-72(%rbp), %r15         # 8-byte Reload
	.align	16, 0x90
.LBB72_16:                              # %for.body.82
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	halide_free@PLT
	movq	(%rbx), %rax
	movq	$0, 8(%rax)
	addq	$-1, %r14
	addq	$-8, %rbx
	cmpq	$1, %r14
	jg	.LBB72_16
	jmp	.LBB72_42
.LBB72_23:                              # %if.then.22
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %r14
	cmpq	(%r14), %rbx
	movq	-104(%rbp), %r12        # 8-byte Reload
	movl	-84(%rbp), %r13d        # 4-byte Reload
	je	.LBB72_36
# BB#24:                                # %if.then.24
	cmpq	$0, 8(%rbx)
	jne	.LBB72_26
# BB#25:                                # %if.then.26
	leaq	.L.str.4.30(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_26:                              # %if.end
	movq	16(%rbx), %rax
	testq	%rax, %rax
	je	.LBB72_28
# BB#27:                                # %if.then.28
	movq	8(%rbx), %rcx
	movq	%rcx, 8(%rax)
	jmp	.LBB72_31
.LBB72_28:                              # %if.else
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	cmpq	%rbx, (%r15)
	je	.LBB72_30
# BB#29:                                # %if.then.33
	leaq	.L.str.5.31(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_30:                              # %if.end.34
	movq	8(%rbx), %rax
	movq	%rax, (%r15)
.LBB72_31:                              # %if.end.36
	movq	8(%rbx), %rax
	testq	%rax, %rax
	jne	.LBB72_33
# BB#32:                                # %if.then.39
	leaq	.L.str.6.32(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	movq	8(%rbx), %rax
.LBB72_33:                              # %if.end.40
	movq	16(%rbx), %rcx
	movq	%rcx, 16(%rax)
	movq	$0, 8(%rbx)
	movq	(%r14), %rax
	movq	%rax, 16(%rbx)
	movq	(%r14), %rax
	testq	%rax, %rax
	je	.LBB72_35
# BB#34:                                # %if.then.47
	movq	%rbx, 8(%rax)
.LBB72_35:                              # %if.end.49
	movq	%rbx, (%r14)
.LBB72_36:                              # %for.cond.52.preheader
	testl	%r13d, %r13d
	jle	.LBB72_39
# BB#37:
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB72_38:                              # %for.body.55
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12), %r15
	movq	%rbx, %rdi
	movl	%r14d, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	64(%rax), %rcx
	movq	%rcx, 64(%r15)
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	%ymm1, 32(%r15)
	vmovups	%ymm0, (%r15)
	addl	$1, %r14d
	addq	$8, %r12
	cmpl	%r14d, %r13d
	jne	.LBB72_38
.LBB72_39:                              # %for.cond.cleanup.54
	addl	%r13d, 44(%rbx)
	xorl	%r12d, %r12d
.LBB72_42:                              # %cleanup.108
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	vzeroupper
	callq	halide_mutex_unlock@PLT
	movl	%r12d, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end72:
	.size	halide_memoization_cache_lookup, .Lfunc_end72-halide_memoization_cache_lookup

	.section	.text.halide_memoization_cache_store,"ax",@progbits
	.weak	halide_memoization_cache_store
	.align	16, 0x90
	.type	halide_memoization_cache_store,@function
halide_memoization_cache_store:         # @halide_memoization_cache_store
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movq	%r9, %r13
	movl	%r8d, %r14d
	movq	%rcx, -80(%rbp)         # 8-byte Spill
	movl	%edx, -100(%rbp)        # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, -120(%rbp)        # 8-byte Spill
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	8(%rax), %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movzbl	%bl, %eax
	movq	%rax, -112(%rbp)        # 8-byte Spill
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %r12
	testq	%r12, %r12
	je	.LBB73_14
# BB#1:                                 # %while.body.lr.ph
	movslq	-100(%rbp), %rax        # 4-byte Folded Reload
	movq	%rax, -56(%rbp)         # 8-byte Spill
	testl	%r14d, %r14d
	jle	.LBB73_16
# BB#2:                                 # %while.body.lr.ph.split.us
	movslq	%r14d, %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB73_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB73_9 Depth 2
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_13
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_13
# BB#5:                                 # %land.lhs.true.12.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#6:                                 # %land.lhs.true.15.us
                                        #   in Loop: Header=BB73_3 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#7:                                 # %land.lhs.true.18.us
                                        #   in Loop: Header=BB73_3 Depth=1
	cmpl	%r14d, 48(%r12)
	jne	.LBB73_13
# BB#8:                                 #   in Loop: Header=BB73_3 Depth=1
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	movq	%r14, -136(%rbp)        # 8-byte Spill
	movb	$1, %al
	movl	%eax, -68(%rbp)         # 4-byte Spill
	xorl	%r15d, %r15d
	movq	%r13, %rbx
	movq	%r13, -128(%rbp)        # 8-byte Spill
	movl	$1, %r13d
	.align	16, 0x90
.LBB73_9:                               # %for.body.us
                                        #   Parent Loop BB73_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r14
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	movb	%al, -41(%rbp)          # 1-byte Spill
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rax
	cmpq	8(%r14), %rax
	jne	.LBB73_11
# BB#10:                                #   in Loop: Header=BB73_9 Depth=2
	movl	$0, -68(%rbp)           # 4-byte Folded Spill
.LBB73_11:                              # %select.mid
                                        #   in Loop: Header=BB73_9 Depth=2
	movb	-41(%rbp), %cl          # 1-byte Reload
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	setl	%al
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %rbx
	testb	%cl, %al
	jne	.LBB73_9
# BB#12:                                # %for.cond.cleanup.us
                                        #   in Loop: Header=BB73_3 Depth=1
	testb	%cl, %cl
	movq	-128(%rbp), %r13        # 8-byte Reload
	movq	-136(%rbp), %r14        # 8-byte Reload
	movq	-88(%rbp), %r15         # 8-byte Reload
	movq	-96(%rbp), %rbx         # 8-byte Reload
	jne	.LBB73_22
	.align	16, 0x90
.LBB73_13:                              # %if.end.56.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_3
	jmp	.LBB73_14
	.align	16, 0x90
.LBB73_16:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_21
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_21
# BB#18:                                # %land.lhs.true.12
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#19:                                # %land.lhs.true.15
                                        #   in Loop: Header=BB73_16 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#20:                                # %land.lhs.true.18
                                        #   in Loop: Header=BB73_16 Depth=1
	cmpl	%r14d, 48(%r12)
	je	.LBB73_24
	.align	16, 0x90
.LBB73_21:                              # %if.end.56
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_16
.LBB73_14:                              # %for.cond.60.preheader
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	xorl	%r15d, %r15d
	testl	%r14d, %r14d
	movq	%r14, %rax
	jle	.LBB73_15
# BB#26:
	movq	%r13, %rbx
	movl	%eax, %r14d
	movq	%rax, %r12
	.align	16, 0x90
.LBB73_27:                              # %for.body.63
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	addq	%rax, %r15
	addq	$8, %rbx
	addl	$-1, %r14d
	jne	.LBB73_27
	jmp	.LBB73_28
.LBB73_15:
	movq	%rax, %r12
.LBB73_28:                              # %for.cond.cleanup.62
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r14
	addq	%r15, (%r14)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r12, %rbx
	leal	-1(%rbx), %eax
	cltq
	leaq	(%rax,%rax,8), %rax
	leaq	200(,%rax,8), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB73_29
# BB#31:                                # %if.end.96
	movslq	-100(%rbp), %rdx        # 4-byte Folded Reload
	movq	%r13, (%rsp)
	movq	%r12, %rdi
	movq	-88(%rbp), %rsi         # 8-byte Reload
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	-80(%rbp), %r8          # 8-byte Reload
	movl	%ebx, %r9d
	callq	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_@PLT
	testb	%al, %al
	je	.LBB73_32
# BB#35:                                # %if.end.120
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rax
	movq	%rax, (%r12)
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	movq	%rcx, 16(%r12)
	testq	%rcx, %rcx
	je	.LBB73_37
# BB#36:                                # %if.then.125
	movq	%r12, 8(%rcx)
.LBB73_37:                              # %if.end.126
	movq	%r12, (%rax)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	$0, (%rax)
	jne	.LBB73_39
# BB#38:                                # %if.then.128
	movq	%r12, (%rax)
.LBB73_39:                              # %if.end.129
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	%r12, (%rcx,%rax,8)
	movl	%ebx, 44(%r12)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_40:                              # %for.body.137
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r12, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_40
	jmp	.LBB73_41
.LBB73_29:                              # %if.then.79
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_30:                              # %for.body.86
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_30
	jmp	.LBB73_41
.LBB73_32:                              # %if.then.103
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_33
	.align	16, 0x90
.LBB73_34:                              # %for.body.110
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_34
.LBB73_33:                              # %for.cond.cleanup.109
	movq	-120(%rbp), %rdi        # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB73_41:                              # %cleanup.153
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%eax, %eax
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB73_22:                              # %if.then.36
	movl	-68(%rbp), %eax         # 4-byte Reload
	testb	$1, %al
	jne	.LBB73_24
# BB#23:                                # %if.then.38
	leaq	.L.str.8.33(%rip), %rsi
	movq	-120(%rbp), %rdi        # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB73_24:                              # %for.cond.42.preheader
	testl	%r14d, %r14d
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_25:                              # %for.body.45
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %r14d
	jne	.LBB73_25
	jmp	.LBB73_41
.Lfunc_end73:
	.size	halide_memoization_cache_store, .Lfunc_end73-halide_memoization_cache_store

	.section	.text.halide_memoization_cache_release,"ax",@progbits
	.weak	halide_memoization_cache_release
	.align	16, 0x90
	.type	halide_memoization_cache_release,@function
halide_memoization_cache_release:       # @halide_memoization_cache_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	%rsi, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	(%rax), %rbx
	testq	%rbx, %rbx
	je	.LBB74_4
# BB#1:                                 # %if.else
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movl	44(%rbx), %eax
	testl	%eax, %eax
	jne	.LBB74_3
# BB#2:                                 # %if.then.6
	leaq	.L.str.11.34(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	44(%rbx), %eax
.LBB74_3:                               # %if.end
	addl	$-1, %eax
	movl	%eax, 44(%rbx)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.LBB74_4:                               # %if.then
	movq	%r14, %rdi
	movq	%rax, %rsi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_free@PLT         # TAILCALL
.Lfunc_end74:
	.size	halide_memoization_cache_release, .Lfunc_end74-halide_memoization_cache_release

	.section	.text.halide_memoization_cache_cleanup,"ax",@progbits
	.weak	halide_memoization_cache_cleanup
	.align	16, 0x90
	.type	halide_memoization_cache_cleanup,@function
halide_memoization_cache_cleanup:       # @halide_memoization_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	xorl	%r14d, %r14d
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB75_1:                               # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB75_2 Depth 2
	movq	(%r15,%r14,8), %rbx
	movq	$0, (%r15,%r14,8)
	testq	%rbx, %rbx
	je	.LBB75_3
	.align	16, 0x90
.LBB75_2:                               # %while.body
                                        #   Parent Loop BB75_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r12
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	%r12, %rbx
	testq	%r12, %r12
	jne	.LBB75_2
.LBB75_3:                               # %while.end
                                        #   in Loop: Header=BB75_1 Depth=1
	addq	$1, %r14
	cmpq	$256, %r14              # imm = 0x100
	jne	.LBB75_1
# BB#4:                                 # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_mutex_destroy@PLT # TAILCALL
.Lfunc_end75:
	.size	halide_memoization_cache_cleanup, .Lfunc_end75-halide_memoization_cache_cleanup

	.section	.text.halide_cache_cleanup,"ax",@progbits
	.weak	halide_cache_cleanup
	.align	16, 0x90
	.type	halide_cache_cleanup,@function
halide_cache_cleanup:                   # @halide_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_memoization_cache_cleanup@PLT # TAILCALL
.Lfunc_end76:
	.size	halide_cache_cleanup, .Lfunc_end76-halide_cache_cleanup

	.section	.text.halide_string_to_string,"ax",@progbits
	.weak	halide_string_to_string
	.align	16, 0x90
	.type	halide_string_to_string,@function
halide_string_to_string:                # @halide_string_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB77_3
# BB#1:                                 # %while.body.preheader
	je	.LBB77_2
	.align	16, 0x90
.LBB77_4:                               # %if.end.3
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rdx), %al
	movb	%al, (%rdi)
	testb	%al, %al
	je	.LBB77_3
# BB#5:                                 # %if.end.6
                                        #   in Loop: Header=BB77_4 Depth=1
	addq	$1, %rdi
	addq	$1, %rdx
	cmpq	%rdi, %rsi
	jne	.LBB77_4
# BB#6:
	movq	%rsi, %rdi
.LBB77_2:                               # %if.then.2
	movb	$0, -1(%rdi)
.LBB77_3:                               # %return
	movq	%rdi, %rax
	popq	%rbp
	retq
.Lfunc_end77:
	.size	halide_string_to_string, .Lfunc_end77-halide_string_to_string

	.section	.text.halide_uint64_to_string,"ax",@progbits
	.weak	halide_uint64_to_string
	.align	16, 0x90
	.type	halide_uint64_to_string,@function
halide_uint64_to_string:                # @halide_uint64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	movb	$0, -1(%rbp)
	leaq	-2(%rbp), %r8
	movl	$1, %r9d
	testq	%rdx, %rdx
	jne	.LBB78_2
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jle	.LBB78_4
	.align	16, 0x90
.LBB78_2:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdx, %r11
	movl	%r9d, %r10d
	movabsq	$-3689348814741910323, %r9 # imm = 0xCCCCCCCCCCCCCCCD
	mulxq	%r9, %rax, %rdx
	shrq	$3, %rdx
	imull	$-10, %edx, %eax
	leal	48(%r11,%rax), %eax
	movb	%al, (%r8)
	addq	$-1, %r8
	leal	1(%r10), %r9d
	cmpq	$9, %r11
	ja	.LBB78_2
# BB#3:                                 # %for.body
                                        #   in Loop: Header=BB78_2 Depth=1
	cmpl	%ecx, %r10d
	jl	.LBB78_2
.LBB78_4:                               # %for.cond.cleanup
	addq	$1, %r8
	movq	%r8, %rdx
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end78:
	.size	halide_uint64_to_string, .Lfunc_end78-halide_uint64_to_string

	.section	.text.halide_int64_to_string,"ax",@progbits
	.weak	halide_int64_to_string
	.align	16, 0x90
	.type	halide_int64_to_string,@function
halide_int64_to_string:                 # @halide_int64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB79_3
# BB#1:                                 # %entry
	testq	%rdx, %rdx
	jns	.LBB79_3
# BB#2:                                 # %if.then
	movb	$45, (%rdi)
	addq	$1, %rdi
	negq	%rdx
.LBB79_3:                               # %if.end
	popq	%rbp
	jmp	halide_uint64_to_string@PLT # TAILCALL
.Lfunc_end79:
	.size	halide_int64_to_string, .Lfunc_end79-halide_int64_to_string

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI80_0:
	.quad	-9223372036854775808    # 0x8000000000000000
	.quad	-9223372036854775808    # 0x8000000000000000
.LCPI80_6:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI80_7:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI80_1:
	.quad	4607182418800017408     # double 1
.LCPI80_2:
	.quad	4621819117588971520     # double 10
.LCPI80_3:
	.quad	4696837146684686336     # double 1.0E+6
.LCPI80_4:
	.quad	4602678819172646912     # double 0.5
.LCPI80_5:
	.quad	4890909195324358656     # double 9.2233720368547758E+18
	.section	.text.halide_double_to_string,"ax",@progbits
	.weak	halide_double_to_string
	.align	16, 0x90
	.type	halide_double_to_string,@function
halide_double_to_string:                # @halide_double_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$552, %rsp              # imm = 0x228
	movl	%edx, %ebx
	vmovapd	%xmm0, -592(%rbp)       # 16-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	vmovsd	%xmm0, -48(%rbp)
	movq	$0, -56(%rbp)
	leaq	-56(%rbp), %rdi
	leaq	-48(%rbp), %rsi
	movl	$8, %edx
	callq	memcpy@PLT
	movq	-56(%rbp), %rax
	movb	$52, %cl
	bzhiq	%rcx, %rax, %r13
	movq	%rax, %r15
	shrq	$52, %r15
	andl	$2047, %r15d            # imm = 0x7FF
	shrq	$63, %rax
	cmpl	$2047, %r15d            # imm = 0x7FF
	jne	.LBB80_9
# BB#1:                                 # %if.then
	testq	%r13, %r13
	je	.LBB80_6
# BB#2:                                 # %if.then.4
	testl	%eax, %eax
	je	.LBB80_5
# BB#3:                                 # %if.then.6
	leaq	.L.str.45(%rip), %rdx
	jmp	.LBB80_4
.LBB80_9:                               # %if.else.15
	testq	%r13, %r13
	jne	.LBB80_18
# BB#10:                                # %if.else.15
	testl	%r15d, %r15d
	jne	.LBB80_18
# BB#11:                                # %if.then.18
	testl	%ebx, %ebx
	je	.LBB80_15
# BB#12:                                # %if.then.20
	testl	%eax, %eax
	je	.LBB80_14
# BB#13:                                # %if.then.22
	leaq	.L.str.4.49(%rip), %rdx
	jmp	.LBB80_4
.LBB80_18:                              # %if.end.32
	testl	%eax, %eax
	je	.LBB80_19
# BB#20:                                # %if.then.34
	leaq	.L.str.8.53(%rip), %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r14
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
	vxorpd	.LCPI80_0(%rip), %xmm2, %xmm2
	vmovlpd	%xmm2, -48(%rbp)
	jmp	.LBB80_21
.LBB80_6:                               # %if.else.9
	testl	%eax, %eax
	je	.LBB80_8
# BB#7:                                 # %if.then.11
	leaq	.L.str.2.47(%rip), %rdx
	jmp	.LBB80_4
.LBB80_5:                               # %if.else
	leaq	.L.str.1.46(%rip), %rdx
	jmp	.LBB80_4
.LBB80_15:                              # %if.else.26
	testl	%eax, %eax
	je	.LBB80_17
# BB#16:                                # %if.then.28
	leaq	.L.str.6.51(%rip), %rdx
	jmp	.LBB80_4
.LBB80_19:
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
.LBB80_21:                              # %if.end.37
	testl	%ebx, %ebx
	je	.LBB80_36
# BB#22:                                # %while.condthread-pre-split
	xorl	%ebx, %ebx
	vmovsd	.LCPI80_1(%rip), %xmm0  # xmm0 = mem[0],zero
	vucomisd	%xmm2, %xmm0
	jbe	.LBB80_26
# BB#23:
	vmovsd	.LCPI80_2(%rip), %xmm1  # xmm1 = mem[0],zero
	.align	16, 0x90
.LBB80_24:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	vmulsd	%xmm1, %xmm2, %xmm2
	addl	$-1, %ebx
	vucomisd	%xmm2, %xmm0
	ja	.LBB80_24
# BB#25:                                # %while.cond.while.cond.41thread-pre-split_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_26:                              # %while.cond.41thread-pre-split
	vucomisd	.LCPI80_2(%rip), %xmm2
	jb	.LBB80_30
# BB#27:
	vmovsd	.LCPI80_2(%rip), %xmm0  # xmm0 = mem[0],zero
	.align	16, 0x90
.LBB80_28:                              # %while.body.43
                                        # =>This Inner Loop Header: Depth=1
	vdivsd	%xmm0, %xmm2, %xmm2
	addl	$1, %ebx
	vucomisd	%xmm0, %xmm2
	jae	.LBB80_28
# BB#29:                                # %while.cond.41.while.end.44_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_30:                              # %while.end.44
	vmovsd	.LCPI80_3(%rip), %xmm0  # xmm0 = mem[0],zero
	vfmadd213sd	.LCPI80_4(%rip), %xmm2, %xmm0
	vmovsd	.LCPI80_5(%rip), %xmm1  # xmm1 = mem[0],zero
	vsubsd	%xmm1, %xmm0, %xmm2
	vcvttsd2si	%xmm2, %rax
	movabsq	$-9223372036854775808, %rcx # imm = 0x8000000000000000
	xorq	%rax, %rcx
	vcvttsd2si	%xmm0, %rdx
	vucomisd	%xmm1, %xmm0
	cmovaeq	%rcx, %rdx
	movabsq	$4835703278458516699, %rax # imm = 0x431BDE82D7B634DB
	mulxq	%rax, %rcx, %rax
	shrq	$18, %rax
	imulq	$-1000000, %rax, %r15   # imm = 0xFFFFFFFFFFF0BDC0
	addq	%rdx, %r15
	movl	$1, %ecx
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	js	.LBB80_32
# BB#31:                                # %if.then.54
	leaq	.L.str.10.55(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	jmp	.LBB80_33
.LBB80_36:                              # %if.else.62
	testl	%r15d, %r15d
	je	.LBB80_37
# BB#38:                                # %if.end.66
	movabsq	$4503599627370496, %rax # imm = 0x10000000000000
	orq	%rax, %r13
	leal	-1075(%r15), %ecx
	cmpl	$1074, %r15d            # imm = 0x432
	ja	.LBB80_39
# BB#40:                                # %if.then.72
	cmpl	$-52, %ecx
	jge	.LBB80_42
# BB#41:
	xorl	%eax, %eax
	jmp	.LBB80_43
.LBB80_8:                               # %if.else.13
	leaq	.L.str.3.48(%rip), %rdx
	jmp	.LBB80_4
.LBB80_14:                              # %if.else.24
	leaq	.L.str.5.50(%rip), %rdx
	jmp	.LBB80_4
.LBB80_32:                              # %if.else.56
	leaq	.L.str.11.56(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	negl	%ebx
.LBB80_33:                              # %if.end.59
	movslq	%ebx, %rdx
	movl	$2, %ecx
	movq	%r12, %rsi
	jmp	.LBB80_34
.LBB80_17:                              # %if.else.30
	leaq	.L.str.7.52(%rip), %rdx
.LBB80_4:                               # %cleanup.148
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	jmp	.LBB80_35
.LBB80_37:                              # %if.then.64
	vxorpd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB80_35
.LBB80_39:
	xorl	%eax, %eax
	movq	%rax, -592(%rbp)        # 8-byte Spill
	movl	%ecx, %ebx
	jmp	.LBB80_44
.LBB80_42:                              # %if.else.76
	movl	$1075, %edx             # imm = 0x433
	subl	%r15d, %edx
	shrxq	%rdx, %r13, %rax
	shlxq	%rdx, %rax, %rdx
	subq	%rdx, %r13
.LBB80_43:                              # %if.end.84
	vmovq	%r13, %xmm0
	vmovdqa	.LCPI80_6(%rip), %xmm1  # xmm1 = [1127219200,1160773632,0,0]
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vmovapd	.LCPI80_7(%rip), %xmm2  # xmm2 = [4.503600e+15,1.934281e+25]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	shlq	$52, %rcx
	movabsq	$4696837146684686336, %rdx # imm = 0x412E848000000000
	addq	%rcx, %rdx
	vmovq	%rdx, %xmm3
	vfmadd213sd	.LCPI80_4(%rip), %xmm0, %xmm3
	vmovsd	.LCPI80_5(%rip), %xmm0  # xmm0 = mem[0],zero
	vsubsd	%xmm0, %xmm3, %xmm4
	vcvttsd2si	%xmm4, %rcx
	movabsq	$-9223372036854775808, %rdx # imm = 0x8000000000000000
	xorq	%rcx, %rdx
	vcvttsd2si	%xmm3, %rcx
	vucomisd	%xmm0, %xmm3
	cmovaeq	%rdx, %rcx
	vmovq	%rcx, %xmm0
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vucomisd	%xmm3, %xmm0
	setnp	%dl
	sete	%bl
	andb	%dl, %bl
	andb	%cl, %bl
	movzbl	%bl, %edx
	subq	%rdx, %rcx
	cmpq	$1000000, %rcx          # imm = 0xF4240
	sete	%dl
	movzbl	%dl, %r13d
	movl	$0, %edx
	cmovneq	%rcx, %rdx
	movq	%rdx, -592(%rbp)        # 8-byte Spill
	addq	%rax, %r13
	xorl	%ebx, %ebx
.LBB80_44:                              # %if.end.105
	leaq	-56(%rbp), %rsi
	leaq	-88(%rbp), %r15
	movl	$1, %ecx
	movq	%r15, %rdi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	movq	%rbx, %r13
	jle	.LBB80_65
# BB#45:                                # %for.cond.112.preheader.preheader
	testb	$1, %r13b
	jne	.LBB80_47
# BB#46:
	xorl	%r8d, %r8d
	jmp	.LBB80_53
.LBB80_47:                              # %for.cond.112.preheader.prol
	movl	$1, %r8d
	cmpq	%r15, %rax
	je	.LBB80_48
# BB#49:                                # %for.body.116.preheader.prol
	movl	$480, %ecx              # imm = 0x1E0
	subq	%rax, %rcx
	leaq	-568(%rbp,%rcx), %r11
	leaq	-1(%rax), %rsi
	xorl	%r9d, %r9d
	.align	16, 0x90
.LBB80_50:                              # %for.body.116.prol
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edi
	addl	%edi, %edi
	orl	%r9d, %edi
	movsbl	%dil, %r10d
	leal	246(%rdi), %ecx
	cmpl	$9, %r10d
	setg	%dl
	movzbl	%dl, %r9d
	cmovlel	%edi, %ecx
	addl	$48, %ecx
	movb	%cl, (%rsi)
	addq	$-1, %rsi
	addq	$1, %r11
	jne	.LBB80_50
# BB#51:                                # %for.cond.cleanup.115.prol
	cmpl	$10, %r10d
	jl	.LBB80_53
# BB#52:                                # %if.then.136.prol
	leaq	-89(%rbp), %r15
	movb	$49, -89(%rbp)
	jmp	.LBB80_53
.LBB80_48:
	movq	%rax, %r15
.LBB80_53:                              # %for.cond.112.preheader.preheader.split
	cmpl	$1, %r13d
	je	.LBB80_65
	.align	16, 0x90
.LBB80_54:                              # %for.cond.112.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB80_55 Depth 2
                                        #     Child Loop BB80_60 Depth 2
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r10
	cmpq	%r15, %rax
	je	.LBB80_59
	.align	16, 0x90
.LBB80_55:                              # %for.body.116
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edx
	addl	%edx, %edx
	orl	%r9d, %edx
	movsbl	%dl, %edi
	leal	246(%rdx), %ebx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%edx, %ebx
	addl	$48, %ebx
	movb	%bl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r15
	jne	.LBB80_55
# BB#56:                                # %for.cond.cleanup.115
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$9, %edi
	jle	.LBB80_58
# BB#57:                                # %if.then.136
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r15)
	addq	$-1, %r15
.LBB80_58:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r15, %r10
.LBB80_59:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r15
	cmpq	%r10, %rax
	je	.LBB80_64
	.align	16, 0x90
.LBB80_60:                              # %for.body.116.1
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %ebx
	addl	%ebx, %ebx
	orl	%r9d, %ebx
	movsbl	%bl, %edi
	leal	246(%rbx), %edx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%ebx, %edx
	addl	$48, %edx
	movb	%dl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r10
	jne	.LBB80_60
# BB#61:                                # %for.cond.cleanup.115.1
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$10, %edi
	jl	.LBB80_63
# BB#62:                                # %if.then.136.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r10)
	addq	$-1, %r10
.LBB80_63:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r10, %r15
.LBB80_64:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	addl	$2, %r8d
	cmpl	%r13d, %r8d
	jne	.LBB80_54
.LBB80_65:                              # %for.cond.cleanup
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	-592(%rbp), %rdx        # 8-byte Reload
.LBB80_34:                              # %cleanup.148
	callq	halide_int64_to_string@PLT
.LBB80_35:                              # %cleanup.148
	addq	$552, %rsp              # imm = 0x228
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end80:
	.size	halide_double_to_string, .Lfunc_end80-halide_double_to_string

	.section	.text.halide_pointer_to_string,"ax",@progbits
	.weak	halide_pointer_to_string
	.align	16, 0x90
	.type	halide_pointer_to_string,@function
halide_pointer_to_string:               # @halide_pointer_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, -32(%rbp)
	movl	$0, -16(%rbp)
	leaq	-14(%rbp), %r10
	leaq	-13(%rbp), %r8
	movl	$1, %ecx
	leaq	.L.str.12.57(%rip), %r9
	.align	16, 0x90
.LBB81_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	%edx, %eax
	andl	$15, %eax
	movb	(%rax,%r9), %al
	movb	%al, (%r10)
	addq	$-1, %r10
	addq	$-1, %r8
	cmpl	$15, %ecx
	jg	.LBB81_3
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB81_1 Depth=1
	shrq	$4, %rdx
	addl	$1, %ecx
	testq	%rdx, %rdx
	jne	.LBB81_1
.LBB81_3:                               # %cleanup
	movq	%r8, %rdx
	addq	$-2, %rdx
	movb	$120, (%r10)
	movb	$48, -2(%r8)
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end81:
	.size	halide_pointer_to_string, .Lfunc_end81-halide_pointer_to_string

	.section	.text.halide_get_device_handle,"ax",@progbits
	.weak	halide_get_device_handle
	.align	16, 0x90
	.type	halide_get_device_handle,@function
halide_get_device_handle:               # @halide_get_device_handle
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB82_2
# BB#1:                                 # %if.end
	movq	(%rdi), %rax
.LBB82_2:                               # %cleanup
	popq	%rbp
	retq
.Lfunc_end82:
	.size	halide_get_device_handle, .Lfunc_end82-halide_get_device_handle

	.section	.text._ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,@function
_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t: # @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %rbx
	movq	%rdi, %r14
	xorl	%eax, %eax
	cmpb	$0, 69(%rbx)
	je	.LBB83_5
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %rcx
	movl	$-14, %eax
	cmpb	$0, 68(%rbx)
	jne	.LBB83_5
# BB#2:                                 # %if.end.10
	movl	$-19, %eax
	testq	%rcx, %rcx
	je	.LBB83_5
# BB#3:                                 # %if.end.16
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	*48(%rcx)
	movl	%eax, %ecx
	movl	$-14, %eax
	testl	%ecx, %ecx
	jne	.LBB83_5
# BB#4:                                 # %if.end.25
	movb	$0, 69(%rbx)
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_buffer_is_initialized@PLT
	xorl	%eax, %eax
.LBB83_5:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end83:
	.size	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t, .Lfunc_end83-_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t

	.section	.text.halide_get_device_interface,"ax",@progbits
	.weak	halide_get_device_interface
	.align	16, 0x90
	.type	halide_get_device_interface,@function
halide_get_device_interface:            # @halide_get_device_interface
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB84_2
# BB#1:                                 # %if.end
	movq	8(%rdi), %rax
.LBB84_2:                               # %return
	popq	%rbp
	retq
.Lfunc_end84:
	.size	halide_get_device_interface, .Lfunc_end84-halide_get_device_interface

	.section	.text.halide_new_device_wrapper,"ax",@progbits
	.weak	halide_new_device_wrapper
	.align	16, 0x90
	.type	halide_new_device_wrapper,@function
halide_new_device_wrapper:              # @halide_new_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$16, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	xorl	%eax, %eax
	testq	%rbx, %rbx
	je	.LBB85_2
# BB#1:                                 # %if.end
	movq	%r14, (%rbx)
	movq	%r15, 8(%rbx)
	callq	*(%r15)
	movq	%rbx, %rax
.LBB85_2:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end85:
	.size	halide_new_device_wrapper, .Lfunc_end85-halide_new_device_wrapper

	.section	.text.halide_delete_device_wrapper,"ax",@progbits
	.weak	halide_delete_device_wrapper
	.align	16, 0x90
	.type	halide_delete_device_wrapper,@function
halide_delete_device_wrapper:           # @halide_delete_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	8(%rbx), %rax
	callq	*8(%rax)
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end86:
	.size	halide_delete_device_wrapper, .Lfunc_end86-halide_delete_device_wrapper

	.section	.text.halide_device_release,"ax",@progbits
	.weak	halide_device_release
	.align	16, 0x90
	.type	halide_device_release,@function
halide_device_release:                  # @halide_device_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmpq	*40(%rsi)               # TAILCALL
.Lfunc_end87:
	.size	halide_device_release, .Lfunc_end87-halide_device_release

	.section	.text.halide_copy_to_host,"ax",@progbits
	.weak	halide_copy_to_host
	.align	16, 0x90
	.type	halide_copy_to_host,@function
halide_copy_to_host:                    # @halide_copy_to_host
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %r15
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end88:
	.size	halide_copy_to_host, .Lfunc_end88-halide_copy_to_host

	.section	.text.halide_copy_to_device,"ax",@progbits
	.weak	halide_copy_to_device
	.align	16, 0x90
	.type	halide_copy_to_device,@function
halide_copy_to_device:                  # @halide_copy_to_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	(%r12), %rdi
	callq	halide_get_device_interface@PLT
	testq	%r15, %r15
	jne	.LBB89_2
# BB#1:                                 # %if.then
	movl	$-19, %ebx
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB89_17
.LBB89_2:                               # %if.end.24
	movq	(%r12), %rcx
	cmpq	%r15, %rax
	je	.LBB89_11
# BB#3:                                 # %if.end.24
	testq	%rcx, %rcx
	je	.LBB89_11
# BB#4:                                 # %if.then.28
	testq	%rax, %rax
	je	.LBB89_9
# BB#5:                                 # %land.lhs.true.34
	cmpb	$0, 69(%r12)
	je	.LBB89_9
# BB#6:                                 # %if.then.37
	cmpb	$0, 68(%r12)
	je	.LBB89_8
# BB#7:                                 # %if.then.40
	leaq	.L.str.25.64(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB89_8:                               # %if.end.41
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_9:                               # %if.end.50
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_device_free@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
# BB#10:                                # %if.end.58
	movb	$1, 68(%r12)
	movq	(%r12), %rcx
.LBB89_11:                              # %if.end.60
	testq	%rcx, %rcx
	jne	.LBB89_13
# BB#12:                                # %if.then.63
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_13:                              # %if.end.72
	xorl	%ebx, %ebx
	cmpb	$0, 68(%r12)
	je	.LBB89_17
# BB#14:                                # %if.then.75
	movl	$-15, %ebx
	cmpb	$0, 69(%r12)
	jne	.LBB89_17
# BB#15:                                # %if.else
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*56(%r15)
	testl	%eax, %eax
	jne	.LBB89_17
# BB#16:                                # %if.then.89
	movb	$0, 68(%r12)
	xorl	%ebx, %ebx
.LBB89_17:                              # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end89:
	.size	halide_copy_to_device, .Lfunc_end89-halide_copy_to_device

	.section	.text.halide_device_free,"ax",@progbits
	.weak	halide_device_free
	.align	16, 0x90
	.type	halide_device_free,@function
halide_device_free:                     # @halide_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r13
	movq	%rdi, %r14
	testq	%r13, %r13
	je	.LBB90_1
# BB#3:                                 # %if.then.8
	movq	(%r13), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB90_2
# BB#4:                                 # %if.then.12
	callq	*(%r12)
	movq	%r14, %rdi
	movq	%r13, %rsi
	callq	*24(%r12)
	movl	%eax, %r15d
	callq	*8(%r12)
	cmpq	$0, (%r13)
	je	.LBB90_6
# BB#5:                                 # %if.then.17
	leaq	.L.str.40(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB90_6:                               # %cleanup.22
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB90_7
.LBB90_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
.LBB90_2:                               # %if.end.23
	movb	$0, 69(%r13)
	xorl	%eax, %eax
.LBB90_7:                               # %cleanup.24
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end90:
	.size	halide_device_free, .Lfunc_end90-halide_device_free

	.section	.text.halide_device_malloc,"ax",@progbits
	.weak	halide_device_malloc
	.align	16, 0x90
	.type	halide_device_malloc,@function
halide_device_malloc:                   # @halide_device_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB91_6
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB91_6
# BB#2:                                 # %if.then
	movl	$1024, %esi             # imm = 0x400
	movq	%r14, %rdi
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB91_3
# BB#4:                                 # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.37(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB91_5
.LBB91_6:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*16(%rbx)
	movl	%eax, %r14d
	callq	*8(%rbx)
	testl	%r14d, %r14d
	movl	$-16, %eax
	cmovel	%r14d, %eax
	jmp	.LBB91_7
.LBB91_3:                               # %if.then.i
	leaq	.L.str.37(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB91_5:                               # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-16, %eax
.LBB91_7:                               # %cleanup.23
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end91:
	.size	halide_device_malloc, .Lfunc_end91-halide_device_malloc

	.section	.text.halide_device_sync,"ax",@progbits
	.weak	halide_device_sync
	.align	16, 0x90
	.type	halide_device_sync,@function
halide_device_sync:                     # @halide_device_sync
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r15
	movl	$-19, %r14d
	testq	%rbx, %rbx
	je	.LBB92_3
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB92_3
# BB#2:                                 # %if.end.2
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	*32(%rax)
	testl	%eax, %eax
	movl	$-17, %r14d
	cmovel	%eax, %r14d
.LBB92_3:                               # %cleanup.7
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end92:
	.size	halide_device_sync, .Lfunc_end92-halide_device_sync

	.section	.text.halide_weak_device_free,"ax",@progbits
	.weak	halide_weak_device_free
	.align	16, 0x90
	.type	halide_weak_device_free,@function
halide_weak_device_free:                # @halide_weak_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end93:
	.size	halide_weak_device_free, .Lfunc_end93-halide_weak_device_free

	.section	.text.halide_device_free_as_destructor,"ax",@progbits
	.weak	halide_device_free_as_destructor
	.align	16, 0x90
	.type	halide_device_free_as_destructor,@function
halide_device_free_as_destructor:       # @halide_device_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end94:
	.size	halide_device_free_as_destructor, .Lfunc_end94-halide_device_free_as_destructor

	.section	.text.halide_device_and_host_malloc,"ax",@progbits
	.weak	halide_device_and_host_malloc
	.align	16, 0x90
	.type	halide_device_and_host_malloc,@function
halide_device_and_host_malloc:          # @halide_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB95_3
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB95_3
# BB#2:                                 # %if.then
	leaq	.L.str.42(%rip), %rsi
	jmp	.LBB95_5
.LBB95_3:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*64(%rbx)
	movl	%eax, %r15d
	callq	*8(%rbx)
	xorl	%eax, %eax
	testl	%r15d, %r15d
	je	.LBB95_6
# BB#4:                                 # %if.then.21
	leaq	.L.str.43(%rip), %rsi
.LBB95_5:                               # %cleanup.22
	movq	%r14, %rdi
	callq	halide_error@PLT
	movl	$-16, %eax
.LBB95_6:                               # %cleanup.22
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end95:
	.size	halide_device_and_host_malloc, .Lfunc_end95-halide_device_and_host_malloc

	.section	.text.halide_device_and_host_free,"ax",@progbits
	.weak	halide_device_and_host_free
	.align	16, 0x90
	.type	halide_device_and_host_free,@function
halide_device_and_host_free:            # @halide_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r12
	movq	%rdi, %r14
	testq	%r12, %r12
	je	.LBB96_1
# BB#2:                                 # %if.then.8
	movq	(%r12), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB96_6
# BB#3:                                 # %if.then.12
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*72(%rbx)
	movl	%eax, %r15d
	callq	*8(%rbx)
	cmpq	$0, (%r12)
	je	.LBB96_5
# BB#4:                                 # %if.then.17
	leaq	.L.str.45.65(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB96_5:                               # %cleanup.28
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB96_9
.LBB96_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
	jmp	.LBB96_8
.LBB96_6:                               # %if.else.21
	movq	8(%r12), %rsi
	testq	%rsi, %rsi
	je	.LBB96_8
# BB#7:                                 # %if.then.23
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%r12)
.LBB96_8:                               # %if.end.29
	movb	$0, 69(%r12)
	xorl	%eax, %eax
.LBB96_9:                               # %cleanup.30
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end96:
	.size	halide_device_and_host_free, .Lfunc_end96-halide_device_and_host_free

	.section	.text.halide_default_device_and_host_malloc,"ax",@progbits
	.weak	halide_default_device_and_host_malloc
	.align	16, 0x90
	.type	halide_default_device_and_host_malloc,@function
halide_default_device_and_host_malloc:  # @halide_default_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	movq	%r14, %rdi
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %rcx
	movq	%rcx, 8(%rbx)
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB97_3
# BB#1:                                 # %if.end
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %r15d
	xorl	%eax, %eax
	testl	%r15d, %r15d
	je	.LBB97_3
# BB#2:                                 # %if.then.5
	movq	8(%rbx), %rsi
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
	movl	%r15d, %eax
.LBB97_3:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end97:
	.size	halide_default_device_and_host_malloc, .Lfunc_end97-halide_default_device_and_host_malloc

	.section	.text.halide_default_device_and_host_free,"ax",@progbits
	.weak	halide_default_device_and_host_free
	.align	16, 0x90
	.type	halide_default_device_and_host_free,@function
halide_default_device_and_host_free:    # @halide_default_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r15
	callq	halide_device_free@PLT
	movl	%eax, %r14d
	movq	8(%rbx), %rsi
	testq	%rsi, %rsi
	je	.LBB98_2
# BB#1:                                 # %if.then
	movq	%r15, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
.LBB98_2:                               # %if.end
	movw	$0, 68(%rbx)
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end98:
	.size	halide_default_device_and_host_free, .Lfunc_end98-halide_default_device_and_host_free

	.section	.text.halide_device_and_host_free_as_destructor,"ax",@progbits
	.weak	halide_device_and_host_free_as_destructor
	.align	16, 0x90
	.type	halide_device_and_host_free_as_destructor,@function
halide_device_and_host_free_as_destructor: # @halide_device_and_host_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_and_host_free@PLT # TAILCALL
.Lfunc_end99:
	.size	halide_device_and_host_free_as_destructor, .Lfunc_end99-halide_device_and_host_free_as_destructor

	.section	.text.halide_device_host_nop_free,"ax",@progbits
	.weak	halide_device_host_nop_free
	.align	16, 0x90
	.type	halide_device_host_nop_free,@function
halide_device_host_nop_free:            # @halide_device_host_nop_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end100:
	.size	halide_device_host_nop_free, .Lfunc_end100-halide_device_host_nop_free

	.section	.text.halide_float16_bits_to_float,"ax",@progbits
	.weak	halide_float16_bits_to_float
	.align	16, 0x90
	.type	halide_float16_bits_to_float,@function
halide_float16_bits_to_float:           # @halide_float16_bits_to_float
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%edi, %eax
	shll	$16, %eax
	andl	$-2147483648, %eax      # imm = 0xFFFFFFFF80000000
	movl	$1290, %ecx             # imm = 0x50A
	bextrl	%ecx, %edi, %ecx
	andl	$1023, %edi             # imm = 0x3FF
	je	.LBB101_3
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jne	.LBB101_3
# BB#2:                                 # %if.then
	lzcntl	%edi, %ecx
	xorl	$31, %ecx
	movl	$-2, %edx
	roll	%cl, %edx
	andl	%edi, %edx
	movl	$23, %esi
	subl	%ecx, %esi
	shlxl	%esi, %edx, %edx
	shll	$23, %ecx
	addl	$864026624, %ecx        # imm = 0x33800000
	orl	%eax, %ecx
	orl	%edx, %ecx
	movl	%ecx, %edi
	jmp	.LBB101_7
.LBB101_3:                              # %if.else
	shll	$13, %edi
	xorl	%edx, %edx
	testl	%ecx, %ecx
	je	.LBB101_6
# BB#4:                                 # %if.else.18
	movl	$2139095040, %edx       # imm = 0x7F800000
	cmpl	$31, %ecx
	je	.LBB101_6
# BB#5:                                 # %if.else.21
	shll	$23, %ecx
	addl	$939524096, %ecx        # imm = 0x38000000
	movl	%ecx, %edx
.LBB101_6:                              # %if.end.23
	orl	%eax, %edi
	orl	%edx, %edi
.LBB101_7:                              # %if.end.28
	vmovd	%edi, %xmm0
	popq	%rbp
	retq
.Lfunc_end101:
	.size	halide_float16_bits_to_float, .Lfunc_end101-halide_float16_bits_to_float

	.section	.text.halide_float16_bits_to_double,"ax",@progbits
	.weak	halide_float16_bits_to_double
	.align	16, 0x90
	.type	halide_float16_bits_to_double,@function
halide_float16_bits_to_double:          # @halide_float16_bits_to_double
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_float16_bits_to_float@PLT
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	popq	%rbp
	retq
.Lfunc_end102:
	.size	halide_float16_bits_to_double, .Lfunc_end102-halide_float16_bits_to_double

	.section	.text.halide_error_bounds_inference_call_failed,"ax",@progbits
	.weak	halide_error_bounds_inference_call_failed
	.align	16, 0x90
	.type	halide_error_bounds_inference_call_failed,@function
halide_error_bounds_inference_call_failed: # @halide_error_bounds_inference_call_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB103_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB103_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.68(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB103_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB103_5
.LBB103_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB103_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end103:
	.size	halide_error_bounds_inference_call_failed, .Lfunc_end103-halide_error_bounds_inference_call_failed

	.section	.text.halide_error_extern_stage_failed,"ax",@progbits
	.weak	halide_error_extern_stage_failed
	.align	16, 0x90
	.type	halide_error_extern_stage_failed,@function
halide_error_extern_stage_failed:       # @halide_error_extern_stage_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB104_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB104_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.2.70(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB104_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB104_5
.LBB104_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB104_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end104:
	.size	halide_error_extern_stage_failed, .Lfunc_end104-halide_error_extern_stage_failed

	.section	.text.halide_error_explicit_bounds_too_small,"ax",@progbits
	.weak	halide_error_explicit_bounds_too_small
	.align	16, 0x90
	.type	halide_error_explicit_bounds_too_small,@function
halide_error_explicit_bounds_too_small: # @halide_error_explicit_bounds_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movl	%r8d, -48(%rbp)         # 4-byte Spill
	movl	%ecx, -52(%rbp)         # 4-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB105_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB105_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.3.71(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.4.72(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.5.73(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r12
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.7.75(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	16(%rbp), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB105_3
# BB#4:                                 # %if.else.i.58
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB105_5
.LBB105_3:                              # %if.then.i.57
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB105_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-2, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end105:
	.size	halide_error_explicit_bounds_too_small, .Lfunc_end105-halide_error_explicit_bounds_too_small

	.section	.text.halide_error_bad_elem_size,"ax",@progbits
	.weak	halide_error_bad_elem_size
	.align	16, 0x90
	.type	halide_error_bad_elem_size,@function
halide_error_bad_elem_size:             # @halide_error_bad_elem_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%r8d, -44(%rbp)         # 4-byte Spill
	movl	%ecx, -48(%rbp)         # 4-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB106_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB106_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.9.77(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.10.78(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.11.79(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB106_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB106_5
.LBB106_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB106_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-3, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end106:
	.size	halide_error_bad_elem_size, .Lfunc_end106-halide_error_bad_elem_size

	.section	.text.halide_error_access_out_of_bounds,"ax",@progbits
	.weak	halide_error_access_out_of_bounds
	.align	16, 0x90
	.type	halide_error_access_out_of_bounds,@function
halide_error_access_out_of_bounds:      # @halide_error_access_out_of_bounds
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movl	%r8d, %r15d
	movl	%ecx, %r14d
	cmpl	%r13d, %r14d
	jge	.LBB107_6
# BB#1:                                 # %if.then
	movq	%rsi, %r15
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_3
# BB#2:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.13.81(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	jmp	.LBB107_4
.LBB107_6:                              # %if.else
	movl	16(%rbp), %r14d
	cmpl	%r14d, %r15d
	jle	.LBB107_12
# BB#7:                                 # %if.then.8
	movq	%rsi, %r13
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_9
# BB#8:                                 # %if.then.i.61
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_9:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit65
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.15.83(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
.LBB107_4:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.14.82(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB107_5
# BB#10:                                # %if.else.i.98
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB107_11
.LBB107_5:                              # %if.then.i.50
	leaq	.L.str.53(%rip), %rsi
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
.LBB107_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit103
	callq	halide_error@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB107_12:                             # %if.end.17
	movl	$-4, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end107:
	.size	halide_error_access_out_of_bounds, .Lfunc_end107-halide_error_access_out_of_bounds

	.section	.text.halide_error_buffer_allocation_too_large,"ax",@progbits
	.weak	halide_error_buffer_allocation_too_large
	.align	16, 0x90
	.type	halide_error_buffer_allocation_too_large,@function
halide_error_buffer_allocation_too_large: # @halide_error_buffer_allocation_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB108_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB108_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.16.84(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB108_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB108_5
.LBB108_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB108_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-5, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end108:
	.size	halide_error_buffer_allocation_too_large, .Lfunc_end108-halide_error_buffer_allocation_too_large

	.section	.text.halide_error_buffer_extents_negative,"ax",@progbits
	.weak	halide_error_buffer_extents_negative
	.align	16, 0x90
	.type	halide_error_buffer_extents_negative,@function
halide_error_buffer_extents_negative:   # @halide_error_buffer_extents_negative
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movl	%edx, %r13d
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB109_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB109_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.19.87(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.88(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.21.89(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB109_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB109_5
.LBB109_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB109_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-28, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end109:
	.size	halide_error_buffer_extents_negative, .Lfunc_end109-halide_error_buffer_extents_negative

	.section	.text.halide_error_buffer_extents_too_large,"ax",@progbits
	.weak	halide_error_buffer_extents_too_large
	.align	16, 0x90
	.type	halide_error_buffer_extents_too_large,@function
halide_error_buffer_extents_too_large:  # @halide_error_buffer_extents_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB110_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB110_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.22.90(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB110_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB110_5
.LBB110_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB110_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-6, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end110:
	.size	halide_error_buffer_extents_too_large, .Lfunc_end110-halide_error_buffer_extents_too_large

	.section	.text.halide_error_constraints_make_required_region_smaller,"ax",@progbits
	.weak	halide_error_constraints_make_required_region_smaller
	.align	16, 0x90
	.type	halide_error_constraints_make_required_region_smaller,@function
halide_error_constraints_make_required_region_smaller: # @halide_error_constraints_make_required_region_smaller
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	16(%rbp), %eax
	leal	-1(%r13,%rax), %edx
	movl	%edx, -60(%rbp)         # 4-byte Spill
	leal	-1(%rcx,%rax), %eax
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB111_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB111_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.23.91(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.24.92(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.25.93(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-60(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.26.94(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.27.95(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	-56(%rbp), %rcx         # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB111_3
# BB#4:                                 # %if.else.i.65
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB111_5
.LBB111_3:                              # %if.then.i.64
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB111_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-7, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end111:
	.size	halide_error_constraints_make_required_region_smaller, .Lfunc_end111-halide_error_constraints_make_required_region_smaller

	.section	.text.halide_error_constraint_violated,"ax",@progbits
	.weak	halide_error_constraint_violated
	.align	16, 0x90
	.type	halide_error_constraint_violated,@function
halide_error_constraint_violated:       # @halide_error_constraint_violated
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movl	%edx, -52(%rbp)         # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB112_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB112_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.29(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.31(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %r15         # 8-byte Reload
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB112_3
# BB#4:                                 # %if.else.i.40
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB112_5
.LBB112_3:                              # %if.then.i.39
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB112_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-8, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end112:
	.size	halide_error_constraint_violated, .Lfunc_end112-halide_error_constraint_violated

	.section	.text.halide_error_param_too_small_i64,"ax",@progbits
	.weak	halide_error_param_too_small_i64
	.align	16, 0x90
	.type	halide_error_param_too_small_i64,@function
halide_error_param_too_small_i64:       # @halide_error_param_too_small_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB113_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB113_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB113_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB113_5
.LBB113_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB113_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end113:
	.size	halide_error_param_too_small_i64, .Lfunc_end113-halide_error_param_too_small_i64

	.section	.text.halide_error_param_too_small_u64,"ax",@progbits
	.weak	halide_error_param_too_small_u64
	.align	16, 0x90
	.type	halide_error_param_too_small_u64,@function
halide_error_param_too_small_u64:       # @halide_error_param_too_small_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB114_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB114_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB114_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB114_5
.LBB114_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB114_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end114:
	.size	halide_error_param_too_small_u64, .Lfunc_end114-halide_error_param_too_small_u64

	.section	.text.halide_error_param_too_small_f64,"ax",@progbits
	.weak	halide_error_param_too_small_f64
	.align	16, 0x90
	.type	halide_error_param_too_small_f64,@function
halide_error_param_too_small_f64:       # @halide_error_param_too_small_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB115_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB115_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB115_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB115_5
.LBB115_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB115_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end115:
	.size	halide_error_param_too_small_f64, .Lfunc_end115-halide_error_param_too_small_f64

	.section	.text.halide_error_param_too_large_i64,"ax",@progbits
	.weak	halide_error_param_too_large_i64
	.align	16, 0x90
	.type	halide_error_param_too_large_i64,@function
halide_error_param_too_large_i64:       # @halide_error_param_too_large_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB116_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB116_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB116_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB116_5
.LBB116_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB116_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end116:
	.size	halide_error_param_too_large_i64, .Lfunc_end116-halide_error_param_too_large_i64

	.section	.text.halide_error_param_too_large_u64,"ax",@progbits
	.weak	halide_error_param_too_large_u64
	.align	16, 0x90
	.type	halide_error_param_too_large_u64,@function
halide_error_param_too_large_u64:       # @halide_error_param_too_large_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB117_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB117_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB117_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB117_5
.LBB117_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB117_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end117:
	.size	halide_error_param_too_large_u64, .Lfunc_end117-halide_error_param_too_large_u64

	.section	.text.halide_error_param_too_large_f64,"ax",@progbits
	.weak	halide_error_param_too_large_f64
	.align	16, 0x90
	.type	halide_error_param_too_large_f64,@function
halide_error_param_too_large_f64:       # @halide_error_param_too_large_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB118_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB118_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB118_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB118_5
.LBB118_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB118_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end118:
	.size	halide_error_param_too_large_f64, .Lfunc_end118-halide_error_param_too_large_f64

	.section	.text.halide_error_out_of_memory,"ax",@progbits
	.weak	halide_error_out_of_memory
	.align	16, 0x90
	.type	halide_error_out_of_memory,@function
halide_error_out_of_memory:             # @halide_error_out_of_memory
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.35(%rip), %rsi
	callq	halide_error@PLT
	movl	$-11, %eax
	popq	%rbp
	retq
.Lfunc_end119:
	.size	halide_error_out_of_memory, .Lfunc_end119-halide_error_out_of_memory

	.section	.text.halide_error_buffer_argument_is_null,"ax",@progbits
	.weak	halide_error_buffer_argument_is_null
	.align	16, 0x90
	.type	halide_error_buffer_argument_is_null,@function
halide_error_buffer_argument_is_null:   # @halide_error_buffer_argument_is_null
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	xorl	%r12d, %r12d
	testq	%rbx, %rbx
	je	.LBB120_2
# BB#1:                                 # %if.then.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
.LBB120_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.36(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.37.96(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB120_3
# BB#4:                                 # %if.else.i.15
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB120_5
.LBB120_3:                              # %if.then.i.14
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB120_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-12, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end120:
	.size	halide_error_buffer_argument_is_null, .Lfunc_end120-halide_error_buffer_argument_is_null

	.section	.text.halide_error_debug_to_file_failed,"ax",@progbits
	.weak	halide_error_debug_to_file_failed
	.align	16, 0x90
	.type	halide_error_debug_to_file_failed,@function
halide_error_debug_to_file_failed:      # @halide_error_debug_to_file_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB121_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB121_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.38(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.39(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.40.97(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB121_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB121_5
.LBB121_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB121_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-13, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end121:
	.size	halide_error_debug_to_file_failed, .Lfunc_end121-halide_error_debug_to_file_failed

	.section	.text.halide_error_unaligned_host_ptr,"ax",@progbits
	.weak	halide_error_unaligned_host_ptr
	.align	16, 0x90
	.type	halide_error_unaligned_host_ptr,@function
halide_error_unaligned_host_ptr:        # @halide_error_unaligned_host_ptr
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r15d
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB122_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB122_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.41(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.42.98(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.43.99(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB122_3
# BB#4:                                 # %if.else.i.23
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB122_5
.LBB122_3:                              # %if.then.i.22
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB122_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-24, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end122:
	.size	halide_error_unaligned_host_ptr, .Lfunc_end122-halide_error_unaligned_host_ptr

	.section	.text.halide_error_bad_fold,"ax",@progbits
	.weak	halide_error_bad_fold
	.align	16, 0x90
	.type	halide_error_bad_fold,@function
halide_error_bad_fold:                  # @halide_error_bad_fold
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB123_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB123_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.44(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.46.101(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB123_3
# BB#4:                                 # %if.else.i.31
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB123_5
.LBB123_3:                              # %if.then.i.30
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB123_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-25, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end123:
	.size	halide_error_bad_fold, .Lfunc_end123-halide_error_bad_fold

	.section	.text.halide_error_fold_factor_too_small,"ax",@progbits
	.weak	halide_error_fold_factor_too_small
	.align	16, 0x90
	.type	halide_error_fold_factor_too_small,@function
halide_error_fold_factor_too_small:     # @halide_error_fold_factor_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movq	%r8, -56(%rbp)          # 8-byte Spill
	movl	%ecx, %r15d
	movq	%rdx, %r13
	movq	%rsi, -64(%rbp)         # 8-byte Spill
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB124_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB124_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.47(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.48(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-64(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.49(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.50(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB124_3
# BB#4:                                 # %if.else.i.48
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB124_5
.LBB124_3:                              # %if.then.i.47
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB124_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-26, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end124:
	.size	halide_error_fold_factor_too_small, .Lfunc_end124-halide_error_fold_factor_too_small

	.section	.text.halide_error_requirement_failed,"ax",@progbits
	.weak	halide_error_requirement_failed
	.align	16, 0x90
	.type	halide_error_requirement_failed,@function
halide_error_requirement_failed:        # @halide_error_requirement_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB125_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB125_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.51(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.52(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB125_3
# BB#4:                                 # %if.else.i.19
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB125_5
.LBB125_3:                              # %if.then.i.18
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB125_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-27, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end125:
	.size	halide_error_requirement_failed, .Lfunc_end125-halide_error_requirement_failed

	.section	.text.halide_profiler_get_state,"ax",@progbits
	.weak	halide_profiler_get_state
	.align	16, 0x90
	.type	halide_profiler_get_state,@function
halide_profiler_get_state:              # @halide_profiler_get_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	_ZZ25halide_profiler_get_stateE1s(%rip), %rax
	popq	%rbp
	retq
.Lfunc_end126:
	.size	halide_profiler_get_state, .Lfunc_end126-halide_profiler_get_state

	.section	.text._ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,@function
_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy: # @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r12
	movl	%esi, %r13d
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	80(%r14), %rax
	jmp	.LBB127_1
	.align	16, 0x90
.LBB127_4:                              # %for.inc
                                        #   in Loop: Header=BB127_1 Depth=1
	movq	64(%rax), %rax
.LBB127_1:                              # %entry
                                        # =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	je	.LBB127_5
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpq	%rbx, 48(%rax)
	jne	.LBB127_4
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpl	%r13d, 72(%rax)
	jne	.LBB127_4
	jmp	.LBB127_16
.LBB127_5:                              # %for.end.critedge
	movl	$96, %edi
	callq	malloc@PLT
	movq	%rax, %r15
	xorl	%eax, %eax
	testq	%r15, %r15
	je	.LBB127_16
# BB#6:                                 # %if.end.7
	movq	80(%r14), %rax
	movq	%rax, 64(%r15)
	movq	%rbx, 48(%r15)
	movl	68(%r14), %eax
	movl	%eax, 76(%r15)
	movl	%r13d, 72(%r15)
	movl	$0, 80(%r15)
	movl	$0, 84(%r15)
	movl	$0, 88(%r15)
	movslq	%r13d, %rax
	shlq	$3, %rax
	leaq	(%rax,%rax,8), %rdi
	vxorps	%ymm0, %ymm0, %ymm0
	vmovups	%ymm0, 16(%r15)
	vmovups	%ymm0, (%r15)
	vzeroupper
	callq	malloc@PLT
	movq	%rax, 56(%r15)
	testq	%rax, %rax
	je	.LBB127_15
# BB#7:                                 # %for.cond.17.preheader
	testl	%r13d, %r13d
	jle	.LBB127_14
# BB#8:                                 # %for.body.20.preheader
	leal	-1(%r13), %r8d
	xorl	%ecx, %ecx
	testb	$3, %r13b
	je	.LBB127_11
# BB#9:                                 # %for.body.20.prol.preheader
	movl	%r13d, %esi
	andl	$3, %esi
	negl	%esi
	xorl	%ecx, %ecx
	vxorps	%ymm0, %ymm0, %ymm0
	movq	%r12, %rdi
	movq	%rax, %rbx
	.align	16, 0x90
.LBB127_10:                             # %for.body.20.prol
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rbx)
	movq	(%rdi), %rdx
	movq	%rdx, 56(%rbx)
	movl	$0, 64(%rbx)
	addq	$1, %rcx
	vmovups	%ymm0, 24(%rbx)
	vmovups	%ymm0, 8(%rbx)
	addq	$72, %rbx
	addq	$8, %rdi
	addl	$1, %esi
	jne	.LBB127_10
.LBB127_11:                             # %for.body.20.preheader.split
	cmpl	$3, %r8d
	jb	.LBB127_14
# BB#12:                                # %for.body.20.preheader.split.split
	movl	%r13d, %edx
	subl	%ecx, %edx
	leaq	(%rcx,%rcx,8), %rsi
	leaq	(%rax,%rsi,8), %rax
	leaq	(%r12,%rcx,8), %rcx
	vxorps	%ymm0, %ymm0, %ymm0
	.align	16, 0x90
.LBB127_13:                             # %for.body.20
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rax)
	movq	(%rcx), %rsi
	movq	%rsi, 56(%rax)
	movl	$0, 64(%rax)
	vmovups	%ymm0, 24(%rax)
	vmovups	%ymm0, 8(%rax)
	movq	$0, 72(%rax)
	movq	8(%rcx), %rsi
	movq	%rsi, 128(%rax)
	movl	$0, 136(%rax)
	vmovups	%ymm0, 96(%rax)
	vmovups	%ymm0, 80(%rax)
	movq	$0, 144(%rax)
	movq	16(%rcx), %rsi
	movq	%rsi, 200(%rax)
	movl	$0, 208(%rax)
	vmovups	%ymm0, 168(%rax)
	vmovups	%ymm0, 152(%rax)
	movq	$0, 216(%rax)
	movq	24(%rcx), %rsi
	movq	%rsi, 272(%rax)
	movl	$0, 280(%rax)
	vmovups	%ymm0, 240(%rax)
	vmovups	%ymm0, 224(%rax)
	addq	$288, %rax              # imm = 0x120
	addq	$32, %rcx
	addl	$-4, %edx
	jne	.LBB127_13
.LBB127_14:                             # %for.cond.cleanup.19
	addl	%r13d, 68(%r14)
	movq	%r15, 80(%r14)
	movq	%r15, %rax
	jmp	.LBB127_16
.LBB127_15:                             # %if.then.15
	movq	%r15, %rdi
	callq	free@PLT
	xorl	%eax, %eax
.LBB127_16:                             # %cleanup.62
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end127:
	.size	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy, .Lfunc_end127-_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy

	.section	.text._ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,@function
_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi: # @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	80(%rdi), %r8
	xorl	%r9d, %r9d
	testq	%r8, %r8
	je	.LBB128_8
# BB#1:
	movq	%r8, %rax
	.align	16, 0x90
.LBB128_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %r11
	movslq	76(%r11), %r10
	cmpl	%esi, %r10d
	jg	.LBB128_7
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB128_2 Depth=1
	movl	72(%r11), %eax
	addl	%r10d, %eax
	cmpl	%esi, %eax
	jg	.LBB128_4
.LBB128_7:                              # %if.end.23
                                        #   in Loop: Header=BB128_2 Depth=1
	movq	64(%r11), %rax
	movq	%r11, %r9
	testq	%rax, %rax
	jne	.LBB128_2
.LBB128_8:                              # %cleanup.25
	popq	%rbp
	retq
.LBB128_4:                              # %if.then
	testq	%r9, %r9
	je	.LBB128_6
# BB#5:                                 # %if.then.4
	movq	64(%r11), %rax
	movq	%rax, 64(%r9)
	movq	%r8, 64(%r11)
	movq	%r11, 80(%rdi)
.LBB128_6:                              # %if.end
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	shlq	$3, %rax
	addq	56(%r11), %rax
	negq	%r10
	leaq	(%r10,%r10,8), %rsi
	addq	%rdx, (%rax,%rsi,8)
	movslq	%ecx, %rcx
	movl	$1, %edi
	vmovq	%rdi, %xmm0
	vmovq	%rcx, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0]
	vpaddq	40(%rax,%rsi,8), %xmm0, %xmm1
	vmovdqu	%xmm1, 40(%rax,%rsi,8)
	addq	%rdx, (%r11)
	incl	84(%r11)
	vpaddq	32(%r11), %xmm0, %xmm0
	vmovdqu	%xmm0, 32(%r11)
	popq	%rbp
	retq
.Lfunc_end128:
	.size	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi, .Lfunc_end128-_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi

	.section	.text._ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,@function
_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv: # @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r13
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	cmpl	$-2, 72(%r13)
	je	.LBB129_11
# BB#1:                                 # %while.body.lr.ph
	leaq	-44(%rbp), %r14
	leaq	-48(%rbp), %r15
	.align	16, 0x90
.LBB129_2:                              # %while.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB129_3 Depth 2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %r12
	jmp	.LBB129_3
	.align	16, 0x90
.LBB129_9:                              # %cleanup.thread
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	64(%r13), %r12d
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%edi, %edi
	movl	%r12d, %esi
	callq	halide_sleep_ms@PLT
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %r12
.LBB129_3:                              # %while.body.3
                                        #   Parent Loop BB129_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	88(%r13), %rax
	testq	%rax, %rax
	je	.LBB129_5
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*%rax
	jmp	.LBB129_6
	.align	16, 0x90
.LBB129_5:                              # %if.else
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	72(%r13), %eax
	movl	%eax, -44(%rbp)
	movl	76(%r13), %eax
	movl	%eax, -48(%rbp)
.LBB129_6:                              # %if.end
                                        #   in Loop: Header=BB129_3 Depth=2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %rbx
	movl	-44(%rbp), %esi
	cmpl	$-2, %esi
	je	.LBB129_10
# BB#7:                                 # %if.else.10
                                        #   in Loop: Header=BB129_3 Depth=2
	testl	%esi, %esi
	js	.LBB129_9
# BB#8:                                 # %if.then.12
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%rbx, %rdx
	subq	%r12, %rdx
	movl	-48(%rbp), %ecx
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi@PLT
	jmp	.LBB129_9
	.align	16, 0x90
.LBB129_10:                             # %cleanup
                                        #   in Loop: Header=BB129_2 Depth=1
	cmpl	$-2, 72(%r13)
	jne	.LBB129_2
.LBB129_11:                             # %while.end.19
	movb	$0, 96(%r13)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end129:
	.size	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv, .Lfunc_end129-_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv

	.section	.text.halide_profiler_get_pipeline_state,"ax",@progbits
	.weak	halide_profiler_get_pipeline_state
	.align	16, 0x90
	.type	halide_profiler_get_pipeline_state,@function
halide_profiler_get_pipeline_state:     # @halide_profiler_get_pipeline_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	80(%r14), %rax
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB130_5
# BB#1:
	xorl	%r15d, %r15d
	.align	16, 0x90
.LBB130_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rbx, 48(%rax)
	je	.LBB130_3
# BB#4:                                 # %for.inc
                                        #   in Loop: Header=BB130_2 Depth=1
	movq	64(%rax), %rax
	testq	%rax, %rax
	jne	.LBB130_2
	jmp	.LBB130_5
.LBB130_3:
	movq	%rax, %r15
.LBB130_5:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end130:
	.size	halide_profiler_get_pipeline_state, .Lfunc_end130-halide_profiler_get_pipeline_state

	.section	.text.halide_profiler_pipeline_start,"ax",@progbits
	.weak	halide_profiler_pipeline_start
	.align	16, 0x90
	.type	halide_profiler_pipeline_start,@function
halide_profiler_pipeline_start:         # @halide_profiler_pipeline_start
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, %r15
	movl	%edx, %r12d
	movq	%rsi, %r13
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 96(%rbx)
	jne	.LBB131_2
# BB#1:                                 # %if.then
	movq	%r14, %rdi
	callq	halide_start_clock@PLT
	movq	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv@GOTPCREL(%rip), %rdi
	xorl	%esi, %esi
	callq	halide_spawn_thread@PLT
	movb	$1, 96(%rbx)
.LBB131_2:                              # %if.end
	movq	%r13, %rdi
	movl	%r12d, %esi
	movq	%r15, %rdx
	callq	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy@PLT
	testq	%rax, %rax
	je	.LBB131_3
# BB#4:                                 # %if.end.9
	incl	80(%rax)
	movl	76(%rax), %r14d
	jmp	.LBB131_5
.LBB131_3:                              # %if.then.7
	movq	%r14, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r14d
.LBB131_5:                              # %cleanup
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end131:
	.size	halide_profiler_pipeline_start, .Lfunc_end131-halide_profiler_pipeline_start

	.section	.text.halide_profiler_stack_peak_update,"ax",@progbits
	.weak	halide_profiler_stack_peak_update
	.align	16, 0x90
	.type	halide_profiler_stack_peak_update,@function
halide_profiler_stack_peak_update:      # @halide_profiler_stack_peak_update
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdx, %r14
	movq	%rsi, %rbx
	testq	%rbx, %rbx
	jne	.LBB132_2
# BB#1:                                 # %if.then
	leaq	.L.str.103(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB132_2:                              # %for.cond.preheader
	movl	72(%rbx), %eax
	testl	%eax, %eax
	jle	.LBB132_10
# BB#3:                                 # %for.body.lr.ph
	xorl	%edx, %edx
	.align	16, 0x90
.LBB132_4:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB132_6 Depth 2
	movq	(%r14,%rdx,8), %rsi
	testq	%rsi, %rsi
	je	.LBB132_9
# BB#5:                                 # %if.then.3
                                        #   in Loop: Header=BB132_4 Depth=1
	movq	56(%rbx), %rax
	leaq	(%rdx,%rdx,8), %rcx
	leaq	32(%rax,%rcx,8), %rdi
	movq	32(%rax,%rcx,8), %rcx
	.align	16, 0x90
.LBB132_6:                              # %while.cond.i
                                        #   Parent Loop BB132_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rsi, %rcx
	jae	.LBB132_8
# BB#7:                                 # %while.body.i
                                        #   in Loop: Header=BB132_6 Depth=2
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB132_6
.LBB132_8:                              # %for.inc.loopexit
                                        #   in Loop: Header=BB132_4 Depth=1
	movl	72(%rbx), %eax
.LBB132_9:                              # %for.inc
                                        #   in Loop: Header=BB132_4 Depth=1
	addq	$1, %rdx
	movslq	%eax, %rcx
	cmpq	%rcx, %rdx
	jl	.LBB132_4
.LBB132_10:                             # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end132:
	.size	halide_profiler_stack_peak_update, .Lfunc_end132-halide_profiler_stack_peak_update

	.section	.text.halide_profiler_memory_allocate,"ax",@progbits
	.weak	halide_profiler_memory_allocate
	.align	16, 0x90
	.type	halide_profiler_memory_allocate,@function
halide_profiler_memory_allocate:        # @halide_profiler_memory_allocate
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%r14, %r14
	je	.LBB133_13
# BB#1:                                 # %if.end
	testq	%rbx, %rbx
	jne	.LBB133_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.1.104(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_3:                              # %if.end.3
	testl	%r15d, %r15d
	jns	.LBB133_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.2.105(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_5:                              # %if.end.6
	cmpl	%r15d, 72(%rbx)
	jg	.LBB133_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.3.106(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_7:                              # %if.end.9
	movq	56(%rbx), %rdx
	lock		addl	$1, 88(%rbx)
	lock		addq	%r14, 24(%rbx)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rbx)
	movslq	%r15d, %rdi
	addq	%r14, %rsi
	movq	16(%rbx), %rcx
	addq	$16, %rbx
	.align	16, 0x90
.LBB133_8:                              # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_10
# BB#9:                                 # %while.body.i
                                        #   in Loop: Header=BB133_8 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rbx)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_8
.LBB133_10:                             # %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit
	leaq	(%rdi,%rdi,8), %rax
	lock		addl	$1, 64(%rdx,%rax,8)
	lock		addq	%r14, 24(%rdx,%rax,8)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rdx,%rax,8)
	addq	%r14, %rsi
	leaq	16(%rdx,%rax,8), %rdi
	movq	16(%rdx,%rax,8), %rcx
	.align	16, 0x90
.LBB133_11:                             # %while.cond.i.37
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_13
# BB#12:                                # %while.body.i.39
                                        #   in Loop: Header=BB133_11 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_11
.LBB133_13:                             # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end133:
	.size	halide_profiler_memory_allocate, .Lfunc_end133-halide_profiler_memory_allocate

	.section	.text.halide_profiler_memory_free,"ax",@progbits
	.weak	halide_profiler_memory_free
	.align	16, 0x90
	.type	halide_profiler_memory_free,@function
halide_profiler_memory_free:            # @halide_profiler_memory_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %rbx
	movl	%edx, %r14d
	movq	%rsi, %r15
	movq	%rdi, %r12
	testq	%rbx, %rbx
	je	.LBB134_8
# BB#1:                                 # %if.end
	testq	%r15, %r15
	jne	.LBB134_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.4.107(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_3:                              # %if.end.3
	testl	%r14d, %r14d
	jns	.LBB134_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.5.108(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_5:                              # %if.end.6
	cmpl	%r14d, 72(%r15)
	jg	.LBB134_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.6.109(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_7:                              # %if.end.9
	movq	56(%r15), %rax
	negq	%rbx
	lock		addq	%rbx, 8(%r15)
	movslq	%r14d, %rcx
	leaq	(%rcx,%rcx,8), %rcx
	lock		addq	%rbx, 8(%rax,%rcx,8)
.LBB134_8:                              # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end134:
	.size	halide_profiler_memory_free, .Lfunc_end134-halide_profiler_memory_free

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI135_0:
	.long	1232348160              # float 1.0E+6
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI135_1:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI135_2:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI135_3:
	.quad	4457293557087583675     # double 1.0E-10
	.section	.text.halide_profiler_report_unlocked,"ax",@progbits
	.weak	halide_profiler_report_unlocked
	.align	16, 0x90
	.type	halide_profiler_report_unlocked,@function
halide_profiler_report_unlocked:        # @halide_profiler_report_unlocked
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1144, %rsp             # imm = 0x478
	movq	%rdi, -1136(%rbp)       # 8-byte Spill
	leaq	-1064(%rbp), %rbx
	movb	$0, -41(%rbp)
	movq	80(%rsi), %r13
	testq	%r13, %r13
	je	.LBB135_50
# BB#1:
	leaq	-41(%rbp), %r15
	leaq	-1064(%rbp), %r12
	leaq	.L.str.20.123(%rip), %r14
	leaq	-1064(%rbp), %rbx
	.align	16, 0x90
.LBB135_2:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB135_13 Depth 2
                                        #     Child Loop BB135_16 Depth 2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movq	%r13, -1072(%rbp)       # 8-byte Spill
	movq	(%r13), %rax
	movl	%eax, %ecx
	andl	$1, %ecx
	testq	%rax, %rax
	js	.LBB135_3
# BB#4:                                 # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	vcvtsi2ssq	%rax, %xmm0, %xmm0
	jmp	.LBB135_5
	.align	16, 0x90
.LBB135_3:                              #   in Loop: Header=BB135_2 Depth=1
	shrq	%rax
	orq	%rax, %rcx
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_5:                              # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 80(%r13)
	je	.LBB135_49
# BB#6:                                 # %if.end
                                        #   in Loop: Header=BB135_2 Depth=1
	vdivss	.LCPI135_0(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movb	$0, -1064(%rbp)
	movq	32(%r13), %rax
	movq	%rax, -1120(%rbp)       # 8-byte Spill
	movq	40(%r13), %rax
	movq	%rax, -1128(%rbp)       # 8-byte Spill
	movq	48(%r13), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.7.110(%rip), %rbx
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.8.111(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.9.112(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.10.113(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	84(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.11.114(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	80(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.12.115(%rip), %rdx
	callq	halide_string_to_string@PLT
	vcvtsi2ssl	80(%r13), %xmm0, %xmm0
	vmovss	-1080(%rbp), %xmm1      # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.13.116(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1128(%rbp), %rdx       # 8-byte Reload
	movq	-1120(%rbp), %rcx       # 8-byte Reload
	cmpq	%rdx, %rcx
	je	.LBB135_8
# BB#7:                                 # %if.then.31
                                        #   in Loop: Header=BB135_2 Depth=1
	vmovq	%rcx, %xmm0
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	%rdx, %xmm1
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.14.117(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
.LBB135_8:                              # %if.end.35
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.15.118(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	88(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.16.119(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	16(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.17.120(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	cmpq	$0, (%r13)
	jne	.LBB135_14
# BB#9:                                 # %lor.end
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpq	$0, 24(%r13)
	jne	.LBB135_14
# BB#10:                                # %for.cond.50.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	movslq	72(%r13), %rax
	testq	%rax, %rax
	jle	.LBB135_49
# BB#11:                                # %for.body.53.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	56(%r13), %rcx
	addq	$32, %rcx
	xorl	%edx, %edx
	.align	16, 0x90
.LBB135_13:                             # %for.body.53
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	$0, (%rcx)
	jne	.LBB135_14
# BB#12:                                # %for.cond.50
                                        #   in Loop: Header=BB135_13 Depth=2
	addq	$1, %rdx
	addq	$72, %rcx
	cmpq	%rax, %rdx
	jl	.LBB135_13
	jmp	.LBB135_49
	.align	16, 0x90
.LBB135_14:                             # %for.cond.62.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 72(%r13)
	jle	.LBB135_49
# BB#15:                                # %if.then.i.352.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB135_16:                             # %if.then.i.352
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movb	$0, -1064(%rbp)
	movq	56(%r13), %rax
	leaq	(%rcx,%rcx,8), %r8
	leaq	(%rax,%r8,8), %rdx
	testl	%ecx, %ecx
	jne	.LBB135_18
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, (%rdx)
	movq	%r12, %rbx
	je	.LBB135_48
.LBB135_18:                             # %if.end.75
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rdx, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, -1096(%rbp)       # 8-byte Spill
	movq	%rax, %r13
	movq	%r12, %rdi
	movq	%r15, %rsi
	leaq	.L.str.18.121(%rip), %rdx
	movq	%r8, %rbx
	movq	%rbx, -1080(%rbp)       # 8-byte Spill
	callq	halide_string_to_string@PLT
	movq	56(%r13,%rbx,8), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.19.122(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$24, %rcx
	ja	.LBB135_20
	.align	16, 0x90
.LBB135_19:                             # %while.body
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$25, %rcx
	jb	.LBB135_19
.LBB135_20:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rcx       # 8-byte Reload
	movq	(%rcx), %rcx
	movl	%ecx, %edx
	andl	$1, %edx
	testq	%rcx, %rcx
	js	.LBB135_21
# BB#22:                                # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	jmp	.LBB135_23
	.align	16, 0x90
.LBB135_21:                             #   in Loop: Header=BB135_16 Depth=2
	shrq	%rcx
	orq	%rcx, %rdx
	vcvtsi2ssq	%rdx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_23:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rcx       # 8-byte Reload
	vcvtsi2ssl	80(%rcx), %xmm0, %xmm1
	vmulss	.LCPI135_0(%rip), %xmm1, %xmm1
	vdivss	%xmm1, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %edi
	testq	%rax, %rax
	je	.LBB135_25
# BB#24:                                # %if.then.i.382
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rdi
.LBB135_25:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy1024EE5eraseEi.exit
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r15, %rsi
	leaq	.L.str.21.124(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$34, %rax
	ja	.LBB135_27
	.align	16, 0x90
.LBB135_26:                             # %while.body.95
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$35, %rax
	jb	.LBB135_26
.LBB135_27:                             # %while.end.97
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rax       # 8-byte Reload
	movq	(%rax), %rsi
	movl	$0, %ebx
	testq	%rsi, %rsi
	je	.LBB135_29
# BB#28:                                # %if.then.100
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rax       # 8-byte Reload
	imulq	$100, (%rax), %rax
	xorl	%edx, %edx
	divq	%rsi
	movq	%rax, %rbx
.LBB135_29:                             # %if.end.106
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.22.125(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	%ebx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.23.126(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$42, %rax
	ja	.LBB135_31
	.align	16, 0x90
.LBB135_30:                             # %while.body.114
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$43, %rax
	jb	.LBB135_30
.LBB135_31:                             # %while.end.116
                                        #   in Loop: Header=BB135_16 Depth=2
	movl	$58, %ebx
	movq	-1128(%rbp), %rax       # 8-byte Reload
	cmpq	%rax, -1120(%rbp)       # 8-byte Folded Reload
	je	.LBB135_36
# BB#32:                                # %if.then.118
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1080(%rbp), %rax       # 8-byte Reload
	vmovq	40(%r13,%rax,8), %xmm0  # xmm0 = mem[0],zero
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	48(%r13,%rax,8), %xmm1  # xmm1 = mem[0],zero
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1088(%rbp)      # 4-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.24.127(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1088(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %ecx
	testq	%rax, %rax
	je	.LBB135_34
# BB#33:                                # %if.then.i.429
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rcx
.LBB135_34:                             # %while.cond.130.preheader
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rax
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$57, %rax
	ja	.LBB135_36
	.align	16, 0x90
.LBB135_35:                             # %while.body.133
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$58, %rax
	jb	.LBB135_35
.LBB135_36:                             # %if.end.136
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r13, %rdi
	movq	-1080(%rbp), %rsi       # 8-byte Reload
	movslq	64(%rdi,%rsi,8), %r8
	movl	$0, %eax
	testq	%r8, %r8
	je	.LBB135_38
# BB#37:                                # %if.then.140
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	24(%rdi,%rsi,8), %rax
	xorl	%edx, %edx
	divq	%r8
.LBB135_38:                             # %if.end.146
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 16(%rdi,%rsi,8)
	movq	-1072(%rbp), %r13       # 8-byte Reload
	je	.LBB135_45
# BB#39:                                # %if.then.149
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, -1104(%rbp)       # 8-byte Spill
	leaq	64(%rdi,%rsi,8), %rax
	movq	%rax, -1112(%rbp)       # 8-byte Spill
	leaq	16(%rdi,%rsi,8), %r13
	movq	%rdi, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.25.128(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	jmp	.LBB135_41
	.align	16, 0x90
.LBB135_40:                             # %while.body.157
                                        #   in Loop: Header=BB135_41 Depth=3
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
.LBB135_41:                             # %while.body.157
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rcx
	subq	%r12, %rcx
	movq	%rax, %rdi
	movq	%r15, %rsi
	cmpq	%rbx, %rcx
	jb	.LBB135_40
# BB#42:                                # %while.end.159
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.26.129(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1112(%rbp), %rcx       # 8-byte Reload
	movslq	(%rcx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	addq	$15, %rbx
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	movq	-1072(%rbp), %r13       # 8-byte Reload
	jae	.LBB135_44
	.align	16, 0x90
.LBB135_43:                             # %while.body.167
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	jb	.LBB135_43
.LBB135_44:                             # %while.end.169
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.27.130(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1104(%rbp), %rcx       # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rcx
	movq	-1088(%rbp), %rdi       # 8-byte Reload
	movq	-1080(%rbp), %rsi       # 8-byte Reload
.LBB135_45:                             # %if.end.172
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 32(%rdi,%rsi,8)
	je	.LBB135_47
# BB#46:                                # %if.then.175
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	32(%rdi,%rsi,8), %rbx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.28.131(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rcx
.LBB135_47:                             # %if.end.179
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.7.110(%rip), %rdx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	movq	-1096(%rbp), %rcx       # 8-byte Reload
.LBB135_48:                             # %cleanup.182
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$1, %rcx
	movslq	72(%r13), %rax
	cmpq	%rax, %rcx
	jl	.LBB135_16
.LBB135_49:                             # %cleanup.191
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	64(%r13), %r13
	testq	%r13, %r13
	jne	.LBB135_2
.LBB135_50:                             # %if.else.i
	movl	$1, %edx
	leaq	-1064(%rbp), %rsi
	subq	%rsi, %rdx
	addq	%rbx, %rdx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	callq	halide_msan_annotate_memory_is_initialized@PLT
	addq	$1144, %rsp             # imm = 0x478
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end135:
	.size	halide_profiler_report_unlocked, .Lfunc_end135-halide_profiler_report_unlocked

	.section	.text.halide_profiler_report,"ax",@progbits
	.weak	halide_profiler_report
	.align	16, 0x90
	.type	halide_profiler_report,@function
halide_profiler_report:                 # @halide_profiler_report
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_profiler_report_unlocked@PLT
	movq	%rbx, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end136:
	.size	halide_profiler_report, .Lfunc_end136-halide_profiler_report

	.section	.text.halide_profiler_reset,"ax",@progbits
	.weak	halide_profiler_reset
	.align	16, 0x90
	.type	halide_profiler_reset,@function
halide_profiler_reset:                  # @halide_profiler_reset
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	jmp	.LBB137_2
	.align	16, 0x90
.LBB137_1:                              # %while.body
                                        #   in Loop: Header=BB137_2 Depth=1
	movq	64(%rbx), %rax
	movq	%rax, 80(%r14)
	movq	56(%rbx), %rdi
	callq	free@PLT
	movq	%rbx, %rdi
	callq	free@PLT
.LBB137_2:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	80(%r14), %rbx
	testq	%rbx, %rbx
	jne	.LBB137_1
# BB#3:                                 # %while.end
	movl	$0, 68(%r14)
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end137:
	.size	halide_profiler_reset, .Lfunc_end137-halide_profiler_reset

	.section	.text.halide_profiler_shutdown,"ax",@progbits
	.weak	halide_profiler_shutdown
	.align	16, 0x90
	.type	halide_profiler_shutdown,@function
halide_profiler_shutdown:               # @halide_profiler_shutdown
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_profiler_get_state@PLT
	cmpb	$0, 96(%rax)
	je	.LBB138_4
# BB#1:                                 # %if.end
	movl	$-2, 72(%rax)
	.align	16, 0x90
.LBB138_2:                              # %do.body
                                        # =>This Inner Loop Header: Depth=1
	mfence
	cmpb	$0, 96(%rax)
	jne	.LBB138_2
# BB#3:                                 # %do.end
	movl	$-1, 72(%rax)
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	halide_profiler_report_unlocked@PLT # TAILCALL
.LBB138_4:                              # %cleanup
	popq	%rbp
	retq
.Lfunc_end138:
	.size	halide_profiler_shutdown, .Lfunc_end138-halide_profiler_shutdown

	.section	.text.halide_profiler_pipeline_end,"ax",@progbits
	.weak	halide_profiler_pipeline_end
	.align	16, 0x90
	.type	halide_profiler_pipeline_end,@function
halide_profiler_pipeline_end:           # @halide_profiler_pipeline_end
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$-1, 72(%rsi)
	popq	%rbp
	retq
.Lfunc_end139:
	.size	halide_profiler_pipeline_end, .Lfunc_end139-halide_profiler_pipeline_end

	.section	.text.halide_msan_annotate_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_memory_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_memory_is_initialized,@function
halide_msan_annotate_memory_is_initialized: # @halide_msan_annotate_memory_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end140:
	.size	halide_msan_annotate_memory_is_initialized, .Lfunc_end140-halide_msan_annotate_memory_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized,@function
halide_msan_annotate_buffer_is_initialized: # @halide_msan_annotate_buffer_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end141:
	.size	halide_msan_annotate_buffer_is_initialized, .Lfunc_end141-halide_msan_annotate_buffer_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized_as_destructor,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized_as_destructor
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized_as_destructor,@function
halide_msan_annotate_buffer_is_initialized_as_destructor: # @halide_msan_annotate_buffer_is_initialized_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end142:
	.size	halide_msan_annotate_buffer_is_initialized_as_destructor, .Lfunc_end142-halide_msan_annotate_buffer_is_initialized_as_destructor

	.section	.text.halide_default_can_use_target_features,"ax",@progbits
	.weak	halide_default_can_use_target_features
	.align	16, 0x90
	.type	halide_default_can_use_target_features,@function
halide_default_can_use_target_features: # @halide_default_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$24, %rsp
	movq	%rdi, %rbx
	movb	_ZZ38halide_default_can_use_target_featuresE11initialized(%rip), %al
	andb	$1, %al
	jne	.LBB143_2
# BB#1:                                 # %if.then
	leaq	-24(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv@PLT
	vmovups	-24(%rbp), %xmm0
	vmovups	%xmm0, _ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip)
	movb	$1, _ZZ38halide_default_can_use_target_featuresE11initialized(%rip)
.LBB143_2:                              # %if.end
	andq	_ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip), %rbx
	je	.LBB143_4
# BB#3:                                 # %if.then.1
	movq	_ZZ38halide_default_can_use_target_featuresE12cpu_features+8(%rip), %rcx
	andq	%rbx, %rcx
	xorl	%eax, %eax
	cmpq	%rbx, %rcx
	jne	.LBB143_5
.LBB143_4:                              # %if.end.6
	movl	$1, %eax
.LBB143_5:                              # %cleanup
	addq	$24, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end143:
	.size	halide_default_can_use_target_features, .Lfunc_end143-halide_default_can_use_target_features

	.section	.text.halide_set_custom_can_use_target_features,"ax",@progbits
	.weak	halide_set_custom_can_use_target_features
	.align	16, 0x90
	.type	halide_set_custom_can_use_target_features,@function
halide_set_custom_can_use_target_features: # @halide_set_custom_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end144:
	.size	halide_set_custom_can_use_target_features, .Lfunc_end144-halide_set_custom_can_use_target_features

	.section	.text.halide_can_use_target_features,"ax",@progbits
	.weak	halide_can_use_target_features
	.align	16, 0x90
	.type	halide_can_use_target_features,@function
halide_can_use_target_features:         # @halide_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end145:
	.size	halide_can_use_target_features, .Lfunc_end145-halide_can_use_target_features

	.section	.text._ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,@function
_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv: # @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	movl	$1, -24(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-24(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -24(%rbp)
	movl	%ebx, -20(%rbp)
	movl	%ecx, -16(%rbp)
	movl	%edx, -12(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-16(%rbp), %eax
	movl	%eax, %ecx
	andl	$524288, %ecx           # imm = 0x80000
	shrq	$15, %rcx
	movl	%eax, %edx
	shrl	$23, %edx
	andl	$32, %edx
	orq	%rcx, %rdx
	movl	%eax, %ecx
	shrl	$20, %ecx
	andl	$512, %ecx              # imm = 0x200
	orq	%rdx, %rcx
	movl	%eax, %r8d
	shrl	$5, %r8d
	andl	$128, %r8d
	orq	%rcx, %r8
	andl	$1879048192, %eax       # imm = 0x70000000
	cmpl	$1879048192, %eax       # imm = 0x70000000
	jne	.LBB146_4
# BB#1:                                 # %if.then.33
	movl	$7, -40(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-40(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -40(%rbp)
	movl	%ebx, -36(%rbp)
	movl	%ecx, -32(%rbp)
	movl	%edx, -28(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-36(%rbp), %ecx
	movl	%ecx, %eax
	andl	$32, %eax
	addq	%rax, %rax
	orq	%r8, %rax
	movl	%ecx, %edx
	andl	$268500992, %edx        # imm = 0x10010000
	cmpl	$268500992, %edx        # imm = 0x10010000
	jne	.LBB146_3
# BB#2:                                 # %if.then.44
	movl	%ecx, %edx
	andl	$469827584, %edx        # imm = 0x1C010000
	cmpl	$469827584, %edx        # imm = 0x1C010000
	movabsq	$824633720832, %rdx     # imm = 0xC000000000
	movabsq	$274877906944, %rsi     # imm = 0x4000000000
	cmoveq	%rdx, %rsi
	orq	%rsi, %rax
	movl	%ecx, %edx
	andl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	movabsq	$1099511627776, %rsi    # imm = 0x10000000000
	orq	%rax, %rsi
	cmpl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	cmovneq	%rax, %rsi
	andl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	movabsq	$2199023255552, %rax    # imm = 0x20000000000
	orq	%rsi, %rax
	cmpl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	cmovneq	%rsi, %rax
.LBB146_3:                              # %if.end.64
	movq	%rax, %r8
.LBB146_4:                              # %if.end.65
	movabsq	$4123168604912, %rax    # imm = 0x3C0000002F0
	movq	%rax, (%rdi)
	movq	%r8, 8(%rdi)
	movq	%rdi, %rax
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end146:
	.size	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv, .Lfunc_end146-_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv

	.section	.rodata,"a",@progbits
	.align	32
.LCPI147_0:
	.long	3                       # 0x3
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI147_1:
	.long	3                       # 0x3
	.long	4096                    # 0x1000
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI147_2:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
	.long	4294967288              # 0xfffffff8
	.long	4294967286              # 0xfffffff6
	.long	4294967284              # 0xfffffff4
	.long	4294967282              # 0xfffffff2
.LCPI147_3:
	.long	4294967280              # 0xfffffff0
	.long	4294967278              # 0xffffffee
	.long	4294967276              # 0xffffffec
	.long	4294967274              # 0xffffffea
	.long	4294967272              # 0xffffffe8
	.long	4294967270              # 0xffffffe6
	.long	4294967268              # 0xffffffe4
	.long	4294967266              # 0xffffffe2
.LCPI147_7:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
.LCPI147_10:
	.long	16                      # 0x10
	.long	18                      # 0x12
	.long	20                      # 0x14
	.long	22                      # 0x16
	.long	24                      # 0x18
	.long	26                      # 0x1a
	.long	28                      # 0x1c
	.long	30                      # 0x1e
.LCPI147_11:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	8                       # 0x8
	.long	10                      # 0xa
	.long	12                      # 0xc
	.long	14                      # 0xe
.LCPI147_12:
	.zero	4
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
.LCPI147_13:
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
	.zero	4
.LCPI147_14:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI147_15:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
.LCPI147_26:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
.LCPI147_27:
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI147_4:
	.long	1                       # 0x1
.LCPI147_5:
	.long	1199570688              # float 65535
.LCPI147_17:
	.long	1065353216              # float 1
.LCPI147_18:
	.long	1073741824              # float 2
.LCPI147_19:
	.long	1048576000              # float 0.25
.LCPI147_20:
	.long	1056964608              # float 0.5
.LCPI147_21:
	.long	2147483647              # 0x7fffffff
.LCPI147_22:
	.long	40                      # 0x28
.LCPI147_23:
	.long	1045220557              # float 0.200000003
.LCPI147_24:
	.long	1042983595              # float 0.166666672
.LCPI147_25:
	.long	1166012416              # float 4095
.LCPI147_28:
	.long	3212836864              # float -1
.LCPI147_29:
	.long	3186723360              # float -0.117939234
.LCPI147_30:
	.long	3190598332              # float -0.16862005
.LCPI147_31:
	.long	3196053750              # float -0.249912113
.LCPI147_32:
	.long	3204448274              # float -0.500001073
.LCPI147_33:
	.long	1028743925              # float 0.0511197634
.LCPI147_34:
	.long	1041846319              # float 0.149719939
.LCPI147_35:
	.long	1045207583              # float 0.199806675
.LCPI147_36:
	.long	1051372237              # float 0.333334357
.LCPI147_37:
	.long	1060205080              # float 0.693147182
.LCPI147_39:
	.long	1069066811              # float 1.44269502
.LCPI147_40:
	.long	3207688704              # float -0.693145751
.LCPI147_41:
	.long	3049242254              # float -1.42860677E-6
.LCPI147_42:
	.long	983314022               # float 0.00119156833
.LCPI147_43:
	.long	1026188988              # float 0.0416018814
.LCPI147_44:
	.long	1056964574              # float 0.499998987
.LCPI147_45:
	.long	967284723               # float 3.19659332E-4
.LCPI147_46:
	.long	1007360298              # float 0.00848988629
.LCPI147_47:
	.long	1042984479              # float 0.166679844
.LCPI147_48:
	.long	2139095040              # float +Inf
.LCPI147_49:
	.long	255                     # 0xff
.LCPI147_50:
	.long	901758606               # float 1.42860677E-6
.LCPI147_51:
	.long	1060205056              # float 0.693145751
.LCPI147_52:
	.long	127                     # 0x7f
.LCPI147_53:
	.long	4286578688              # float -Inf
.LCPI147_54:
	.long	1132396544              # float 255
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI147_6:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI147_8:
	.byte	0                       # 0x0
	.byte	2                       # 0x2
	.byte	4                       # 0x4
	.byte	6                       # 0x6
	.byte	8                       # 0x8
	.byte	10                      # 0xa
	.byte	12                      # 0xc
	.byte	14                      # 0xe
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
.LCPI147_9:
	.zero	16,1
.LCPI147_16:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI147_55:
	.zero	16,255
	.section	.rodata.cst8,"aM",@progbits,8
	.align	4
.LCPI147_38:
	.long	4286578688              # float -inf
	.long	2143289344              # float nan
	.section	.text.__sharpi,"ax",@progbits
	.globl	__sharpi
	.align	16, 0x90
	.type	__sharpi,@function
__sharpi:                               # @__sharpi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$6528, %rsp             # imm = 0x1980
	testq	%rdi, %rdi
	je	.LBB147_1
# BB#29:                                # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB147_30
# BB#31:                                # %assert succeeded11
	testq	%r8, %r8
	je	.LBB147_32
# BB#33:                                # %assert succeeded30
	testq	%r9, %r9
	je	.LBB147_34
# BB#35:                                # %assert succeeded49
	movq	%rcx, 5608(%rsp)        # 8-byte Spill
	movq	%r9, 5528(%rsp)         # 8-byte Spill
	cmpq	$0, 88(%rbp)
	je	.LBB147_36
# BB#37:                                # %assert succeeded68
	movq	112(%rbp), %r11
	testq	%r11, %r11
	je	.LBB147_38
# BB#39:                                # %assert succeeded87
	movslq	16(%rdi), %r13
	movq	%r13, 4840(%rsp)        # 8-byte Spill
	movl	48(%rdi), %r12d
	movq	%r12, 5536(%rsp)        # 8-byte Spill
	movl	16(%r11), %eax
	movq	%rax, 1136(%rsp)        # 8-byte Spill
	movslq	20(%r11), %r10
	movq	%r10, 448(%rsp)         # 8-byte Spill
	movl	48(%r11), %ebx
	movq	%rbx, 464(%rsp)         # 8-byte Spill
	movslq	52(%r11), %r14
	movq	%r14, 728(%rsp)         # 8-byte Spill
	leal	(%r14,%r10), %ecx
	movq	%rcx, 408(%rsp)         # 8-byte Spill
	cmpl	%edx, %ecx
	movl	%ecx, %r9d
	cmovll	%edx, %r9d
	movq	%r9, 5392(%rsp)         # 8-byte Spill
	movq	%rdx, 712(%rsp)         # 8-byte Spill
	leal	(%rbx,%rax), %eax
	movq	%rax, 424(%rsp)         # 8-byte Spill
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	%rsi, 720(%rsp)         # 8-byte Spill
	movl	%ebx, %r15d
	sarl	$31, %r15d
	andl	%ebx, %r15d
	movq	%r15, 5288(%rsp)        # 8-byte Spill
	leal	-8(%r15), %edx
	movq	%rdx, 3000(%rsp)        # 8-byte Spill
	leal	-1(%r12,%r13), %ebx
	movl	%ebx, 4288(%rsp)        # 4-byte Spill
	cmpl	%edx, %ebx
	movl	%ebx, %ecx
	cmovgl	%edx, %ecx
	cmpl	%r12d, %ecx
	cmovll	%r12d, %ecx
	leal	(%r13,%r13), %edx
	movl	$2, %esi
	subl	%edx, %esi
	cmpl	$1, %edx
	leal	-2(%r13,%r13), %edx
	movl	%edx, 4272(%rsp)        # 4-byte Spill
	cmovgl	%edx, %esi
	subl	%r13d, %esi
	leal	-1(%r13), %edx
	cmpl	%esi, %edx
	cmoval	%edx, %esi
	leal	-1(%r13,%r12), %edx
	subl	%esi, %edx
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	movl	%edx, 4896(%rsp)        # 4-byte Spill
	cmpl	$1, %eax
	leal	-1(%rax), %eax
	movl	%eax, 384(%rsp)         # 4-byte Spill
	movl	$0, %ecx
	cmovgl	%eax, %ecx
	subl	%r15d, %ecx
	movq	%rcx, 5440(%rsp)        # 8-byte Spill
	leal	16(%rcx), %eax
	andl	$-32, %eax
	leal	23(%r15,%rax), %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%r12d, %eax
	cmovll	%r12d, %eax
	leal	(%r12,%r13), %ecx
	movq	%rcx, 2096(%rsp)        # 8-byte Spill
	cmpl	%ecx, %eax
	leal	1(%rax), %eax
	cmovll	%ecx, %eax
	movl	%eax, 4880(%rsp)        # 4-byte Spill
	movl	%r14d, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	movq	%rax, 632(%rsp)         # 8-byte Spill
	movslq	20(%rdi), %rbx
	movq	%rbx, 1344(%rsp)        # 8-byte Spill
	movslq	52(%rdi), %rsi
	movq	%rsi, 1352(%rsp)        # 8-byte Spill
	leal	-8(%rax), %ecx
	movl	%ecx, 5696(%rsp)        # 4-byte Spill
	leal	-1(%rsi,%rbx), %r12d
	movl	%r12d, 1340(%rsp)       # 4-byte Spill
	cmpl	%ecx, %r12d
	movl	%r12d, %eax
	cmovgl	%ecx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	leal	(%rbx,%rbx), %ecx
	movl	$2, %edx
	subl	%ecx, %edx
	cmpl	$1, %ecx
	leal	-2(%rbx,%rbx), %ecx
	movl	%ecx, 1332(%rsp)        # 4-byte Spill
	cmovgl	%ecx, %edx
	movl	%edx, 1336(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	subl	%ebx, %ecx
	leal	-1(%rbx), %edx
	cmpl	%ecx, %edx
	cmoval	%edx, %ecx
	leal	-1(%rbx,%rsi), %edx
	movl	%edx, 1328(%rsp)        # 4-byte Spill
	subl	%ecx, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	movl	%edx, 4872(%rsp)        # 4-byte Spill
	cmpl	$1, %r9d
	leal	-1(%r9), %eax
	movl	%eax, 388(%rsp)         # 4-byte Spill
	movl	$0, %ecx
	cmovgl	%eax, %ecx
	movq	%rcx, 5632(%rsp)        # 8-byte Spill
	movl	$0, %eax
	cmovgl	%r9d, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	leal	8(%rcx), %edx
	movl	%edx, 5152(%rsp)        # 4-byte Spill
	cmpl	%edx, %r12d
	movl	%r12d, %eax
	cmovgl	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	leal	(%rsi,%rbx), %ecx
	movl	%ecx, 708(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	leal	1(%rax), %eax
	cmovll	%ecx, %eax
	movl	%eax, 4864(%rsp)        # 4-byte Spill
	leal	-32(%r14,%r10), %eax
	movl	%eax, 380(%rsp)         # 4-byte Spill
	cmpl	%r14d, %eax
	cmovgl	%r14d, %eax
	movl	%eax, 460(%rsp)         # 4-byte Spill
	leal	-1(%r10), %eax
	movq	%rax, 440(%rsp)         # 8-byte Spill
	orl	$31, %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	leal	(%rax,%r14), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	-1(%r14,%r10), %ecx
	movl	%ecx, 404(%rsp)         # 4-byte Spill
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 4912(%rsp)        # 4-byte Spill
	movq	5608(%rsp), %rdx        # 8-byte Reload
	movl	48(%rdx), %r13d
	movq	%r13, 4680(%rsp)        # 8-byte Spill
	movl	$1, %eax
	subl	%r13d, %eax
	movq	(%rdi), %rcx
	movq	%rcx, 5248(%rsp)        # 8-byte Spill
	movq	8(%rdi), %rcx
	movq	%rcx, 4984(%rsp)        # 8-byte Spill
	movl	32(%rdi), %ecx
	movl	%ecx, 4384(%rsp)        # 4-byte Spill
	movslq	36(%rdi), %rcx
	movq	%rcx, 1680(%rsp)        # 8-byte Spill
	movl	64(%rdi), %ecx
	movl	%ecx, 4576(%rsp)        # 4-byte Spill
	movq	%rdi, 4768(%rsp)        # 8-byte Spill
	movq	(%rdx), %rcx
	movq	%rcx, 5328(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 5464(%rsp)        # 8-byte Spill
	movslq	16(%rdx), %r9
	movq	%r9, 4712(%rsp)         # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 5616(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4512(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 4608(%rsp)        # 8-byte Spill
	movl	52(%rdx), %r10d
	movq	%r10, 4672(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4616(%rsp)        # 4-byte Spill
	movq	(%r8), %rcx
	movq	%rcx, 5344(%rsp)        # 8-byte Spill
	movq	8(%r8), %rcx
	movq	%rcx, 1824(%rsp)        # 8-byte Spill
	movslq	16(%r8), %rcx
	movq	%rcx, 5408(%rsp)        # 8-byte Spill
	movslq	20(%r8), %rcx
	movq	%rcx, 1784(%rsp)        # 8-byte Spill
	movl	32(%r8), %ecx
	movl	%ecx, 4480(%rsp)        # 4-byte Spill
	movslq	36(%r8), %rcx
	movq	%rcx, 1816(%rsp)        # 8-byte Spill
	movl	48(%r8), %ecx
	movq	%rcx, 5424(%rsp)        # 8-byte Spill
	movl	52(%r8), %ecx
	movq	%rcx, 1752(%rsp)        # 8-byte Spill
	movl	64(%r8), %ecx
	movl	%ecx, 4584(%rsp)        # 4-byte Spill
	movq	%r8, 4784(%rsp)         # 8-byte Spill
	movq	5528(%rsp), %rdx        # 8-byte Reload
	movq	(%rdx), %rcx
	movq	%rcx, 5184(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 840(%rsp)         # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 4800(%rsp)        # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 4632(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4320(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 4688(%rsp)        # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 4720(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4560(%rsp)        # 4-byte Spill
	movq	88(%rbp), %rcx
	movq	%rcx, %rdx
	movq	(%rdx), %rcx
	movq	%rcx, 5312(%rsp)        # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 5568(%rsp)        # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 4640(%rsp)        # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 4648(%rsp)        # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 4416(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 4832(%rsp)        # 8-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 4656(%rsp)        # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 4816(%rsp)        # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 4568(%rsp)        # 4-byte Spill
	movq	(%r11), %rcx
	movq	%rcx, 5296(%rsp)        # 8-byte Spill
	movq	8(%r11), %rcx
	movq	%rcx, 848(%rsp)         # 8-byte Spill
	movl	24(%r11), %ecx
	movq	%rcx, 4448(%rsp)        # 8-byte Spill
	movl	32(%r11), %ecx
	movl	%ecx, 4304(%rsp)        # 4-byte Spill
	movslq	36(%r11), %rcx
	movq	%rcx, 392(%rsp)         # 8-byte Spill
	movl	40(%r11), %ecx
	movl	%ecx, 4352(%rsp)        # 4-byte Spill
	movl	56(%r11), %ecx
	movq	%rcx, 4624(%rsp)        # 8-byte Spill
	movl	64(%r11), %ecx
	movl	%ecx, 4552(%rsp)        # 4-byte Spill
	leal	(%r9,%r9), %r11d
	cltd
	idivl	%r11d
	movl	%r11d, %eax
	negl	%eax
	movl	%r9d, %edi
	sarl	$31, %edi
	andnl	%r11d, %edi, %ecx
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	movl	%ebx, 4128(%rsp)        # 4-byte Spill
	movl	%r11d, %esi
	subl	%ebx, %esi
	addl	$-1, %esi
	movl	%esi, 4248(%rsp)        # 4-byte Spill
	cmpl	%ebx, %esi
	movl	%ebx, %eax
	cmovgel	%esi, %eax
	movl	%eax, 5664(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	movl	%r13d, %eax
	negl	%eax
	cltd
	idivl	%r11d
	movl	%ecx, %r14d
	sarl	$31, %r14d
	andl	%edi, %r14d
	addl	%ecx, %r14d
	movl	%r14d, 4104(%rsp)       # 4-byte Spill
	movl	%edx, %r15d
	sarl	$31, %r15d
	andl	%edi, %r15d
	addl	%edx, %r15d
	movl	%r15d, 4096(%rsp)       # 4-byte Spill
	movl	%r11d, %r12d
	subl	%r14d, %r12d
	addl	$-1, %r12d
	movl	%r12d, 4160(%rsp)       # 4-byte Spill
	cmpl	%r12d, %r14d
	movl	%r14d, %r8d
	cmovgl	%r12d, %r8d
	addl	%r13d, %r8d
	leal	-1(%r13,%r9), %eax
	movl	%eax, 5376(%rsp)        # 4-byte Spill
	cmpl	%r8d, %eax
	cmovlel	%eax, %r8d
	subl	%r15d, %r11d
	addl	$-1, %r11d
	movl	%r11d, 4824(%rsp)       # 4-byte Spill
	cmpl	%r11d, %r15d
	movl	%r15d, %edx
	cmovgl	%r11d, %edx
	addl	%r13d, %edx
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	cmpl	%esi, %ebx
	movl	%ebx, %ecx
	cmovgl	%esi, %ecx
	addl	%r13d, %ecx
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	%ecx, %eax
	sarl	$31, %eax
	cmpl	$3, %r8d
	movl	$2, %esi
	cmovll	%r8d, %esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	movl	%edx, %r9d
	sarl	$31, %r9d
	andl	%edx, %r9d
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	movl	%esi, %ebx
	sarl	$31, %ebx
	testl	%esi, %esi
	setg	%dil
	andl	%esi, %ebx
	movzbl	%dil, %esi
	orl	%esi, %ebx
	cmpl	%edx, %ebx
	cmovgl	%edx, %ebx
	movl	%ebx, %edi
	sarl	$31, %edi
	andl	%ebx, %edi
	cmpl	%ecx, %edi
	cmovgl	%ecx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	testl	%edi, %edi
	setg	%bl
	andl	%edi, %esi
	movzbl	%bl, %edi
	orl	%edi, %esi
	cmpl	%r8d, %esi
	cmovgl	%r8d, %esi
	cmpl	$3, %esi
	movl	$2, %edi
	cmovgel	%edi, %esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	movl	%esi, %edi
	sarl	$31, %edi
	testl	%esi, %esi
	setg	%bl
	andl	%esi, %edi
	movzbl	%bl, %esi
	orl	%esi, %edi
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	movl	%edi, %edx
	sarl	$31, %edx
	andl	%edi, %edx
	cmpl	%r8d, %edx
	cmovgl	%r8d, %edx
	testl	%ecx, %ecx
	setg	%bl
	andl	%ecx, %eax
	movzbl	%bl, %ecx
	orl	%ecx, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	$3, %eax
	movl	$2, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r13d, %eax
	cmovll	%r13d, %eax
	movl	%eax, 4752(%rsp)        # 4-byte Spill
	movl	5664(%rsp), %edx        # 4-byte Reload
	addl	%r13d, %edx
	cmpl	%r14d, %r12d
	cmovgel	%r12d, %r14d
	addl	%r13d, %r14d
	cmpl	$1, %r14d
	cmovlel	%ecx, %r14d
	cmpl	%r14d, %edx
	cmovgel	%edx, %r14d
	cmpl	%r15d, %r11d
	cmovgel	%r11d, %r15d
	addl	%r13d, %r15d
	movl	$0, %ecx
	cmovsl	%ecx, %r15d
	cmpl	%r15d, %r14d
	cmovgel	%r14d, %r15d
	cmpl	%edx, %r15d
	cmovll	%edx, %r15d
	cmpl	$2, %r15d
	setl	%al
	cmpl	$1, %r15d
	cmovlel	%ecx, %r15d
	movzbl	%al, %eax
	orl	%eax, %r15d
	movl	5376(%rsp), %eax        # 4-byte Reload
	cmpl	%r15d, %eax
	cmovlel	%eax, %r15d
	cmpl	%r13d, %r15d
	cmovll	%r13d, %r15d
	movl	%r15d, 5360(%rsp)       # 4-byte Spill
	movq	5616(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rdx), %esi
	movl	%esi, 4256(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%esi, %ecx, %edi
	andl	%eax, %ecx
	orl	%edi, %ecx
	movq	5440(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %edi
	andl	$-8, %edi
	movq	%rdi, 5120(%rsp)        # 8-byte Spill
	movl	%esi, %r9d
	subl	%ecx, %r9d
	cmovgel	%esi, %ecx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	7(%rdi,%r12), %eax
	movq	%rdi, %r8
	leal	-1(%r10,%rdx), %esi
	movq	%rdx, %r15
	movl	%esi, 4192(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	leal	-1(%rcx,%r10), %edi
	movl	%edi, 4992(%rsp)        # 4-byte Spill
	addl	%r10d, %ecx
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5136(%rsp)        # 4-byte Spill
	leal	12(%rbx), %eax
	movq	%rbx, %r11
	andl	$-8, %eax
	movq	%rax, 5088(%rsp)        # 8-byte Spill
	movl	%eax, %r14d
	movq	%r12, %rbx
	addl	%ebx, %r14d
	leal	1(%rax,%rbx), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5104(%rsp)        # 4-byte Spill
	leal	3(%rax,%rbx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5056(%rsp)        # 4-byte Spill
	leal	(%r10,%r15), %edx
	movq	%rdx, 5472(%rsp)        # 8-byte Spill
	cmpl	%r14d, %edx
	movl	%edx, %eax
	movq	%rdx, %r12
	cmovgl	%r14d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	addl	$-1, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, 5040(%rsp)        # 4-byte Spill
	leal	4(%r11), %ecx
	andl	$-8, %ecx
	movq	%rcx, 5032(%rsp)        # 8-byte Spill
	movq	%rbx, %rax
	leal	5(%rcx,%rax), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	movl	%edi, %ecx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5008(%rsp)        # 4-byte Spill
	leal	9(%r8,%rax), %edx
	movq	%rax, %rdi
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5072(%rsp)        # 4-byte Spill
	movl	%r9d, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%r10d, %eax
	leal	2(%rdi), %ecx
	movq	%rcx, 4600(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 4960(%rsp)        # 4-byte Spill
	movq	%rdi, %rcx
	cmpl	%eax, %ecx
	movq	%rcx, %rdx
	cmovgl	%eax, %edi
	leal	1(%rdx), %ecx
	movq	%rcx, 4592(%rsp)        # 8-byte Spill
	cmovgel	%eax, %ecx
	movl	%ecx, 5664(%rsp)        # 4-byte Spill
	cmpl	%edi, %esi
	cmovlel	%esi, %edi
	cmpl	%r10d, %edi
	cmovll	%r10d, %edi
	movl	%edi, 4944(%rsp)        # 4-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 4856(%rsp)        # 8-byte Spill
	movq	%rdx, %r8
	cmpl	%eax, %ecx
	movl	%ecx, %r9d
	cmovgl	%eax, %r9d
	cmpl	%r9d, %esi
	cmovlel	%esi, %r9d
	cmpl	%r10d, %r9d
	cmovll	%r10d, %r9d
	cmpl	%eax, %esi
	movl	%esi, %edi
	cmovgl	%eax, %edi
	movl	%edi, 4928(%rsp)        # 4-byte Spill
	movq	464(%rsp), %rbx         # 8-byte Reload
	cmpl	%ebx, %r12d
	movl	%r12d, %ecx
	movq	%r12, %r15
	cmovgl	%ebx, %ecx
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	addl	$-1, %ebx
	cmpl	%edi, %ebx
	cmovgl	%edi, %ebx
	cmpl	%r10d, %ebx
	cmovll	%r10d, %ebx
	leal	-6(%r8), %ecx
	movq	%rcx, 4120(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	movl	%ecx, %r11d
	cmovgl	%eax, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	cmpl	%r10d, %r11d
	cmovll	%r10d, %r11d
	leal	-4(%r8), %r12d
	cmpl	%eax, %r12d
	cmovgl	%eax, %r12d
	cmpl	%r12d, %esi
	cmovlel	%esi, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	movq	3000(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	movl	%edx, %r13d
	cmovgl	%eax, %r13d
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	movl	5664(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 5664(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r15d
	movl	%r15d, %r8d
	cmovgl	%ecx, %r8d
	addl	$-1, %r8d
	cmpl	%edi, %r8d
	cmovgl	%edi, %r8d
	cmpl	%r10d, %r8d
	cmovll	%r10d, %r8d
	movq	5288(%rsp), %r15        # 8-byte Reload
	leal	-5(%r15), %ecx
	movq	%rcx, 4112(%rsp)        # 8-byte Spill
	cmpl	%eax, %ecx
	movl	%ecx, %edx
	cmovgl	%eax, %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movq	4856(%rsp), %rdi        # 8-byte Reload
	movq	5472(%rsp), %rcx        # 8-byte Reload
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	addl	$-1, %ecx
	movl	4928(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	leal	-1(%r15), %edi
	movq	%rdi, 4848(%rsp)        # 8-byte Spill
	cmpl	%eax, %edi
	cmovlel	%edi, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	4944(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%r8d, %eax
	cmovgl	%r8d, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%ecx, %edx
	movl	4960(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	movl	5664(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, 4960(%rsp)        # 4-byte Spill
	movq	5032(%rsp), %rdx        # 8-byte Reload
	leal	6(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	4992(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	5008(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	leal	4(%rdx,%r15), %ecx
	movq	%rdx, %rbx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	7(%rbx,%r15), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	leal	3(%rbx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movq	5088(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx,%r15), %ecx
	movl	5104(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5056(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	5040(%rsp), %r8d        # 4-byte Reload
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	cmpl	%r14d, %esi
	cmovlel	%esi, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	cmpl	%r10d, %r14d
	cmovll	%r10d, %r14d
	cmpl	%r14d, %ecx
	cmovgel	%ecx, %r14d
	cmpl	%edx, %r14d
	cmovll	%edx, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	movl	5136(%rsp), %eax        # 4-byte Reload
	cmpl	%r14d, %eax
	cmovgel	%eax, %r14d
	movl	%eax, %r8d
	movl	5072(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %ebx
	movq	5120(%rsp), %rdx        # 8-byte Reload
	leal	5(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %r14d
	cmovgel	%r14d, %eax
	leal	6(%rdx,%r15), %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	8(%rdx,%r15), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5008(%rsp)        # 4-byte Spill
	movq	5424(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %eax
	negl	%eax
	movq	5408(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx), %r9d
	cltd
	idivl	%r9d
	movl	%r9d, %eax
	negl	%eax
	movl	%ecx, %esi
	movq	%rcx, %r11
	sarl	$31, %esi
	andnl	%r9d, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%edi, 3920(%rsp)        # 4-byte Spill
	movl	%r9d, %ecx
	subl	%edi, %ecx
	addl	$-1, %ecx
	movl	%ecx, 4944(%rsp)        # 4-byte Spill
	cmpl	%edi, %ecx
	movl	%edi, %r8d
	cmovgel	%ecx, %r8d
	movl	$1, %eax
	subl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	sarl	$31, %r14d
	andl	%esi, %r14d
	addl	%edx, %r14d
	movl	%r14d, 3928(%rsp)       # 4-byte Spill
	movl	$2, %eax
	subl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	sarl	$31, %r15d
	andl	%esi, %r15d
	addl	%edx, %r15d
	movl	%r15d, 3904(%rsp)       # 4-byte Spill
	movl	%r9d, %r10d
	subl	%r15d, %r10d
	addl	$-1, %r10d
	movl	%r10d, 4928(%rsp)       # 4-byte Spill
	cmpl	%r10d, %r15d
	movl	%r15d, %eax
	cmovgl	%r10d, %eax
	movq	%rbx, %rsi
	addl	%esi, %eax
	leal	-1(%rsi,%r11), %r11d
	movl	%r11d, 4080(%rsp)       # 4-byte Spill
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	cmpl	%ecx, %edi
	movl	%edi, %edx
	cmovgl	%ecx, %edx
	addl	%esi, %edx
	movq	%rsi, %r12
	cmpl	%edx, %r11d
	cmovlel	%r11d, %edx
	subl	%r14d, %r9d
	addl	$-1, %r9d
	movl	%r9d, 4088(%rsp)        # 4-byte Spill
	cmpl	%r9d, %r14d
	movl	%r14d, %esi
	cmovgl	%r9d, %esi
	addl	%r12d, %esi
	cmpl	%esi, %r11d
	cmovlel	%r11d, %esi
	movl	%esi, %edi
	sarl	$31, %edi
	testl	%esi, %esi
	setg	%bl
	andl	%esi, %edi
	movzbl	%bl, %ebx
	orl	%ebx, %edi
	cmpl	%edx, %edi
	movl	%edi, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebx
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%edx, %edi
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	cmpl	$3, %ebx
	movl	$2, %r13d
	cmovgel	%r13d, %ebx
	cmpl	%esi, %ebx
	cmovgl	%esi, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	testl	%ebx, %ebx
	setg	%sil
	andl	%ebx, %ecx
	movzbl	%sil, %esi
	orl	%esi, %ecx
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, %edx
	sarl	$31, %edx
	andl	%ecx, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	cmpl	$3, %edx
	cmovgel	%r13d, %edx
	movl	$2, %esi
	movq	%r12, %rcx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%edx, 4736(%rsp)        # 4-byte Spill
	addl	%ecx, %r8d
	cmpl	%r14d, %r9d
	movl	%r14d, %eax
	cmovgel	%r9d, %eax
	addl	%ecx, %eax
	movq	%rcx, %rdx
	cmpl	$2, %eax
	setl	%cl
	cmpl	$1, %eax
	movl	$0, %edi
	cmovlel	%edi, %eax
	movzbl	%cl, %ecx
	orl	%ecx, %eax
	cmpl	%eax, %r8d
	cmovgel	%r8d, %eax
	cmpl	%r15d, %r10d
	cmovgel	%r10d, %r15d
	addl	%edx, %r15d
	cmpl	$1, %r15d
	cmovlel	%esi, %r15d
	cmpl	%r15d, %eax
	cmovgel	%eax, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	testl	%r15d, %r15d
	cmovsl	%edi, %r15d
	cmpl	%r15d, %r11d
	cmovlel	%r11d, %r15d
	cmpl	%edx, %r15d
	cmovll	%edx, %r15d
	movl	%r15d, 4992(%rsp)       # 4-byte Spill
	movq	1784(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx), %edx
	movq	%rdx, 1760(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%ecx, %esi
	movq	%rcx, %rdi
	sarl	$31, %esi
	andnl	%edx, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 1772(%rsp)        # 4-byte Spill
	movl	%edx, %r9d
	subl	%esi, %r9d
	cmovgel	%edx, %esi
	movq	%rdi, %rcx
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	movq	%rsi, %rax
	movq	%rcx, %r8
	movq	5632(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx), %r14d
	movq	1752(%rsp), %rdx        # 8-byte Reload
	leal	-1(%rdx,%r8), %esi
	movl	%esi, 1740(%rsp)        # 4-byte Spill
	cmpl	%r14d, %esi
	movl	%esi, %edi
	cmovgl	%r14d, %edi
	leal	-1(%rax,%rdx), %ebx
	movl	%ebx, 5056(%rsp)        # 4-byte Spill
	addl	%edx, %eax
	movq	%rax, 5072(%rsp)        # 8-byte Spill
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	movl	%edi, 5136(%rsp)        # 4-byte Spill
	leal	4(%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5120(%rsp)        # 4-byte Spill
	cmpl	%ecx, %esi
	movl	%esi, %eax
	cmovgl	%ecx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5104(%rsp)        # 4-byte Spill
	leal	6(%rcx), %r12d
	cmpl	%r12d, %esi
	movl	%esi, %eax
	cmovgl	%r12d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	%eax, 5088(%rsp)        # 4-byte Spill
	movl	%r9d, %r15d
	sarl	$31, %r15d
	andl	%r9d, %r15d
	addl	%edx, %r15d
	movq	632(%rsp), %rax         # 8-byte Reload
	leal	2(%rax), %ecx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	%ecx, 5040(%rsp)        # 4-byte Spill
	cmpl	%r15d, %eax
	movl	%eax, %edi
	cmovgl	%r15d, %edi
	leal	1(%rax), %ebx
	movq	%rax, %rcx
	cmovgel	%r15d, %ebx
	movl	%ebx, 5664(%rsp)        # 4-byte Spill
	cmpl	%edi, %esi
	cmovlel	%esi, %edi
	movl	%edi, 5032(%rsp)        # 4-byte Spill
	leal	-2(%rcx), %r13d
	cmpl	%r15d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r15d, %ebx
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%r15d, %esi
	movl	%esi, %r9d
	cmovgl	%r15d, %r9d
	leal	(%rdx,%r8), %edx
	movq	%rdx, 1744(%rsp)        # 8-byte Spill
	movq	728(%rsp), %r8          # 8-byte Reload
	cmpl	%r8d, %edx
	movl	%edx, %eax
	movq	%rdx, %rdi
	cmovgl	%r8d, %eax
	movl	%eax, %r8d
	sarl	$31, %r8d
	andl	%eax, %r8d
	addl	$-1, %r8d
	cmpl	%r9d, %r8d
	cmovgl	%r9d, %r8d
	leal	-4(%rcx), %r10d
	cmpl	%r15d, %r10d
	cmovgl	%r15d, %r10d
	cmpl	%r10d, %esi
	cmovlel	%esi, %r10d
	leal	-6(%rcx), %edx
	cmpl	%r15d, %edx
	movl	%edx, %r11d
	cmovgl	%r15d, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%r15d, %eax
	cmovgl	%r15d, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	%eax, 5696(%rsp)        # 4-byte Spill
	movl	5664(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	%eax, 5664(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	addl	$-1, %edx
	cmpl	%r9d, %edx
	cmovgl	%r9d, %edx
	leal	-5(%rcx), %ecx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	addl	$-1, %eax
	cmpl	%r9d, %eax
	cmovgl	%r9d, %eax
	cmpl	%r13d, %r15d
	cmovlel	%r15d, %r13d
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	cmpl	%r13d, %ebx
	cmovlel	%ebx, %r13d
	movl	5032(%rsp), %eax        # 4-byte Reload
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	movl	%eax, %edi
	cmpl	%r13d, %r10d
	cmovlel	%r10d, %r13d
	cmpl	%r13d, %r11d
	cmovlel	%r11d, %r13d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r13d
	cmpl	%edx, %r13d
	cmovgl	%edx, %r13d
	cmpl	%r13d, %r10d
	cmovlel	%r10d, %r13d
	cmpl	%r13d, %eax
	cmovlel	%eax, %r13d
	cmpl	%r13d, %r11d
	cmovlel	%r11d, %r13d
	cmpl	%edi, %r13d
	cmovgl	%edi, %r13d
	movl	5040(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	cmpl	%ebx, %r13d
	cmovgl	%ebx, %r13d
	cmpl	%r8d, %r13d
	cmovgl	%r8d, %r13d
	movl	5664(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r13d
	cmpl	%r8d, %r13d
	cmovgl	%r8d, %r13d
	cmpl	%ebx, %r13d
	cmovgl	%ebx, %r13d
	cmpl	%edi, %r13d
	cmovgl	%edi, %r13d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r13d
	movq	1752(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movq	5632(%rsp), %rdx        # 8-byte Reload
	leal	3(%rdx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	5056(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	%ecx, %edi
	movq	1744(%rsp), %r8         # 8-byte Reload
	cmpl	%r14d, %r8d
	cmovlel	%r8d, %r14d
	movq	5072(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	movq	%rcx, %rbx
	addl	$-1, %r14d
	cmpl	%r14d, %eax
	cmovgel	%eax, %r14d
	movl	5136(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r9d
	movl	5120(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r15d
	movl	5104(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r10d
	movl	5088(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movl	%eax, %r11d
	movl	5152(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %r14d
	cmovgel	%r14d, %eax
	movl	%eax, %ecx
	leal	7(%rdx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r12d, %r8d
	cmovlel	%r8d, %r12d
	cmpl	%ebx, %r12d
	cmovll	%ebx, %r12d
	addl	$-1, %r12d
	cmpl	%r12d, %eax
	cmovgel	%eax, %r12d
	cmpl	%r15d, %r12d
	cmovll	%r15d, %r12d
	cmpl	%r11d, %r12d
	cmovll	%r11d, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	cmpl	%r9d, %r12d
	cmovll	%r9d, %r12d
	movq	%rdx, %rcx
	leal	-2(%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	cmpl	%ecx, %r8d
	movq	%rcx, %rdx
	movl	%r8d, %ecx
	cmovgl	%edx, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	addl	$-1, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	5392(%rsp), %rax        # 8-byte Reload
	cmpl	$2, %eax
	setl	%al
	movzbl	%al, %eax
	orl	5216(%rsp), %eax        # 4-byte Folded Reload
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movq	1752(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movq	840(%rsp), %rcx         # 8-byte Reload
	orq	%rcx, 5184(%rsp)        # 8-byte Folded Spill
	sete	%r11b
	jne	.LBB147_41
# BB#40:                                # %true_bb
	vxorps	%xmm8, %xmm8, %xmm8
	movq	5528(%rsp), %rcx        # 8-byte Reload
	vmovups	%xmm8, (%rcx)
	movl	$4, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	$0, 48(%rcx)
	movl	$0, 52(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 60(%rcx)
	vmovaps	.LCPI147_0(%rip), %ymm8 # ymm8 = [3,3,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rcx)
.LBB147_41:                             # %after_bb
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orq	4984(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r8b
	jne	.LBB147_43
# BB#42:                                # %true_bb105
	movl	4880(%rsp), %ecx        # 4-byte Reload
	movl	4896(%rsp), %esi        # 4-byte Reload
	subl	%esi, %ecx
	movl	4864(%rsp), %edx        # 4-byte Reload
	movl	4872(%rsp), %edi        # 4-byte Reload
	subl	%edi, %edx
	vxorps	%xmm8, %xmm8, %xmm8
	movq	4768(%rsp), %rbx        # 8-byte Reload
	vmovups	%xmm8, (%rbx)
	movl	$2, 64(%rbx)
	movb	$0, 68(%rbx)
	movb	$0, 69(%rbx)
	movl	%esi, 48(%rbx)
	movl	%ecx, 16(%rbx)
	movl	$1, 32(%rbx)
	movl	%edi, 52(%rbx)
	movl	%edx, 20(%rbx)
	movl	%ecx, 36(%rbx)
	movl	$0, 56(%rbx)
	movl	$0, 24(%rbx)
	movl	$0, 40(%rbx)
	movl	$0, 60(%rbx)
	movl	$0, 28(%rbx)
	movl	$0, 44(%rbx)
.LBB147_43:                             # %after_bb107
	movq	5296(%rsp), %rcx        # 8-byte Reload
	orq	848(%rsp), %rcx         # 8-byte Folded Reload
	sete	%r14b
	movq	4672(%rsp), %r10        # 8-byte Reload
	movl	5360(%rsp), %edi        # 4-byte Reload
	jne	.LBB147_45
# BB#44:                                # %true_bb108
	movl	$1, %ecx
	movl	460(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %ecx
	addl	4912(%rsp), %ecx        # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	112(%rbp), %rdx
	movq	%rdx, %rsi
	vmovups	%xmm8, (%rsi)
	movl	$1, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movq	464(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 48(%rsi)
	movq	1136(%rsp), %rdx        # 8-byte Reload
	movl	%edx, 16(%rsi)
	movl	$3, 32(%rsi)
	movl	%ebx, 52(%rsi)
	movl	%ecx, 20(%rsi)
	movl	%edx, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$3, 24(%rsi)
	movl	$1, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_45:                             # %after_bb110
	movq	5312(%rsp), %rcx        # 8-byte Reload
	orq	5568(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r9b
	jne	.LBB147_47
# BB#46:                                # %true_bb111
	vxorps	%xmm8, %xmm8, %xmm8
	movq	88(%rbp), %rcx
	vmovups	%xmm8, (%rcx)
	movl	$1, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	$0, 48(%rcx)
	movl	$0, 52(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 60(%rcx)
	vmovaps	.LCPI147_1(%rip), %ymm8 # ymm8 = [3,4096,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rcx)
.LBB147_47:                             # %after_bb113
	movq	5328(%rsp), %rcx        # 8-byte Reload
	orq	5464(%rsp), %rcx        # 8-byte Folded Reload
	sete	%r12b
	jne	.LBB147_49
# BB#48:                                # %true_bb114
	movl	%edi, %ecx
	movl	4752(%rsp), %edi        # 4-byte Reload
	subl	%edi, %ecx
	addl	$1, %ecx
	movl	$1, %ebx
	movl	4960(%rsp), %edx        # 4-byte Reload
	subl	%edx, %ebx
	addl	5008(%rsp), %ebx        # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	%xmm8, (%rsi)
	movl	$4, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movl	%edi, 48(%rsi)
	movl	%ecx, 16(%rsi)
	movl	$1, 32(%rsi)
	movl	%edx, 52(%rsi)
	movl	%ebx, 20(%rsi)
	movl	%ecx, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$0, 24(%rsi)
	movl	$0, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_49:                             # %after_bb116
	movq	5344(%rsp), %rcx        # 8-byte Reload
	orq	1824(%rsp), %rcx        # 8-byte Folded Reload
	sete	%bl
	jne	.LBB147_51
# BB#50:                                # %after_bb119.thread
	movl	4736(%rsp), %edx        # 4-byte Reload
	movl	4992(%rsp), %esi        # 4-byte Reload
	subl	%edx, %esi
	addl	$1, %esi
	addl	$1, %eax
	subl	%r13d, %eax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	4784(%rsp), %rcx        # 8-byte Reload
	vmovups	%xmm0, (%rcx)
	movl	$4, 64(%rcx)
	movb	$0, 68(%rcx)
	movb	$0, 69(%rcx)
	movl	%edx, 48(%rcx)
	movl	%esi, 16(%rcx)
	movl	$1, 32(%rcx)
	movl	%r13d, 52(%rcx)
	movl	%eax, 20(%rcx)
	movl	%esi, 36(%rcx)
	movl	$0, 56(%rcx)
	movl	$0, 24(%rcx)
	movl	$0, 40(%rcx)
	movl	$0, 60(%rcx)
	movl	$0, 28(%rcx)
	movl	$0, 44(%rcx)
	jmp	.LBB147_1558
.LBB147_51:                             # %after_bb119
	orb	%r11b, %r8b
	orb	%r14b, %r8b
	orb	%r8b, %r9b
	orb	%r9b, %r12b
	xorl	%esi, %esi
	orb	%r12b, %bl
	movl	$0, %edx
	movl	$0, %ecx
	movq	%rcx, 5032(%rsp)        # 8-byte Spill
	movl	$0, %ecx
	movq	%rcx, 4664(%rsp)        # 8-byte Spill
	movl	$0, %ecx
	movq	%rcx, 4704(%rsp)        # 8-byte Spill
	movl	$0, %ecx
	movq	%rcx, 4696(%rsp)        # 8-byte Spill
	movl	$0, %r12d
	movl	$0, %ecx
	movq	%rcx, 5608(%rsp)        # 8-byte Spill
	movl	$0, %ecx
	movl	$0, %edi
	jne	.LBB147_227
# BB#52:                                # %true_bb120
	movl	4560(%rsp), %ecx        # 4-byte Reload
	cmpl	$4, %ecx
	jne	.LBB147_53
# BB#55:                                # %assert succeeded124
	movl	4576(%rsp), %ebx        # 4-byte Reload
	cmpl	$2, %ebx
	movq	728(%rsp), %r9          # 8-byte Reload
	movq	1352(%rsp), %r15        # 8-byte Reload
	movq	5536(%rsp), %r14        # 8-byte Reload
	movq	4840(%rsp), %r11        # 8-byte Reload
	movq	4720(%rsp), %rdi        # 8-byte Reload
	movq	4688(%rsp), %rdx        # 8-byte Reload
	movq	4632(%rsp), %rsi        # 8-byte Reload
	movq	4800(%rsp), %r8         # 8-byte Reload
	movq	4624(%rsp), %r12        # 8-byte Reload
	movl	4616(%rsp), %ecx        # 4-byte Reload
	jne	.LBB147_56
# BB#57:                                # %assert succeeded126
	movl	4552(%rsp), %ebx        # 4-byte Reload
	cmpl	$1, %ebx
	jne	.LBB147_58
# BB#63:                                # %assert succeeded128
	movl	4568(%rsp), %ebx        # 4-byte Reload
	cmpl	$1, %ebx
	jne	.LBB147_64
# BB#65:                                # %assert succeeded130
	cmpl	$4, %ecx
	jne	.LBB147_66
# BB#67:                                # %assert succeeded132
	movl	4584(%rsp), %ebx        # 4-byte Reload
	cmpl	$4, %ebx
	jne	.LBB147_68
# BB#69:                                # %assert succeeded134
	testl	%edx, %edx
	jg	.LBB147_71
# BB#70:                                # %assert succeeded134
	movl	$3, %ebx
	subl	%r8d, %ebx
	cmpl	%edx, %ebx
	jg	.LBB147_71
# BB#72:                                # %assert succeeded136
	testl	%r8d, %r8d
	js	.LBB147_73
# BB#76:                                # %assert succeeded138
	testl	%edi, %edi
	movq	4448(%rsp), %r8         # 8-byte Reload
	movq	%r11, %rdx
	movq	%rdi, %r11
	jg	.LBB147_78
# BB#77:                                # %assert succeeded138
	movl	$3, %ecx
	subl	%esi, %ecx
	cmpl	%r11d, %ecx
	jg	.LBB147_78
# BB#81:                                # %assert succeeded140
	testl	%esi, %esi
	js	.LBB147_82
# BB#83:                                # %assert succeeded142
	cmpl	4896(%rsp), %r14d       # 4-byte Folded Reload
	movq	4656(%rsp), %r11        # 8-byte Reload
	jg	.LBB147_85
# BB#84:                                # %assert succeeded142
	movl	4880(%rsp), %ecx        # 4-byte Reload
	subl	%edx, %ecx
	cmpl	%r14d, %ecx
	jg	.LBB147_85
# BB#86:                                # %assert succeeded144
	testl	%edx, %edx
	js	.LBB147_87
# BB#88:                                # %assert succeeded146
	cmpl	4872(%rsp), %r15d       # 4-byte Folded Reload
	movq	1344(%rsp), %rdx        # 8-byte Reload
	movq	448(%rsp), %rbx         # 8-byte Reload
	jg	.LBB147_90
# BB#89:                                # %assert succeeded146
	movl	4864(%rsp), %ecx        # 4-byte Reload
	subl	%edx, %ecx
	cmpl	%r15d, %ecx
	jg	.LBB147_90
# BB#91:                                # %assert succeeded148
	testl	%edx, %edx
	movq	%rdx, %rcx
	js	.LBB147_92
# BB#94:                                # %assert succeeded150
	movq	1136(%rsp), %rcx        # 8-byte Reload
	testl	%ecx, %ecx
	movq	4640(%rsp), %rdx        # 8-byte Reload
	js	.LBB147_95
# BB#96:                                # %assert succeeded152
	cmpl	460(%rsp), %r9d         # 4-byte Folded Reload
	jg	.LBB147_98
# BB#97:                                # %assert succeeded152
	movl	4912(%rsp), %ecx        # 4-byte Reload
	subl	%ebx, %ecx
	cmpl	%r9d, %ecx
	jge	.LBB147_98
# BB#99:                                # %assert succeeded154
	testl	%ebx, %ebx
	js	.LBB147_100
# BB#101:                               # %assert succeeded156
	testl	%r12d, %r12d
	jg	.LBB147_103
# BB#102:                               # %assert succeeded156
	movl	$3, %ecx
	subl	%r8d, %ecx
	cmpl	%r12d, %ecx
	jg	.LBB147_103
# BB#104:                               # %assert succeeded158
	movq	%rbx, %r9
	testl	%r8d, %r8d
	js	.LBB147_105
# BB#106:                               # %assert succeeded160
	movq	%r12, %r14
	testl	%r11d, %r11d
	jg	.LBB147_108
# BB#107:                               # %assert succeeded160
	movl	$3, %ecx
	subl	%edx, %ecx
	cmpl	%r11d, %ecx
	jg	.LBB147_108
# BB#109:                               # %assert succeeded162
	movq	%r8, %r11
	movq	%rsi, %r12
	testl	%edx, %edx
	js	.LBB147_110
# BB#111:                               # %assert succeeded164
	movq	4816(%rsp), %rdi        # 8-byte Reload
	testl	%edi, %edi
	movq	4712(%rsp), %rcx        # 8-byte Reload
	movq	5408(%rsp), %rbx        # 8-byte Reload
	movq	4648(%rsp), %rsi        # 8-byte Reload
	jg	.LBB147_113
# BB#112:                               # %assert succeeded164
	movl	$4096, %edx             # imm = 0x1000
	subl	%esi, %edx
	cmpl	%edi, %edx
	jg	.LBB147_113
# BB#114:                               # %assert succeeded166
	testl	%esi, %esi
	js	.LBB147_115
# BB#116:                               # %assert succeeded168
	movl	5360(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %esi
	subl	%ecx, %esi
	movq	4680(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %esi
	movq	5616(%rsp), %rdi        # 8-byte Reload
	jge	.LBB147_117
# BB#118:                               # %assert succeeded170
	testl	%ecx, %ecx
	js	.LBB147_119
# BB#120:                               # %assert succeeded172
	cmpl	4960(%rsp), %r10d       # 4-byte Folded Reload
	jg	.LBB147_122
# BB#121:                               # %assert succeeded172
	movl	5008(%rsp), %edx        # 4-byte Reload
	subl	%edi, %edx
	cmpl	%r10d, %edx
	jge	.LBB147_122
# BB#123:                               # %assert succeeded174
	testl	%edi, %edi
	js	.LBB147_124
# BB#125:                               # %assert succeeded176
	movl	4992(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %esi
	subl	%ebx, %esi
	movq	5424(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %esi
	jge	.LBB147_126
# BB#127:                               # %assert succeeded178
	movq	%rcx, %rsi
	testl	%ebx, %ebx
	movq	%rbx, %r10
	js	.LBB147_128
# BB#129:                               # %assert succeeded180
	movl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %ecx
	movq	1752(%rsp), %rbx        # 8-byte Reload
	cmpl	%ebx, %ecx
	jge	.LBB147_130
# BB#131:                               # %assert succeeded182
	testl	%edx, %edx
	movq	%rdx, %rcx
	movq	%rsi, %r13
	js	.LBB147_132
# BB#133:                               # %assert succeeded184
	movl	4320(%rsp), %eax        # 4-byte Reload
	cmpl	$1, %eax
	movl	4512(%rsp), %edx        # 4-byte Reload
	movl	4384(%rsp), %ebx        # 4-byte Reload
	jne	.LBB147_134
# BB#138:                               # %assert succeeded186
	cmpl	$1, %ebx
	jne	.LBB147_139
# BB#141:                               # %assert succeeded188
	movl	4304(%rsp), %eax        # 4-byte Reload
	cmpl	$3, %eax
	jne	.LBB147_142
# BB#143:                               # %assert succeeded190
	movl	4352(%rsp), %ebx        # 4-byte Reload
	cmpl	$1, %ebx
	jne	.LBB147_144
# BB#145:                               # %assert succeeded192
	testl	%r14d, %r14d
	jne	.LBB147_146
# BB#147:                               # %assert succeeded194
	cmpl	$3, %r11d
	movq	1344(%rsp), %rsi        # 8-byte Reload
	jne	.LBB147_148
# BB#149:                               # %assert succeeded196
	movl	4416(%rsp), %eax        # 4-byte Reload
	cmpl	$1, %eax
	jne	.LBB147_150
# BB#151:                               # %assert succeeded198
	cmpl	$1, %edx
	jne	.LBB147_152
# BB#154:                               # %assert succeeded200
	movl	4480(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	jne	.LBB147_155
# BB#156:                               # %assert succeeded204
	movq	832(%rsp), %rax         # 8-byte Reload
	imulq	%r12, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_157
# BB#160:                               # %assert succeeded206
	imulq	4800(%rsp), %r12        # 8-byte Folded Reload
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r12
	jge	.LBB147_161
# BB#162:                               # %assert succeeded210
	movq	1680(%rsp), %rax        # 8-byte Reload
	imulq	%rsi, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_163
# BB#164:                               # %assert succeeded212
	movq	%rsi, %rdx
	movq	4840(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_165
# BB#168:                               # %assert succeeded214
	movq	1136(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %r8
	leaq	(%r8,%r8,2), %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	movq	4608(%rsp), %rsi        # 8-byte Reload
	jne	.LBB147_169
# BB#170:                               # %assert succeeded216
	movq	392(%rsp), %rax         # 8-byte Reload
	imulq	%r9, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_169
# BB#171:                               # %assert succeeded218
	movq	%r9, %rdx
	imulq	%r8, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_172
# BB#173:                               # %assert succeeded220
	cmpq	$715827883, %rdx        # imm = 0x2AAAAAAB
	movq	5440(%rsp), %r12        # 8-byte Reload
	movq	4648(%rsp), %rbx        # 8-byte Reload
	jge	.LBB147_174
# BB#175:                               # %assert succeeded224
	movq	4832(%rsp), %rax        # 8-byte Reload
	imulq	%rbx, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_176
# BB#177:                               # %assert succeeded226
	imulq	4640(%rsp), %rbx        # 8-byte Folded Reload
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rbx
	jge	.LBB147_178
# BB#179:                               # %assert succeeded230
	movq	%rsi, %rax
	imulq	%rdi, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_180
# BB#181:                               # %assert succeeded232
	movq	%rdi, %rdx
	imulq	%r13, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_182
# BB#183:                               # %assert succeeded236
	movq	1816(%rsp), %rax        # 8-byte Reload
	imulq	%rcx, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_184
# BB#185:                               # %assert succeeded238
	movq	%rcx, %rdx
	imulq	%r10, %rdx
	movq	%r10, %r14
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_186
# BB#187:                               # %assert succeeded240
	vmovss	%xmm1, 5296(%rsp)       # 4-byte Spill
	vmovss	%xmm4, 5312(%rsp)       # 4-byte Spill
	vmovss	%xmm7, 5328(%rsp)       # 4-byte Spill
	vmovss	%xmm2, 5344(%rsp)       # 4-byte Spill
	vmovss	%xmm5, 5360(%rsp)       # 4-byte Spill
	vmovss	%xmm0, 5528(%rsp)       # 4-byte Spill
	vmovss	%xmm3, 5608(%rsp)       # 4-byte Spill
	vmovss	%xmm6, 5696(%rsp)       # 4-byte Spill
	movq	5632(%rsp), %rsi        # 8-byte Reload
	leal	-31(%rsi), %eax
	movl	%eax, 580(%rsp)         # 4-byte Spill
	movq	728(%rsp), %rcx         # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	movl	%ecx, 360(%rsp)         # 4-byte Spill
	movl	460(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	%eax, 460(%rsp)         # 4-byte Spill
	movl	%eax, %edx
	sarl	$31, %edx
	andl	%eax, %edx
	movl	%edx, 704(%rsp)         # 4-byte Spill
	movl	%esi, %r13d
	movq	632(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %r13d
	movl	%r13d, %eax
	andl	$-32, %eax
	leal	31(%rcx,%rax), %eax
	cmpl	%esi, %eax
	cmovgl	%esi, %eax
	movq	712(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rcx), %esi
	movl	%esi, 916(%rsp)         # 4-byte Spill
	xorl	%ebx, %ebx
	cmpl	$1, %ecx
	movl	$0, %edi
	cmovgl	%esi, %edi
	movl	4912(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovgel	%ecx, %edi
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	subl	%edx, %edi
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	-1(%rax), %esi
	cmpl	$1, %eax
	cmovgl	%esi, %ebx
	movq	%rbx, 816(%rsp)         # 8-byte Spill
	leal	-7(%rbx), %ecx
	movl	%ecx, 664(%rsp)         # 4-byte Spill
	movq	464(%rsp), %rax         # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, 356(%rsp)         # 4-byte Spill
	movl	%eax, %edx
	sarl	$31, %edx
	andl	%eax, %edx
	movl	%edx, 364(%rsp)         # 4-byte Spill
	movl	%ebx, %eax
	orl	$7, %eax
	movl	%eax, 376(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	movq	424(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rcx), %ecx
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	%r12d, %ebx
	andl	$-32, %ebx
	movq	%rbx, 5664(%rsp)        # 8-byte Spill
	movq	5288(%rsp), %rcx        # 8-byte Reload
	leal	31(%rbx,%rcx), %ecx
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	subl	%edx, %eax
	addl	$1, %edi
	cltq
	addq	$1, %rax
	movq	%rax, 1008(%rsp)        # 8-byte Spill
	movl	%eax, %eax
	imulq	%rdi, %rax
	leaq	(%rax,%rax,2), %rbx
	cmpq	$2147483647, %rbx       # imm = 0x7FFFFFFF
	movl	%eax, %ecx
	ja	.LBB147_189
# BB#188:                               # %assert succeeded240
	shrq	$32, %rax
	leaq	(%rcx,%rcx,2), %rcx
	shrq	$32, %rcx
	leaq	(%rax,%rax,2), %rax
	addq	%rcx, %rax
	movabsq	$30064771072, %rcx      # imm = 0x700000000
	andq	%rax, %rcx
	jne	.LBB147_189
# BB#190:                               # %assert succeeded242
	movq	%rdi, 368(%rsp)         # 8-byte Spill
	movq	%r8, 344(%rsp)          # 8-byte Spill
	movl	%esi, 912(%rsp)         # 4-byte Spill
	leaq	1(%rbx), %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	addq	$1, %rbx
	je	.LBB147_193
# BB#191:                               # %assert succeeded242
	testq	%rax, %rax
	je	.LBB147_192
.LBB147_193:                            # %assert succeeded244
	movq	%rax, 1176(%rsp)        # 8-byte Spill
	sarl	$5, %r13d
	movl	%r13d, 616(%rsp)        # 4-byte Spill
	movl	%r12d, %eax
	sarl	$5, %eax
	movq	%rax, 824(%rsp)         # 8-byte Spill
	testl	%r13d, %r13d
	vmovss	5328(%rsp), %xmm6       # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	5296(%rsp), %xmm7       # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	movq	%r14, %rcx
	js	.LBB147_1469
# BB#194:                               # %for f0.s0.v11.v14.preheader
	movq	2096(%rsp), %rdx        # 8-byte Reload
	leal	8(%rdx), %eax
	vmovd	%eax, %xmm0
	movq	5536(%rsp), %rsi        # 8-byte Reload
	leal	8(%rsi), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_2(%rip), %ymm2 # ymm2 = [0,4294967294,4294967292,4294967290,4294967288,4294967286,4294967284,4294967282]
	vpaddd	%ymm2, %ymm0, %ymm3
	vmovdqa	%ymm3, 4512(%rsp)       # 32-byte Spill
	vmovdqa	.LCPI147_3(%rip), %ymm3 # ymm3 = [4294967280,4294967278,4294967276,4294967274,4294967272,4294967270,4294967268,4294967266]
	vpaddd	%ymm3, %ymm0, %ymm0
	vmovdqa	%ymm0, 4480(%rsp)       # 32-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm4
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm4, %ymm0, %ymm4
	vpcmpeqd	%ymm5, %ymm5, %ymm5
	vpaddd	%ymm5, %ymm4, %ymm4
	vmovdqa	%ymm4, 4448(%rsp)       # 32-byte Spill
	vpaddd	%ymm2, %ymm1, %ymm4
	vmovdqa	%ymm4, 4032(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 4000(%rsp)       # 32-byte Spill
	leal	7(%rdx), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm4
	vmovdqa	%ymm4, 4416(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 4384(%rsp)       # 32-byte Spill
	leal	7(%rsi), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm2
	vmovdqa	%ymm2, 3968(%rsp)       # 32-byte Spill
	vpaddd	%ymm3, %ymm1, %ymm1
	vmovdqa	%ymm1, 3936(%rsp)       # 32-byte Spill
	vmovss	.LCPI147_5(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm1, %xmm2
	vmulss	%xmm6, %xmm2, %xmm3
	vmovss	5312(%rsp), %xmm4       # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm7, %xmm3, %xmm8
	vmovss	32(%rbp), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm3, %xmm3
	vmulss	%xmm3, %xmm2, %xmm2
	vdivss	%xmm2, %xmm4, %xmm9
	vmovss	16(%rbp), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	5344(%rsp), %xmm5       # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm4
	vmulss	%xmm2, %xmm4, %xmm3
	vmovss	5360(%rsp), %xmm6       # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vdivss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm5, %xmm3, %xmm10
	vmovss	40(%rbp), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm2, %xmm4, %xmm2
	vdivss	%xmm2, %xmm6, %xmm11
	vmovss	5528(%rsp), %xmm5       # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm1
	vmovss	5696(%rsp), %xmm3       # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm1, %xmm2
	vmovss	5608(%rsp), %xmm4       # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm5, %xmm2, %xmm12
	vmovss	24(%rbp), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm2, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm13
	movl	3920(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	movl	4944(%rsp), %esi        # 4-byte Reload
	cmovgl	%eax, %esi
	movq	5424(%rsp), %r9         # 8-byte Reload
	addl	%r9d, %esi
	movl	4080(%rsp), %edx        # 4-byte Reload
	cmpl	%esi, %edx
	cmovlel	%edx, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	movl	%esi, 4944(%rsp)        # 4-byte Spill
	movq	%rcx, %r14
	leal	(%r9,%r14), %ebx
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovlel	%edx, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	testl	%ebx, %ebx
	cmovlel	%esi, %eax
	movl	%eax, 5696(%rsp)        # 4-byte Spill
	movq	4712(%rsp), %r11        # 8-byte Reload
	movl	4096(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r11d
	movl	4824(%rsp), %ecx        # 4-byte Reload
	cmovgl	%eax, %ecx
	movq	4680(%rsp), %rsi        # 8-byte Reload
	addl	%esi, %ecx
	movl	5376(%rsp), %r8d        # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 4824(%rsp)        # 4-byte Spill
	leal	(%rsi,%r11), %edi
	testl	%edi, %edi
	movl	$0, %eax
	cmovlel	%r8d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%edi, %edi
	cmovlel	%ecx, %eax
	movl	%eax, 5632(%rsp)        # 4-byte Spill
	movl	3904(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	movl	4928(%rsp), %ecx        # 4-byte Reload
	cmovgl	%eax, %ecx
	addl	%r9d, %ecx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	movl	%ecx, 4928(%rsp)        # 4-byte Spill
	cmpl	$3, %ebx
	movl	$2, %r12d
	cmovll	%edx, %r12d
	cmpl	%r9d, %r12d
	cmovll	%r9d, %r12d
	cmpl	$3, %ebx
	cmovll	%ecx, %r12d
	movl	4104(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r11d
	movl	4160(%rsp), %r15d       # 4-byte Reload
	cmovgl	%eax, %r15d
	addl	%esi, %r15d
	cmpl	%r15d, %r8d
	cmovlel	%r8d, %r15d
	cmpl	%esi, %r15d
	cmovll	%esi, %r15d
	cmpl	$3, %edi
	movl	$2, %r13d
	cmovll	%r8d, %r13d
	cmpl	%esi, %r13d
	cmovll	%esi, %r13d
	cmpl	$3, %edi
	cmovll	%r15d, %r13d
	movl	3928(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r14d
	movl	4088(%rsp), %r10d       # 4-byte Reload
	cmovgl	%eax, %r10d
	addl	%r9d, %r10d
	cmpl	%r10d, %edx
	cmovlel	%edx, %r10d
	cmpl	%r9d, %r10d
	cmovll	%r9d, %r10d
	cmpl	$1, %ebx
	setg	%al
	cmpl	$2, %ebx
	movl	$0, %r14d
	cmovgel	%r14d, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	$2, %ebx
	cmovll	%r10d, %edx
	movl	4128(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r11d
	movl	4248(%rsp), %r11d       # 4-byte Reload
	cmovgl	%eax, %r11d
	addl	%esi, %r11d
	cmpl	%r11d, %r8d
	cmovlel	%r8d, %r11d
	cmpl	%esi, %r11d
	cmovll	%esi, %r11d
	cmpl	$1, %edi
	setg	%al
	cmpl	$2, %edi
	cmovgel	%r14d, %r8d
	movzbl	%al, %eax
	orl	%eax, %r8d
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	cmpl	$2, %edi
	cmovll	%r11d, %r8d
	movq	5664(%rsp), %r14        # 8-byte Reload
	leal	64(%r14), %ecx
	movq	%rcx, 1648(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	shlq	$6, %rax
	movabsq	$68719474688, %rdi      # imm = 0xFFFFFF800
	andq	%rax, %rdi
	leaq	(%rdi,%rdi,2), %rdi
	shrq	$32, %rdi
	movq	%rcx, %rbx
	shrq	$30, %rbx
	leaq	(%rbx,%rbx,2), %rbx
	shlq	$4, %rbx
	addq	%rdi, %rbx
	movl	%ecx, %edi
	shll	$6, %edi
	leal	(%rdi,%rdi,2), %edi
	andl	$-2048, %edi            # imm = 0xFFFFFFFFFFFFF800
	leaq	(%rdi,%rdi,2), %rdi
	leaq	(%rbx,%rbx,2), %rbx
	shrq	$32, %rdi
	addq	%rbx, %rdi
	movabsq	$64424509440, %rbx      # imm = 0xF00000000
	andq	%rdi, %rbx
	leaq	(%rax,%rax,8), %rcx
	movq	%rcx, 336(%rsp)         # 8-byte Spill
	movq	%rcx, %rax
	shrq	$31, %rax
	orq	%rax, %rbx
	sete	579(%rsp)               # 1-byte Folded Spill
	orq	$4, %rcx
	movq	%rcx, 568(%rsp)         # 8-byte Spill
	leal	40(%r14), %eax
	movq	%rax, 1168(%rsp)        # 8-byte Spill
	shlq	$8, %rax
	movq	%rax, 608(%rsp)         # 8-byte Spill
	leal	48(%r14), %eax
	movq	%rax, 808(%rsp)         # 8-byte Spill
	shlq	$8, %rax
	movq	%rax, 600(%rsp)         # 8-byte Spill
	movl	%r14d, %edi
	orl	$31, %edi
	movq	5288(%rsp), %rcx        # 8-byte Reload
	leal	(%r14,%rcx), %eax
	movq	%rcx, %r14
	addl	%eax, %edi
	leal	33(%rax), %ebx
	cmpl	%edi, %ebx
	cmovgel	%ebx, %edi
	leal	37(%rax), %ebx
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	movq	5440(%rsp), %rcx        # 8-byte Reload
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	andl	%r14d, %ecx
	orl	%eax, %ecx
	movq	5664(%rsp), %rax        # 8-byte Reload
	subl	%r14d, %eax
	leal	32(%rax,%rcx), %ebx
	addl	%ecx, %eax
	movq	%rax, 5528(%rsp)        # 8-byte Spill
	movq	464(%rsp), %rax         # 8-byte Reload
	cltq
	movq	%rax, %rcx
	sarq	$63, %rcx
	andq	%rax, %rcx
	movq	%rcx, 4712(%rsp)        # 8-byte Spill
	subl	%r14d, %edi
	leal	3(%rdi), %eax
	movq	%rax, 1288(%rsp)        # 8-byte Spill
	shlq	$8, %rax
	movq	%rax, 584(%rsp)         # 8-byte Spill
	shlq	$8, %rbx
	movq	%rbx, 592(%rsp)         # 8-byte Spill
	movq	5664(%rsp), %rbx        # 8-byte Reload
	leal	79(%rbx), %eax
	sarl	$5, %eax
	movl	%eax, 1676(%rsp)        # 4-byte Spill
	movq	1352(%rsp), %rax        # 8-byte Reload
	movl	%eax, %ecx
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	movq	5536(%rsp), %rax        # 8-byte Reload
	addl	%eax, %ecx
	movl	%ecx, 4696(%rsp)        # 4-byte Spill
	movq	824(%rsp), %rax         # 8-byte Reload
	movslq	%eax, %rcx
	movq	%rcx, 5312(%rsp)        # 8-byte Spill
	movq	%rcx, %r14
	shlq	$5, %r14
	movq	%rcx, %rax
	shlq	$9, %rax
	movq	%rax, 4912(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	shlq	$10, %rax
	movq	%rax, 4896(%rsp)        # 8-byte Spill
	movq	1752(%rsp), %rax        # 8-byte Reload
	movq	1816(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %eax
	leal	43(%rbx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 2184(%rsp)        # 4-byte Spill
	addl	%r9d, %eax
	cmpl	$1, %esi
	cmovgl	%r11d, %r8d
	movq	4608(%rsp), %rcx        # 8-byte Reload
	movq	4672(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %ecx
	addl	%esi, %ecx
	cmpl	$1, %r9d
	cmovgl	%r10d, %edx
	movslq	%eax, %r11
	movslq	%edx, %rax
	subq	%r11, %rax
	movq	%rax, 1776(%rsp)        # 8-byte Spill
	cmpl	$2, %esi
	cmovgl	%r15d, %r13d
	cmpl	$2, %r9d
	cmovgl	4928(%rsp), %r12d       # 4-byte Folded Reload
	movslq	%r12d, %rax
	subq	%r11, %rax
	movq	%rax, 1808(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	movl	5632(%rsp), %eax        # 4-byte Reload
	cmovgl	4824(%rsp), %eax        # 4-byte Folded Reload
	movslq	%ecx, %rcx
	movq	%rcx, 5608(%rsp)        # 8-byte Spill
	movslq	%r8d, %rsi
	movq	%rsi, 5152(%rsp)        # 8-byte Spill
	movslq	%r13d, %rdx
	movq	%rdx, 5296(%rsp)        # 8-byte Spill
	cltq
	movq	%rax, 5408(%rsp)        # 8-byte Spill
	vmovd	%ecx, %xmm1
	vmovd	%esi, %xmm2
	vpsubd	%xmm1, %xmm2, %xmm2
	vmovd	%edx, %xmm7
	vpsubd	%xmm1, %xmm7, %xmm7
	vmovd	%eax, %xmm3
	vpsubd	%xmm1, %xmm3, %xmm1
	testl	%r9d, %r9d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmovgl	4944(%rsp), %eax        # 4-byte Folded Reload
	cltq
	subq	%r11, %rax
	movq	%rax, 1800(%rsp)        # 8-byte Spill
	movq	832(%rsp), %rcx         # 8-byte Reload
	movq	4720(%rsp), %rsi        # 8-byte Reload
	imull	%ecx, %esi
	movq	4688(%rsp), %rax        # 8-byte Reload
	addl	%eax, %esi
	movq	4816(%rsp), %rdx        # 8-byte Reload
	movq	4832(%rsp), %rax        # 8-byte Reload
	imull	%eax, %edx
	movq	4656(%rsp), %rax        # 8-byte Reload
	addl	%eax, %edx
	movq	%rdx, 4816(%rsp)        # 8-byte Spill
	movslq	460(%rsp), %rax         # 4-byte Folded Reload
	movq	%rax, %rdx
	sarq	$63, %rdx
	andq	%rax, %rdx
	movq	%rdx, 520(%rsp)         # 8-byte Spill
	movq	5664(%rsp), %rdx        # 8-byte Reload
	leal	51(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, 2188(%rsp)        # 4-byte Spill
	leaq	64(%r14), %rax
	movq	%rax, 1656(%rsp)        # 8-byte Spill
	leaq	40(%r14), %rax
	movq	%rax, 1712(%rsp)        # 8-byte Spill
	addq	$48, %r14
	movq	%r14, 1720(%rsp)        # 8-byte Spill
	movslq	%edi, %rax
	addq	$3, %rax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	leal	39(%rdx), %eax
	movq	%rdx, %r12
	sarl	$3, %eax
	movl	%eax, 1736(%rsp)        # 4-byte Spill
	movq	5528(%rsp), %rax        # 8-byte Reload
	cltq
	addq	$32, %rax
	movq	%rax, 1728(%rsp)        # 8-byte Spill
	movslq	%esi, %rax
	leaq	(%rcx,%rcx), %rdx
	subq	%rax, %rdx
	movq	%rdx, 792(%rsp)         # 8-byte Spill
	subq	%rax, %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	movl	$2, %ecx
	subq	%rax, %rcx
	movq	%rcx, 784(%rsp)         # 8-byte Spill
	movl	$1, %ecx
	subq	%rax, %rcx
	movq	%rcx, 776(%rsp)         # 8-byte Spill
	negq	%rax
	movq	%rax, 800(%rsp)         # 8-byte Spill
	movq	5536(%rsp), %rsi        # 8-byte Reload
	leal	39(%rsi), %eax
	movq	5288(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	sarl	$5, %eax
	movl	$0, %ebx
	cmovsl	%ebx, %eax
	movq	%rax, 4928(%rsp)        # 8-byte Spill
	movq	2096(%rsp), %rdx        # 8-byte Reload
	subl	%ecx, %edx
	movq	4840(%rsp), %rax        # 8-byte Reload
	subl	%ecx, %eax
	movq	%rcx, %rdi
	addl	%esi, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	addl	$-24, %eax
	leal	-23(%rdx), %ecx
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	6(%rdx), %ecx
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	addl	$7, %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	leal	47(%r12), %eax
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	sarl	$5, %edx
	addl	$1, %edx
	movq	%rdx, 2096(%rsp)        # 8-byte Spill
	movq	4672(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r13d
	subl	%edi, %r13d
	movl	%r13d, %ecx
	sarl	$3, %ecx
	movq	%rcx, 4784(%rsp)        # 8-byte Spill
	leal	6(%r13), %eax
	sarl	$3, %eax
	movl	%eax, 4800(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	leal	1(%rcx), %edx
	movl	%edx, 4768(%rsp)        # 4-byte Spill
	movl	%eax, %ecx
	cmovgel	%edx, %ecx
	leal	7(%r13), %eax
	sarl	$3, %eax
	movl	%eax, 4872(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	leal	9(%r13), %eax
	sarl	$3, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 1128(%rsp)        # 8-byte Spill
	leal	10(%r13), %eax
	sarl	$3, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	leal	11(%r13), %ecx
	sarl	$3, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	testl	%ecx, %ecx
	cmovsl	%ebx, %ecx
	movq	%rcx, 5032(%rsp)        # 8-byte Spill
	movq	5616(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %r8d
	subl	%edi, %r8d
	movq	5472(%rsp), %r9         # 8-byte Reload
	subl	%edi, %r9d
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	(%r8,%rcx), %eax
	movl	%eax, 4960(%rsp)        # 4-byte Spill
	cmpl	%eax, %r9d
	movl	%r9d, %r14d
	cmovgl	%eax, %r14d
	movl	%r14d, %eax
	sarl	$3, %eax
	movq	%rax, 4992(%rsp)        # 8-byte Spill
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-7(%r8,%rcx), %edx
	sarl	$3, %edx
	movl	%edx, 5008(%rsp)        # 4-byte Spill
	leal	-1(%rax), %eax
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-6(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5040(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-5(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5056(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-4(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-3(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5184(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-7(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5440(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-6(%r9), %r11d
	movl	%r11d, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5072(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-5(%r9), %r10d
	movl	%r10d, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5088(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-4(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5344(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-3(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5360(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-2(%r9), %esi
	movl	%esi, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5696(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	-1(%r9), %edi
	movl	%edi, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5528(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	1(%r9), %r15d
	movl	%r15d, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5376(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	2(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5328(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	leal	3(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5632(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	movl	%r9d, %ecx
	sarl	$3, %ecx
	movl	%ecx, 5120(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	addl	$35, %r12d
	movq	%r12, 5664(%rsp)        # 8-byte Spill
	movl	%r12d, %ebx
	sarl	$3, %ebx
	movl	%ebx, 4864(%rsp)        # 4-byte Spill
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	movq	5152(%rsp), %r12        # 8-byte Reload
	movq	5608(%rsp), %rcx        # 8-byte Reload
	subq	%rcx, %r12
	movq	%r12, 4248(%rsp)        # 8-byte Spill
	movq	5296(%rsp), %rbx        # 8-byte Reload
	subq	%rcx, %rbx
	movq	%rbx, 4688(%rsp)        # 8-byte Spill
	movq	5408(%rsp), %rbx        # 8-byte Reload
	subq	%rcx, %rbx
	movq	%rbx, 4824(%rsp)        # 8-byte Spill
	addl	$1, %edx
	movq	1128(%rsp), %r12        # 8-byte Reload
	testl	%r12d, %r12d
	movl	$0, %ebx
	cmovsl	%ebx, %r12d
	movq	%r12, 1128(%rsp)        # 8-byte Spill
	addl	$-14, %r14d
	leal	-13(%r9), %r12d
	cmpl	%r14d, %r12d
	cmovlel	%r12d, %r14d
	leal	-12(%r9), %r12d
	cmpl	%r14d, %r12d
	cmovlel	%r12d, %r14d
	leal	-11(%r9), %r12d
	cmpl	%r14d, %r12d
	cmovlel	%r12d, %r14d
	cmpl	%r14d, %r11d
	cmovlel	%r11d, %r14d
	cmpl	%r14d, %r10d
	cmovlel	%r10d, %r14d
	cmpl	%r14d, %esi
	cmovlel	%esi, %r14d
	cmpl	%r14d, %edi
	cmovlel	%edi, %r14d
	cmpl	%r14d, %r9d
	cmovll	%r15d, %r14d
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	movq	5664(%rsp), %rcx        # 8-byte Reload
	cmpl	%r14d, %ecx
	cmovlel	%ecx, %r14d
	sarl	$3, %r14d
	addl	$1, %r14d
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	13(%rcx), %ecx
	movq	5288(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %ecx
	sarl	$3, %ecx
	cmovsl	%ebx, %ecx
	movq	%rcx, 4944(%rsp)        # 8-byte Spill
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-10(%r8,%rcx), %edi
	sarl	$3, %edi
	movl	%edi, 5296(%rsp)        # 4-byte Spill
	cmpl	%eax, %edi
	cmovgl	%eax, %edi
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-9(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5152(%rsp)        # 4-byte Spill
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5008(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	%ecx, %r12d
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-2(%r8,%rcx), %ebx
	sarl	$3, %ebx
	movl	%ebx, 5408(%rsp)        # 4-byte Spill
	cmpl	%edi, %ebx
	cmovgl	%edi, %ebx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	-1(%r8,%rcx), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5104(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	leal	-10(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5608(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	leal	-9(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5664(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	movl	5440(%rsp), %r10d       # 4-byte Reload
	cmpl	%ebx, %r10d
	cmovlel	%r10d, %ebx
	movl	5696(%rsp), %ecx        # 4-byte Reload
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	movl	5528(%rsp), %r11d       # 4-byte Reload
	cmpl	%ebx, %r11d
	cmovlel	%r11d, %ebx
	movl	5328(%rsp), %ecx        # 4-byte Reload
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	movl	%ecx, %r15d
	movl	5632(%rsp), %ecx        # 4-byte Reload
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	leal	4(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5424(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	leal	5(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 5136(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	15(%rcx), %ecx
	subl	%esi, %ecx
	sarl	$3, %ecx
	movl	$0, %esi
	cmovsl	%esi, %ecx
	movq	%rcx, 4840(%rsp)        # 8-byte Spill
	movl	4960(%rsp), %ecx        # 4-byte Reload
	sarl	$3, %ecx
	movl	%ecx, 4960(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5296(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5152(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	movl	5216(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5184(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5408(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5104(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leal	1(%r8,%rcx), %r12d
	sarl	$3, %r12d
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	movl	5608(%rsp), %r8d        # 4-byte Reload
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	movl	5664(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r10d
	cmovlel	%r10d, %eax
	movl	5344(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5360(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5696(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r11d
	cmovlel	%r11d, %eax
	movl	5376(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%eax, %r15d
	cmovlel	%r15d, %eax
	movl	5632(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5424(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	5136(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	6(%r9), %ecx
	sarl	$3, %ecx
	movl	%ecx, 4880(%rsp)        # 4-byte Spill
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	addl	$7, %r9d
	sarl	$3, %r9d
	movq	%r9, 4704(%rsp)         # 8-byte Spill
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	5120(%rsp), %ecx        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	movl	%ecx, %r9d
	leal	4(%r13), %ecx
	sarl	$3, %ecx
	movq	4784(%rsp), %rsi        # 8-byte Reload
	cmpl	%ecx, %esi
	cmovgel	4768(%rsp), %ecx        # 4-byte Folded Reload
	addl	$5, %r13d
	sarl	$3, %r13d
	cmpl	%r13d, %ecx
	cmovgel	%ecx, %r13d
	movl	4800(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	movl	4872(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	movl	5248(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	testl	%r13d, %r13d
	movl	$0, %ecx
	cmovsl	%ecx, %r13d
	movq	%r13, 5248(%rsp)        # 8-byte Spill
	movl	5040(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5056(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	cmpl	%edi, %r8d
	cmovlel	%r8d, %edi
	movl	5664(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	%r10d, %r15d
	cmpl	%edi, %r15d
	cmovlel	%r15d, %edi
	movl	5072(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5088(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5344(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5360(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	5696(%rsp), %r10d       # 4-byte Reload
	cmpl	%edi, %r10d
	cmovlel	%r10d, %edi
	cmpl	%edi, %r11d
	cmovlel	%r11d, %edi
	movl	%r11d, %r13d
	movl	5376(%rsp), %ecx        # 4-byte Reload
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movl	%r9d, %ecx
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	movq	824(%rsp), %rcx         # 8-byte Reload
	leal	3(,%rcx,4), %ecx
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	addl	$1, %edi
	movl	$31, %r8d
	movq	5392(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %r8d
	movq	5312(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %ecx
	shll	$10, %ecx
	movq	%rcx, 4680(%rsp)        # 8-byte Spill
	movl	%esi, %ecx
	shll	$9, %ecx
	movq	%rcx, 624(%rsp)         # 8-byte Spill
	cmpl	$1, %r9d
	movl	$30, %esi
	cmovgl	%r8d, %esi
	movl	%esi, 564(%rsp)         # 4-byte Spill
	movl	2184(%rsp), %esi        # 4-byte Reload
	movl	%esi, %ecx
	notl	%ecx
	movl	%ecx, 5392(%rsp)        # 4-byte Spill
	movq	4992(%rsp), %rcx        # 8-byte Reload
	negl	%ecx
	movq	%rcx, 4992(%rsp)        # 8-byte Spill
	movq	%rcx, %r9
	movl	5008(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5008(%rsp)        # 4-byte Spill
	movl	%ecx, %r11d
	notl	%r15d
	movl	%r15d, 5440(%rsp)       # 4-byte Spill
	notl	%r10d
	movl	%r10d, 5696(%rsp)       # 4-byte Spill
	notl	%r13d
	movl	%r13d, 5528(%rsp)       # 4-byte Spill
	notl	5328(%rsp)              # 4-byte Folded Spill
	notl	5632(%rsp)              # 4-byte Folded Spill
	movq	1128(%rsp), %rcx        # 8-byte Reload
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	cmpl	%r14d, %ecx
	cmovll	%r14d, %ecx
	movl	%ecx, 4664(%rsp)        # 4-byte Spill
	movq	4944(%rsp), %rcx        # 8-byte Reload
	movl	2188(%rsp), %esi        # 4-byte Reload
	cmpl	%esi, %ecx
	movl	%ecx, %r8d
	cmovgl	%esi, %r8d
	movl	5296(%rsp), %r13d       # 4-byte Reload
	notl	%r13d
	movl	%r13d, 5296(%rsp)       # 4-byte Spill
	movq	%r9, %rsi
	cmpl	%r13d, %esi
	cmovgel	%esi, %r13d
	movl	5152(%rsp), %r9d        # 4-byte Reload
	notl	%r9d
	movl	%r9d, 5152(%rsp)        # 4-byte Spill
	cmpl	%r9d, %r13d
	cmovgel	%r13d, %r9d
	movl	%r11d, %ecx
	cmpl	%ecx, %r9d
	movl	5408(%rsp), %esi        # 4-byte Reload
	notl	%esi
	movl	%esi, 5408(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r9d
	cmpl	%esi, %r9d
	movl	5104(%rsp), %r11d       # 4-byte Reload
	notl	%r11d
	movl	%r11d, 5104(%rsp)       # 4-byte Spill
	cmovll	%esi, %r9d
	cmpl	%r11d, %r9d
	movl	5608(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5608(%rsp)        # 4-byte Spill
	cmovll	%r11d, %r9d
	cmpl	%ecx, %r9d
	movl	5664(%rsp), %esi        # 4-byte Reload
	notl	%esi
	movl	%esi, 5664(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r9d
	cmpl	%esi, %r9d
	cmovll	%esi, %r9d
	cmpl	%r15d, %r9d
	cmovll	%r15d, %r9d
	cmpl	%r10d, %r9d
	cmovll	%r10d, %r9d
	movl	5528(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r9d
	cmovll	%ecx, %r9d
	movl	5328(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r9d
	cmovll	%ecx, %r9d
	movl	5632(%rsp), %esi        # 4-byte Reload
	cmpl	%esi, %r9d
	movl	5424(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5424(%rsp)        # 4-byte Spill
	cmovll	%esi, %r9d
	cmpl	%ecx, %r9d
	movl	5136(%rsp), %esi        # 4-byte Reload
	notl	%esi
	movl	%esi, 5136(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r9d
	cmpl	%esi, %r9d
	cmovll	%esi, %r9d
	movl	5392(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r9d
	cmovll	%ecx, %r9d
	negl	%r9d
	cmpl	%r9d, %r8d
	cmovgel	%r8d, %r9d
	movl	$2, %ecx
	subq	4712(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 1160(%rsp)        # 8-byte Spill
	movl	1676(%rsp), %ecx        # 4-byte Reload
	movq	4928(%rsp), %rsi        # 8-byte Reload
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	%ecx, 1272(%rsp)        # 4-byte Spill
	movq	2096(%rsp), %rsi        # 8-byte Reload
	cmpl	%esi, %ecx
	cmovgel	%ecx, %esi
	movl	%esi, 1268(%rsp)        # 4-byte Spill
	movl	2184(%rsp), %esi        # 4-byte Reload
	movq	5032(%rsp), %r8         # 8-byte Reload
	cmpl	%r8d, %esi
	movl	%esi, %ecx
	cmovgl	%r8d, %ecx
	movl	%ecx, 2172(%rsp)        # 4-byte Spill
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movq	1128(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %esi
	movl	%esi, %ecx
	cmovgl	%edx, %ecx
	movl	%ecx, 1264(%rsp)        # 4-byte Spill
	cmpl	%r14d, %ecx
	movl	%r14d, %edx
	cmovgel	%ecx, %edx
	movl	%edx, 1260(%rsp)        # 4-byte Spill
	movl	2188(%rsp), %edx        # 4-byte Reload
	movq	4944(%rsp), %r8         # 8-byte Reload
	cmpl	%r8d, %edx
	movl	%edx, %ecx
	cmovgl	%r8d, %ecx
	movl	%ecx, 1256(%rsp)        # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	addl	$1, %ebx
	cmpl	%ebx, %ecx
	cmovgel	%ecx, %ebx
	movl	%ebx, 1276(%rsp)        # 4-byte Spill
	movq	4840(%rsp), %rbx        # 8-byte Reload
	cmpl	%ebx, %edx
	movl	%edx, %ecx
	cmovgl	%ebx, %ecx
	movl	%ecx, 1252(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	addl	$1, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	%eax, 1284(%rsp)        # 4-byte Spill
	movl	1736(%rsp), %eax        # 4-byte Reload
	movq	5248(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, 3100(%rsp)        # 4-byte Spill
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	movl	%edi, 1388(%rsp)        # 4-byte Spill
	movl	$-7, %eax
	movq	5536(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	movq	%rax, 3928(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	subl	%ecx, %eax
	movq	%rax, 3920(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rcx        # 8-byte Reload
	imull	$3264, %ecx, %eax       # imm = 0xCC0
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 516(%rsp)         # 4-byte Spill
	movl	%ecx, %edx
	imull	$1728, %ecx, %eax       # imm = 0x6C0
	movq	%rcx, %rsi
	shll	$5, %edx
	movq	%rdx, 4872(%rsp)        # 8-byte Spill
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 512(%rsp)         # 4-byte Spill
	movq	824(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movl	%eax, 620(%rsp)         # 4-byte Spill
	shll	$6, %eax
	addl	$384, %eax              # imm = 0x180
	movl	%eax, 508(%rsp)         # 4-byte Spill
	movq	4672(%rsp), %rax        # 8-byte Reload
	negl	%eax
	movq	%rax, 3072(%rsp)        # 8-byte Spill
	movl	$1, %ecx
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movq	%rcx, 3088(%rsp)        # 8-byte Spill
	movl	$-4, %ecx
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movq	%rcx, 4640(%rsp)        # 8-byte Spill
	movl	$-3, %ecx
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movq	%rcx, 4632(%rsp)        # 8-byte Spill
	movl	$-2, %ecx
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movq	%rcx, 3080(%rsp)        # 8-byte Spill
	movq	728(%rsp), %r15         # 8-byte Reload
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%r15,%rax), %edi
	movq	712(%rsp), %r11         # 8-byte Reload
	cmpl	%edi, %r11d
	cmovgel	%r11d, %edi
	testl	%edi, %edi
	movl	$1, %r8d
	cmovlel	%r8d, %edi
	movl	$31, %eax
	movq	%rsi, %rbx
	imull	$3136, %ebx, %ecx       # imm = 0xC40
	subl	%edi, %eax
	movl	%eax, 560(%rsp)         # 4-byte Spill
	movq	5288(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %esi
	imull	$1600, %ebx, %edx       # imm = 0x640
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %esi
	movq	%rsi, 4584(%rsp)        # 8-byte Spill
	subl	$-128, %ecx
	movl	%ecx, 504(%rsp)         # 4-byte Spill
	subl	$-128, %edx
	movl	%edx, 500(%rsp)         # 4-byte Spill
	movl	%ebx, %esi
	shll	$6, %esi
	movl	%esi, %ecx
	subl	$-128, %ecx
	movl	%ecx, 772(%rsp)         # 4-byte Spill
	movl	$-7, %eax
	movq	4672(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %eax
	movq	%rax, 4104(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	movq	4672(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %eax
	movq	%rax, 4096(%rsp)        # 8-byte Spill
	movl	$-6, %eax
	movq	4672(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %eax
	movq	%rax, 4088(%rsp)        # 8-byte Spill
	movl	$-5, %eax
	movq	4672(%rsp), %rdx        # 8-byte Reload
	subl	%edx, %eax
	movq	%rax, 4080(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	37(%r10,%rax), %edx
	leal	33(%r10,%rax), %ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leal	31(%r10,%rsi), %eax
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	addl	$3, %ecx
	subl	%r10d, %ecx
	movl	%ecx, 1700(%rsp)        # 4-byte Spill
	movq	464(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %edx
	notl	%edx
	movq	720(%rsp), %rbx         # 8-byte Reload
	testl	%ebx, %ebx
	movl	$1, %eax
	cmovgl	%ebx, %eax
	movl	$7, %ebx
	subl	%eax, %ebx
	cmpl	%ebx, %edx
	cmovgel	%edx, %ebx
	notl	%ebx
	movslq	%ebx, %rcx
	notq	%rcx
	cmpq	$-2, %rcx
	movq	$-1, %rdx
	cmovleq	%rdx, %rcx
	movq	%rcx, 4656(%rsp)        # 8-byte Spill
	movq	1136(%rsp), %rdx        # 8-byte Reload
	leal	-1(%rsi,%rdx), %ebx
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leal	31(%r10,%rcx), %edx
	cmpl	%edx, %ebx
	cmovgel	%ebx, %edx
	movl	376(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	negl	%eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	%eax, %edx
	movl	364(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %edx
	movl	%edx, 4648(%rsp)        # 4-byte Spill
	movl	%r15d, %edx
	notl	%edx
	movl	$31, %ebx
	subl	%r15d, %ebx
	movq	448(%rsp), %r10         # 8-byte Reload
	subl	%r10d, %ebx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	movl	360(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	notl	%ebx
	movslq	%ebx, %rdx
	notq	%rdx
	cmpq	$-2, %rdx
	movq	$-1, %rsi
	cmovleq	%rsi, %rdx
	addq	$1, %rdx
	movq	%rdx, 488(%rsp)         # 8-byte Spill
	addl	$1, %eax
	subl	%ecx, %eax
	testl	%r11d, %r11d
	cmovgl	%r11d, %r8d
	movl	$-32, %edx
	movq	632(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %edx
	movl	616(%rsp), %ebx         # 4-byte Reload
	shll	$5, %ebx
	subl	%ebx, %edx
	addl	$-1, %r8d
	negl	%edi
	cmpl	%edx, %edi
	cmovgel	%edi, %edx
	notl	%edx
	cmpl	%edx, %r8d
	cmovgel	%r8d, %edx
	movq	432(%rsp), %rsi         # 8-byte Reload
	leal	(%r15,%rsi), %esi
	notl	%esi
	movl	%r10d, %ecx
	negl	%ecx
	subl	%r15d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	addl	$1, %ecx
	subl	704(%rsp), %ecx         # 4-byte Folded Reload
	imull	%eax, %ecx
	movl	%ecx, 4576(%rsp)        # 4-byte Spill
	movl	1676(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %eax
	notl	%eax
	movq	4928(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	%ecx, %eax
	movq	%rcx, %rdi
	shll	$5, %eax
	movq	5288(%rsp), %rsi        # 8-byte Reload
	leal	-32(%rsi), %ecx
	movl	%ecx, %edx
	subl	%eax, %edx
	movl	%edx, 1124(%rsp)        # 4-byte Spill
	subl	4696(%rsp), %ecx        # 4-byte Folded Reload
	subl	%eax, %ecx
	movq	%rcx, 696(%rsp)         # 8-byte Spill
	movl	%edi, %eax
	notl	%eax
	movl	%eax, 1120(%rsp)        # 4-byte Spill
	movq	2096(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	1(%rdi,%rcx), %eax
	movl	%eax, 1116(%rsp)        # 4-byte Spill
	movl	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, 1112(%rsp)        # 4-byte Spill
	shll	$5, %ecx
	movq	%rcx, 2096(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	movq	5536(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %eax
	leal	-7(%rax), %ecx
	movq	%rcx, 2072(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rdx        # 8-byte Reload
	imull	$3456, %edx, %ecx       # imm = 0xD80
	addl	$-8, %eax
	movq	%rax, 2080(%rsp)        # 8-byte Spill
	imull	$1920, %edx, %eax       # imm = 0x780
	addl	$768, %ecx              # imm = 0x300
	movl	%ecx, 484(%rsp)         # 4-byte Spill
	addl	$768, %eax              # imm = 0x300
	movl	%eax, 480(%rsp)         # 4-byte Spill
	movl	620(%rsp), %eax         # 4-byte Reload
	shll	$7, %eax
	addl	$768, %eax              # imm = 0x300
	movl	%eax, 620(%rsp)         # 4-byte Spill
	movq	5032(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	movl	5392(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 5032(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	leal	(,%rcx,8), %eax
	movq	%rcx, %rbx
	movl	$-8, %ecx
	subl	%eax, %ecx
	movq	%rcx, 688(%rsp)         # 8-byte Spill
	movq	3000(%rsp), %rcx        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 1108(%rsp)        # 4-byte Spill
	movq	4992(%rsp), %rcx        # 8-byte Reload
	movl	5008(%rsp), %edi        # 4-byte Reload
	cmpl	%edi, %ecx
	movl	5040(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5040(%rsp)        # 4-byte Spill
	movl	%edi, %r15d
	cmovgel	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5056(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5056(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	movl	5216(%rsp), %edx        # 4-byte Reload
	notl	%edx
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%edx, %r15d
	movl	5184(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	cmovll	%edx, %r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5440(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5072(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5072(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5088(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5088(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	movl	5344(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5344(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	5360(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	movl	%ecx, 5360(%rsp)        # 4-byte Spill
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5528(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5376(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5376(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	movl	%ecx, %r10d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5328(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	movl	5632(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %r15d
	movl	5120(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movl	%eax, 5120(%rsp)        # 4-byte Spill
	cmovll	%ecx, %r15d
	cmpl	%eax, %r15d
	movl	4864(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmovll	%eax, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	movl	%ebx, %eax
	notl	%eax
	negl	%r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	leal	(%rsi,%r15,8), %ecx
	movq	%rcx, 2088(%rsp)        # 8-byte Spill
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movq	%rcx, 2064(%rsp)        # 8-byte Spill
	movl	2184(%rsp), %eax        # 4-byte Reload
	subl	%r15d, %eax
	movl	%eax, 1104(%rsp)        # 4-byte Spill
	movq	1128(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %r8d
	cmovgel	%r8d, %ecx
	movq	%rcx, 1128(%rsp)        # 8-byte Spill
	leal	(,%rcx,8), %eax
	negl	%eax
	movl	%eax, 1100(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	notl	%eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	movq	%rax, 4568(%rsp)        # 8-byte Spill
	movl	2188(%rsp), %esi        # 4-byte Reload
	notl	%esi
	movq	4944(%rsp), %r11        # 8-byte Reload
	notl	%r11d
	cmpl	%r11d, %esi
	cmovgel	%esi, %r11d
	movl	%r11d, %ecx
	notl	%ecx
	movl	%ecx, 1096(%rsp)        # 4-byte Spill
	leal	(,%r11,8), %eax
	negl	%eax
	movl	%eax, 1092(%rsp)        # 4-byte Spill
	movl	5152(%rsp), %r8d        # 4-byte Reload
	cmpl	%r8d, %r13d
	cmovll	%r8d, %r13d
	cmpl	%edi, %r13d
	cmovll	%edi, %r13d
	movl	5408(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovgel	%r13d, %eax
	movl	5104(%rsp), %ebx        # 4-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	5608(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5664(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5440(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5696(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	%r10d, %edx
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5328(%rsp), %r14d       # 4-byte Reload
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	movl	5632(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5424(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	5136(%rsp), %r10d       # 4-byte Reload
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	5392(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	negl	%eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movq	5312(%rsp), %rdx        # 8-byte Reload
	imull	$3584, %edx, %ecx       # imm = 0xE00
	leal	1(%rax,%r11), %eax
	movl	%eax, 1088(%rsp)        # 4-byte Spill
	addl	$1024, %ecx             # imm = 0x400
	movl	%ecx, 476(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	shll	$11, %eax
	orl	$1024, %eax             # imm = 0x400
	movl	%eax, 472(%rsp)         # 4-byte Spill
	movq	624(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rcx,2), %eax
	movq	%rax, 4864(%rsp)        # 8-byte Spill
	leal	-8(%rcx,%rcx,2), %eax
	movq	%rax, 2160(%rsp)        # 8-byte Spill
	addl	$1024, %ecx             # imm = 0x400
	movq	%rcx, 624(%rsp)         # 8-byte Spill
	movq	4840(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %esi
	cmovgel	%esi, %ecx
	movq	%rcx, 4840(%rsp)        # 8-byte Spill
	leal	(,%rcx,8), %eax
	movq	%rcx, %rdx
	movq	5288(%rsp), %rcx        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 1084(%rsp)        # 4-byte Spill
	negl	%eax
	movq	%rax, 680(%rsp)         # 8-byte Spill
	movl	4960(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movq	4992(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	5296(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	5216(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	5184(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	5408(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	notl	%r12d
	cmpl	%r12d, %eax
	cmovgel	%eax, %r12d
	movl	5608(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5664(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5440(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5344(%rsp), %r11d       # 4-byte Reload
	cmpl	%r11d, %r12d
	cmovll	%r11d, %r12d
	movl	5360(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	%eax, %ebx
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5528(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5376(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	%eax, %r8d
	cmpl	%r14d, %r12d
	cmovll	%r14d, %r12d
	movl	5632(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	5424(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	movl	4880(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	movq	4672(%rsp), %rdi        # 8-byte Reload
	movq	4704(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	5120(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	%eax, %r10d
	movl	5392(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	%edx, %eax
	notl	%eax
	movl	%eax, 1080(%rsp)        # 4-byte Spill
	negl	%ecx
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 4704(%rsp)        # 8-byte Spill
	movl	2188(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	movl	%eax, 1076(%rsp)        # 4-byte Spill
	movl	1736(%rsp), %eax        # 4-byte Reload
	notl	%eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 5248(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	leal	(,%rcx,8), %edx
	movq	%rcx, %rsi
	subl	%edx, %eax
	movq	%rax, 2136(%rsp)        # 8-byte Spill
	movl	5040(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5056(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5608(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5664(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5440(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5072(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5088(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	cmpl	%r11d, %r13d
	cmovll	%r11d, %r13d
	cmpl	%ebx, %r13d
	cmovll	%ebx, %r13d
	movl	5696(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movl	5528(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	cmpl	%r8d, %r13d
	cmovll	%r8d, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %eax
	movl	%eax, 1156(%rsp)        # 4-byte Spill
	shll	$2, %ecx
	movl	$-4, %eax
	subl	%ecx, %eax
	cmpl	%eax, %r13d
	cmovgel	%r13d, %eax
	negl	%eax
	movl	%esi, %r8d
	notl	%r8d
	cmpl	%r8d, %eax
	cmovgel	%eax, %r8d
	movq	5288(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r14d
	movq	%rax, %rcx
	subl	%edx, %r14d
	movl	4272(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %ymm3
	vmovaps	%ymm3, 4352(%rsp)       # 32-byte Spill
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1584(%rsp)       # 16-byte Spill
	vpbroadcastd	.LCPI147_4(%rip), %ymm3
	vmovdqa	%ymm3, 736(%rsp)        # 32-byte Spill
	vpsubd	%ymm0, %ymm3, %ymm0
	vmovdqa	%ymm0, 4320(%rsp)       # 32-byte Spill
	vmovd	4288(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4304(%rsp)       # 16-byte Spill
	movq	5536(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4288(%rsp)       # 16-byte Spill
	movq	4912(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	movq	4896(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 4616(%rsp)        # 8-byte Spill
	movl	4256(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm4
	vmovdqa	%xmm4, 5392(%rsp)       # 16-byte Spill
	movq	4608(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5360(%rsp)       # 16-byte Spill
	movq	5472(%rsp), %rdx        # 8-byte Reload
	leal	2(%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm3
	vmovdqa	.LCPI147_6(%rip), %xmm0 # xmm0 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5136(%rsp)       # 16-byte Spill
	leal	2(%rdi), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5088(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm5
	vmovdqa	%xmm5, 5344(%rsp)       # 16-byte Spill
	movq	5616(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 5328(%rsp)       # 16-byte Spill
	movl	4192(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm6
	vmovdqa	%xmm6, 5312(%rsp)       # 16-byte Spill
	leal	1(%rdx), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5120(%rsp)       # 16-byte Spill
	leal	1(%rdi), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 5072(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 5104(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm2
	vmovaps	%xmm2, 5440(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm2
	vmovaps	%xmm2, 5408(%rsp)       # 16-byte Spill
	leal	3(%rdx), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4960(%rsp)       # 16-byte Spill
	leal	3(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4800(%rsp)       # 16-byte Spill
	movq	%rdx, %rax
	leal	4(%rax), %edx
	leal	6(%rax), %esi
	leal	5(%rax), %r10d
	leal	8(%rax), %r13d
	leal	7(%rax), %r12d
	leal	-2(%rax), %r11d
	leal	-3(%rax), %ebx
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5056(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4944(%rsp)       # 16-byte Spill
	leal	4(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4784(%rsp)       # 16-byte Spill
	leal	-1(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 5008(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4928(%rsp)       # 16-byte Spill
	leal	6(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4768(%rsp)       # 16-byte Spill
	vmovd	%r10d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4912(%rsp)       # 16-byte Spill
	leal	5(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4752(%rsp)       # 16-byte Spill
	vmovd	%r13d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4272(%rsp)       # 16-byte Spill
	leal	8(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 3904(%rsp)       # 16-byte Spill
	vmovd	%r12d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4256(%rsp)       # 16-byte Spill
	leal	7(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 3888(%rsp)       # 16-byte Spill
	vmovd	%r11d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4896(%rsp)       # 16-byte Spill
	leal	-2(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4736(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4880(%rsp)       # 16-byte Spill
	leal	-3(%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	%xmm2, 4720(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm5, %xmm2
	vmovdqa	%xmm2, 4992(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm6, %xmm0
	vmovdqa	%xmm0, 5040(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vmovdqa	%xmm0, 5376(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 5696(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 5664(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vmovdqa	%xmm0, 5424(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 5632(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 5616(%rsp)       # 16-byte Spill
	movq	4832(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 5536(%rsp)       # 32-byte Spill
	movslq	4664(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1064(%rsp)        # 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, 1056(%rsp)        # 8-byte Spill
	movq	632(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	notl	%edx
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	64(%rax), %r13d
	movl	%r13d, 1768(%rsp)       # 4-byte Spill
	movl	%edi, %r12d
	notl	%r12d
	movq	%r12, 3064(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rax        # 8-byte Reload
	movq	4656(%rsp), %rsi        # 8-byte Reload
	leaq	1(%rsi,%rax), %rax
	movq	%rax, 656(%rsp)         # 8-byte Spill
	movslq	4648(%rsp), %r10        # 4-byte Folded Reload
	movslq	4576(%rsp), %rbx        # 4-byte Folded Reload
	movq	%rbx, 5528(%rsp)        # 8-byte Spill
	movq	5032(%rsp), %rax        # 8-byte Reload
	leal	1(%rax,%r15), %ebx
	movl	%ebx, 1052(%rsp)        # 4-byte Spill
	movq	4680(%rsp), %r11        # 8-byte Reload
	leal	(%r11,%r11,2), %ebx
	movq	%rbx, 4872(%rsp)        # 8-byte Spill
	leal	(,%r15,8), %eax
	movq	%rax, 2048(%rsp)        # 8-byte Spill
	leal	(%rbx,%r15,8), %eax
	movq	%rax, 2040(%rsp)        # 8-byte Spill
	movq	4864(%rsp), %r9         # 8-byte Reload
	leal	(%r9,%r15,8), %eax
	movq	%rax, 2032(%rsp)        # 8-byte Spill
	movq	%rcx, %rsi
	leal	1(%rsi,%r15,8), %eax
	movq	%rax, 2024(%rsp)        # 8-byte Spill
	leal	-4(%rsi,%r15,8), %eax
	movq	%rax, 2016(%rsp)        # 8-byte Spill
	leal	-3(%rsi,%r15,8), %eax
	movq	%rax, 2008(%rsp)        # 8-byte Spill
	leal	-1(%rsi,%r15,8), %eax
	movq	%rax, 2000(%rsp)        # 8-byte Spill
	leal	-2(%rsi,%r15,8), %eax
	movq	%rax, 1992(%rsp)        # 8-byte Spill
	leal	-8(%r11,%r11,2), %eax
	movq	%rax, 2152(%rsp)        # 8-byte Spill
	movq	4568(%rsp), %rax        # 8-byte Reload
	movq	1128(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx,%rax), %eax
	movl	%eax, 1048(%rsp)        # 4-byte Spill
	movq	4704(%rsp), %rcx        # 8-byte Reload
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	1(%rcx,%rax), %eax
	movl	%eax, 1044(%rsp)        # 4-byte Spill
	leal	(,%rcx,8), %eax
	movq	%rax, 1032(%rsp)        # 8-byte Spill
	leal	(%rbx,%rcx,8), %eax
	movq	%rax, 1024(%rsp)        # 8-byte Spill
	leal	(%r9,%rcx,8), %eax
	movq	%rax, 1016(%rsp)        # 8-byte Spill
	leal	(%rsi,%rcx,8), %r15d
	movq	%r15, 1984(%rsp)        # 8-byte Spill
	leal	-3(%rsi,%rcx,8), %eax
	movq	%rax, 1976(%rsp)        # 8-byte Spill
	leal	-7(%rsi,%rcx,8), %eax
	movq	%rax, 1968(%rsp)        # 8-byte Spill
	leal	-5(%rsi,%rcx,8), %eax
	movq	%rax, 1960(%rsp)        # 8-byte Spill
	leal	-8(%rsi,%rcx,8), %eax
	movq	%rax, 1952(%rsp)        # 8-byte Spill
	leal	-4(%rsi,%rcx,8), %eax
	movq	%rax, 1944(%rsp)        # 8-byte Spill
	leal	-6(%rsi,%rcx,8), %eax
	movq	%rax, 1936(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rax        # 8-byte Reload
	leal	1(%rax,%r8), %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movl	1736(%rsp), %eax        # 4-byte Reload
	subl	%r8d, %eax
	movl	%eax, 1244(%rsp)        # 4-byte Spill
	leal	(,%r8,8), %eax
	movq	%rax, 1232(%rsp)        # 8-byte Spill
	leal	(%rsi,%r8,8), %ebx
	movq	%rbx, 2992(%rsp)        # 8-byte Spill
	leal	3(%rsi,%r8,8), %eax
	movq	%rax, 2984(%rsp)        # 8-byte Spill
	leal	1(%rsi,%r8,8), %eax
	movq	%rax, 2976(%rsp)        # 8-byte Spill
	leal	-1(%rsi,%r8,8), %eax
	movq	%rax, 2968(%rsp)        # 8-byte Spill
	leal	-2(%rsi,%r8,8), %eax
	movq	%rax, 2960(%rsp)        # 8-byte Spill
	leal	2(%rsi,%r8,8), %eax
	movq	%rsi, %r9
	movq	%rax, 2952(%rsp)        # 8-byte Spill
	movl	$3, %ecx
	subl	%edi, %ecx
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2944(%rsp)        # 8-byte Spill
	movl	$2, %ecx
	subl	%edi, %ecx
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2936(%rsp)        # 8-byte Spill
	movl	%ebx, %eax
	subl	%edi, %eax
	movq	%rax, 2928(%rsp)        # 8-byte Spill
	addl	$-8, %r14d
	movq	%r14, 2056(%rsp)        # 8-byte Spill
	leaq	1(%r10), %rax
	movq	%rax, 2144(%rsp)        # 8-byte Spill
	leaq	2(%r10,%r10), %rax
	movq	%rax, 672(%rsp)         # 8-byte Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 552(%rsp)         # 8-byte Spill
	movq	600(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	movq	584(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 536(%rsp)         # 8-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 528(%rsp)         # 8-byte Spill
	movq	1760(%rsp), %rax        # 8-byte Reload
	leal	-1(%rax), %eax
	movl	%eax, 1796(%rsp)        # 4-byte Spill
	movq	1752(%rsp), %rsi        # 8-byte Reload
	leal	-1(%rsi), %eax
	movl	%eax, 1640(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax), %ecx
	movl	%ecx, 1708(%rsp)        # 4-byte Spill
	leal	-2(%rsi), %ecx
	movl	%ecx, 1636(%rsp)        # 4-byte Spill
	leal	2(%rax), %ecx
	movl	%ecx, 1704(%rsp)        # 4-byte Spill
	leal	2(%rsi), %ecx
	movl	%ecx, 1644(%rsp)        # 4-byte Spill
	movq	4608(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rcx), %rcx
	movq	%rcx, 4680(%rsp)        # 8-byte Spill
	leal	-3(%rax), %eax
	movl	%eax, 976(%rsp)         # 4-byte Spill
	movslq	1340(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1320(%rsp)        # 8-byte Spill
	movslq	708(%rsp), %rax         # 4-byte Folded Reload
	movq	%rax, 1312(%rsp)        # 8-byte Spill
	movq	4584(%rsp), %rcx        # 8-byte Reload
	leal	-2(%rcx), %eax
	movq	%rax, 4840(%rsp)        # 8-byte Spill
	leal	-1(%rcx), %eax
	movq	%rax, 4832(%rsp)        # 8-byte Spill
	leal	-6(%rcx), %eax
	movq	%rax, 3880(%rsp)        # 8-byte Spill
	leal	-5(%rcx), %eax
	movq	%rax, 3872(%rsp)        # 8-byte Spill
	leal	3(%r9), %eax
	movq	%rax, 4576(%rsp)        # 8-byte Spill
	leal	3(%rcx), %eax
	movq	%rax, 4568(%rsp)        # 8-byte Spill
	leal	1(%rcx), %eax
	movq	%rax, 4560(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	movq	%rax, 4552(%rsp)        # 8-byte Spill
	movq	2096(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	movq	%rax, 1928(%rsp)        # 8-byte Spill
	leal	-8(%rcx), %eax
	movq	%rax, 1920(%rsp)        # 8-byte Spill
	movq	3088(%rsp), %r8         # 8-byte Reload
	movq	2088(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %eax
	movq	%rax, 1912(%rsp)        # 8-byte Spill
	movq	4640(%rsp), %r10        # 8-byte Reload
	leal	(%rcx,%r10), %eax
	movq	%rax, 1904(%rsp)        # 8-byte Spill
	movq	4632(%rsp), %rdi        # 8-byte Reload
	leal	(%rcx,%rdi), %eax
	movq	%rax, 1896(%rsp)        # 8-byte Spill
	leal	(%rcx,%r12), %eax
	movq	%rax, 1888(%rsp)        # 8-byte Spill
	movq	3080(%rsp), %r9         # 8-byte Reload
	leal	(%rcx,%r9), %eax
	movq	%rax, 1880(%rsp)        # 8-byte Spill
	movq	1784(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %eax
	movl	%eax, 668(%rsp)         # 4-byte Spill
	movl	$0, %eax
	movslq	%r13d, %rcx
	movq	%rcx, 1688(%rsp)        # 8-byte Spill
	leal	(%r15,%rdi), %ecx
	movq	%rcx, 1872(%rsp)        # 8-byte Spill
	movq	4104(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movq	%rcx, 1864(%rsp)        # 8-byte Spill
	movq	4096(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movq	%rcx, 1856(%rsp)        # 8-byte Spill
	leal	(%r15,%r10), %ecx
	movq	%rcx, 1848(%rsp)        # 8-byte Spill
	movq	4088(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movq	%rcx, 1840(%rsp)        # 8-byte Spill
	movq	4080(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movq	%rcx, 1832(%rsp)        # 8-byte Spill
	leal	(%rbx,%r8), %ecx
	movq	%rcx, 2920(%rsp)        # 8-byte Spill
	leal	(%rbx,%r12), %ecx
	movq	%rcx, 2912(%rsp)        # 8-byte Spill
	leal	(%rbx,%r9), %ecx
	movq	%rcx, 2904(%rsp)        # 8-byte Spill
	vmovd	4696(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovaps	%ymm0, 1536(%rsp)       # 32-byte Spill
	movq	4816(%rsp), %rcx        # 8-byte Reload
	vmovd	%ecx, %xmm0
	vmovdqa	%ymm0, 5472(%rsp)       # 32-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	%xmm0, 5296(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB147_195:                            # %for f0.s0.v11.v14
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_229 Depth 2
                                        #       Child Loop BB147_231 Depth 3
                                        #     Child Loop BB147_258 Depth 2
                                        #       Child Loop BB147_260 Depth 3
                                        #     Child Loop BB147_295 Depth 2
                                        #       Child Loop BB147_298 Depth 3
                                        #     Child Loop BB147_317 Depth 2
                                        #       Child Loop BB147_320 Depth 3
                                        #     Child Loop BB147_339 Depth 2
                                        #       Child Loop BB147_341 Depth 3
                                        #     Child Loop BB147_367 Depth 2
                                        #       Child Loop BB147_363 Depth 3
                                        #         Child Loop BB147_364 Depth 4
                                        #     Child Loop BB147_361 Depth 2
                                        #       Child Loop BB147_369 Depth 3
                                        #     Child Loop BB147_404 Depth 2
                                        #       Child Loop BB147_407 Depth 3
                                        #     Child Loop BB147_444 Depth 2
                                        #       Child Loop BB147_445 Depth 3
                                        #         Child Loop BB147_446 Depth 4
                                        #     Child Loop BB147_466 Depth 2
                                        #       Child Loop BB147_468 Depth 3
                                        #         Child Loop BB147_470 Depth 4
                                        #       Child Loop BB147_498 Depth 3
                                        #         Child Loop BB147_500 Depth 4
                                        #         Child Loop BB147_527 Depth 4
                                        #         Child Loop BB147_554 Depth 4
                                        #       Child Loop BB147_582 Depth 3
                                        #         Child Loop BB147_584 Depth 4
                                        #       Child Loop BB147_611 Depth 3
                                        #         Child Loop BB147_613 Depth 4
                                        #       Child Loop BB147_649 Depth 3
                                        #         Child Loop BB147_651 Depth 4
                                        #         Child Loop BB147_686 Depth 4
                                        #         Child Loop BB147_705 Depth 4
                                        #       Child Loop BB147_741 Depth 3
                                        #         Child Loop BB147_743 Depth 4
                                        #       Child Loop BB147_778 Depth 3
                                        #         Child Loop BB147_823 Depth 4
                                        #       Child Loop BB147_782 Depth 3
                                        #         Child Loop BB147_784 Depth 4
                                        #         Child Loop BB147_803 Depth 4
                                        #         Child Loop BB147_843 Depth 4
                                        #       Child Loop BB147_863 Depth 3
                                        #         Child Loop BB147_866 Depth 4
                                        #       Child Loop BB147_886 Depth 3
                                        #         Child Loop BB147_931 Depth 4
                                        #       Child Loop BB147_890 Depth 3
                                        #         Child Loop BB147_892 Depth 4
                                        #         Child Loop BB147_911 Depth 4
                                        #         Child Loop BB147_951 Depth 4
                                        #       Child Loop BB147_971 Depth 3
                                        #         Child Loop BB147_974 Depth 4
                                        #       Child Loop BB147_994 Depth 3
                                        #         Child Loop BB147_996 Depth 4
                                        #       Child Loop BB147_1016 Depth 3
                                        #         Child Loop BB147_1018 Depth 4
                                        #         Child Loop BB147_1037 Depth 4
                                        #         Child Loop BB147_1056 Depth 4
                                        #       Child Loop BB147_1076 Depth 3
                                        #         Child Loop BB147_1078 Depth 4
                                        #       Child Loop BB147_1104 Depth 3
                                        #         Child Loop BB147_1100 Depth 4
                                        #           Child Loop BB147_1101 Depth 5
                                        #       Child Loop BB147_1098 Depth 3
                                        #         Child Loop BB147_1106 Depth 4
                                        #       Child Loop BB147_1142 Depth 3
                                        #         Child Loop BB147_1144 Depth 4
                                        #         Child Loop BB147_1179 Depth 4
                                        #         Child Loop BB147_1199 Depth 4
                                        #       Child Loop BB147_1235 Depth 3
                                        #         Child Loop BB147_1238 Depth 4
                                        #       Child Loop BB147_1274 Depth 3
                                        #         Child Loop BB147_1277 Depth 4
                                        #       Child Loop BB147_1313 Depth 3
                                        #         Child Loop BB147_1315 Depth 4
                                        #         Child Loop BB147_1350 Depth 4
                                        #         Child Loop BB147_1370 Depth 4
                                        #       Child Loop BB147_1406 Depth 3
                                        #         Child Loop BB147_1409 Depth 4
                                        #       Child Loop BB147_1446 Depth 3
                                        #         Child Loop BB147_1447 Depth 4
                                        #           Child Loop BB147_1448 Depth 5
	movl	%edx, 644(%rsp)         # 4-byte Spill
	movq	%rax, 648(%rsp)         # 8-byte Spill
	movl	560(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1528(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 864(%rsp)         # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1520(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movl	%esi, 860(%rsp)         # 4-byte Spill
	cmpl	%ecx, %edx
	movl	%ecx, %esi
	cmovgel	%edx, %esi
	movq	%rsi, 1608(%rsp)        # 8-byte Spill
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	%rcx, 1504(%rsp)        # 8-byte Spill
	movl	564(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %edx
	movl	%ecx, %r13d
	cmovgel	%edx, %r13d
	movl	%eax, %ecx
	shll	$5, %ecx
	movq	632(%rsp), %rax         # 8-byte Reload
	addl	%eax, %ecx
	movl	580(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movq	%rcx, 1624(%rsp)        # 8-byte Spill
	cmpb	$0, 579(%rsp)           # 1-byte Folded Reload
	je	.LBB147_196
# BB#197:                               # %assert succeeded246
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	568(%rsp), %rsi         # 8-byte Reload
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, %r14
	testq	%r14, %r14
	je	.LBB147_198
# BB#199:                               # %assert succeeded248
                                        #   in Loop: Header=BB147_195 Depth=1
	testq	$-2147483648, 608(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	552(%rsp), %rax         # 8-byte Reload
	jne	.LBB147_200
# BB#201:                               # %assert succeeded250
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB147_202
# BB#204:                               # %assert succeeded252
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB147_205
# BB#206:                               # %assert succeeded256
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r12, 4816(%rsp)        # 8-byte Spill
	testq	$-2147483648, 600(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	544(%rsp), %rcx         # 8-byte Reload
	jne	.LBB147_207
# BB#208:                               # %assert succeeded258
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	movq	%rcx, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB147_209
# BB#211:                               # %assert succeeded260
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r15
	testq	%r15, %r15
	je	.LBB147_212
# BB#213:                               # %assert succeeded264
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	testq	$-2147483648, 584(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_214
# BB#215:                               # %assert succeeded266
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	536(%rsp), %rsi         # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, %r14
	testq	%r14, %r14
	je	.LBB147_216
# BB#218:                               # %assert succeeded268
                                        #   in Loop: Header=BB147_195 Depth=1
	testq	$-2147483648, 592(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	movq	528(%rsp), %rax         # 8-byte Reload
	jne	.LBB147_219
# BB#220:                               # %assert succeeded270
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	movq	%rax, %rbx
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 4656(%rsp)        # 8-byte Spill
	testq	%rax, %rax
	je	.LBB147_221
# BB#224:                               # %assert succeeded272
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	movq	%r15, 4664(%rsp)        # 8-byte Spill
	movq	%r14, 5032(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 4648(%rsp)        # 8-byte Spill
	testq	%rax, %rax
	je	.LBB147_225
# BB#228:                               # %assert succeeded276
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1528(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	notl	%eax
	cltq
	movq	%rax, 1432(%rsp)        # 8-byte Spill
	movl	$63, %eax
	subl	%ecx, %eax
	movl	%eax, 960(%rsp)         # 4-byte Spill
	movl	$7, %ecx
	movq	864(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 968(%rsp)         # 4-byte Spill
	movq	1520(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	notl	%eax
	cltq
	movq	%rax, 1464(%rsp)        # 8-byte Spill
	movl	$3, %eax
	subl	%ecx, %eax
	movl	%eax, 964(%rsp)         # 4-byte Spill
	movl	860(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	488(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	2144(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	movl	$57, %ecx
	movq	1608(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 2176(%rsp)        # 4-byte Spill
	movl	$1, %ecx
	subl	%r13d, %ecx
	movl	%ecx, 996(%rsp)         # 4-byte Spill
	leal	-4(%r13), %eax
	movl	%eax, 992(%rsp)         # 4-byte Spill
	movl	$7, %eax
	subl	%r13d, %eax
	movl	%eax, 988(%rsp)         # 4-byte Spill
	movl	$3, %eax
	subl	%r13d, %eax
	movl	%eax, 984(%rsp)         # 4-byte Spill
	movl	$9, %eax
	subl	%r13d, %eax
	movl	%eax, 1228(%rsp)        # 4-byte Spill
	movl	$-9, %eax
	subl	%r13d, %eax
	leal	-10(%r13), %edx
	movl	%edx, 980(%rsp)         # 4-byte Spill
	leal	-6(%r13), %edx
	movl	%edx, 1212(%rsp)        # 4-byte Spill
	leal	-12(%r13), %edx
	movl	%edx, 1208(%rsp)        # 4-byte Spill
	notl	%r13d
	movslq	%r13d, %rdx
	movq	%rdx, 1472(%rsp)        # 8-byte Spill
	cltq
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	movq	1624(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rsi
	movq	%rsi, 1416(%rsp)        # 8-byte Spill
	movl	$8, %edx
	subq	%rsi, %rdx
	movq	%rdx, 1664(%rsp)        # 8-byte Spill
	movslq	%ecx, %rcx
	movq	%rcx, 952(%rsp)         # 8-byte Spill
	leal	9(%rax), %eax
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_229:                            # %for deinterleaved$1.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_231 Depth 3
	cmpl	$0, 1676(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_256
# BB#230:                               # %for deinterleaved$1.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_229 Depth=2
	movq	3104(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3536(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	movq	1352(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	1332(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1336(%rsp), %eax        # 4-byte Folded Reload
	movq	1344(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movl	1328(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	cmpq	%rdi, 1320(%rsp)        # 8-byte Folded Reload
	movl	1340(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%rdi, 1312(%rsp)        # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %rdi
	cmovll	%eax, %ecx
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	vmovd	%ecx, %xmm0
	vpabsd	1584(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3008(%rsp)       # 32-byte Spill
	vpsubd	1536(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3488(%rsp)       # 32-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movl	1676(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r15d
	.align	16, 0x90
.LBB147_231:                            # %for deinterleaved$1.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_229 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r15, 3424(%rsp)        # 8-byte Spill
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	cmpl	$0, 3536(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %esi
	andl	$1, %esi
	sete	%cl
	movl	%ecx, 5248(%rsp)        # 4-byte Spill
	movl	%r15d, %eax
	movq	3104(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	movq	3920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4352(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3200(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 5152(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3184(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vmovdqa	.LCPI147_10(%rip), %ymm1 # ymm1 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovdqa	%ymm1, %ymm12
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3584(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4192(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 5152(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4160(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 4128(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	3808(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3840(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r12d, %xmm0, %xmm11
	vmovd	%ebx, %xmm0
	vpinsrd	$1, %r14d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r10d, %xmm0
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r11d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	leal	-8(%r15), %eax
	vmovd	%eax, %xmm5
	movq	3928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	andb	5184(%rsp), %sil        # 1-byte Folded Reload
	movl	%esi, 3408(%rsp)        # 4-byte Spill
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4512(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4480(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4032(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3008(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4320(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4448(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm12, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4304(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4288(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3488(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3808(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4128(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3840(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4160(%rsp)        # 8-byte Spill
	testl	3536(%rsp), %r15d       # 4-byte Folded Reload
	vpbroadcastd	%xmm6, %ymm1
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3216(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3200(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3168(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3184(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3152(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3232(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3136(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3248(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, 3120(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3744(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vmovd	%xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vpaddd	.LCPI147_10(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r8d
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vmovd	%xmm1, %eax
	cltd
	idivl	3584(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r11d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	vmovd	3152(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3168(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3136(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3120(%rsp), %xmm1, %xmm3 # 4-byte Folded Reload
	vmovd	%r12d, %xmm1
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpinsrd	$2, %r13d, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %r8d, %xmm1, %xmm1
	vmovd	%r10d, %xmm2
	vpinsrd	$1, %r9d, %xmm2, %xmm2
	vpinsrd	$2, %r11d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	leal	-7(%r15), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4416(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4384(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	3968(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	3936(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3776(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3488(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, %rax
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3776(%rsp)        # 8-byte Spill
	movslq	%r15d, %r10
	subq	4712(%rsp), %r10        # 8-byte Folded Reload
	addq	2848(%rsp), %r10        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4624(%rsp), %rdx        # 8-byte Reload
	leaq	(%r10,%rdx), %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	je	.LBB147_233
# BB#232:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_233:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovd	%ebx, %xmm0
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm1
	movl	3408(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_235
# BB#234:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_235:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movl	3216(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_237
# BB#236:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_237:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_239
# BB#238:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_239:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm1, 3136(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_241
# BB#240:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_241:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm0, 3152(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_243
# BB#242:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_243:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm1, 3216(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_245
# BB#244:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm2, %xmm0
.LBB147_245:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rsi
	movq	%rsi, 5248(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rdx,%rsi,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3280(%rsp)        # 8-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3376(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rsi
	movq	%rsi, 3392(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r13
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rdx,%r9,2), %xmm1, %xmm1
	movq	4128(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdx,%r15,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rdx,%r13,2), %xmm1, %xmm1
	movq	4160(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rdx,%rdi,2), %xmm2, %xmm2
	movq	5184(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rdx,%rbx,2), %xmm2, %xmm2
	movq	5152(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rdx,%rsi,2), %xmm2, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_247
# BB#246:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm3, %xmm5
.LBB147_247:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movslq	%r11d, %r11
	movslq	%r12d, %r12
	movslq	%r14d, %r8
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r14
	movq	3232(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	movq	%rbx, 3328(%rsp)        # 8-byte Spill
	movq	3200(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rdi
	movq	%rdi, 3344(%rsp)        # 8-byte Spill
	movq	3248(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	movq	%rdx, %rsi
	movzwl	(%rsi,%r14,2), %edx
	vmovd	%edx, %xmm0
	movq	3680(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rsi,%rbx,2), %xmm0, %xmm0
	movq	3744(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rsi,%rdi,2), %xmm0, %xmm0
	movq	3712(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$6, (%rsi,%rcx,2), %xmm0, %xmm0
	movq	%rax, %rcx
	movq	%r8, %rax
	movzwl	(%rsi,%r11,2), %edx
	vmovd	%edx, %xmm1
	movq	3776(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %r8d
	vpinsrw	$7, %r8d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3552(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r12,2), %xmm1, %xmm1
	movq	3616(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rax,2), %xmm1, %xmm1
	movq	3584(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rcx,2), %xmm1, %xmm1
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	%rsi, %rdx
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	%ymm1, (%rsi,%r10,4)
	vmovups	%ymm3, 32(%rsi,%r10,4)
	vmovups	%ymm0, 64(%rsi,%r10,4)
	vmovups	%ymm2, 96(%rsi,%r10,4)
	je	.LBB147_249
# BB#248:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	%xmm8, %xmm7
.LBB147_249:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movq	%rdx, %rsi
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %edx
	vmovd	%edx, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rsi,%r9,2), %xmm0, %xmm0
	movq	4128(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rsi,%r15,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm0, %xmm0
	vpinsrw	$6, (%rsi,%r13,2), %xmm0, %xmm0
	movq	4160(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm0, %xmm0
	movq	5248(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rsi,%rdx,2), %edx
	vmovd	%edx, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	5216(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm1, %xmm2
	movq	%rsi, %rdx
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_251
# BB#250:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	3136(%rsp), %xmm6       # 16-byte Reload
.LBB147_251:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movq	%rdx, %rsi
	movzwl	(%rsi,%r11,2), %edx
	vmovd	%edx, %xmm5
	movq	3552(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$2, (%rsi,%r12,2), %xmm5, %xmm5
	movq	3616(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rsi,%rax,2), %xmm5, %xmm5
	movq	3584(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rdx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3648(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rdx,2), %xmm5, %xmm7
	movzwl	(%rsi,%r14,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm5, %xmm5
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$6, (%rsi,%rcx,2), %xmm5, %xmm5
	vpinsrw	$7, %r8d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	5608(%rsp), %rdx        # 8-byte Reload
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rdx,%rcx,4)
	vmovups	%ymm4, 12320(%rdx,%rcx,4)
	vmovups	%ymm2, 12352(%rdx,%rcx,4)
	vmovups	%ymm6, 12384(%rdx,%rcx,4)
	je	.LBB147_253
# BB#252:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	3152(%rsp), %xmm9       # 16-byte Reload
.LBB147_253:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_255
# BB#254:                               # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	vmovdqa	3216(%rsp), %xmm15      # 16-byte Reload
.LBB147_255:                            # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_231 Depth=3
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	3776(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rcx,%rsi,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4616(%rsp), %r10        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rdx,%r10,4)
	vmovups	%ymm2, 24608(%rdx,%r10,4)
	vmovups	%ymm1, 24640(%rdx,%r10,4)
	vmovups	%ymm4, 24672(%rdx,%r10,4)
	movq	3424(%rsp), %r15        # 8-byte Reload
	addl	$32, %r15d
	movl	3456(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_231
.LBB147_256:                            # %end for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB147_229 Depth=2
	movq	3104(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	addq	$1, %rax
	cmpl	2832(%rsp), %ecx        # 4-byte Folded Reload
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	jne	.LBB147_229
# BB#257:                               # %produce gH
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1624(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax), %edx
	movl	%edx, 1512(%rsp)        # 4-byte Spill
	leal	4(%rax), %ecx
	movl	%ecx, 1616(%rsp)        # 4-byte Spill
	movl	$8, %ecx
	subl	%eax, %ecx
	movq	%rcx, 1144(%rsp)        # 8-byte Spill
	movl	508(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1496(%rsp)        # 4-byte Spill
	movl	512(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1488(%rsp)        # 4-byte Spill
	movl	516(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1480(%rsp)        # 4-byte Spill
	movl	%edx, 2192(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_258:                            # %for gH.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_260 Depth 3
	cmpl	$0, 2184(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_293
# BB#259:                               # %for gH.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_258 Depth=2
	movl	2192(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1740(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1744(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2112(%rsp)       # 16-byte Spill
	cltq
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1712(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2104(%rsp)        # 8-byte Spill
	movl	2184(%rsp), %eax        # 4-byte Reload
	movq	5288(%rsp), %r15        # 8-byte Reload
	movl	1496(%rsp), %ecx        # 4-byte Reload
	movl	1488(%rsp), %r9d        # 4-byte Reload
	movl	1480(%rsp), %edi        # 4-byte Reload
	.align	16, 0x90
.LBB147_260:                            # %for gH.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_258 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%ecx, 5248(%rsp)        # 4-byte Spill
	movl	%edi, 3616(%rsp)        # 4-byte Spill
	movl	%r9d, 3648(%rsp)        # 4-byte Spill
	movl	%eax, 3680(%rsp)        # 4-byte Spill
	cmpl	$0, 3808(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r13d
	andl	$1, %r13d
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	3080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	movq	3064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vmovd	4160(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4192(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 4128(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3840(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3840(%rsp)       # 16-byte Spill
	leal	-2(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4128(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	vmovd	%r14d, %xmm0
	vpinsrd	$1, 3536(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3424(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r11d, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm3, %xmm5
	leal	-1(%r15), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	leal	-3(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4160(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2112(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5296(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5344(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm0
	vmovdqa	5360(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm1
	vmovdqa	5104(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpcmpgtd	%xmm1, %xmm14, %xmm4
	vpsubd	%xmm1, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	4160(%rsp), %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	%xmm12, %xmm1, %xmm0
	vmovdqa	%xmm0, 4160(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm11, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movl	2192(%rsp), %ebx        # 4-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	3088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vmovdqa	3840(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm5
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	5088(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	4128(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm2, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	sete	4128(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r8b        # 1-byte Reload
	movb	5152(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, %r8b
	vpaddd	%xmm1, %xmm11, %xmm5
	vmovq	%xmm5, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm5, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movb	5184(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r13b
	movl	%r13d, 5216(%rsp)       # 4-byte Spill
	movl	3808(%rsp), %r14d       # 4-byte Reload
	testl	%r15d, %r14d
	setne	3840(%rsp)              # 1-byte Folded Spill
	leal	1(%r15), %r13d
	movl	%r13d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	sete	%r12b
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	3392(%rsp)              # 4-byte Folded Reload
	vmovd	3552(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3584(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3488(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3456(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	3424(%rsp), %xmm5       # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vpinsrd	$1, 3408(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$2, 3376(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$3, 3360(%rsp), %xmm5, %xmm6 # 4-byte Folded Reload
	leal	-4(%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpinsrd	$2, %esi, %xmm7, %xmm7
	vpsrad	$31, %xmm4, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpinsrd	$3, %edx, %xmm7, %xmm0
	vpaddd	%xmm4, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	orl	%ebx, %eax
	movb	%r12b, %bl
	testb	$1, %al
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	%r10b, %bl
	andb	%r11b, %r9b
	testl	%r13d, %r14d
	vmovd	%r13d, %xmm1
	vpsrad	$31, %xmm6, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vmovdqa	4944(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm6
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm6, %xmm6
	vmovdqa	4784(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm2, %xmm5, %xmm2
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	5040(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vmovdqa	5008(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm2, %xmm5
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm3
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm6
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	setne	%r14b
	vmovq	%xmm0, %r8
	movq	%r8, 2304(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2352(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3536(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm1, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 2368(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2432(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	movslq	5248(%rsp), %rcx        # 4-byte Folded Reload
	movq	%rcx, %rdx
	orq	$4, %rdx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2496(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2576(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2592(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2608(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2656(%rsp)        # 8-byte Spill
	movq	%rcx, %rdx
	orq	$6, %rdx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	vmovdqa	4160(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2816(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3200(%rsp)        # 8-byte Spill
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm10
	vmovaps	%xmm10, 4160(%rsp)      # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_262
# BB#261:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm10, %xmm10, %xmm10
.LBB147_262:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	movzbl	4128(%rsp), %r13d       # 1-byte Folded Reload
	vmovd	%r13d, %xmm0
	movzbl	3840(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm8
	vmovaps	%xmm8, 3840(%rsp)       # 16-byte Spill
	je	.LBB147_264
# BB#263:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_264:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_266
# BB#265:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_266:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	je	.LBB147_268
# BB#267:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_268:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm1, 2208(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2224(%rsp)       # 16-byte Spill
	movzbl	%bl, %edi
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_270
# BB#269:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_270:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm0, 2240(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	%r14b, %edi
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, %xmm2
	movq	4816(%rsp), %r14        # 8-byte Reload
	je	.LBB147_272
# BB#271:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_272:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm2, 2256(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	movzbl	%r9b, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_274
# BB#273:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_274:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm2, 2272(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movl	3648(%rsp), %r9d        # 4-byte Reload
	je	.LBB147_276
# BB#275:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_276:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movq	2832(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3136(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2848(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2784(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3008(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3152(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm12 # xmm12 = xmm2[0,1,2],mem[0]
	movq	2688(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2720(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2704(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2736(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm15 # xmm15 = xmm3[0,1,2],mem[0]
	movq	2480(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2528(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2512(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2544(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm11 # xmm11 = xmm3[0,1,2],mem[0]
	movslq	%r9d, %rdi
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rdi,4), %xmm3
	vmovups	12328(%rdx,%rdi,4), %xmm6
	vmovups	12304(%rdx,%rdi,4), %xmm7
	vmovups	12320(%rdx,%rdi,4), %xmm1
	vshufps	$136, 12336(%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,2],mem[0,2]
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm0, %xmm9, %xmm0
	vshufps	$221, %xmm6, %xmm3, %xmm2 # xmm2 = xmm3[1,3],xmm6[1,3]
	vmovaps	5408(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$221, %xmm1, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm1[1,3]
	vmulps	%xmm12, %xmm9, %xmm1
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm6, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm15, %xmm9, %xmm3
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm7
	vsubps	%xmm4, %xmm14, %xmm6
	cmpl	$0, 104(%rbp)
	je	.LBB147_278
# BB#277:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm10, 4192(%rsp)      # 16-byte Spill
.LBB147_278:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm9
	vmaxps	%xmm1, %xmm2, %xmm0
	vmulps	3776(%rsp), %xmm11, %xmm12 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm6, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm5
	je	.LBB147_280
# BB#279:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	%xmm8, 4128(%rsp)       # 16-byte Spill
.LBB147_280:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	movq	2304(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2320(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	movq	2336(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2352(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	movq	2368(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	2384(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r10,4), %xmm6, %xmm15 # xmm15 = xmm6[0,1,2],mem[0]
	movl	3616(%rsp), %edi        # 4-byte Reload
	movslq	%edi, %rax
	vmovups	24592(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vmovups	24608(%rdx,%rax,4), %xmm10
	vmovups	24624(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 2832(%rsp)       # 16-byte Spill
	vmovups	24600(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3120(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm6
	vmovaps	%xmm6, 3104(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	vaddps	%xmm9, %xmm0, %xmm7
	vmovaps	%xmm7, 2720(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm12, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm5, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm0
	vshufps	$136, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm10[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm7, %xmm1
	vshufps	$136, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm15, %xmm7, %xmm2
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_282
# BB#281:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	2208(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
.LBB147_282:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rdx
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2432(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],mem[0]
	movq	2464(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 3136(%rsp)       # 16-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2576(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2592(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	2608(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2672(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3008(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm9
	vmovups	48(%rsi,%rcx,4), %xmm11
	vmovaps	%xmm11, 2784(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%rcx,4), %xmm14
	vmovaps	%xmm14, 2848(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm12, %xmm15
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm1
	vshufps	$136, %xmm9, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm9[0,2]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$136, %xmm11, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm12, %xmm4
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vmulps	5216(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm4
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm5, %xmm1, %xmm8
	je	.LBB147_284
# BB#283:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	2224(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3840(%rsp)       # 16-byte Spill
.LBB147_284:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vsubps	%xmm0, %xmm15, %xmm1
	vmovdqa	4160(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vfmadd213ps	%xmm2, %xmm14, %xmm7
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm4, %xmm6
	vmovdqa	3840(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm8, %xmm5, %xmm0
	je	.LBB147_286
# BB#285:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	2240(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 5184(%rsp)       # 16-byte Spill
.LBB147_286:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovdqa	4128(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vfmadd213ps	%xmm2, %xmm14, %xmm1
	vblendvps	%xmm15, %xmm7, %xmm0, %xmm0
	vaddps	%xmm6, %xmm8, %xmm15
	je	.LBB147_288
# BB#287:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovaps	2256(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 5152(%rsp)       # 16-byte Spill
.LBB147_288:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm7
	movq	2752(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3744(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm1
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 2832(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm10[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3120(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3104(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm2, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm12, %xmm4
	vminps	%xmm13, %xmm1, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm1
	vmulps	5216(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3408(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_290
# BB#289:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovdqa	2272(%rsp), %xmm10      # 16-byte Reload
.LBB147_290:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vblendvps	%xmm0, %xmm8, %xmm7, %xmm8
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 2784(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm9[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3008(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm4, %xmm12
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3136(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm9[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vsubps	%xmm0, %xmm12, %xmm0
	vfmadd213ps	%xmm6, %xmm14, %xmm1
	vfmadd213ps	%xmm6, %xmm14, %xmm0
	vmovdqa	5184(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vpslld	$31, %xmm10, %xmm5
	vmovdqa	3392(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_292
# BB#291:                               # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vmovdqa	2288(%rsp), %xmm6       # 16-byte Reload
.LBB147_292:                            # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_260 Depth=3
	vpslld	$31, %xmm6, %xmm6
	vmovaps	3152(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm6, %xmm7, %xmm2, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm7, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	2104(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r14,%rax,4)
	addl	$8, %edi
	addl	$8, %r9d
	movl	5248(%rsp), %ecx        # 4-byte Reload
	addl	$8, %ecx
	addl	$8, %r15d
	movl	3680(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_260
.LBB147_293:                            # %end for gH.s0.v10.v10
                                        #   in Loop: Header=BB147_258 Depth=2
	movl	2192(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2192(%rsp)        # 4-byte Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1480(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1488(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1496(%rsp)        # 4-byte Folded Spill
	cmpl	1616(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_258
# BB#294:                               # %for gV.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	7(%rax), %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 2336(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	leal	10(%rax), %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	addl	$9, %eax
	movq	%rax, 1504(%rsp)        # 8-byte Spill
	movl	1512(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	movl	2184(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_295:                            # %for gV.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_298 Depth 3
	testl	%eax, %eax
	jle	.LBB147_296
# BB#297:                               # %for gV.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_295 Depth=2
	movq	%r8, 2816(%rsp)         # 8-byte Spill
	movl	%r8d, %ebx
	movq	1752(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ebx
	leal	-1(%rbx), %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edi
	movl	%ecx, %r11d
	subl	%eax, %edi
	movq	1784(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %esi
	cmovgl	%r8d, %esi
	addl	$-1, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ecx
	cmovll	%edi, %esi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r8d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %ecx
	cmovlel	%edx, %ebx
	movl	%r8d, %ecx
	subl	%r9d, %ecx
	cmovll	%edx, %ebx
	cmovlel	%edi, %esi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r12d, %edi
	addl	%edx, %edi
	movl	%r11d, %eax
	subl	%edi, %eax
	cmpl	%edi, %r10d
	cmovgl	%edi, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	leal	1(%r8), %edi
	movl	%edi, 2592(%rsp)        # 4-byte Spill
	cmpl	%edi, %r13d
	movl	%r13d, %edx
	cmovgl	%edi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%eax, %edx
	movl	%r8d, %edi
	andl	$1, %edi
	movl	%edi, 5184(%rsp)        # 4-byte Spill
	movslq	%ebx, %r11
	movq	1816(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2800(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rbx
	movq	%rbx, 5248(%rsp)        # 8-byte Spill
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5248(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r11), %r12
	movl	1796(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %edx
	subl	%esi, %edx
	cmpl	%esi, %r10d
	cmovgl	%esi, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	leal	2(%r8), %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %esi
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rax,%rdx), %r15
	subl	%ecx, %ebx
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	leal	-2(%r8), %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ebx, %ecx
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %ecx
	movslq	%ecx, %rcx
	imulq	%r14, %rcx
	leaq	(%rax,%rcx), %rbx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r15,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1712(%rsp), %rcx        # 8-byte Folded Reload
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2736(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2720(%rsp)        # 8-byte Spill
	subq	4712(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2752(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1504(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2656(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2640(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	xorl	%r9d, %r9d
	movl	2184(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_298:                            # %for gV.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_295 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 5184(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %r15d
	movl	%r15d, 3712(%rsp)       # 4-byte Spill
	movl	%r15d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r11d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2800(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5328(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	2816(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	sete	%r8b
	andb	5216(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5248(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5184(%rsp), %r15d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r14
	movq	%r14, %r15
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r13
	movq	%r13, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2672(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movq	2640(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %r10d
	movq	2608(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3120(%rsp)        # 4-byte Spill
	movq	2704(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3200(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3216(%rsp)        # 4-byte Spill
	movq	2624(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r9), %ebx
	movl	%ebx, 3232(%rsp)        # 4-byte Spill
	je	.LBB147_300
# BB#299:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_300:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movzbl	%r8b, %r8d
	vmovd	%r8d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_302
# BB#301:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_302:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_304
# BB#303:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_304:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3008(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_306
# BB#305:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_306:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r10d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	movslq	3120(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r14d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r13d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5408(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2768(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_308
# BB#307:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovdqa	2848(%rsp), %xmm9       # 16-byte Reload
.LBB147_308:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5408(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4696(%rsp), %rcx        # 8-byte Reload
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3680(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3248(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_310
# BB#309:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovdqa	2832(%rsp), %xmm7       # 16-byte Reload
.LBB147_310:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	3360(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4160(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3232(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5152(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_312
# BB#311:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovdqa	3008(%rsp), %xmm6       # 16-byte Reload
.LBB147_312:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3776(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	je	.LBB147_314
# BB#313:                               # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovdqa	3104(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
.LBB147_314:                            # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB147_298 Depth=3
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3424(%rsp), %xmm11      # 16-byte Reload
	vmulps	4160(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4128(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3648(%rsp), %xmm7       # 16-byte Reload
	vmulps	3808(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3776(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vmulps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3552(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3536(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3584(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5248(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3712(%rsp), %rax        # 4-byte Folded Reload
	movq	2752(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r9d
	movl	3744(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_298
	jmp	.LBB147_315
.LBB147_296:                            # %for gV.s0.v11.end for gV.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_295 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2592(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_315:                            #   in Loop: Header=BB147_295 Depth=2
	movl	2188(%rsp), %ecx        # 4-byte Reload
	movl	992(%rsp), %edx         # 4-byte Reload
	movl	984(%rsp), %esi         # 4-byte Reload
	movl	2592(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	1616(%rsp), %eax        # 4-byte Folded Reload
	movl	2184(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_295
# BB#316:                               # %produce dV
                                        #   in Loop: Header=BB147_195 Depth=1
	movl	%esi, 984(%rsp)         # 4-byte Spill
	movl	%edx, 992(%rsp)         # 4-byte Spill
	movq	1624(%rsp), %rax        # 8-byte Reload
	leal	-6(%rax), %edx
	movl	%edx, 2544(%rsp)        # 4-byte Spill
	leal	8(%rax), %eax
	movl	%eax, 2368(%rsp)        # 4-byte Spill
	movl	%edx, %r9d
	.align	16, 0x90
.LBB147_317:                            # %for dV.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_320 Depth 3
	testl	%ecx, %ecx
	jle	.LBB147_318
# BB#319:                               # %for dV.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_317 Depth=2
	movq	%r9, 2688(%rsp)         # 8-byte Spill
	movl	%r9d, %edi
	movq	1752(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1760(%rsp), %r10        # 8-byte Reload
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %r15d
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r11d
	subl	%eax, %esi
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1740(%rsp), %r8d        # 4-byte Reload
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %r13d
	cmovgl	%r9d, %r13d
	addl	$-1, %r13d
	cmpl	%ebx, %r13d
	cmovll	%ebx, %r13d
	cmpl	%r9d, %ecx
	cmovll	%esi, %r13d
	movl	%edi, %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, %r8d
	movl	%r8d, %edi
	cmovgl	%r9d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r9d, %ecx
	cmovlel	%edx, %edi
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r13d
	leal	1(%rcx), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	leal	1(%r9), %eax
	movl	%eax, 2288(%rsp)        # 4-byte Spill
	cmpl	%eax, %r8d
	movl	%r8d, %r14d
	cmovgl	%eax, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r9d, %r8d
	cmovlel	%esi, %r14d
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movslq	%edi, %rdi
	movq	1816(%rsp), %r12        # 8-byte Reload
	imulq	%r12, %rdi
	movq	%rdi, 5248(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2672(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%rdi), %rax
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	movl	%r11d, %edi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	sarl	$31, %edi
	movl	1772(%rsp), %eax        # 4-byte Reload
	andl	%eax, %edi
	addl	%r11d, %edi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	addl	%edx, %ecx
	movl	1796(%rsp), %r10d       # 4-byte Reload
	movl	%r10d, %eax
	subl	%edi, %eax
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovgl	%edi, %eax
	addl	%ebx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	2(%r9), %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%eax, %edx
	cmpl	%r9d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r11
	movq	1824(%rsp), %rax        # 8-byte Reload
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	imulq	%r12, %r11
	leaq	(%r15,%r11), %rdx
	movq	%rdx, 5184(%rsp)        # 8-byte Spill
	subl	%ecx, %r10d
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%ecx, %edx
	cmovgl	%ecx, %r10d
	addl	%ebx, %r10d
	cmpl	%r10d, %r8d
	cmovlel	%r8d, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	leal	-2(%r9), %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%r10d, %ecx
	cmpl	%r9d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%r10d, %ecx
	movslq	%ecx, %rcx
	imulq	%r12, %rcx
	leaq	(%r15,%rcx), %rdi
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	cmpl	%r9d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r14d
	movslq	%r14d, %rdx
	imulq	%r12, %rdx
	leaq	(%rdx,%r15), %rdx
	movslq	%r13d, %rsi
	imulq	%r12, %rsi
	leaq	(%rsi,%r15), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r10
	leaq	(%rdi,%rcx), %rsi
	movq	5248(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1800(%rsp), %rbx        # 8-byte Reload
	leaq	(%r11,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	2352(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2528(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2496(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2480(%rsp)        # 8-byte Spill
	movq	2336(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2464(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2448(%rsp)        # 8-byte Spill
	movq	1504(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2432(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	2188(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_320:                            # %for dV.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_317 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5216(%rsp)          # 4-byte Folded Reload
	setne	3680(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r14d
	movl	%r14d, 3776(%rsp)       # 4-byte Spill
	movl	%r14d, %r11d
	andl	$1, %r11d
	sete	%r12b
	movq	3872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	movq	3880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r15d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r11d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2672(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4928(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5296(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4120(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	2688(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4112(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	sete	3328(%rsp)              # 1-byte Folded Spill
	andb	3680(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_55(%rip), %xmm3, %xmm3
	vmovdqa	4752(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5216(%rsp), %r14d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r15
	movq	%r15, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r14
	movq	%r14, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3152(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r11d
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r10d
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movq	2560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r12d
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movq	2400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3296(%rsp)        # 4-byte Spill
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	je	.LBB147_322
# BB#321:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_322:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movzbl	3328(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	je	.LBB147_324
# BB#323:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_324:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_326
# BB#325:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_326:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	%xmm4, 3312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	je	.LBB147_328
# BB#327:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_328:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r11d, %rcx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3008(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3232(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2816(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 2848(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movslq	%r12d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm2, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm2[0,2]
	vmovaps	5408(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5440(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2640(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm5, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm5[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2784(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2800(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2592(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2768(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	3008(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2848(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3248(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3312(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_330
# BB#329:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovdqa	2720(%rsp), %xmm10      # 16-byte Reload
.LBB147_330:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vandps	3360(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3280(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3264(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3328(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_332
# BB#331:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovdqa	2704(%rsp), %xmm11      # 16-byte Reload
.LBB147_332:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5152(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3536(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3344(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3264(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3680(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_334
# BB#333:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovdqa	2736(%rsp), %xmm5       # 16-byte Reload
.LBB147_334:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vmulps	4160(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4128(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm7
	vmovaps	%xmm7, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3840(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm14
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3312(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmulps	3360(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmulps	3232(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	je	.LBB147_336
# BB#335:                               # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
.LBB147_336:                            # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB147_320 Depth=3
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	5184(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3536(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5152(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	4160(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4128(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3680(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3456(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3392(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2576(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_320
	jmp	.LBB147_337
.LBB147_318:                            # %for dV.s0.v11.end for dV.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_317 Depth=2
	addl	$1, %r9d
	movl	%r9d, %eax
	movl	%eax, 2288(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_337:                            #   in Loop: Header=BB147_317 Depth=2
	movq	5032(%rsp), %rax        # 8-byte Reload
	movl	996(%rsp), %edx         # 4-byte Reload
	movl	988(%rsp), %esi         # 4-byte Reload
	movl	980(%rsp), %edi         # 4-byte Reload
	movl	2288(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %r9d
	cmpl	2368(%rsp), %ecx        # 4-byte Folded Reload
	movl	2188(%rsp), %ecx        # 4-byte Reload
	jne	.LBB147_317
# BB#338:                               #   in Loop: Header=BB147_195 Depth=1
	movl	%edi, 980(%rsp)         # 4-byte Spill
	movl	%esi, 988(%rsp)         # 4-byte Spill
	movl	%edx, 996(%rsp)         # 4-byte Spill
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	movl	772(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2480(%rsp)        # 4-byte Spill
	movl	500(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2464(%rsp)        # 4-byte Spill
	movl	504(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2448(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_339:                            # %for dh.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_341 Depth 3
	cmpl	$0, 2188(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_358
# BB#340:                               # %for dh.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_339 Depth=2
	movl	2544(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1740(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1744(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 2528(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5248(%rsp)       # 16-byte Spill
	cltq
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	movl	2188(%rsp), %eax        # 4-byte Reload
	movq	5288(%rsp), %r13        # 8-byte Reload
	movl	2480(%rsp), %r15d       # 4-byte Reload
	movl	2464(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, 3840(%rsp)        # 4-byte Spill
	movl	2448(%rsp), %r8d        # 4-byte Reload
	.align	16, 0x90
.LBB147_341:                            # %for dh.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_339 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%r8d, 3744(%rsp)        # 4-byte Spill
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	movl	2528(%rsp), %r10d       # 4-byte Reload
	testl	%r10d, %r10d
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r13d, %r11d
	andl	$1, %r11d
	sete	5152(%rsp)              # 1-byte Folded Spill
	movq	4080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4928(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5344(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovdqa	5360(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3776(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	4944(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4784(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	movq	4104(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3616(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	leal	-8(%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r9d
	sarq	$32, %rbx
	movq	%rbx, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3408(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	leal	-7(%r13), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2768(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4272(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3904(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4752(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3712(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3680(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3888(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	orl	2544(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	2688(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %bl         # 1-byte Reload
	andb	5216(%rsp), %bl         # 1-byte Folded Reload
	andb	5184(%rsp), %r11b       # 1-byte Folded Reload
	movl	%r11d, 5184(%rsp)       # 4-byte Spill
	testl	%r13d, %r10d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5216(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	4960(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %r8
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %r12
	sarq	$32, %r12
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3776(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2624(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r14
	movq	%r14, 2640(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vmovdqa	3488(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3536(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_343
# BB#342:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_343:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movzbl	2688(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	5216(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_345
# BB#344:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_345:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movl	5184(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_347
# BB#346:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_347:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	%xmm1, 2592(%rsp)       # 16-byte Spill
	movl	%r15d, 3776(%rsp)       # 4-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	je	.LBB147_349
# BB#348:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_349:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5464(%rsp), %r15        # 8-byte Reload
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3456(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2768(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2752(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2800(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	3840(%rsp), %rbx        # 4-byte Folded Reload
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12296(%rdx,%rbx,4), %xmm7
	vmovups	12312(%rdx,%rbx,4), %xmm0
	vmovups	12304(%rdx,%rbx,4), %xmm3
	vmovups	12320(%rdx,%rbx,4), %xmm14
	vmovups	12288(%rdx,%rbx,4), %xmm5
	movq	2832(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3104(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3136(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	movq	3008(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3168(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3184(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r10d, %rbx
	vmovss	(%r15,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r11d, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r15,%r12,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2512(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5184(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_351
# BB#350:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	2576(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
.LBB147_351:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movl	3744(%rsp), %r8d        # 4-byte Reload
	vmovdqa	2688(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_353
# BB#352:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovdqa	2560(%rsp), %xmm15      # 16-byte Reload
.LBB147_353:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	movq	2624(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2640(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r14,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2656(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%r15,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2672(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	%r8d, %rcx
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	24584(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3152(%rsp)       # 16-byte Spill
	vmovups	24576(%rsi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3184(%rsp)      # 16-byte Spill
	vmovups	24592(%rsi,%rcx,4), %xmm9
	vmovups	24608(%rsi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3616(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3136(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3120(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_355
# BB#354:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_355:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm7
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm11
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r15, %rdi
	vmovups	(%rsi,%rax,4), %xmm15
	vmovaps	%xmm15, 3360(%rsp)      # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3456(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vaddps	5184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3136(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmulps	3552(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovdqa	3120(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmulps	3104(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movl	3776(%rsp), %r15d       # 4-byte Reload
	je	.LBB147_357
# BB#356:                               # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_357:                            # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_341 Depth=3
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3680(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3712(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3648(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3408(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5216(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4128(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3456(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3536(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4192(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3424(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3584(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3488(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	2496(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4664(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	addl	$8, 3840(%rsp)          # 4-byte Folded Spill
	addl	$8, %r15d
	addl	$8, %r13d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_341
.LBB147_358:                            # %end for dh.s0.v10.v10
                                        #   in Loop: Header=BB147_339 Depth=2
	movl	2544(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2544(%rsp)        # 4-byte Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	addl	%eax, 2448(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2464(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2480(%rsp)        # 4-byte Folded Spill
	cmpl	2368(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_339
# BB#359:                               # %for f4.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movl	2184(%rsp), %r14d       # 4-byte Reload
	testl	%r14d, %r14d
	movq	808(%rsp), %r11         # 8-byte Reload
	movq	4664(%rsp), %rsi        # 8-byte Reload
	movq	%rsi, %r12
	movq	4704(%rsp), %rsi        # 8-byte Reload
	movq	4696(%rsp), %r13        # 8-byte Reload
	vmovaps	736(%rsp), %ymm2        # 32-byte Reload
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vmovdqu	.LCPI147_22(%rip), %xmm3
	movl	1512(%rsp), %eax        # 4-byte Reload
	jle	.LBB147_360
	.align	16, 0x90
.LBB147_367:                            # %for f4.s0.v10.v10.preheader.us
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_363 Depth 3
                                        #         Child Loop BB147_364 Depth 4
	movl	%eax, 1512(%rsp)        # 4-byte Spill
	movl	%eax, %r9d
	andl	$63, %r9d
	movl	%r9d, %r8d
	movq	1168(%rsp), %rax        # 8-byte Reload
	imull	%eax, %r8d
	imulq	1200(%rsp), %r9         # 8-byte Folded Reload
	subq	4712(%rsp), %r9         # 8-byte Folded Reload
	xorl	%r10d, %r10d
	.align	16, 0x90
.LBB147_363:                            # %for f4.s0.v10.v10.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_367 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_364 Depth 4
	leal	(,%r10,8), %r15d
	vxorps	%ymm0, %ymm0, %ymm0
	movl	$9, %edi
	movl	2176(%rsp), %ebx        # 4-byte Reload
	.align	16, 0x90
.LBB147_364:                            # %for sum.s1.r4$y.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_367 Depth=2
                                        #       Parent Loop BB147_363 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ebx, %ecx
	andl	$63, %ecx
	imull	%r11d, %ecx
	leal	(%r15,%rcx), %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r12,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$1, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$2, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$3, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$4, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$5, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%eax, %edx
	orl	$6, %edx
	movslq	%edx, %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	vcmpnltps	(%r12,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	orl	$7, %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r12,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	leal	8(%r15,%rcx), %eax
	cltq
	vmovups	(%rsi,%rax,4), %ymm1
	vcmpnltps	(%r12,%rax,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	addl	$1, %ebx
	addl	$-1, %edi
	jne	.LBB147_364
# BB#365:                               # %consume sum.us
                                        #   in Loop: Header=BB147_363 Depth=3
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %eax
	addl	%r8d, %r15d
	vpbroadcastd	%xmm3, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm0
	movslq	%r15d, %rcx
	movq	4816(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm1
	vblendvps	%ymm0, (%r13,%rcx,4), %ymm1, %ymm0
	cltq
	leaq	(%rax,%r9), %rax
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r10
	cmpl	%r14d, %r10d
	jne	.LBB147_363
# BB#366:                               # %end for f4.s0.v10.v10.us
                                        #   in Loop: Header=BB147_367 Depth=2
	movl	1512(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	addl	$1, 2176(%rsp)          # 4-byte Folded Spill
	cmpl	1616(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_367
.LBB147_360:                            # %produce f7
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1624(%rsp), %rax        # 8-byte Reload
	leal	2(%rax), %ecx
	movl	%ecx, 2176(%rsp)        # 4-byte Spill
	movq	1608(%rsp), %rcx        # 8-byte Reload
	leal	10(%rcx), %edx
	movq	%rdx, 2112(%rsp)        # 8-byte Spill
	leal	8(%rcx), %edx
	movq	%rdx, 2104(%rsp)        # 8-byte Spill
	leal	7(%rcx), %edx
	movq	%rdx, 1616(%rsp)        # 8-byte Spill
	leal	11(%rcx), %edx
	movq	%rdx, 1512(%rsp)        # 8-byte Spill
	addl	$9, %ecx
	movq	%rcx, 1608(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_361:                            # %for f7.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_369 Depth 3
	testl	%eax, %eax
	jle	.LBB147_362
# BB#368:                               # %for f7.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_361 Depth=2
	movq	%r8, 2384(%rsp)         # 8-byte Spill
	movl	%r8d, %edi
	movq	1752(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r10d
	subl	%eax, %esi
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r9
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %r14d
	cmovgl	%r8d, %r14d
	addl	$-1, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %ecx
	cmovll	%esi, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r9d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %edi
	cmovgl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r8d, %ecx
	cmovlel	%edx, %edi
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r14d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	leal	1(%r8), %eax
	movl	%eax, 2192(%rsp)        # 4-byte Spill
	cmpl	%eax, %r13d
	movl	%r13d, %r9d
	cmovgl	%eax, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	cmpl	%r8d, %r13d
	cmovlel	%r11d, %r9d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3104(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2368(%rsp)       # 16-byte Spill
	movslq	%edi, %r10
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%edi, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1816(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %r10
	movq	1800(%rsp), %r12        # 8-byte Reload
	leaq	(%r10,%r12), %rcx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%esi, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%esi, %edx
	cmovgl	%esi, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	2(%r8), %r10d
	cmpl	%r10d, %r13d
	movl	%r13d, %edx
	cmovgl	%r10d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r15, %rcx
	leaq	(%rcx,%r12), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	-2(%r8), %esi
	cmpl	%esi, %r13d
	movl	%r13d, %eax
	cmovgl	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %eax
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %eax
	cltq
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movslq	%r14d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%r11d, %r9d
	movslq	%r9d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movq	2400(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1728(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	movl	2192(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	movq	2112(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1768(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2288(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	movq	2104(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2256(%rsp)        # 8-byte Spill
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	movq	1616(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2240(%rsp)        # 8-byte Spill
	andl	$63, %r10d
	imull	%eax, %r10d
	movq	%r10, 2352(%rsp)        # 8-byte Spill
	movq	1512(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2224(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2208(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_369:                            # %for f7.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_361 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3008(%rsp)        # 4-byte Spill
	cmpl	$0, 3104(%rsp)          # 4-byte Folded Reload
	sete	3776(%rsp)              # 1-byte Folded Spill
	setne	3680(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 5152(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3744(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r14d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2368(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5152(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5056(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4992(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5296(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5312(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3744(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r13d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3712(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4896(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4736(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r12d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r14d
	movl	%r14d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2736(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2784(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r12d
	vmovq	%xmm1, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5040(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movb	3680(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5216(%rsp)       # 1-byte Folded Spill
	movl	5152(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2384(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	movq	2208(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r14
	sete	3200(%rsp)              # 1-byte Folded Spill
	movl	3104(%rsp), %r13d       # 4-byte Reload
	testl	%ecx, %r13d
	setne	3168(%rsp)              # 1-byte Folded Spill
	movb	3776(%rsp), %r10b       # 1-byte Reload
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	movl	%r8d, %r12d
	andl	$1, %r12d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r13d
	movzbl	3200(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r12b
	movq	2224(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	movslq	%ecx, %r10
	movq	2240(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %edi
	movslq	%edi, %rcx
	movq	%r10, %rsi
	orq	$6, %rsi
	movq	%rsi, 3280(%rsp)        # 8-byte Spill
	movq	%rcx, %r13
	movq	%rcx, %rsi
	orq	$6, %r13
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	movl	%ecx, 2768(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %edi
	movq	2336(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	movl	%ecx, 2672(%rsp)        # 4-byte Spill
	movq	2272(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	movl	%ecx, 3184(%rsp)        # 4-byte Spill
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	movl	%ecx, 3200(%rsp)        # 4-byte Spill
	je	.LBB147_371
# BB#370:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_371:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm0, 2416(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_373
# BB#372:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_373:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm1, 2432(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	movzbl	3168(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_375
# BB#374:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_375:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3120(%rsp)       # 16-byte Spill
	je	.LBB147_377
# BB#376:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_377:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm1, 2448(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_379
# BB#378:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_379:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	je	.LBB147_381
# BB#380:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_381:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, %r9
	je	.LBB147_383
# BB#382:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_383:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2816(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	je	.LBB147_385
# BB#384:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_385:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2624(%rsp)       # 16-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	32(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovups	48(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	8(%rcx,%r8,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovups	24(%rcx,%r8,4), %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4160(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rdx,%r10,4), %xmm14
	vmovups	48(%rdx,%r10,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rdx,%r9,4), %xmm6
	vmovups	48(%rdx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovups	40(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovups	56(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	movq	%rdx, %rbx
	movq	%rcx, %rdi
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2736(%rsp)      # 16-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movslq	2672(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	48(%rbx,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm14
	vmovaps	%xmm14, 3712(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3776(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovups	48(%rbx,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	movslq	2768(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5184(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3184(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	movslq	3200(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2608(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4192(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3264(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_387
# BB#386:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	2416(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
.LBB147_387:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vsubps	3408(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2656(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	3424(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm9 # xmm9 = xmm13[1,3],xmm10[1,3]
	vmulps	%xmm0, %xmm15, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vmulps	5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3216(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2592(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3424(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3488(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
	vmovaps	%xmm13, 2768(%rsp)      # 16-byte Spill
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3328(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3744(%rsp), %xmm13, %xmm13 # 16-byte Folded Reload
	je	.LBB147_389
# BB#388:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm9, 3584(%rsp)       # 16-byte Spill
	vmovaps	2432(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3120(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm9       # 16-byte Reload
.LBB147_389:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	%xmm9, 3584(%rsp)       # 16-byte Spill
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2576(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2560(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm10, %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vmulps	4160(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rbx,%r13,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4192(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vaddps	3376(%rsp), %xmm13, %xmm8 # 16-byte Folded Reload
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	%xmm9, %xmm5, %xmm9
	vaddps	3312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3296(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_391
# BB#390:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovdqa	2448(%rsp), %xmm6       # 16-byte Reload
.LBB147_391:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2576(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vsubps	2560(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	3232(%rsp), %xmm3       # 16-byte Reload
	vmulps	3808(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3360(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3216(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2816(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_393
# BB#392:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovdqa	2464(%rsp), %xmm4       # 16-byte Reload
.LBB147_393:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3120(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3264(%rsp), %xmm4       # 16-byte Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2848(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_395
# BB#394:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	2480(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
.LBB147_395:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4192(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm4       # 16-byte Reload
	vmulps	5184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3216(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3136(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_397
# BB#396:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	2496(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3152(%rsp)       # 16-byte Spill
.LBB147_397:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3776(%rsp), %xmm4       # 16-byte Reload
	vmulps	4160(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	4128(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2656(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vaddps	3376(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3744(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5424(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_399
# BB#398:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovdqa	2512(%rsp), %xmm15      # 16-byte Reload
.LBB147_399:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vaddps	3488(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2752(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vmulps	3808(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5248(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vaddps	3264(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3168(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	movq	4656(%rsp), %rcx        # 8-byte Reload
	vmovdqa	2832(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_401
# BB#400:                               # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovdqa	2528(%rsp), %xmm0       # 16-byte Reload
.LBB147_401:                            # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB147_369 Depth=3
	vmovaps	3392(%rsp), %xmm7       # 16-byte Reload
	vaddps	3424(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5152(%rsp), %rax        # 4-byte Folded Reload
	movq	2320(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	3008(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_369
	jmp	.LBB147_402
.LBB147_362:                            # %for f7.s0.v11.end for f7.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_361 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2192(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_402:                            #   in Loop: Header=BB147_361 Depth=2
	movq	4664(%rsp), %rcx        # 8-byte Reload
	movq	4704(%rsp), %rdx        # 8-byte Reload
	movq	4696(%rsp), %rsi        # 8-byte Reload
	movl	2192(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	2176(%rsp), %eax        # 4-byte Folded Reload
	movl	1736(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_361
# BB#403:                               # %for f8.s0.v11.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	%rsi, 4696(%rsp)        # 8-byte Spill
	movq	%rdx, 4704(%rsp)        # 8-byte Spill
	movq	%rcx, 4664(%rsp)        # 8-byte Spill
	movq	1624(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	.align	16, 0x90
.LBB147_404:                            # %for f8.s0.v11
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_407 Depth 3
	testl	%eax, %eax
	jle	.LBB147_405
# BB#406:                               # %for f8.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_404 Depth=2
	movq	%r8, 2464(%rsp)         # 8-byte Spill
	movl	%r8d, %eax
	movq	1752(%rsp), %r11        # 8-byte Reload
	subl	%r11d, %eax
	addl	$-1, %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %esi
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	movl	%ecx, %r10d
	subl	%eax, %edx
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movq	1744(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %r9d
	cmovgl	%r8d, %r9d
	addl	$-1, %r9d
	cmpl	%r11d, %r9d
	cmovll	%r11d, %r9d
	cmpl	%r8d, %ebx
	cmovll	%edx, %r9d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovlel	%edx, %r9d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r10d, %esi
	subl	%edi, %esi
	cmpl	%edi, %r14d
	cmovgl	%edi, %esi
	addl	%r11d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	leal	1(%r8), %ecx
	movl	%ecx, 2256(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r13d
	movl	%r13d, %edi
	cmovgl	%ecx, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%esi, %edi
	movl	%r13d, %r10d
	cmovgl	%r8d, %r10d
	cmpl	%r11d, %r10d
	cmovll	%r11d, %r10d
	cmpl	%r8d, %ebx
	cmovlel	%edx, %r10d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovll	%edx, %r10d
	movl	%r8d, %r12d
	andl	$1, %r12d
	movl	%r12d, 2272(%rsp)       # 4-byte Spill
	leal	-2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2448(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	movl	%r14d, %ebx
	addl	$2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	sarl	$31, %ebx
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %ebx
	addl	%r14d, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %edi
	movslq	%edi, %rcx
	movq	1816(%rsp), %rdi        # 8-byte Reload
	imulq	%rdi, %rcx
	movq	1808(%rsp), %rsi        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	1824(%rsp), %r14        # 8-byte Reload
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%ebx, %ecx
	movq	1784(%rsp), %r15        # 8-byte Reload
	cmpl	%ebx, %r15d
	cmovgl	%ebx, %ecx
	addl	%r11d, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	leal	-2(%r8), %r9d
	cmpl	%r9d, %r13d
	movl	%r13d, %edx
	cmovgl	%r9d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	movslq	%r10d, %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%r14,%rdx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	leal	2(%r8), %ebx
	cmpl	%ebx, %r13d
	movl	%r13d, %eax
	cmovgl	%ebx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %eax
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %eax
	cltq
	imulq	%rdi, %rax
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movq	2480(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1728(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1700(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2416(%rsp)        # 8-byte Spill
	movq	1512(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1768(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	subq	4712(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r9d
	imull	%esi, %r9d
	movq	%r9, 2432(%rsp)         # 8-byte Spill
	movq	1616(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2368(%rsp)        # 8-byte Spill
	movq	2104(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2352(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	movl	2256(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2304(%rsp)        # 8-byte Spill
	movq	2112(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2288(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_407:                            # %for f8.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_404 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	sete	5152(%rsp)              # 1-byte Folded Spill
	setne	3776(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3744(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3712(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3680(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3648(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3584(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2448(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3808(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5328(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5296(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5040(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5008(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5344(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5360(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5376(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3744(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3680(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5088(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3120(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3104(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3648(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5056(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4992(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4896(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4736(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movb	3776(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5184(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2464(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	2272(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3280(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %r14b       # 1-byte Reload
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	%r14b, %al
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3488(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3536(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movl	%r11d, %r12d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r14b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3456(%rsp)        # 4-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %edi
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %esi
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2784(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r14d
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3264(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r11d
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3296(%rsp)        # 4-byte Spill
	je	.LBB147_409
# BB#408:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_409:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5216(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	je	.LBB147_411
# BB#410:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_411:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3280(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_413
# BB#412:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_413:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_415
# BB#414:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_415:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm1, 2528(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_417
# BB#416:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_417:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_419
# BB#418:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_419:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm2, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3232(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_421
# BB#420:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_421:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2592(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_423
# BB#422:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_423:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3008(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3104(%rsp)      # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5032(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4160(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2784(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2784(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2736(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2720(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2688(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2752(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3120(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3008(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movslq	2832(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movslq	3776(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3776(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	movslq	3264(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	4128(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movslq	3296(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3584(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	movslq	3456(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_425
# BB#424:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	2496(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
.LBB147_425:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vsubps	3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vsubps	2688(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vsubps	2704(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3552(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3424(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_427
# BB#426:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	2512(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3184(%rsp)       # 16-byte Spill
.LBB147_427:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5664(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmulps	4192(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4160(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3264(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmulps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2672(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2656(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2640(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2624(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3328(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vaddps	3536(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3552(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5152(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_429
# BB#428:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovdqa	2528(%rsp), %xmm7       # 16-byte Reload
.LBB147_429:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3104(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4128(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3408(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3136(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_431
# BB#430:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovdqa	2544(%rsp), %xmm0       # 16-byte Reload
.LBB147_431:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2800(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2672(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3280(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3392(%rsp)      # 16-byte Spill
	vaddps	3344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3264(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5152(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_433
# BB#432:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovdqa	2576(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
.LBB147_433:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3312(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_435
# BB#434:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	2560(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
.LBB147_435:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3376(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2816(%rsp), %xmm15      # 16-byte Reload
	vmulps	4192(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3008(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4160(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vaddps	3456(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3536(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_437
# BB#436:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
.LBB147_437:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vaddps	3280(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3120(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2768(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2704(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3344(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3152(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_439
# BB#438:                               # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovdqa	2608(%rsp), %xmm5       # 16-byte Reload
.LBB147_439:                            # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_407 Depth=3
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3104(%rsp), %xmm2       # 16-byte Reload
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	3168(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_407
	jmp	.LBB147_440
.LBB147_405:                            # %for f8.s0.v11.end for f8.s0.v10.v10_crit_edge
                                        #   in Loop: Header=BB147_404 Depth=2
	addl	$1, %r8d
	movl	%r8d, %eax
	movl	%eax, 2256(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_440:                            # %end for f8.s0.v10.v10
                                        #   in Loop: Header=BB147_404 Depth=2
	movl	2256(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	2176(%rsp), %eax        # 4-byte Folded Reload
	movl	1736(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_404
# BB#441:                               # %consume f8
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	824(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_442
# BB#443:                               # %for f0.s0.v10.v10.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	840(%rsp), %rax         # 8-byte Reload
	movq	792(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	8(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3808(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3776(%rsp)       # 32-byte Spill
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3744(%rsp)       # 32-byte Spill
	movq	832(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	8(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3712(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3680(%rsp)       # 32-byte Spill
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3648(%rsp)       # 32-byte Spill
	movq	784(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3616(%rsp)       # 32-byte Spill
	movq	776(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3584(%rsp)       # 32-byte Spill
	movq	800(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %ymm0
	vmovaps	%ymm0, 3552(%rsp)       # 32-byte Spill
	movq	1472(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %esi
	andl	$63, %esi
	movq	%rsi, %rcx
	movq	1728(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	addq	$1, %rdx
	movl	%edx, %r13d
	andl	$63, %r13d
	movq	%r13, %r12
	imulq	%rax, %r12
	movq	1416(%rsp), %rcx        # 8-byte Reload
	subq	%rcx, %rdx
	shlq	$5, %rdx
	movq	%rdx, 1472(%rsp)        # 8-byte Spill
	leaq	8(%rdx), %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	leaq	16(%rdx), %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rsi
	movq	%rsi, 3536(%rsp)        # 8-byte Spill
	imulq	%rax, %r13
	leaq	24(%rdx), %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	subq	520(%rsp), %rcx         # 8-byte Folded Reload
	movq	%rcx, 1416(%rsp)        # 8-byte Spill
	movq	656(%rsp), %rax         # 8-byte Reload
	movq	1456(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r11d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB147_444:                            # %for f0.s0.v10.v10
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_445 Depth 3
                                        #         Child Loop BB147_446 Depth 4
	movl	%r14d, %eax
	shll	$5, %eax
	movq	5288(%rsp), %rcx        # 8-byte Reload
	addl	%ecx, %eax
	cltq
	movq	1160(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rcx
	movq	3536(%rsp), %rdi        # 8-byte Reload
	leaq	(%rcx,%rdi), %rdx
	movq	5032(%rsp), %rsi        # 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm0
	leaq	(%rcx,%r13), %rdx
	vmovups	(%rsi,%rdx,4), %ymm1
	leaq	8(%rdi,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm2
	leaq	8(%r13,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm3
	leaq	16(%rdi,%rcx), %rdx
	vmovdqu	(%rsi,%rdx,4), %ymm4
	leaq	16(%r13,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm5
	leaq	24(%rdi,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm6
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	leaq	24(%r13,%rcx), %rcx
	vmovdqu	(%rsi,%rcx,4), %ymm7
	movq	3488(%rsp), %rbx        # 8-byte Reload
	leaq	(%rax,%rbx), %rcx
	movq	4656(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm8
	vmovaps	%ymm8, 5728(%rsp)
	movq	4648(%rsp), %rsi        # 8-byte Reload
	vmovdqu	(%rsi,%rcx,4), %ymm8
	leaq	(%rax,%r12), %rcx
	vmovups	(%rdx,%rcx,4), %ymm9
	movq	1472(%rsp), %rdi        # 8-byte Reload
	vmovaps	%ymm9, 5728(%rsp,%rdi,4)
	vmovups	(%rsi,%rcx,4), %ymm9
	leaq	8(%rbx,%rax), %rcx
	vmovups	(%rdx,%rcx,4), %ymm10
	vmovaps	%ymm10, 5760(%rsp)
	vmovups	(%rsi,%rcx,4), %ymm10
	leaq	8(%rax,%r12), %r8
	vmovups	(%rdx,%r8,4), %ymm11
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm11, 5728(%rsp,%rcx,4)
	vmovdqu	(%rsi,%r8,4), %ymm11
	leaq	16(%rax,%rbx), %rcx
	vmovups	(%rdx,%rcx,4), %ymm12
	vmovaps	%ymm12, 5792(%rsp)
	vmovups	(%rsi,%rcx,4), %ymm12
	leaq	16(%rax,%r12), %r8
	vmovups	(%rdx,%r8,4), %ymm13
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm13, 5728(%rsp,%rcx,4)
	vmovdqu	(%rsi,%r8,4), %ymm13
	leaq	24(%rax,%rbx), %rcx
	vmovups	(%rdx,%rcx,4), %ymm14
	vmovaps	%ymm14, 5824(%rsp)
	leaq	24(%rax,%r12), %rax
	vmovups	(%rdx,%rax,4), %ymm14
	movq	3408(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm14, 5728(%rsp,%rdx,4)
	vmovaps	%ymm0, 5984(%rsp)
	vmovaps	%ymm1, 5984(%rsp,%rdi,4)
	vmovdqa	%ymm2, 6016(%rsp)
	vmovdqa	%ymm3, 6016(%rsp,%rdi,4)
	vmovdqa	%ymm4, 6048(%rsp)
	vmovaps	%ymm5, 6048(%rsp,%rdi,4)
	vmovaps	%ymm6, 6080(%rsp)
	vmovdqa	%ymm7, 6080(%rsp,%rdi,4)
	vmovdqa	%ymm8, 6240(%rsp)
	vmovaps	%ymm9, 6240(%rsp,%rdi,4)
	vmovaps	%ymm10, 6272(%rsp)
	vmovdqa	%ymm11, 6272(%rsp,%rdi,4)
	vmovaps	%ymm12, 6304(%rsp)
	vmovdqa	%ymm13, 6304(%rsp,%rdi,4)
	vmovups	(%rsi,%rcx,4), %ymm0
	movslq	%r11d, %rcx
	movq	3392(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %r8
	vmovaps	%ymm0, 6336(%rsp)
	vmovups	(%rsi,%rax,4), %ymm0
	vmovaps	%ymm0, 6336(%rsp,%rdi,4)
	xorl	%eax, %eax
	.align	16, 0x90
.LBB147_445:                            # %for f0.s0.v11.v13.yii
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_444 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_446 Depth 4
	movq	%rax, %rcx
	shlq	$7, %rcx
	vmovaps	5728(%rsp,%rcx), %ymm6
	vmovaps	%ymm6, 4128(%rsp)       # 32-byte Spill
	vmovaps	5760(%rsp,%rcx), %ymm1
	vmovaps	%ymm1, 4160(%rsp)       # 32-byte Spill
	vmovaps	5792(%rsp,%rcx), %ymm0
	vmovaps	%ymm0, 4192(%rsp)       # 32-byte Spill
	vmovaps	5824(%rsp,%rcx), %ymm13
	vmovaps	%ymm13, 3840(%rsp)      # 32-byte Spill
	vmovaps	5984(%rsp,%rcx), %ymm4
	vmovaps	6016(%rsp,%rcx), %ymm7
	vmovaps	6048(%rsp,%rcx), %ymm8
	vmovaps	6080(%rsp,%rcx), %ymm3
	vmovaps	6240(%rsp,%rcx), %ymm9
	vmovaps	6272(%rsp,%rcx), %ymm10
	vmovaps	6304(%rsp,%rcx), %ymm12
	vmovaps	3744(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm0, %ymm0
	vmulps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm5, %ymm6, %ymm2
	vmovaps	%ymm6, %ymm14
	vmulps	%ymm5, %ymm13, %ymm5
	vmovaps	3776(%rsp), %ymm11      # 32-byte Reload
	vmovaps	%ymm11, %ymm6
	vfmadd213ps	%ymm5, %ymm3, %ymm6
	vmovaps	%ymm11, %ymm5
	vfmadd213ps	%ymm2, %ymm4, %ymm5
	vmovaps	%ymm11, %ymm2
	vfmadd213ps	%ymm1, %ymm7, %ymm2
	vmovaps	%ymm11, %ymm1
	vfmadd213ps	%ymm0, %ymm8, %ymm1
	vmovaps	3808(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm11
	vfmadd213ps	%ymm1, %ymm12, %ymm11
	vmovaps	%ymm11, 5248(%rsp)      # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm2, %ymm10, %ymm1
	vmovaps	%ymm1, 5216(%rsp)       # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm5, %ymm9, %ymm1
	vmovaps	%ymm1, 5184(%rsp)       # 32-byte Spill
	vmovaps	6336(%rsp,%rcx), %ymm15
	vfmadd213ps	%ymm6, %ymm15, %ymm0
	vmovaps	%ymm0, 5152(%rsp)       # 32-byte Spill
	vmovaps	3648(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm13, %ymm0
	vmovaps	3680(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm11
	vfmadd213ps	%ymm0, %ymm3, %ymm11
	vmulps	%ymm5, %ymm14, %ymm0
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm0, %ymm4, %ymm6
	vmovaps	4160(%rsp), %ymm14      # 32-byte Reload
	vmulps	%ymm5, %ymm14, %ymm0
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm0, %ymm7, %ymm1
	vmulps	4192(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm5
	vfmadd213ps	%ymm0, %ymm8, %ymm5
	vmovaps	3712(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm0
	vfmadd213ps	%ymm5, %ymm12, %ymm0
	vmovaps	%ymm2, %ymm5
	vfmadd213ps	%ymm1, %ymm10, %ymm5
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm6, %ymm9, %ymm1
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm11, %ymm15, %ymm6
	vmovaps	3552(%rsp), %ymm13      # 32-byte Reload
	vmulps	3840(%rsp), %ymm13, %ymm11 # 32-byte Folded Reload
	vmovaps	3584(%rsp), %ymm2       # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm2, %ymm3
	vmulps	4128(%rsp), %ymm13, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm11, %ymm2, %ymm4
	vmulps	%ymm13, %ymm14, %ymm11
	vfmadd213ps	%ymm11, %ymm2, %ymm7
	vmulps	4192(%rsp), %ymm13, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm11, %ymm2, %ymm8
	vmovaps	3616(%rsp), %ymm2       # 32-byte Reload
	vfmadd213ps	%ymm8, %ymm2, %ymm12
	vfmadd213ps	%ymm7, %ymm2, %ymm10
	vfmadd213ps	%ymm4, %ymm2, %ymm9
	vfmadd213ps	%ymm3, %ymm2, %ymm15
	xorl	%esi, %esi
	movl	$3, %edx
	movq	%r8, %rcx
	.align	16, 0x90
.LBB147_446:                            # %for f0.s0.v12
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_444 Depth=2
                                        #       Parent Loop BB147_445 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovaps	%ymm1, %ymm3
	cmpl	$1, %esi
	je	.LBB147_448
# BB#447:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	5184(%rsp), %ymm3       # 32-byte Reload
.LBB147_448:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm5, %ymm4
	je	.LBB147_450
# BB#449:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	5216(%rsp), %ymm4       # 32-byte Reload
.LBB147_450:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm0, %ymm8
	je	.LBB147_452
# BB#451:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	5248(%rsp), %ymm8       # 32-byte Reload
.LBB147_452:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm6, %ymm11
	je	.LBB147_454
# BB#453:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	5152(%rsp), %ymm11      # 32-byte Reload
.LBB147_454:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm15, %ymm7
	testl	%esi, %esi
	je	.LBB147_456
# BB#455:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm11, %ymm7
.LBB147_456:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm12, %ymm11
	je	.LBB147_458
# BB#457:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm8, %ymm11
.LBB147_458:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm10, %ymm8
	je	.LBB147_460
# BB#459:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm4, %ymm8
.LBB147_460:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm9, %ymm4
	je	.LBB147_462
# BB#461:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vmovaps	%ymm3, %ymm4
.LBB147_462:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB147_446 Depth=4
	vbroadcastss	.LCPI147_25(%rip), %ymm3
	vminps	%ymm3, %ymm4, %ymm4
	vminps	%ymm3, %ymm8, %ymm8
	vminps	%ymm3, %ymm11, %ymm11
	vminps	%ymm3, %ymm7, %ymm3
	vxorps	%ymm2, %ymm2, %ymm2
	vmaxps	%ymm2, %ymm4, %ymm4
	vmaxps	%ymm2, %ymm8, %ymm7
	vmaxps	%ymm2, %ymm11, %ymm8
	vmaxps	%ymm2, %ymm3, %ymm3
	vcvttps2dq	%ymm4, %ymm4
	vmovdqa	.LCPI147_7(%rip), %ymm2 # ymm2 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm2, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vcvttps2dq	%ymm7, %ymm7
	vpshufb	%ymm2, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vcvttps2dq	%ymm8, %ymm8
	vpshufb	%ymm2, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vcvttps2dq	%ymm3, %ymm3
	vpshufb	%ymm2, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm8, %ymm8    # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero,xmm8[4],zero,xmm8[5],zero,xmm8[6],zero,xmm8[7],zero
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vmovdqa	5536(%rsp), %ymm2       # 32-byte Reload
	vpmulld	%ymm2, %ymm4, %ymm11
	vpmulld	%ymm2, %ymm7, %ymm7
	vpmulld	%ymm2, %ymm8, %ymm4
	vpmulld	%ymm2, %ymm3, %ymm3
	vmovd	%esi, %xmm8
	vpsubd	5472(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpbroadcastd	%xmm8, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm8
	vpaddd	%ymm4, %ymm13, %ymm4
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	%ymm11, %ymm13, %ymm3
	vmovq	%xmm4, %r9
	movslq	%r9d, %r10
	movq	5568(%rsp), %r15        # 8-byte Reload
	movzbl	(%r15,%r10), %edi
	vmovd	%edi, %xmm2
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %r9
	vpinsrb	$1, (%r15,%r9), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm4, %xmm4
	vpinsrb	$2, (%r15,%rbx), %xmm2, %xmm2
	vmovq	%xmm4, %rbx
	vpinsrb	$3, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrb	$4, (%r15,%rdi), %xmm2, %xmm2
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rbx
	vpinsrb	$5, (%r15,%rbx), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vpinsrb	$6, (%r15,%rbx), %xmm2, %xmm2
	vmovq	%xmm8, %rbx
	vpinsrb	$7, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrb	$8, (%r15,%rdi), %xmm2, %xmm2
	vpextrq	$1, %xmm8, %rdi
	sarq	$32, %rbx
	vpinsrb	$9, (%r15,%rbx), %xmm2, %xmm2
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm8, %xmm4
	vpinsrb	$10, (%r15,%rbx), %xmm2, %xmm2
	vmovq	%xmm4, %rbx
	vpinsrb	$11, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	vpinsrb	$12, (%r15,%rdi), %xmm2, %xmm2
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rbx
	vpinsrb	$13, (%r15,%rbx), %xmm2, %xmm2
	movslq	%edi, %rbx
	vpinsrb	$14, (%r15,%rbx), %xmm2, %xmm2
	vmovq	%xmm3, %rbx
	sarq	$32, %rdi
	vpinsrb	$15, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ebx, %rdi
	movzbl	(%r15,%rdi), %edi
	vmovd	%edi, %xmm4
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rbx
	vpinsrb	$1, (%r15,%rbx), %xmm4, %xmm4
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm3, %xmm3
	vpinsrb	$2, (%r15,%rbx), %xmm4, %xmm4
	vmovq	%xmm3, %rbx
	vpinsrb	$3, (%r15,%rdi), %xmm4, %xmm4
	movslq	%ebx, %rdi
	vpinsrb	$4, (%r15,%rdi), %xmm4, %xmm4
	vpextrq	$1, %xmm3, %rdi
	sarq	$32, %rbx
	vpinsrb	$5, (%r15,%rbx), %xmm4, %xmm3
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vpinsrb	$6, (%r15,%rbx), %xmm3, %xmm3
	vmovq	%xmm7, %rbx
	vpinsrb	$7, (%r15,%rdi), %xmm3, %xmm3
	movslq	%ebx, %rdi
	vpinsrb	$8, (%r15,%rdi), %xmm3, %xmm3
	vpextrq	$1, %xmm7, %rdi
	sarq	$32, %rbx
	vpinsrb	$9, (%r15,%rbx), %xmm3, %xmm3
	movslq	%edi, %rbx
	sarq	$32, %rdi
	vextracti128	$1, %ymm7, %xmm4
	vpinsrb	$10, (%r15,%rbx), %xmm3, %xmm3
	vmovq	%xmm4, %rbx
	vpinsrb	$11, (%r15,%rdi), %xmm3, %xmm3
	movslq	%ebx, %rdi
	vpinsrb	$12, (%r15,%rdi), %xmm3, %xmm3
	vpextrq	$1, %xmm4, %rdi
	sarq	$32, %rbx
	vpinsrb	$13, (%r15,%rbx), %xmm3, %xmm3
	movslq	%edi, %rbx
	vpinsrb	$14, (%r15,%rbx), %xmm3, %xmm3
	sarq	$32, %rdi
	vpinsrb	$15, (%r15,%rdi), %xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovdqu	%ymm2, (%rcx)
	addq	5528(%rsp), %rcx        # 8-byte Folded Reload
	addl	$1, %esi
	addq	$-1, %rdx
	jne	.LBB147_446
# BB#463:                               # %end for f0.s0.v12
                                        #   in Loop: Header=BB147_445 Depth=3
	addq	$1, %rax
	addq	2144(%rsp), %r8         # 8-byte Folded Reload
	cmpq	$2, %rax
	jne	.LBB147_445
# BB#464:                               # %end for f0.s0.v11.v13.yii
                                        #   in Loop: Header=BB147_444 Depth=2
	addq	$1, %r14
	addl	$32, %r11d
	cmpl	1156(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB147_444
	jmp	.LBB147_465
.LBB147_442:                            # %consume f8.for f0.s0.v11.v13.v13.preheader_crit_edge
                                        #   in Loop: Header=BB147_195 Depth=1
	movq	1416(%rsp), %rax        # 8-byte Reload
	subq	520(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1416(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_465:                            # %for f0.s0.v11.v13.v13.preheader
                                        #   in Loop: Header=BB147_195 Depth=1
	addl	$8, 860(%rsp)           # 4-byte Folded Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	leal	9(%rax), %ecx
	movq	%rcx, 1616(%rsp)        # 8-byte Spill
	leal	7(%rax), %ecx
	movq	%rcx, 1512(%rsp)        # 8-byte Spill
	movl	$6, %ecx
	movq	1464(%rsp), %rdx        # 8-byte Reload
	subq	%rdx, %rcx
	movq	%rcx, 1504(%rsp)        # 8-byte Spill
	movl	$10, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1496(%rsp)        # 8-byte Spill
	movl	$7, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1488(%rsp)        # 8-byte Spill
	movl	$9, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1480(%rsp)        # 8-byte Spill
	movl	$8, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1472(%rsp)        # 8-byte Spill
	movl	$9, %ecx
	movq	1432(%rsp), %rdx        # 8-byte Reload
	subq	%rdx, %rcx
	movq	%rcx, 1464(%rsp)        # 8-byte Spill
	movl	$7, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1456(%rsp)        # 8-byte Spill
	movl	$6, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1448(%rsp)        # 8-byte Spill
	movl	$10, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1440(%rsp)        # 8-byte Spill
	movl	$8, %ecx
	subq	%rdx, %rcx
	movq	%rcx, 1432(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 1424(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 1608(%rsp)        # 8-byte Spill
	addl	$10, %eax
	movq	%rax, 1520(%rsp)        # 8-byte Spill
	movq	864(%rsp), %rax         # 8-byte Reload
	addl	$9, %eax
	movq	%rax, 864(%rsp)         # 8-byte Spill
	movq	1416(%rsp), %rdx        # 8-byte Reload
	addq	$2, %rdx
	imulq	2144(%rsp), %rdx        # 8-byte Folded Reload
	movq	1528(%rsp), %rax        # 8-byte Reload
	leal	10(%rax), %ecx
	movq	%rcx, 1416(%rsp)        # 8-byte Spill
	leal	8(%rax), %ecx
	movq	%rcx, 1408(%rsp)        # 8-byte Spill
	leal	7(%rax), %ecx
	movq	%rcx, 1400(%rsp)        # 8-byte Spill
	leal	11(%rax), %ecx
	movq	%rcx, 1392(%rsp)        # 8-byte Spill
	addl	$9, %eax
	movq	%rax, 1528(%rsp)        # 8-byte Spill
	movq	656(%rsp), %rax         # 8-byte Reload
	leaq	(%rdx,%rax), %rax
	movq	%rax, 1192(%rsp)        # 8-byte Spill
	movl	$1, %r12d
	movq	624(%rsp), %rax         # 8-byte Reload
	movl	%eax, 948(%rsp)         # 4-byte Spill
	movl	472(%rsp), %eax         # 4-byte Reload
	movl	%eax, 944(%rsp)         # 4-byte Spill
	movl	476(%rsp), %eax         # 4-byte Reload
	movl	%eax, 940(%rsp)         # 4-byte Spill
	movl	620(%rsp), %eax         # 4-byte Reload
	movl	%eax, 936(%rsp)         # 4-byte Spill
	movl	480(%rsp), %eax         # 4-byte Reload
	movl	%eax, 932(%rsp)         # 4-byte Spill
	movl	484(%rsp), %eax         # 4-byte Reload
	movl	%eax, 928(%rsp)         # 4-byte Spill
	movl	$1, %eax
	movq	%rax, 920(%rsp)         # 8-byte Spill
	movq	5608(%rsp), %r14        # 8-byte Reload
	movq	%r14, %rsi
	movl	968(%rsp), %eax         # 4-byte Reload
	movl	964(%rsp), %ecx         # 4-byte Reload
	movl	996(%rsp), %r9d         # 4-byte Reload
	movl	992(%rsp), %r8d         # 4-byte Reload
	movl	988(%rsp), %r13d        # 4-byte Reload
	movl	984(%rsp), %edi         # 4-byte Reload
	movl	1228(%rsp), %r10d       # 4-byte Reload
	movl	980(%rsp), %r11d        # 4-byte Reload
	movl	1212(%rsp), %r14d       # 4-byte Reload
	movl	1208(%rsp), %r15d       # 4-byte Reload
	.align	16, 0x90
.LBB147_466:                            # %for f0.s0.v11.v13.v13
                                        #   Parent Loop BB147_195 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_468 Depth 3
                                        #         Child Loop BB147_470 Depth 4
                                        #       Child Loop BB147_498 Depth 3
                                        #         Child Loop BB147_500 Depth 4
                                        #         Child Loop BB147_527 Depth 4
                                        #         Child Loop BB147_554 Depth 4
                                        #       Child Loop BB147_582 Depth 3
                                        #         Child Loop BB147_584 Depth 4
                                        #       Child Loop BB147_611 Depth 3
                                        #         Child Loop BB147_613 Depth 4
                                        #       Child Loop BB147_649 Depth 3
                                        #         Child Loop BB147_651 Depth 4
                                        #         Child Loop BB147_686 Depth 4
                                        #         Child Loop BB147_705 Depth 4
                                        #       Child Loop BB147_741 Depth 3
                                        #         Child Loop BB147_743 Depth 4
                                        #       Child Loop BB147_778 Depth 3
                                        #         Child Loop BB147_823 Depth 4
                                        #       Child Loop BB147_782 Depth 3
                                        #         Child Loop BB147_784 Depth 4
                                        #         Child Loop BB147_803 Depth 4
                                        #         Child Loop BB147_843 Depth 4
                                        #       Child Loop BB147_863 Depth 3
                                        #         Child Loop BB147_866 Depth 4
                                        #       Child Loop BB147_886 Depth 3
                                        #         Child Loop BB147_931 Depth 4
                                        #       Child Loop BB147_890 Depth 3
                                        #         Child Loop BB147_892 Depth 4
                                        #         Child Loop BB147_911 Depth 4
                                        #         Child Loop BB147_951 Depth 4
                                        #       Child Loop BB147_971 Depth 3
                                        #         Child Loop BB147_974 Depth 4
                                        #       Child Loop BB147_994 Depth 3
                                        #         Child Loop BB147_996 Depth 4
                                        #       Child Loop BB147_1016 Depth 3
                                        #         Child Loop BB147_1018 Depth 4
                                        #         Child Loop BB147_1037 Depth 4
                                        #         Child Loop BB147_1056 Depth 4
                                        #       Child Loop BB147_1076 Depth 3
                                        #         Child Loop BB147_1078 Depth 4
                                        #       Child Loop BB147_1104 Depth 3
                                        #         Child Loop BB147_1100 Depth 4
                                        #           Child Loop BB147_1101 Depth 5
                                        #       Child Loop BB147_1098 Depth 3
                                        #         Child Loop BB147_1106 Depth 4
                                        #       Child Loop BB147_1142 Depth 3
                                        #         Child Loop BB147_1144 Depth 4
                                        #         Child Loop BB147_1179 Depth 4
                                        #         Child Loop BB147_1199 Depth 4
                                        #       Child Loop BB147_1235 Depth 3
                                        #         Child Loop BB147_1238 Depth 4
                                        #       Child Loop BB147_1274 Depth 3
                                        #         Child Loop BB147_1277 Depth 4
                                        #       Child Loop BB147_1313 Depth 3
                                        #         Child Loop BB147_1315 Depth 4
                                        #         Child Loop BB147_1350 Depth 4
                                        #         Child Loop BB147_1370 Depth 4
                                        #       Child Loop BB147_1406 Depth 3
                                        #         Child Loop BB147_1409 Depth 4
                                        #       Child Loop BB147_1446 Depth 3
                                        #         Child Loop BB147_1447 Depth 4
                                        #           Child Loop BB147_1448 Depth 5
	movq	%r12, 1216(%rsp)        # 8-byte Spill
	movl	%r15d, 1208(%rsp)       # 4-byte Spill
	movl	%r14d, 1212(%rsp)       # 4-byte Spill
	movl	%r11d, 980(%rsp)        # 4-byte Spill
	movl	%r10d, 1228(%rsp)       # 4-byte Spill
	movl	%edi, 984(%rsp)         # 4-byte Spill
	movl	%r13d, 988(%rsp)        # 4-byte Spill
	movl	%r8d, 992(%rsp)         # 4-byte Spill
	movl	%r9d, 996(%rsp)         # 4-byte Spill
	movl	%ecx, 964(%rsp)         # 4-byte Spill
	movl	%eax, 968(%rsp)         # 4-byte Spill
	movq	1752(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	movl	%eax, %ebx
	cmovgel	%edx, %ebx
	movl	668(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	movq	%rbx, 904(%rsp)         # 8-byte Spill
	cmpl	%r13d, %edx
	movl	%r13d, %ebx
	cmovgel	%edx, %ebx
	notl	%ebx
	cmpl	%ebx, %r11d
	cmovgel	%r11d, %ebx
	movl	%ebx, 900(%rsp)         # 4-byte Spill
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	%rcx, 880(%rsp)         # 8-byte Spill
	cmpl	%edi, %edx
	cmovgel	%edx, %edi
	notl	%edi
	cmpl	%edi, %r14d
	cmovgel	%r14d, %edi
	movl	%edi, 876(%rsp)         # 4-byte Spill
	movq	1352(%rsp), %rcx        # 8-byte Reload
	cmpl	%r10d, %ecx
	cmovgel	%ecx, %r10d
	movl	%r10d, %eax
	notl	%eax
	cmpl	%eax, %r15d
	cmovgel	%r15d, %eax
	movl	%eax, 2784(%rsp)        # 4-byte Spill
	movl	1644(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %r9d
	movl	%eax, %edi
	cmovgel	%r9d, %edi
	notl	%edi
	cmpl	%edi, %r8d
	cmovgel	%r8d, %edi
	movl	%edi, 972(%rsp)         # 4-byte Spill
	cmpl	%eax, %r13d
	cmovgel	%r13d, %eax
	notl	%eax
	cmpl	%eax, %r11d
	cmovgel	%r11d, %eax
	movl	%eax, 896(%rsp)         # 4-byte Spill
	cmpl	%r9d, %edx
	movl	%r9d, %eax
	cmovgel	%edx, %eax
	movl	$-3, %edx
	subl	%eax, %edx
	cmpl	%edx, %r14d
	cmovgel	%r14d, %edx
	movl	%edx, 892(%rsp)         # 4-byte Spill
	movl	708(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %r10d
	cmovll	%eax, %r10d
	notl	%r10d
	cmpl	%r10d, %r15d
	cmovgel	%r15d, %r10d
	movl	%r10d, 2736(%rsp)       # 4-byte Spill
	movq	1624(%rsp), %rdx        # 8-byte Reload
	leal	8(%rdx,%r12,2), %edi
	movl	%edi, 1368(%rsp)        # 4-byte Spill
	cmpl	%edi, %ecx
	movl	%edi, %ebx
	cmovgel	%ecx, %ebx
	leal	10(%rdx,%r12,2), %ecx
	movl	%ecx, 2768(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	movl	%ecx, %edx
	cmovgl	%ebx, %edx
	movl	%edx, 2816(%rsp)        # 4-byte Spill
	cmpl	%ebx, %eax
	cmovgel	%eax, %ebx
	movl	%ebx, 2800(%rsp)        # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovgl	%ebx, %ecx
	movl	%ecx, 2752(%rsp)        # 4-byte Spill
	cmpl	%edx, %edi
	jge	.LBB147_496
# BB#467:                               # %for deinterleaved$1.s0.v11291.preheader
                                        #   in Loop: Header=BB147_466 Depth=2
	movslq	1228(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	movl	1368(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_468:                            # %for deinterleaved$1.s0.v11291
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_470 Depth 4
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	cmpl	$0, 1676(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_495
# BB#469:                               # %for deinterleaved$1.s0.v10.v10293.preheader
                                        #   in Loop: Header=BB147_468 Depth=3
	movq	3104(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	movq	1352(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	1332(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1336(%rsp), %eax        # 4-byte Folded Reload
	movq	1344(%rsp), %rcx        # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movl	1328(%rsp), %eax        # 4-byte Reload
	subl	%ecx, %eax
	cmpq	%rdi, 1320(%rsp)        # 8-byte Folded Reload
	movl	1340(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%rdi, 1312(%rsp)        # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %rdi
	cmovll	%eax, %ecx
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	vmovd	%ecx, %xmm0
	vpabsd	1584(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3008(%rsp)       # 32-byte Spill
	vpsubd	1536(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3456(%rsp)       # 32-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movl	1676(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_470:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_468 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 3424(%rsp)        # 4-byte Spill
	cmpl	$0, 3488(%rsp)          # 4-byte Folded Reload
	setne	5184(%rsp)              # 1-byte Folded Spill
	sete	5152(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r14d
	andl	$1, %r14d
	sete	%cl
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	3104(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	movq	3920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4352(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %r9d
	movl	%r9d, 3584(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r10d
	movl	%r10d, 3552(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3296(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4160(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4128(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3840(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r15d, %xmm0
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	5184(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andb	5152(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 3408(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4512(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4480(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4032(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3008(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4320(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4448(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4304(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4288(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3456(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4160(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5152(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5184(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4128(%rsp)        # 8-byte Spill
	testl	3488(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3184(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3184(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3536(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3168(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3280(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3152(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3248(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3136(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3232(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vmovd	%xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3584(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	vmovd	3152(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3168(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3136(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r15d, %xmm1
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4416(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4384(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	3968(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	3936(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3744(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3456(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4712(%rsp), %r13        # 8-byte Folded Reload
	addq	2848(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4624(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	je	.LBB147_472
# BB#471:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_472:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovd	%ebx, %xmm0
	movl	5216(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3408(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_474
# BB#473:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_474:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movl	3184(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_476
# BB#475:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_476:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_478
# BB#477:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_478:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm1, 3120(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_480
# BB#479:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_480:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm0, 3136(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_482
# BB#481:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_482:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm1, 3152(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_484
# BB#483:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_484:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5216(%rsp)        # 8-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3248(%rsp)        # 8-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3376(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3392(%rsp)         # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	4128(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4160(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	5152(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5184(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_486
# BB#485:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_486:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3200(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3328(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3344(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3744(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5608(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_488
# BB#487:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_488:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movq	%rsi, %rcx
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3776(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3184(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3216(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	4128(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5216(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4160(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3248(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_490
# BB#489:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	3120(%rsp), %xmm6       # 16-byte Reload
.LBB147_490:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movq	%rsi, %rdx
	movq	3200(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_492
# BB#491:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	3136(%rsp), %xmm9       # 16-byte Reload
.LBB147_492:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_494
# BB#493:                               # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	vmovdqa	3152(%rsp), %xmm15      # 16-byte Reload
.LBB147_494:                            # %for deinterleaved$1.s0.v10.v10293
                                        #   in Loop: Header=BB147_470 Depth=4
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	3744(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4616(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r13,4)
	vmovups	%ymm2, 24608(%rsi,%r13,4)
	vmovups	%ymm1, 24640(%rsi,%r13,4)
	vmovups	%ymm4, 24672(%rsi,%r13,4)
	movq	5248(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3424(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_470
.LBB147_495:                            # %end for deinterleaved$1.s0.v10.v10294
                                        #   in Loop: Header=BB147_468 Depth=3
	movl	2832(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	addq	$1, 3104(%rsp)          # 8-byte Folded Spill
	cmpl	2816(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_468
.LBB147_496:                            # %end for deinterleaved$1.s0.v11292
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	2752(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 2816(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_580
# BB#497:                               #   in Loop: Header=BB147_466 Depth=2
	movl	2784(%rsp), %edx        # 4-byte Reload
	notl	%edx
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%edx, %eax
	movq	696(%rsp), %rcx         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movl	%eax, 2784(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_498:                            # %for deinterleaved$1.s0.v11297
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_500 Depth 4
                                        #         Child Loop BB147_527 Depth 4
                                        #         Child Loop BB147_554 Depth 4
	cmpl	$0, 1272(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_525
# BB#499:                               # %for deinterleaved$1.s0.v10.v10299.preheader
                                        #   in Loop: Header=BB147_498 Depth=3
	movq	5184(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 3456(%rsp)        # 4-byte Spill
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1584(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3008(%rsp)       # 32-byte Spill
	vpsubd	1536(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3424(%rsp)       # 32-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	1656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movl	1120(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_500:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_498 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 3408(%rsp)        # 4-byte Spill
	cmpl	$0, 3456(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r14d
	andl	$1, %r14d
	sete	%cl
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	5184(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3712(%rsp)        # 4-byte Spill
	movq	3920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4352(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3216(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3584(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ebx
	movl	%ebx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r9d
	movl	%r9d, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3280(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4128(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4160(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3840(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3808(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3776(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r15d, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	5152(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andb	4192(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 3392(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4512(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4480(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4032(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3008(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4320(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4448(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4304(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4288(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3424(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4128(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4160(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5152(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	testl	3456(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3168(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3488(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3152(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3264(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3136(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3232(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3120(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3216(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vmovd	%xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3584(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3536(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3248(%rsp)              # 4-byte Folded Reload
	vmovd	3136(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3152(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3120(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r15d, %xmm1
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4416(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4384(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	3968(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	3936(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3712(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3424(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4712(%rsp), %r13        # 8-byte Folded Reload
	addq	2848(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4624(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	je	.LBB147_502
# BB#501:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_502:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovd	%ebx, %xmm0
	movl	5216(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3392(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_504
# BB#503:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_504:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movl	3168(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_506
# BB#505:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_506:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_508
# BB#507:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_508:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm1, 3104(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_510
# BB#509:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_510:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm0, 3120(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_512
# BB#511:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_512:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm1, 3136(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_514
# BB#513:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_514:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5216(%rsp)        # 8-byte Spill
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3232(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3360(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3376(%rsp)         # 8-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4128(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4160(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5152(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_516
# BB#515:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_516:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3184(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3312(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3152(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3328(%rsp)        # 8-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3712(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5608(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_518
# BB#517:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_518:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movq	%rsi, %rcx
	movq	3392(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3744(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3168(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3776(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3200(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5216(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4128(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3232(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4160(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_520
# BB#519:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	3104(%rsp), %xmm6       # 16-byte Reload
.LBB147_520:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movq	%rsi, %rdx
	movq	3184(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_522
# BB#521:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	3120(%rsp), %xmm9       # 16-byte Reload
.LBB147_522:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_524
# BB#523:                               # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	vmovdqa	3136(%rsp), %xmm15      # 16-byte Reload
.LBB147_524:                            # %for deinterleaved$1.s0.v10.v10299
                                        #   in Loop: Header=BB147_500 Depth=4
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	3712(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4616(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r13,4)
	vmovups	%ymm2, 24608(%rsi,%r13,4)
	vmovups	%ymm1, 24640(%rsi,%r13,4)
	vmovups	%ymm4, 24672(%rsi,%r13,4)
	movq	5248(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3408(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_500
.LBB147_525:                            # %end for deinterleaved$1.s0.v10.v10300
                                        #   in Loop: Header=BB147_498 Depth=3
	movl	1268(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1272(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_552
# BB#526:                               # %for deinterleaved$1.s0.v10.v10303.preheader
                                        #   in Loop: Header=BB147_498 Depth=3
	movq	5184(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r9d
	andl	$1, %r9d
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	imulq	1656(%rsp), %r8         # 8-byte Folded Reload
	movl	1116(%rsp), %r10d       # 4-byte Reload
	movl	2784(%rsp), %r11d       # 4-byte Reload
	movl	1124(%rsp), %r15d       # 4-byte Reload
	.align	16, 0x90
.LBB147_527:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_498 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rsi, %r12
	testl	%r9d, %r9d
	setne	%cl
	sete	%dl
	movl	%r15d, %edi
	andl	$1, %edi
	sete	%al
	movl	%r15d, %ebx
	movq	5184(%rsp), %rsi        # 8-byte Reload
	orl	%esi, %ebx
	testb	$1, %bl
	sete	%sil
	andb	%cl, %al
	andb	%dl, %dil
	testl	%r15d, %r9d
	setne	%dl
	movslq	%r15d, %rbx
	subq	4712(%rsp), %rbx        # 8-byte Folded Reload
	addq	%r8, %rbx
	movq	4624(%rsp), %rcx        # 8-byte Reload
	leaq	(%rbx,%rcx), %r14
	movzbl	%sil, %ecx
	vmovd	%esi, %xmm0
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, 5248(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_529
# BB#528:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm5, %xmm5, %xmm5
.LBB147_529:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovd	%ecx, %xmm0
	movzbl	%al, %ecx
	vmovd	%ecx, %xmm2
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm4
	vmovd	%edi, %xmm1
	vpbroadcastb	%xmm1, %xmm3
	vmovdqa	%xmm3, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_531
# BB#530:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_531:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm6
	vpor	%xmm6, %xmm0, %xmm0
	vpor	%xmm4, %xmm2, %xmm2
	vpbroadcastb	%xmm2, %xmm4
	vmovdqa	%xmm4, %xmm1
	je	.LBB147_533
# BB#532:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_533:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm1, 4128(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm2
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm0
	je	.LBB147_535
# BB#534:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_535:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm0, 4160(%rsp)       # 16-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastb	%xmm2, %xmm2
	vmovdqa	%xmm2, %xmm1
	je	.LBB147_537
# BB#536:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_537:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm0, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_539
# BB#538:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_539:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	cmpl	$0, 104(%rbp)
	je	.LBB147_541
# BB#540:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm5, %xmm0
.LBB147_541:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm1, 5152(%rsp)       # 16-byte Spill
	movslq	%r11d, %rax
	movq	4984(%rsp), %rcx        # 8-byte Reload
	vmovdqu	-16(%rcx,%rax,2), %ymm5
	vpblendw	$170, 14(%rcx,%rax,2), %ymm5, %ymm5 # ymm5 = ymm5[0],mem[1],ymm5[2],mem[3],ymm5[4],mem[5],ymm5[6],mem[7],ymm5[8],mem[9],ymm5[10],mem[11],ymm5[12],mem[13],ymm5[14],mem[15]
	vpshufb	.LCPI147_26(%rip), %ymm5, %ymm7 # ymm7 = ymm5[0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15,16,17,20,21,24,25,28,29,18,19,22,23,26,27,30,31]
	vperm2i128	$35, %ymm5, %ymm0, %ymm5 # ymm5 = ymm5[2,3,0,1]
	vpshufb	.LCPI147_27(%rip), %ymm5, %ymm5 # ymm5 = ymm5[2,3,6,7,10,11,14,15,0,1,4,5,8,9,12,13,18,19,22,23,26,27,30,31,16,17,20,21,24,25,28,29]
	vpblendd	$60, %ymm5, %ymm7, %ymm7 # ymm7 = ymm7[0,1],ymm5[2,3,4,5],ymm7[6,7]
	vextracti128	$1, %ymm7, %xmm5
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm5, %ymm5
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vpmovzxbd	%xmm0, %ymm10   # ymm10 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm10, %ymm10
	vxorps	%ymm9, %ymm9, %ymm9
	vblendvps	%ymm10, %ymm7, %ymm9, %ymm14
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm15
	je	.LBB147_543
# BB#542:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	%xmm3, %xmm2
.LBB147_543:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqu	-14(%rcx,%rax,2), %ymm0
	vpblendw	$170, 16(%rcx,%rax,2), %ymm0, %ymm0 # ymm0 = ymm0[0],mem[1],ymm0[2],mem[3],ymm0[4],mem[5],ymm0[6],mem[7],ymm0[8],mem[9],ymm0[10],mem[11],ymm0[12],mem[13],ymm0[14],mem[15]
	vpshufb	.LCPI147_26(%rip), %ymm0, %ymm3 # ymm3 = ymm0[0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15,16,17,20,21,24,25,28,29,18,19,22,23,26,27,30,31]
	vperm2i128	$35, %ymm0, %ymm0, %ymm0 # ymm0 = ymm0[2,3,0,1]
	vpshufb	.LCPI147_27(%rip), %ymm0, %ymm0 # ymm0 = ymm0[2,3,6,7,10,11,14,15,0,1,4,5,8,9,12,13,18,19,22,23,26,27,30,31,16,17,20,21,24,25,28,29]
	vpblendd	$60, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0,1],ymm0[2,3,4,5],ymm3[6,7]
	vextracti128	$1, %ymm0, %xmm3
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vcvtdq2ps	%ymm3, %ymm10
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm11
	vpmovzxbd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,zero,zero,xmm2[1],zero,zero,zero,xmm2[2],zero,zero,zero,xmm2[3],zero,zero,zero,xmm2[4],zero,zero,zero,xmm2[5],zero,zero,zero,xmm2[6],zero,zero,zero,xmm2[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm0, %ymm11, %ymm3, %ymm0
	vpunpckhbw	%xmm2, %xmm2, %xmm2 # xmm2 = xmm2[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm10, %ymm3, %ymm2
	vmovaps	.LCPI147_12(%rip), %ymm12 # ymm12 = <u,4,u,5,u,6,u,7>
	vpermps	%ymm2, %ymm12, %ymm3
	vmovaps	.LCPI147_13(%rip), %ymm13 # ymm13 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm15, %ymm13, %ymm9
	vblendps	$170, %ymm3, %ymm9, %ymm3 # ymm3 = ymm9[0],ymm3[1],ymm9[2],ymm3[3],ymm9[4],ymm3[5],ymm9[6],ymm3[7]
	vmovaps	.LCPI147_14(%rip), %ymm8 # ymm8 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm2, %ymm8, %ymm2
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm15, %ymm1, %ymm9
	vblendps	$170, %ymm2, %ymm9, %ymm2 # ymm2 = ymm9[0],ymm2[1],ymm9[2],ymm2[3],ymm9[4],ymm2[5],ymm9[6],ymm2[7]
	vpermps	%ymm0, %ymm12, %ymm9
	vpermps	%ymm14, %ymm13, %ymm15
	vblendps	$170, %ymm9, %ymm15, %ymm9 # ymm9 = ymm15[0],ymm9[1],ymm15[2],ymm9[3],ymm15[4],ymm9[5],ymm15[6],ymm9[7]
	vpermps	%ymm0, %ymm8, %ymm0
	vmovaps	%ymm8, %ymm15
	vpermps	%ymm14, %ymm1, %ymm14
	vblendps	$170, %ymm0, %ymm14, %ymm0 # ymm0 = ymm14[0],ymm0[1],ymm14[2],ymm0[3],ymm14[4],ymm0[5],ymm14[6],ymm0[7]
	vmovups	%ymm0, (%r12,%rbx,4)
	vmovups	%ymm9, 32(%r12,%rbx,4)
	vmovups	%ymm2, 64(%r12,%rbx,4)
	vmovups	%ymm3, 96(%r12,%rbx,4)
	movq	%r12, %rax
	je	.LBB147_545
# BB#544:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	4128(%rsp), %xmm6       # 16-byte Reload
.LBB147_545:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpmovzxbd	%xmm6, %ymm0    # ymm0 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm0, %ymm7, %ymm3, %ymm0
	vpunpckhbw	%xmm6, %xmm6, %xmm2 # xmm2 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm5, %ymm3, %ymm2
	je	.LBB147_547
# BB#546:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	4160(%rsp), %xmm4       # 16-byte Reload
.LBB147_547:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpmovzxbd	%xmm4, %ymm3    # ymm3 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm6, %ymm6, %ymm6
	vblendvps	%ymm3, %ymm11, %ymm6, %ymm3
	vpunpckhbw	%xmm4, %xmm4, %xmm4 # xmm4 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm10, %ymm6, %ymm4
	vpermps	%ymm2, %ymm13, %ymm6
	vpermps	%ymm4, %ymm12, %ymm9
	vblendps	$170, %ymm9, %ymm6, %ymm6 # ymm6 = ymm6[0],ymm9[1],ymm6[2],ymm9[3],ymm6[4],ymm9[5],ymm6[6],ymm9[7]
	vpermps	%ymm2, %ymm1, %ymm2
	vpermps	%ymm4, %ymm15, %ymm4
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm0, %ymm13, %ymm4
	vpermps	%ymm3, %ymm12, %ymm9
	vblendps	$170, %ymm9, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm9[1],ymm4[2],ymm9[3],ymm4[4],ymm9[5],ymm4[6],ymm9[7]
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	%ymm1, %ymm8
	vpermps	%ymm3, %ymm15, %ymm3
	vmovaps	%ymm15, %ymm9
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	vmovups	%ymm0, 12288(%rax,%r14,4)
	vmovups	%ymm4, 12320(%rax,%r14,4)
	vmovups	%ymm2, 12352(%rax,%r14,4)
	vmovups	%ymm6, 12384(%rax,%r14,4)
	movq	%rax, %rsi
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_549
# BB#548:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
.LBB147_549:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpmovzxbd	%xmm1, %ymm0    # ymm0 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm2, %ymm2, %ymm2
	vblendvps	%ymm0, %ymm7, %ymm2, %ymm0
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm5, %ymm2, %ymm1
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	je	.LBB147_551
# BB#550:                               # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vmovdqa	5152(%rsp), %xmm3       # 16-byte Reload
.LBB147_551:                            # %for deinterleaved$1.s0.v10.v10303
                                        #   in Loop: Header=BB147_527 Depth=4
	vpmovzxbd	%xmm3, %ymm2    # ymm2 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm2, %ymm11, %ymm4, %ymm2
	vpunpckhbw	%xmm3, %xmm3, %xmm3 # xmm3 = xmm3[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm10, %ymm4, %ymm3
	vpermps	%ymm1, %ymm13, %ymm4
	vpermps	%ymm3, %ymm12, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm8, %ymm1
	vpermps	%ymm3, %ymm9, %ymm3
	vblendps	$170, %ymm3, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm3[1],ymm1[2],ymm3[3],ymm1[4],ymm3[5],ymm1[6],ymm3[7]
	vpermps	%ymm0, %ymm13, %ymm3
	vpermps	%ymm2, %ymm12, %ymm5
	vblendps	$170, %ymm5, %ymm3, %ymm3 # ymm3 = ymm3[0],ymm5[1],ymm3[2],ymm5[3],ymm3[4],ymm5[5],ymm3[6],ymm5[7]
	vpermps	%ymm0, %ymm8, %ymm0
	vpermps	%ymm2, %ymm9, %ymm2
	vblendps	$170, %ymm2, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm2[1],ymm0[2],ymm2[3],ymm0[4],ymm2[5],ymm0[6],ymm2[7]
	addq	4616(%rsp), %rbx        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%rbx,4)
	vmovups	%ymm3, 24608(%rsi,%rbx,4)
	vmovups	%ymm1, 24640(%rsi,%rbx,4)
	vmovups	%ymm4, 24672(%rsi,%rbx,4)
	addl	$32, %r15d
	addl	$32, %r11d
	addl	$-1, %r10d
	jne	.LBB147_527
.LBB147_552:                            # %end for deinterleaved$1.s0.v10.v10304
                                        #   in Loop: Header=BB147_498 Depth=3
	movl	1268(%rsp), %eax        # 4-byte Reload
	cmpl	1676(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_579
# BB#553:                               # %for deinterleaved$1.s0.v10.v10307.preheader
                                        #   in Loop: Header=BB147_498 Depth=3
	movq	5184(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 3424(%rsp)        # 4-byte Spill
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1584(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3008(%rsp)       # 32-byte Spill
	vpsubd	1536(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 2848(%rsp)       # 32-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	1656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	movl	1112(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	.align	16, 0x90
.LBB147_554:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_498 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rdx, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 3408(%rsp)        # 4-byte Spill
	cmpl	$0, 3424(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movq	2096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movl	%eax, 3680(%rsp)        # 4-byte Spill
	movl	%eax, %r14d
	andl	$1, %r14d
	sete	%cl
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	movq	5184(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3712(%rsp)        # 4-byte Spill
	movq	2080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm13 # ymm13 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm13, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4352(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3216(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3184(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3200(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %esi
	movl	%esi, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %esi
	movl	%esi, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %esi
	movl	%esi, 3584(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %esi
	movl	%esi, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vmovdqa	.LCPI147_10(%rip), %ymm12 # ymm12 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm12, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movq	1920(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovd	4128(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 4160(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3840(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3808(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3776(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm2, %xmm2
	vpinsrd	$3, %r15d, %xmm2, %xmm2
	vmovd	%esi, %xmm3
	vpinsrd	$1, %r11d, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vmovd	%r9d, %xmm4
	vpinsrd	$1, %ebx, %xmm4, %xmm4
	vpinsrd	$2, %r10d, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	movq	2072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm5
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	5152(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andb	4192(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 3392(%rsp)       # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm12, %ymm0, %ymm6
	vpaddd	%ymm13, %ymm0, %ymm0
	vmovdqa	4304(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm7
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm10, %xmm0, %xmm0
	vmovdqa	4288(%rsp), %xmm11      # 16-byte Reload
	vpmaxsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm7, %ymm7
	vpminsd	%xmm10, %xmm6, %xmm0
	vextracti128	$1, %ymm6, %xmm6
	vpminsd	%xmm10, %xmm6, %xmm6
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm11, %xmm6, %xmm6
	vinserti128	$1, %xmm6, %ymm0, %ymm6
	movl	3680(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	4512(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vmovdqa	3008(%rsp), %ymm14      # 32-byte Reload
	vpand	%ymm4, %ymm14, %ymm4
	vmovdqa	4320(%rsp), %ymm9       # 32-byte Reload
	vpaddd	%ymm3, %ymm9, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpsrad	$31, %ymm1, %ymm4
	vpand	%ymm4, %ymm14, %ymm4
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm4, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm4
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm4, %ymm1
	vpabsd	%xmm3, %xmm4
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vmovdqa	4480(%rsp), %ymm4       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm4, %ymm4
	vmovdqa	4448(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm1, %ymm8, %ymm1
	vblendvps	%ymm2, %ymm7, %ymm1, %ymm1
	vblendvps	%ymm4, %ymm6, %ymm3, %ymm2
	vmovdqa	2848(%rsp), %ymm15      # 32-byte Reload
	vpaddd	%ymm2, %ymm15, %ymm2
	vpaddd	%ymm1, %ymm15, %ymm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4128(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4160(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5152(%rsp)        # 8-byte Spill
	vmovq	%xmm2, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3744(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	testl	3424(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	%xmm5, %ymm1
	vpaddd	%ymm13, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3232(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3152(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3216(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3136(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3184(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3120(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3200(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, 3104(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vmovd	%xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3584(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm12, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3536(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3488(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3248(%rsp)              # 4-byte Folded Reload
	movq	1928(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %r11        # 8-byte Reload
	leal	(%rax,%r11), %eax
	vmovd	%eax, %xmm1
	vmovd	3136(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 3152(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, 3120(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$3, 3104(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vmovd	%r15d, %xmm3
	vpinsrd	$1, %r14d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpinsrd	$3, %r13d, %xmm3, %xmm3
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %ebx, %xmm4, %xmm4
	vmovd	%r9d, %xmm5
	vpinsrd	$1, %r8d, %xmm5, %xmm5
	vpinsrd	$2, %r10d, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpbroadcastd	%xmm1, %ymm6
	vpaddd	%ymm13, %ymm6, %ymm1
	vpminsd	%xmm10, %xmm1, %xmm7
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm11, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm7, %ymm1
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsrad	$31, %ymm2, %ymm3
	vpand	%ymm14, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm3, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovdqa	4416(%rsp), %ymm3       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm3, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	vpaddd	%ymm12, %ymm6, %ymm2
	vpminsd	%xmm10, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm11, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vinserti128	$1, %xmm4, %ymm5, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpand	%ymm14, %ymm4, %ymm4
	vpaddd	%ymm3, %ymm9, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpabsd	%xmm3, %xmm4
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vmovdqa	4384(%rsp), %ymm4       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm4, %ymm0
	vpsubd	%ymm3, %ymm8, %ymm3
	vblendvps	%ymm0, %ymm2, %ymm3, %ymm0
	movl	3712(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vpaddd	%ymm0, %ymm15, %ymm0
	vpaddd	%ymm1, %ymm15, %ymm1
	vmovq	%xmm1, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	movslq	3680(%rsp), %r15        # 4-byte Folded Reload
	subq	4712(%rsp), %r15        # 8-byte Folded Reload
	addq	2832(%rsp), %r15        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4624(%rsp), %rsi        # 8-byte Reload
	leaq	(%r15,%rsi), %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	je	.LBB147_556
# BB#555:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_556:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovd	%ebx, %xmm0
	movl	5216(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3392(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_558
# BB#557:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_558:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movl	3168(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_560
# BB#559:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_560:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_562
# BB#561:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_562:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm1, 3120(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_564
# BB#563:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_564:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm0, 3136(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_566
# BB#565:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_566:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm1, 3152(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_568
# BB#567:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_568:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5216(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3280(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3360(%rsp)        # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%r8,2), %xmm1, %xmm1
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4128(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4160(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%rdx,2), %xmm2, %xmm2
	movq	5152(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vpxor	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_570
# BB#569:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_570:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movslq	%r12d, %r13
	movq	%r13, 3264(%rsp)        # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3312(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movq	3216(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %r12
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3328(%rsp)        # 8-byte Spill
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%rbx, %rax
	movq	%r10, %rbx
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%rax,2), %xmm0, %xmm0
	movzwl	(%r11,%r13,2), %esi
	vmovd	%esi, %xmm1
	movq	3712(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5608(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r15,4)
	vmovups	%ymm3, 32(%rcx,%r15,4)
	vmovups	%ymm0, 64(%rcx,%r15,4)
	vmovups	%ymm2, 96(%rcx,%r15,4)
	movq	%rcx, %r11
	je	.LBB147_572
# BB#571:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_572:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movq	%rsi, %rcx
	movq	3392(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3744(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rcx,%r8,2), %xmm0, %xmm0
	movq	3776(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3168(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5216(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4128(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4160(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_574
# BB#573:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	3120(%rsp), %xmm6       # 16-byte Reload
.LBB147_574:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movq	%rsi, %rdx
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %esi
	vmovd	%esi, %xmm5
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%rax,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	%r11, %rsi
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%rsi,%rcx,4)
	vmovups	%ymm4, 12320(%rsi,%rcx,4)
	vmovups	%ymm2, 12352(%rsi,%rcx,4)
	vmovups	%ymm6, 12384(%rsi,%rcx,4)
	je	.LBB147_576
# BB#575:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	3136(%rsp), %xmm9       # 16-byte Reload
.LBB147_576:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	movq	5248(%rsp), %rdx        # 8-byte Reload
	je	.LBB147_578
# BB#577:                               # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	vmovdqa	3152(%rsp), %xmm15      # 16-byte Reload
.LBB147_578:                            # %for deinterleaved$1.s0.v10.v10307
                                        #   in Loop: Header=BB147_554 Depth=4
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	3712(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rcx,%rdi,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4616(%rsp), %r15        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rsi,%r15,4)
	vmovups	%ymm2, 24608(%rsi,%r15,4)
	vmovups	%ymm1, 24640(%rsi,%r15,4)
	vmovups	%ymm4, 24672(%rsi,%r15,4)
	addl	$32, %edx
	movl	3408(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_554
.LBB147_579:                            # %end for deinterleaved$1.s0.v10.v10308
                                        #   in Loop: Header=BB147_498 Depth=3
	movl	2816(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2816(%rsp)        # 4-byte Spill
	addq	$1, 5184(%rsp)          # 8-byte Folded Spill
	movq	1680(%rsp), %rax        # 8-byte Reload
	addl	%eax, 2784(%rsp)        # 4-byte Folded Spill
	cmpl	2752(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_498
.LBB147_580:                            # %end for deinterleaved$1.s0.v11298
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1624(%rsp), %rax        # 8-byte Reload
	movq	1216(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx,2), %eax
	movq	%rax, 1000(%rsp)        # 8-byte Spill
	movl	2768(%rsp), %eax        # 4-byte Reload
	cmpl	2800(%rsp), %eax        # 4-byte Folded Reload
	jle	.LBB147_610
# BB#581:                               #   in Loop: Header=BB147_466 Depth=2
	movl	2736(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_582:                            # %for deinterleaved$1.s0.v11311
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_584 Depth 4
	cmpl	$0, 1676(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_609
# BB#583:                               # %for deinterleaved$1.s0.v10.v10314.preheader
                                        #   in Loop: Header=BB147_582 Depth=3
	movq	3104(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	cmpq	%rdi, 1320(%rsp)        # 8-byte Folded Reload
	movl	1340(%rsp), %ecx        # 4-byte Reload
	cmovgl	%edi, %ecx
	movq	1352(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	%edi, %eax
	subl	%edx, %eax
	cltd
	idivl	1332(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1336(%rsp), %eax        # 4-byte Folded Reload
	movq	1344(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %edx
	leal	(%rdx,%rax), %esi
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %esi
	notl	%esi
	cmovgl	%eax, %esi
	movl	1328(%rsp), %eax        # 4-byte Reload
	subl	%esi, %eax
	cmpq	%rdi, 1312(%rsp)        # 8-byte Folded Reload
	cmovgl	%ecx, %eax
	movq	1680(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %eax
	vmovd	%eax, %xmm0
	vpabsd	1584(%rsp), %xmm1       # 16-byte Folded Reload
	vinserti128	$1, %xmm1, %ymm1, %ymm1
	vmovdqa	%ymm1, 3008(%rsp)       # 32-byte Spill
	vpsubd	1536(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 3456(%rsp)       # 32-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	1656(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movl	1676(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_584:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_582 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 3424(%rsp)        # 4-byte Spill
	cmpl	$0, 3488(%rsp)          # 4-byte Folded Reload
	setne	5184(%rsp)              # 1-byte Folded Spill
	sete	5152(%rsp)              # 1-byte Folded Spill
	movl	%eax, %r14d
	andl	$1, %r14d
	sete	%cl
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	movq	%rax, %rcx
	movq	%rcx, %rdx
	movq	3104(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	movq	3920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI147_11(%rip), %ymm1 # ymm1 = [0,2,4,6,8,10,12,14]
	vmovdqa	%ymm1, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm1
	vmovdqa	%ymm2, %ymm9
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	vmovdqa	4352(%rsp), %ymm4       # 32-byte Reload
	vextracti128	$1, %ymm4, %xmm3
	vpextrd	$1, %xmm3, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vmovd	%xmm2, %eax
	vmovd	%xmm3, %ecx
	movl	%ecx, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm3, %ecx
	movl	%ecx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm3, %ecx
	movl	%ecx, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm1, %eax
	vpextrd	$1, %xmm4, %ecx
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3808(%rsp)        # 4-byte Spill
	vmovd	%xmm1, %eax
	vmovd	%xmm4, %ecx
	movl	%ecx, 3680(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm1, %eax
	vpextrd	$2, %xmm4, %ecx
	movl	%ecx, 3648(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$3, %xmm1, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3616(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	vmovdqa	.LCPI147_10(%rip), %ymm6 # ymm6 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm6, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$1, %xmm0, %eax
	vpextrd	$1, %xmm4, %ebx
	movl	%ebx, 3584(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	vmovd	%xmm4, %r9d
	movl	%r9d, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm4, %r10d
	movl	%r10d, 3296(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm4, %ecx
	movl	%ecx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	4160(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 4128(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3840(%rsp), %xmm0, %xmm10 # 4-byte Folded Reload
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 3808(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, %r15d, %xmm0, %xmm11
	vmovd	%esi, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %r8d, %xmm0, %xmm3
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm4
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm5
	movq	3928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	5184(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	andb	5152(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 3408(%rsp)       # 4-byte Spill
	movq	%rcx, %rax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	4512(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm7
	vmovdqa	.LCPI147_7(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4480(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI147_8(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vmovdqa	4032(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	4000(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm2, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm14, %xmm8, %xmm2
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vmovdqa	.LCPI147_9(%rip), %xmm15 # xmm15 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	vpxor	%xmm15, %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm1
	vinserti128	$1, %xmm10, %ymm11, %ymm2
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpsrad	$31, %ymm3, %ymm4
	vpsrad	$31, %ymm2, %ymm7
	vmovdqa	3008(%rsp), %ymm15      # 32-byte Reload
	vpand	%ymm7, %ymm15, %ymm7
	vpand	%ymm4, %ymm15, %ymm4
	vmovdqa	4320(%rsp), %ymm10      # 32-byte Reload
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm3, %ymm10, %ymm3
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm7, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vpabsd	%xmm3, %xmm7
	vextracti128	$1, %ymm3, %xmm3
	vpabsd	%xmm3, %xmm3
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm7, %ymm3
	vmovdqa	4448(%rsp), %ymm8       # 32-byte Reload
	vpsubd	%ymm3, %ymm8, %ymm3
	vpsubd	%ymm2, %ymm8, %ymm2
	vpbroadcastd	%xmm5, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm5
	vpaddd	%ymm9, %ymm4, %ymm4
	vmovdqa	4304(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm7
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	4288(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm7, %ymm4
	vpminsd	%xmm11, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpmovzxbd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,zero,zero,xmm1[1],zero,zero,zero,xmm1[2],zero,zero,zero,xmm1[3],zero,zero,zero,xmm1[4],zero,zero,zero,xmm1[5],zero,zero,zero,xmm1[6],zero,zero,zero,xmm1[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm2
	vpunpckhbw	%xmm1, %xmm1, %xmm1 # xmm1 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpslld	$31, %ymm1, %ymm1
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm1
	vmovdqa	3456(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm2, %ymm3, %ymm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4160(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5152(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 5184(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3840(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3808(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 4128(%rsp)        # 8-byte Spill
	testl	3488(%rsp), %eax        # 4-byte Folded Reload
	vpbroadcastd	3184(%rsp), %ymm1 # 16-byte Folded Reload
	vpaddd	%ymm9, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	setne	%al
	movl	%eax, 3184(%rsp)        # 4-byte Spill
	vpextrd	$1, %xmm3, %eax
	cltd
	movl	3536(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, 3168(%rsp)        # 4-byte Spill
	vmovd	%xmm3, %eax
	cltd
	movl	3280(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 3152(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	movl	3248(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, 3136(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm3, %eax
	cltd
	movl	3232(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	3712(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r14d
	vmovd	%xmm2, %eax
	cltd
	idivl	3680(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r15d
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	3648(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r12d
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	3616(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3584(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r9d
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r10d
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	vmovd	3152(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 3168(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3136(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm1, %xmm3
	vmovd	%r15d, %xmm1
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpinsrd	$2, %r12d, %xmm1, %xmm1
	vpinsrd	$3, %r13d, %xmm1, %xmm4
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%r9d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %r10d, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	-7(%rcx), %eax
	vmovd	%eax, %xmm5
	vmovdqa	4416(%rsp), %ymm6       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm6, %ymm6
	vpshufb	%ymm13, %ymm6, %ymm6
	vpermq	$232, %ymm6, %ymm6      # ymm6 = ymm6[0,2,2,3]
	vmovdqa	4384(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vpshufb	%xmm14, %xmm7, %xmm7
	vpshufb	%xmm14, %xmm6, %xmm6
	vpunpcklqdq	%xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0]
	vmovdqa	3968(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	3936(%rsp), %ymm9       # 32-byte Reload
	vpcmpgtd	%ymm0, %ymm9, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	vpshufb	%xmm14, %xmm0, %xmm0
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0],xmm0[0]
	vpxor	.LCPI147_9(%rip), %xmm6, %xmm6
	vpor	%xmm6, %xmm0, %xmm6
	vinserti128	$1, %xmm3, %ymm4, %ymm0
	vpsrad	$31, %ymm0, %ymm3
	vpand	%ymm15, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm3, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm8, %ymm0
	vpbroadcastd	%xmm5, %ymm3
	vpaddd	.LCPI147_11(%rip), %ymm3, %ymm4
	vpminsd	%xmm11, %xmm4, %xmm5
	vextracti128	$1, %ymm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpmovzxbd	%xmm6, %ymm5    # ymm5 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm0, %ymm4, %ymm0
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm15, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm10, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	.LCPI147_10(%rip), %ymm3, %ymm2
	vpminsd	%xmm11, %xmm2, %xmm3
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpsubd	%ymm1, %ymm8, %ymm1
	vpunpckhbw	%xmm6, %xmm6, %xmm3 # xmm3 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm2, %ymm1
	movl	3744(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	movzbl	%al, %ebx
	vmovdqa	3456(%rsp), %ymm3       # 32-byte Reload
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm0, %ymm3, %ymm0
	vmovq	%xmm0, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, %rax
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r15
	movq	%r15, %rax
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	movslq	%ecx, %r13
	subq	4712(%rsp), %r13        # 8-byte Folded Reload
	addq	2848(%rsp), %r13        # 8-byte Folded Reload
	vpbroadcastb	%xmm2, %xmm15
	vmovdqa	%xmm15, %xmm2
	cmpl	$1, 104(%rbp)
	movq	4624(%rsp), %rsi        # 8-byte Reload
	leaq	(%r13,%rsi), %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	je	.LBB147_586
# BB#585:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm2, %xmm2, %xmm2
.LBB147_586:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovd	%ebx, %xmm0
	movl	5216(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	movl	3408(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm5
	vmovd	%ecx, %xmm3
	vpbroadcastb	%xmm3, %xmm9
	vmovdqa	%xmm9, %xmm3
	je	.LBB147_588
# BB#587:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm3, %xmm3, %xmm3
.LBB147_588:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movl	3184(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ebx
	vmovd	%ebx, %xmm6
	vpor	%xmm6, %xmm0, %xmm7
	vpor	%xmm5, %xmm1, %xmm0
	vpbroadcastb	%xmm0, %xmm6
	vmovdqa	%xmm6, %xmm8
	je	.LBB147_590
# BB#589:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm8, %xmm8, %xmm8
.LBB147_590:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovd	%ecx, %xmm0
	vpbroadcastb	%xmm7, %xmm7
	vmovdqa	%xmm7, %xmm1
	je	.LBB147_592
# BB#591:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_592:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm1, 3120(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm1
	vpbroadcastb	%xmm0, %xmm5
	vmovdqa	%xmm5, %xmm0
	je	.LBB147_594
# BB#593:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_594:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm0, 3136(%rsp)       # 16-byte Spill
	vpbroadcastb	%xmm1, %xmm0
	vmovdqa	%xmm0, %xmm1
	je	.LBB147_596
# BB#595:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_596:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm1, 3152(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	je	.LBB147_598
# BB#597:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm2, %xmm0
.LBB147_598:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 5216(%rsp)        # 8-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%rcx,2), %ebx
	vmovd	%ebx, %xmm1
	movzwl	(%rsi,%rdx,2), %ebx
	vmovd	%ebx, %xmm2
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3248(%rsp)        # 8-byte Spill
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 3376(%rsp)        # 8-byte Spill
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r8
	movq	%r8, 3392(%rsp)         # 8-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r9
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movq	3776(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rsi,%r9,2), %xmm1, %xmm1
	movq	3840(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rsi,%rdx,2), %xmm1, %xmm1
	movq	3808(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%rsi,%rax,2), %xmm1, %xmm1
	movq	4128(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	movq	4160(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$2, (%rsi,%rdi,2), %xmm2, %xmm2
	movq	5152(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$4, (%rsi,%rbx,2), %xmm2, %xmm2
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rsi,%rcx,2), %xmm2, %xmm2
	vpinsrw	$6, (%rsi,%r8,2), %xmm2, %xmm2
	movq	5184(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rsi,%rcx,2), %xmm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm0, %ymm12   # ymm12 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
	vpslld	$31, %ymm12, %ymm12
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm12, %ymm2, %ymm4, %ymm12
	vpunpckhbw	%xmm0, %xmm0, %xmm0 # xmm0 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm4, %ymm13
	je	.LBB147_600
# BB#599:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm3, %xmm5
.LBB147_600:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movslq	%r12d, %r8
	movq	%r8, 3200(%rsp)         # 8-byte Spill
	movslq	%r11d, %r14
	movq	%r14, 3328(%rsp)        # 8-byte Spill
	movslq	%r10d, %r10
	movslq	%r15d, %r12
	movq	3168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	movq	%rdi, 3344(%rsp)        # 8-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %r15
	movq	%rsi, %r11
	movzwl	(%r11,%rax,2), %esi
	vmovd	%esi, %xmm0
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%r11,%rdx,2), %xmm0, %xmm0
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$4, (%r11,%rdi,2), %xmm0, %xmm0
	movq	%r10, %rbx
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm0, %xmm0
	vpinsrw	$6, (%r11,%r15,2), %xmm0, %xmm0
	movzwl	(%r11,%r8,2), %esi
	vmovd	%esi, %xmm1
	movq	3744(%rsp), %rcx        # 8-byte Reload
	movzwl	(%r11,%rcx,2), %r10d
	vpinsrw	$7, %r10d, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%r11,%r14,2), %xmm1, %xmm1
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$4, (%r11,%rbx,2), %xmm1, %xmm1
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%r11,%rcx,2), %xmm1, %xmm1
	vpinsrw	$6, (%r11,%r12,2), %xmm1, %xmm1
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%r11,%rcx,2), %xmm1, %xmm1
	movq	%r11, %rsi
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vpxor	%ymm3, %ymm3, %ymm3
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm2 # xmm2 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm3, %ymm0
	vmovaps	.LCPI147_12(%rip), %ymm2 # ymm2 = <u,4,u,5,u,6,u,7>
	vmovaps	%ymm2, %ymm4
	vpermps	%ymm0, %ymm4, %ymm2
	vmovaps	.LCPI147_13(%rip), %ymm10 # ymm10 = <4,u,5,u,6,u,7,u>
	vpermps	%ymm13, %ymm10, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vmovaps	.LCPI147_14(%rip), %ymm11 # ymm11 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm11, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm14 # ymm14 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm14, %ymm3
	vblendps	$170, %ymm0, %ymm3, %ymm0 # ymm0 = ymm3[0],ymm0[1],ymm3[2],ymm0[3],ymm3[4],ymm0[5],ymm3[6],ymm0[7]
	vpermps	%ymm1, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm13
	vpermps	%ymm12, %ymm10, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm1, %ymm11, %ymm1
	vpermps	%ymm12, %ymm14, %ymm5
	vblendps	$170, %ymm1, %ymm5, %ymm1 # ymm1 = ymm5[0],ymm1[1],ymm5[2],ymm1[3],ymm5[4],ymm1[5],ymm5[6],ymm1[7]
	movq	5608(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm1, (%rcx,%r13,4)
	vmovups	%ymm3, 32(%rcx,%r13,4)
	vmovups	%ymm0, 64(%rcx,%r13,4)
	vmovups	%ymm2, 96(%rcx,%r13,4)
	movq	%rcx, %r11
	je	.LBB147_602
# BB#601:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	%xmm8, %xmm7
.LBB147_602:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movq	%rsi, %rcx
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm0
	movq	3776(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	movq	3840(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3184(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rax,2), %xmm0, %xmm0
	movq	3808(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	3216(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	movq	4128(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	movq	5216(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %esi
	vmovd	%esi, %xmm1
	movq	4160(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$1, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3248(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$2, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$3, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3376(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$4, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	4192(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rdx,2), %xmm1, %xmm1
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$7, (%rcx,%rdx,2), %xmm1, %xmm2
	movq	%rcx, %rsi
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm1
	vpmovzxwd	%xmm2, %ymm0    # ymm0 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	vpmovzxbd	%xmm7, %ymm0    # ymm0 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm7, %xmm7, %xmm2 # xmm2 = xmm7[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm2
	je	.LBB147_604
# BB#603:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	3120(%rsp), %xmm6       # 16-byte Reload
.LBB147_604:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movq	%rsi, %rdx
	movq	3200(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %esi
	vmovd	%esi, %xmm5
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$4, (%rdx,%rbx,2), %xmm5, %xmm5
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r12,2), %xmm5, %xmm5
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rcx,2), %xmm5, %xmm7
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rdx,%rcx,2), %ecx
	vmovd	%ecx, %xmm5
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm5, %xmm5
	movq	3680(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rcx,2), %xmm5, %xmm5
	vpinsrw	$6, (%rdx,%r15,2), %xmm5, %xmm5
	vpinsrw	$7, %r10d, %xmm5, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm4
	vpmovzxbd	%xmm6, %ymm8    # ymm8 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	vpslld	$31, %ymm8, %ymm8
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vcvtdq2ps	%ymm7, %ymm7
	vxorps	%ymm12, %ymm12, %ymm12
	vblendvps	%ymm8, %ymm7, %ymm12, %ymm8
	vpunpckhbw	%xmm6, %xmm6, %xmm6 # xmm6 = xmm6[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm12, %ymm4
	vpermps	%ymm4, %ymm13, %ymm6
	vpermps	%ymm2, %ymm10, %ymm12
	vblendps	$170, %ymm6, %ymm12, %ymm6 # ymm6 = ymm12[0],ymm6[1],ymm12[2],ymm6[3],ymm12[4],ymm6[5],ymm12[6],ymm6[7]
	vpermps	%ymm4, %ymm11, %ymm4
	vpermps	%ymm2, %ymm14, %ymm2
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	vpermps	%ymm8, %ymm13, %ymm4
	vpermps	%ymm0, %ymm10, %ymm12
	vblendps	$170, %ymm4, %ymm12, %ymm4 # ymm4 = ymm12[0],ymm4[1],ymm12[2],ymm4[3],ymm12[4],ymm4[5],ymm12[6],ymm4[7]
	vpermps	%ymm8, %ymm11, %ymm8
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm8, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm8[1],ymm0[2],ymm8[3],ymm0[4],ymm8[5],ymm0[6],ymm8[7]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, 12288(%r11,%rcx,4)
	vmovups	%ymm4, 12320(%r11,%rcx,4)
	vmovups	%ymm2, 12352(%r11,%rcx,4)
	vmovups	%ymm6, 12384(%r11,%rcx,4)
	je	.LBB147_606
# BB#605:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	3136(%rsp), %xmm9       # 16-byte Reload
.LBB147_606:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vpmovzxbd	%xmm9, %ymm0    # ymm0 = xmm9[0],zero,zero,zero,xmm9[1],zero,zero,zero,xmm9[2],zero,zero,zero,xmm9[3],zero,zero,zero,xmm9[4],zero,zero,zero,xmm9[5],zero,zero,zero,xmm9[6],zero,zero,zero,xmm9[7],zero,zero,zero
	vpslld	$31, %ymm0, %ymm0
	vxorps	%ymm4, %ymm4, %ymm4
	vblendvps	%ymm0, %ymm3, %ymm4, %ymm0
	vpunpckhbw	%xmm9, %xmm9, %xmm2 # xmm2 = xmm9[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	je	.LBB147_608
# BB#607:                               # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	vmovdqa	3152(%rsp), %xmm15      # 16-byte Reload
.LBB147_608:                            # %for deinterleaved$1.s0.v10.v10314
                                        #   in Loop: Header=BB147_584 Depth=4
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	3744(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rcx,%rdx,2), %ecx
	vpinsrw	$7, %ecx, %xmm5, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vpmovzxbd	%xmm15, %ymm3   # ymm3 = xmm15[0],zero,zero,zero,xmm15[1],zero,zero,zero,xmm15[2],zero,zero,zero,xmm15[3],zero,zero,zero,xmm15[4],zero,zero,zero,xmm15[5],zero,zero,zero,xmm15[6],zero,zero,zero,xmm15[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vpxor	%ymm5, %ymm5, %ymm5
	vblendvps	%ymm3, %ymm7, %ymm5, %ymm3
	vpunpckhbw	%xmm15, %xmm15, %xmm4 # xmm4 = xmm15[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm2, %ymm5, %ymm2
	vpermps	%ymm1, %ymm10, %ymm4
	vpermps	%ymm2, %ymm13, %ymm5
	vblendps	$170, %ymm5, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm5[1],ymm4[2],ymm5[3],ymm4[4],ymm5[5],ymm4[6],ymm5[7]
	vpermps	%ymm1, %ymm14, %ymm1
	vpermps	%ymm2, %ymm11, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm2
	vpermps	%ymm0, %ymm10, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm3
	vpermps	%ymm0, %ymm14, %ymm0
	vblendps	$170, %ymm3, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm3[1],ymm0[2],ymm3[3],ymm0[4],ymm3[5],ymm0[6],ymm3[7]
	addq	4616(%rsp), %r13        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%r11,%r13,4)
	vmovups	%ymm2, 24608(%r11,%r13,4)
	vmovups	%ymm1, 24640(%r11,%r13,4)
	vmovups	%ymm4, 24672(%r11,%r13,4)
	movq	5248(%rsp), %rax        # 8-byte Reload
	addl	$32, %eax
	movl	3424(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_584
.LBB147_609:                            # %end for deinterleaved$1.s0.v10.v10315
                                        #   in Loop: Header=BB147_582 Depth=3
	movl	2800(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2800(%rsp)        # 4-byte Spill
	addq	$1, 3104(%rsp)          # 8-byte Folded Spill
	cmpl	2768(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_582
.LBB147_610:                            # %produce gH319
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1000(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rcx
	leal	2(%rcx), %edx
	movl	%edx, 1372(%rsp)        # 4-byte Spill
	movq	1752(%rsp), %rax        # 8-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	leal	4(%rcx), %ecx
	movl	%ecx, 1364(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	movl	%ecx, %edi
	cmovgl	%esi, %edi
	movl	%edi, 1376(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 2176(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	%ecx, 1188(%rsp)        # 4-byte Spill
	movl	936(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1308(%rsp)        # 4-byte Spill
	movl	932(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1304(%rsp)        # 4-byte Spill
	movl	928(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1300(%rsp)        # 4-byte Spill
	movl	%edx, 2192(%rsp)        # 4-byte Spill
	cmpl	%edi, %edx
	jge	.LBB147_647
	.align	16, 0x90
.LBB147_611:                            # %for gH.s0.v11321
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_613 Depth 4
	cmpl	$0, 2184(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_646
# BB#612:                               # %for gH.s0.v10.v10323.preheader
                                        #   in Loop: Header=BB147_611 Depth=3
	movl	2192(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1740(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1744(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2112(%rsp)       # 16-byte Spill
	cltq
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1712(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2104(%rsp)        # 8-byte Spill
	movl	2184(%rsp), %eax        # 4-byte Reload
	movq	5288(%rsp), %r10        # 8-byte Reload
	movl	1308(%rsp), %ecx        # 4-byte Reload
	movl	1304(%rsp), %r9d        # 4-byte Reload
	movl	1300(%rsp), %edi        # 4-byte Reload
	.align	16, 0x90
.LBB147_613:                            # %for gH.s0.v10.v10323
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_611 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 5248(%rsp)        # 4-byte Spill
	movl	%edi, 3616(%rsp)        # 4-byte Spill
	movl	%r9d, 3648(%rsp)        # 4-byte Spill
	movl	%eax, 3680(%rsp)        # 4-byte Spill
	cmpl	$0, 3808(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r10d, %r15d
	andl	$1, %r15d
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	3080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3376(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3840(%rsp)        # 4-byte Spill
	movq	3064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm11, %xmm10
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vmovd	4160(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4192(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 4128(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3840(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3840(%rsp)       # 16-byte Spill
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 4128(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	vmovd	%r11d, %xmm0
	vpinsrd	$1, 3488(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3424(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r14d, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3392(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm3, %xmm5
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	leal	-3(%r10), %eax
	vmovd	%eax, %xmm11
	vmovd	%r10d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2112(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5296(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5344(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vmovdqa	%xmm10, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm0
	vmovdqa	5360(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm1
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpcmpgtd	%xmm1, %xmm14, %xmm4
	vpsubd	%xmm1, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	%xmm11, %xmm4
	vmovdqa	%xmm6, %xmm11
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	%xmm12, %xmm1, %xmm0
	vmovdqa	%xmm0, 4160(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	movl	%r10d, %eax
	movl	2192(%rsp), %ebx        # 4-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	3088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm4
	vmovdqa	3840(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovdqa	5136(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	5088(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	4128(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm2, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	sete	4128(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r13b       # 1-byte Reload
	movb	5152(%rsp), %r14b       # 1-byte Reload
	andb	%r14b, %r13b
	vpaddd	%xmm1, %xmm10, %xmm5
	vmovq	%xmm5, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm5, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movb	5184(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r15b
	movl	%r15d, 5216(%rsp)       # 4-byte Spill
	movl	3808(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3840(%rsp)              # 1-byte Folded Spill
	leal	1(%r10), %r15d
	movl	%r15d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	sete	%r12b
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	3376(%rsp)              # 4-byte Folded Reload
	vmovd	3552(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3584(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3536(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3456(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	3424(%rsp), %xmm5       # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vpinsrd	$1, 3408(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$2, 3392(%rsp), %xmm5, %xmm5 # 4-byte Folded Reload
	vpinsrd	$3, 3360(%rsp), %xmm5, %xmm6 # 4-byte Folded Reload
	leal	-4(%r10), %eax
	vmovd	%eax, %xmm5
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpinsrd	$2, %esi, %xmm7, %xmm7
	vpsrad	$31, %xmm4, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm11, %xmm3, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpinsrd	$3, %edx, %xmm7, %xmm0
	vpaddd	%xmm4, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2528(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	%ebx, %eax
	movb	%r12b, %bl
	testb	$1, %al
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	%r14b, %bl
	andb	%r11b, %r9b
	testl	%r15d, %r8d
	vmovd	%r15d, %xmm1
	vpsrad	$31, %xmm6, %xmm2
	vpand	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm6
	vpsubd	%xmm2, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vmovdqa	4944(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm6
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vpxor	%xmm10, %xmm6, %xmm6
	vmovdqa	4784(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm2, %xmm5, %xmm2
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	5040(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vmovdqa	5008(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm6, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm2, %xmm5
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm3
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm6
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	setne	%r13b
	vmovq	%xmm0, %r8
	movq	%r8, 2304(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2352(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3488(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm1, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 2368(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %r11
	movq	%r11, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2432(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	movslq	5248(%rsp), %rcx        # 4-byte Folded Reload
	movq	%rcx, %rdx
	orq	$4, %rdx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2496(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2576(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2592(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2608(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2656(%rsp)        # 8-byte Spill
	movq	%rcx, %rdx
	orq	$6, %rdx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	vmovdqa	4160(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2816(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3200(%rsp)        # 8-byte Spill
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm10
	vmovaps	%xmm10, 4160(%rsp)      # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_615
# BB#614:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm10, %xmm10, %xmm10
.LBB147_615:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	movzbl	4128(%rsp), %r15d       # 1-byte Folded Reload
	vmovd	%r15d, %xmm0
	movzbl	3840(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm8
	vmovaps	%xmm8, 3840(%rsp)       # 16-byte Spill
	je	.LBB147_617
# BB#616:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_617:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_619
# BB#618:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_619:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	je	.LBB147_621
# BB#620:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_621:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm1, 2208(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2224(%rsp)       # 16-byte Spill
	movzbl	%bl, %edi
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_623
# BB#622:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_623:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm0, 2240(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	%r13b, %edi
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, %xmm2
	movq	4816(%rsp), %r15        # 8-byte Reload
	je	.LBB147_625
# BB#624:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_625:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm2, 2256(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	movzbl	%r9b, %edi
	vmovd	%edi, %xmm0
	je	.LBB147_627
# BB#626:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_627:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm2, 2272(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movl	3648(%rsp), %r9d        # 4-byte Reload
	je	.LBB147_629
# BB#628:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_629:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movq	2832(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3136(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2848(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2784(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3008(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3152(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm12 # xmm12 = xmm2[0,1,2],mem[0]
	movq	2688(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2720(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2704(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2736(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm15 # xmm15 = xmm3[0,1,2],mem[0]
	movq	2480(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vmovss	(%rbx,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2528(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2512(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2544(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm11 # xmm11 = xmm3[0,1,2],mem[0]
	movslq	%r9d, %rdi
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rdi,4), %xmm3
	vmovups	12328(%rdx,%rdi,4), %xmm6
	vmovups	12304(%rdx,%rdi,4), %xmm7
	vmovups	12320(%rdx,%rdi,4), %xmm1
	vshufps	$136, 12336(%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,2],mem[0,2]
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm0, %xmm9, %xmm0
	vshufps	$221, %xmm6, %xmm3, %xmm2 # xmm2 = xmm3[1,3],xmm6[1,3]
	vmovaps	5408(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$221, %xmm1, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm1[1,3]
	vmulps	%xmm12, %xmm9, %xmm1
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm6, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm15, %xmm9, %xmm3
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm7
	vsubps	%xmm4, %xmm14, %xmm6
	cmpl	$0, 104(%rbp)
	je	.LBB147_631
# BB#630:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm10, 4192(%rsp)      # 16-byte Spill
.LBB147_631:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm9
	vmaxps	%xmm1, %xmm2, %xmm0
	vmulps	3776(%rsp), %xmm11, %xmm12 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm6, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm5
	je	.LBB147_633
# BB#632:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	%xmm8, 4128(%rsp)       # 16-byte Spill
.LBB147_633:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	movq	2304(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2320(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	movq	2336(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2352(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r12,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	movq	2368(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	2384(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm6, %xmm15 # xmm15 = xmm6[0,1,2],mem[0]
	movl	3616(%rsp), %edi        # 4-byte Reload
	movslq	%edi, %rax
	movq	%rdx, %rsi
	vmovups	24592(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vmovups	24608(%rsi,%rax,4), %xmm10
	vmovups	24624(%rsi,%rax,4), %xmm2
	vmovaps	%xmm2, 2832(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rax,4), %xmm4
	vmovaps	%xmm4, 3120(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm6
	vmovaps	%xmm6, 3104(%rsp)       # 16-byte Spill
	vaddps	%xmm9, %xmm0, %xmm7
	vmovaps	%xmm7, 2720(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm12, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm5, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm0
	vshufps	$136, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm10[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm7, %xmm1
	vshufps	$136, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm15, %xmm7, %xmm2
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_635
# BB#634:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	2208(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
.LBB147_635:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rdx
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2432(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],mem[0]
	movq	2464(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 3136(%rsp)       # 16-byte Spill
	movq	2496(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	2576(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2592(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	2608(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2672(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3008(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm9
	vmovups	48(%rsi,%rcx,4), %xmm11
	vmovaps	%xmm11, 2784(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%rcx,4), %xmm14
	vmovaps	%xmm14, 2848(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm12, %xmm15
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm1
	vshufps	$136, %xmm9, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm9[0,2]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm2, %xmm0, %xmm2
	vshufps	$136, %xmm11, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm12, %xmm4
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vmulps	5216(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm4
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm5, %xmm1, %xmm8
	je	.LBB147_637
# BB#636:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	2224(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3840(%rsp)       # 16-byte Spill
.LBB147_637:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vsubps	%xmm0, %xmm15, %xmm1
	vmovdqa	4160(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vfmadd213ps	%xmm2, %xmm14, %xmm7
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm4, %xmm6
	vmovdqa	3840(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm8, %xmm5, %xmm0
	je	.LBB147_639
# BB#638:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	2240(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 5184(%rsp)       # 16-byte Spill
.LBB147_639:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovdqa	4128(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vfmadd213ps	%xmm2, %xmm14, %xmm1
	vblendvps	%xmm15, %xmm7, %xmm0, %xmm0
	vaddps	%xmm6, %xmm8, %xmm15
	je	.LBB147_641
# BB#640:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovaps	2256(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 5152(%rsp)       # 16-byte Spill
.LBB147_641:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm7
	movq	2752(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2768(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3744(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm1
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 2832(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm10[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3120(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3104(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm2, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm12, %xmm4
	vminps	%xmm13, %xmm1, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm1
	vmulps	5216(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3408(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_643
# BB#642:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovdqa	2272(%rsp), %xmm10      # 16-byte Reload
.LBB147_643:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vblendvps	%xmm0, %xmm8, %xmm7, %xmm8
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 2784(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm9[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3008(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3424(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm0, %xmm4, %xmm12
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3136(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm9[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vsubps	%xmm0, %xmm12, %xmm0
	vfmadd213ps	%xmm6, %xmm14, %xmm1
	vfmadd213ps	%xmm6, %xmm14, %xmm0
	vmovdqa	5184(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vpslld	$31, %xmm10, %xmm5
	vmovdqa	3392(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_645
# BB#644:                               # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vmovdqa	2288(%rsp), %xmm6       # 16-byte Reload
.LBB147_645:                            # %for gH.s0.v10.v10323
                                        #   in Loop: Header=BB147_613 Depth=4
	vpslld	$31, %xmm6, %xmm6
	vmovaps	3152(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm6, %xmm7, %xmm2, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vblendvps	%xmm4, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm7, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r10d, %rax
	movq	2104(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r15,%rax,4)
	addl	$8, %edi
	addl	$8, %r9d
	movl	5248(%rsp), %ecx        # 4-byte Reload
	addl	$8, %ecx
	addl	$8, %r10d
	movl	3680(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_613
.LBB147_646:                            # %end for gH.s0.v10.v10324
                                        #   in Loop: Header=BB147_611 Depth=3
	movl	2192(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2192(%rsp)        # 4-byte Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1300(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1304(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1308(%rsp)        # 4-byte Folded Spill
	cmpl	1376(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_611
.LBB147_647:                            # %end for gH.s0.v11322
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	1188(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1376(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_739
# BB#648:                               #   in Loop: Header=BB147_466 Depth=2
	movl	860(%rsp), %esi         # 4-byte Reload
	movl	876(%rsp), %edx         # 4-byte Reload
	subl	%edx, %esi
	movl	1768(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	imull	%esi, %eax
	imull	%ecx, %esi
	movl	%esi, 1308(%rsp)        # 4-byte Spill
	notl	%edx
	movq	688(%rsp), %rcx         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movl	%eax, 1304(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_649:                            # %for gH.s0.v11327
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_651 Depth 4
                                        #         Child Loop BB147_686 Depth 4
                                        #         Child Loop BB147_705 Depth 4
	cmpl	$0, 2172(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_684
# BB#650:                               # %for gH.s0.v10.v10329.preheader
                                        #   in Loop: Header=BB147_649 Depth=3
	movq	1144(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rdi        # 8-byte Reload
	leal	(%rax,%rdi), %ecx
	movq	1648(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2256(%rsp)        # 8-byte Spill
	movl	%edi, %eax
	andl	$1, %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2240(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1712(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2224(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	.align	16, 0x90
.LBB147_651:                            # %for gH.s0.v10.v10329
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_649 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	cmpl	$0, 3840(%rsp)          # 4-byte Folded Reload
	setne	4160(%rsp)              # 1-byte Folded Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%rax,8), %r15d
	movl	%r15d, 3712(%rsp)       # 4-byte Spill
	movl	%r15d, %r11d
	andl	$1, %r11d
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r13d
	movq	%rax, %r10
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r13d
	leal	-2(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3408(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 5152(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	leal	-1(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vmovdqa	%xmm9, %xmm12
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	4128(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpinsrd	$1, 5152(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3680(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3648(%rsp), %xmm1, %xmm10 # 4-byte Folded Reload
	leal	-2(%r12,%r10,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	vmovd	3424(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3488(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3376(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3456(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r8d, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	vpinsrd	$1, 3360(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r14d, %xmm2, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3360(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm2, %xmm5
	leal	-1(%r12,%r10,8), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	leal	-3(%r12,%r10,8), %eax
	movq	%r10, %rbx
	vmovd	%eax, %xmm11
	vmovd	%r15d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2240(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm3, %xmm3
	vmovdqa	5072(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5296(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5344(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vmovdqa	%xmm12, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vmovdqa	5360(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm1
	vmovdqa	%xmm1, 4128(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm0
	vmovdqa	5104(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm1, %xmm7, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm11, %xmm3
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 3680(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	5248(%rsp), %r8         # 8-byte Reload
	orl	%r8d, %eax
	testb	$1, %al
	vpsrad	$31, %xmm10, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpxor	%xmm5, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	5088(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	5152(%rsp), %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm6, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	sete	3488(%rsp)              # 1-byte Folded Spill
	movb	4160(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, 5184(%rsp)       # 1-byte Folded Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	movb	4192(%rsp), %r14b       # 1-byte Reload
	andb	%r14b, %r11b
	movl	%r11d, 5152(%rsp)       # 4-byte Spill
	movl	3840(%rsp), %r11d       # 4-byte Reload
	testl	%r15d, %r11d
	setne	3456(%rsp)              # 1-byte Folded Spill
	leal	1(%r12,%rbx,8), %ecx
	movl	%ecx, %r9d
	andl	$1, %r9d
	sete	%r15b
	addl	$1, %r13d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm6, %xmm10
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	3408(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3392(%rsp)              # 4-byte Folded Reload
	vmovd	3584(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3616(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3552(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3536(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3376(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3424(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3360(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3344(%rsp), %xmm4, %xmm3 # 4-byte Folded Reload
	leal	-4(%r12,%rbx,8), %eax
	movl	%r9d, %r12d
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm4
	vpinsrd	$1, %r13d, %xmm4, %xmm4
	vpinsrd	$2, %esi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm0, %xmm14, %xmm6
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm6, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm2, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpaddd	%xmm4, %xmm7, %xmm0
	vmovq	%xmm0, %rdi
	movq	%rdi, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	orl	%r8d, %eax
	testb	$1, %al
	sete	2432(%rsp)              # 1-byte Folded Spill
	andb	%r10b, %r15b
	movb	%r15b, 2720(%rsp)       # 1-byte Spill
	andb	%r14b, %r12b
	testl	%ecx, %r11d
	vmovd	%ecx, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm11
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm6
	vblendvps	%xmm7, %xmm1, %xmm6, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpxor	%xmm0, %xmm6, %xmm6
	vmovdqa	4784(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpbroadcastd	3616(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm5, %xmm6
	vpand	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm5, %xmm14, %xmm6
	vpsubd	%xmm5, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vmovdqa	5040(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	vpxor	%xmm0, %xmm6, %xmm6
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm2
	vpor	%xmm6, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vpbroadcastd	4192(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm5, %xmm6, %xmm2
	vpmulld	%xmm12, %xmm1, %xmm1
	vpmulld	%xmm12, %xmm2, %xmm2
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm5
	setne	%dl
	vmovq	%xmm5, %r10
	movq	%r10, %rbx
	sarq	$32, %rbx
	vpextrq	$1, %xmm5, %r8
	movq	%r8, %rax
	sarq	$32, %rax
	vpaddd	%xmm4, %xmm3, %xmm5
	vmovq	%xmm5, %r11
	movq	%r11, 2368(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm5, %r13
	movq	%r13, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vmovdqa	3648(%rsp), %xmm0       # 16-byte Reload
	vpaddd	%xmm0, %xmm3, %xmm5
	vmovq	%xmm5, %r15
	movq	%r15, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm5, %r9
	movq	%r9, 5184(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vmovdqa	5424(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm1, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2496(%rsp)        # 8-byte Spill
	movq	2256(%rsp), %rcx        # 8-byte Reload
	movq	5216(%rsp), %rsi        # 8-byte Reload
	leal	(%rcx,%rsi,8), %esi
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movl	%ecx, 2352(%rsp)        # 4-byte Spill
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movl	%ecx, 2400(%rsp)        # 4-byte Spill
	movslq	%esi, %rsi
	movq	%rsi, %r14
	orq	$4, %r14
	vpaddd	%xmm4, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2544(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2528(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2576(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2624(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2608(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	movq	%rsi, %rcx
	orq	$6, %rcx
	movq	%rcx, 2656(%rsp)        # 8-byte Spill
	vmovdqa	3680(%rsp), %xmm6       # 16-byte Reload
	vpaddd	%xmm6, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2768(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2784(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	vmovdqa	4128(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3120(%rsp)        # 8-byte Spill
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm5, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm11, %xmm11
	vmovaps	%xmm11, 2592(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	je	.LBB147_653
# BB#652:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_653:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	movzbl	3488(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	movzbl	3456(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_655
# BB#654:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_655:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movl	5152(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_657
# BB#656:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_657:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm0, 2272(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	je	.LBB147_659
# BB#658:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_659:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movzbl	2720(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_661
# BB#660:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_661:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm1, 2304(%rsp)       # 16-byte Spill
	movzbl	2432(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm1
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	je	.LBB147_663
# BB#662:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_663:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm2, 2320(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5152(%rsp)       # 16-byte Spill
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_665
# BB#664:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_665:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm3, 2720(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2336(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_667
# BB#666:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_667:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm0, 2432(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	3136(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3200(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3184(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],mem[0]
	movq	2672(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2688(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movslq	2352(%rsp), %rcx        # 4-byte Folded Reload
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	12312(%rdi,%rcx,4), %xmm5
	vmovups	12328(%rdi,%rcx,4), %xmm6
	vmovups	12304(%rdi,%rcx,4), %xmm7
	vmovups	12320(%rdi,%rcx,4), %xmm0
	vshufps	$136, 12336(%rdi,%rcx,4), %xmm0, %xmm15 # xmm15 = xmm0[0,2],mem[0,2]
	vmovaps	3808(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm6[1,3]
	vmovaps	5408(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	5440(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm1, %xmm6
	vmulps	%xmm4, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm3, %xmm7
	vsubps	%xmm2, %xmm15, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_669
# BB#668:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm11, 4160(%rsp)      # 16-byte Spill
.LBB147_669:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3808(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_671
# BB#670:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	%xmm8, 4128(%rsp)       # 16-byte Spill
.LBB147_671:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	movslq	%r10d, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%r8d, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2368(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2384(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	5184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r9,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2400(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24592(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vmovups	24608(%rdi,%rax,4), %xmm13
	vmovups	24624(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm2
	vmovaps	%xmm2, 3296(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 3184(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovdqa	2592(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_673
# BB#672:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	2272(%rsp), %xmm2       # 16-byte Reload
.LBB147_673:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rdx, %rcx
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2480(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2464(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2496(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	vmovups	(%rdi,%r14,4), %xmm6
	vmovaps	%xmm6, 3312(%rsp)       # 16-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2544(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2528(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2576(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2624(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2608(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3264(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm10
	vmovups	48(%rdi,%rsi,4), %xmm9
	vmovaps	%xmm9, 3216(%rsp)       # 16-byte Spill
	vmovups	40(%rdi,%rsi,4), %xmm14
	vmovaps	%xmm14, 3248(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3744(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vmulps	5184(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	3136(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	3168(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	movq	4816(%rsp), %rsi        # 8-byte Reload
	vmovdqa	2720(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_675
# BB#674:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	2288(%rsp), %xmm6       # 16-byte Reload
.LBB147_675:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_677
# BB#676:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	2304(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5152(%rsp)       # 16-byte Spill
.LBB147_677:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	4128(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_679
# BB#678:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovaps	2320(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
.LBB147_679:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2736(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2768(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2784(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3200(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	2800(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2816(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3232(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2832(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3008(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3104(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3296(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3152(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5184(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4160(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3488(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_681
# BB#680:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	2336(%rsp), %xmm13      # 16-byte Reload
.LBB147_681:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3216(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3264(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3616(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3584(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3456(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_683
# BB#682:                               # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vmovdqa	2432(%rsp), %xmm1       # 16-byte Reload
.LBB147_683:                            # %for gH.s0.v10.v10329
                                        #   in Loop: Header=BB147_651 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3712(%rsp), %rax        # 4-byte Folded Reload
	movq	2224(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rsi,%rax,4)
	movq	5216(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	cmpl	2172(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_651
.LBB147_684:                            # %end for gH.s0.v10.v10330
                                        #   in Loop: Header=BB147_649 Depth=3
	movl	2172(%rsp), %eax        # 4-byte Reload
	cmpl	1280(%rsp), %eax        # 4-byte Folded Reload
	movq	4608(%rsp), %r8         # 8-byte Reload
	movq	4688(%rsp), %r12        # 8-byte Reload
	movq	4824(%rsp), %r14        # 8-byte Reload
	jge	.LBB147_703
# BB#685:                               # %for gH.s0.v10.v10333.preheader
                                        #   in Loop: Header=BB147_649 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3280(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1712(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 3248(%rsp)        # 8-byte Spill
	movl	1052(%rsp), %ecx        # 4-byte Reload
	movl	1108(%rsp), %r15d       # 4-byte Reload
	movl	1304(%rsp), %eax        # 4-byte Reload
	movl	%eax, %esi
	.align	16, 0x90
.LBB147_686:                            # %for gH.s0.v10.v10333
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_649 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rsi, 4128(%rsp)        # 8-byte Spill
	movl	%ecx, 4160(%rsp)        # 4-byte Spill
	movl	3280(%rsp), %eax        # 4-byte Reload
	testl	%eax, %eax
	setne	%dil
	sete	%r13b
	movslq	%r15d, %rcx
	movq	%rcx, 3776(%rsp)        # 8-byte Spill
	andl	$1, %r15d
	sete	%dl
	leaq	-1(%rcx), %r11
	imulq	%r8, %r11
	movq	%r11, 3808(%rsp)        # 8-byte Spill
	leaq	-3(%rcx), %r9
	imulq	%r8, %r9
	movq	%r9, 3840(%rsp)         # 8-byte Spill
	movl	%ecx, %ebx
	movq	%r14, %r10
	movq	%rsi, %r14
	movq	5248(%rsp), %rsi        # 8-byte Reload
	orl	%esi, %ebx
	testb	$1, %bl
	sete	5184(%rsp)              # 1-byte Folded Spill
	leaq	-2(%rcx), %rbx
	imulq	%r8, %rbx
	andb	%dil, %dl
	movq	4248(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%rbx), %rsi
	movq	%rsi, 3456(%rsp)        # 8-byte Spill
	andb	%r13b, %r15b
	testl	%ecx, %eax
	movq	%r8, %rsi
	setne	%r8b
	movq	%rcx, %r13
	imulq	%rsi, %r13
	leaq	(%rdi,%r13), %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	leaq	-4(%rcx), %rax
	imulq	%rsi, %rax
	leaq	(%r12,%rax), %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	leaq	(%r12,%r13), %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	leaq	(%r12,%rbx), %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	leaq	(%rax,%r10), %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movslq	%r14d, %rax
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	movq	%rax, %rcx
	orq	$4, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	leaq	(%r13,%r10), %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	leaq	(%rbx,%r10), %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	movq	%rax, %r10
	orq	$6, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm14
	vxorps	%xmm12, %xmm12, %xmm12
	vmovdqa	%xmm14, %xmm9
	cmpl	$1, 104(%rbp)
	leaq	(%rdi,%r11), %r11
	movq	4864(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r13d
	leaq	(%rdi,%r9), %r12
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3648(%rsp)        # 4-byte Spill
	je	.LBB147_688
# BB#687:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vpxor	%xmm9, %xmm9, %xmm9
.LBB147_688:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	movzbl	5184(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	movzbl	%r8b, %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %xmm10
	vmovdqa	%xmm10, 5184(%rsp)      # 16-byte Spill
	je	.LBB147_690
# BB#689:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vpxor	%xmm10, %xmm10, %xmm10
.LBB147_690:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vpbroadcastd	%xmm0, %xmm11
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm0
	vmovdqa	%xmm11, %xmm1
	movq	%rsi, %r8
	je	.LBB147_692
# BB#691:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vpxor	%xmm1, %xmm1, %xmm1
.LBB147_692:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	%xmm1, 3360(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm13
	vmovdqa	%xmm13, %xmm0
	je	.LBB147_694
# BB#693:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_694:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11,4), %rax
	movq	4680(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx,4), %rbx
	leaq	(%rbx,%rcx,4), %rdx
	vmovss	(%rdi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3264(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm5
	movslq	%r13d, %r9
	movq	5608(%rsp), %rax        # 8-byte Reload
	vmovups	12312(%rax,%r9,4), %xmm15
	vmovups	12328(%rax,%r9,4), %xmm0
	vshufps	$221, %xmm0, %xmm15, %xmm6 # xmm6 = xmm15[1,3],xmm0[1,3]
	vmovaps	5408(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm6, %xmm6
	vmovaps	5440(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm5, %xmm1
	leaq	(%rdi,%r12,4), %rdx
	leaq	(%rdx,%rcx,4), %rbx
	leaq	(%rbx,%rcx,4), %rsi
	vmovss	(%rdi,%r12,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rbx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm7, %xmm5
	vmovups	12304(%rax,%r9,4), %xmm6
	vmovups	12320(%rax,%r9,4), %xmm3
	vshufps	$221, %xmm3, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm3[1,3]
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm6
	movq	3456(%rsp), %rbx        # 8-byte Reload
	leaq	(%rdi,%rbx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %r11
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm7, %xmm1
	vshufps	$136, %xmm0, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm0[0,2]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	movq	3744(%rsp), %rbx        # 8-byte Reload
	leaq	(%rdi,%rbx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %r11
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%rcx, %rdx
	vmulps	%xmm1, %xmm7, %xmm1
	vshufps	$136, 12336(%rax,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm1, %xmm7
	cmpl	$0, 104(%rbp)
	je	.LBB147_696
# BB#695:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	%xmm9, %xmm11
.LBB147_696:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	%xmm14, %xmm9
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm7, %xmm7
	vaddps	%xmm5, %xmm6, %xmm5
	movq	4688(%rsp), %r12        # 8-byte Reload
	movq	4824(%rsp), %r14        # 8-byte Reload
	je	.LBB147_698
# BB#697:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	%xmm10, %xmm13
.LBB147_698:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovaps	%xmm6, 3744(%rsp)       # 16-byte Spill
	vmaxps	%xmm12, %xmm0, %xmm15
	vmaxps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3376(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm0
	vmovdqa	%xmm0, 3344(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	movq	%rdx, %rbx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	5152(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	movslq	3648(%rsp), %r11        # 4-byte Folded Reload
	vmovups	24592(%rax,%r11,4), %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovups	24608(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 3648(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm4, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm1
	movq	3536(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %r9
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm14, %xmm0
	vmovups	24624(%rax,%r11,4), %xmm6
	vmovaps	%xmm6, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm6[0,2]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm2
	movq	5216(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %r9
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm14, %xmm0
	vmovups	24600(%rax,%r11,4), %xmm6
	vmovaps	%xmm6, 3408(%rsp)       # 16-byte Spill
	vmovups	24616(%rax,%r11,4), %xmm7
	vmovaps	%xmm7, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm4, %xmm7, %xmm7
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm7
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vfmsub213ps	%xmm2, %xmm0, %xmm7
	vsubps	%xmm1, %xmm7, %xmm0
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vbroadcastss	.LCPI147_20(%rip), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm5, %xmm10
	vmovdqa	%xmm9, %xmm13
	je	.LBB147_700
# BB#699:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	3360(%rsp), %xmm13      # 16-byte Reload
.LBB147_700:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	movq	3552(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	32(%rax,%r10,4), %xmm11
	vshufps	$136, %xmm11, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm11[0,2]
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	movq	3584(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm4, %xmm5
	vmovups	48(%rax,%r10,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm11, %xmm3 # xmm3 = xmm11[0,2],xmm3[0,2]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	movq	3616(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r9
	vmovss	(%rdi,%rsi,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%rbx, %rsi
	vmulps	%xmm5, %xmm4, %xmm4
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm7
	vmovaps	%xmm7, 3312(%rsp)       # 16-byte Spill
	vmovups	40(%rax,%r10,4), %xmm5
	vshufps	$136, %xmm5, %xmm7, %xmm7 # xmm7 = xmm7[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm4
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vsubps	%xmm2, %xmm4, %xmm9
	vfmadd213ps	%xmm10, %xmm14, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm10, %xmm14, %xmm9
	vmovaps	%xmm14, 3584(%rsp)      # 16-byte Spill
	vaddps	3456(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 3712(%rsp)      # 16-byte Spill
	vmovdqa	3376(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm2
	vpsrad	$31, %xmm2, %xmm0
	vmovdqa	%xmm0, 3376(%rsp)       # 16-byte Spill
	je	.LBB147_702
# BB#701:                               # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	vmovdqa	3296(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5184(%rsp)       # 16-byte Spill
.LBB147_702:                            # %for gH.s0.v10.v10333
                                        #   in Loop: Header=BB147_686 Depth=4
	movq	3776(%rsp), %r10        # 8-byte Reload
	leaq	1(%r10), %r9
	imulq	%r8, %r9
	leaq	(%r12,%r9), %rcx
	leaq	(%rdi,%rcx,4), %rdx
	movq	%rsi, %rbx
	leaq	(%rdx,%rbx,4), %rsi
	leaq	(%rsi,%rbx,4), %rax
	vmovss	(%rdi,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	5152(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3648(%rsp), %xmm13      # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm13, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm13[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm10      # 16-byte Reload
	vsubps	%xmm10, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	movq	3808(%rsp), %r11        # 8-byte Reload
	leaq	(%r12,%r11), %rcx
	leaq	(%rdi,%rcx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	vmovss	(%rdi,%rcx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rsi,%rbx,4), %rcx
	vinsertps	$32, (%rsi,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm15, %xmm7
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm7
	vminps	%xmm8, %xmm4, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vfmsub213ps	%xmm7, %xmm0, %xmm2
	leaq	(%r9,%r14), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm4, %xmm14, %xmm4
	vshufps	$221, 3328(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm11[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3312(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm7, %xmm5 # xmm5 = xmm7[1,3],xmm5[1,3]
	leaq	(%r11,%r14), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm0, %xmm5
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm13[1,3]
	movq	3840(%rsp), %rdx        # 8-byte Reload
	leaq	(%r12,%rdx), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm15, %xmm4
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	3360(%rsp), %xmm2       # 16-byte Reload
	vmulps	3536(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	leaq	(%rdx,%r14), %rax
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm11[1,3]
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3584(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm2, %xmm1, %xmm0
	vfmadd213ps	%xmm2, %xmm1, %xmm4
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3712(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm1, %xmm6, %xmm12, %xmm2
	vmovaps	3376(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm9, %xmm2, %xmm2
	vmovaps	3744(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm5, %xmm3, %xmm12, %xmm5
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm1
	vmovaps	3456(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3488(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3248(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	4816(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	4128(%rsp), %rsi        # 8-byte Reload
	addl	$8, %esi
	addl	$8, %r10d
	movl	%r10d, %r15d
	movl	4160(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_686
.LBB147_703:                            # %end for gH.s0.v10.v10334
                                        #   in Loop: Header=BB147_649 Depth=3
	movl	1280(%rsp), %eax        # 4-byte Reload
	cmpl	2184(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_738
# BB#704:                               # %for gH.s0.v10.v10337.preheader
                                        #   in Loop: Header=BB147_649 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2112(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1712(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2104(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1104(%rsp), %ecx        # 4-byte Reload
	movl	1308(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_705:                            # %for gH.s0.v10.v10337
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_649 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r13, 3536(%rsp)        # 8-byte Spill
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	movl	%ecx, 3712(%rsp)        # 4-byte Spill
	cmpl	$0, 3840(%rsp)          # 4-byte Folded Reload
	setne	4192(%rsp)              # 1-byte Folded Spill
	sete	5152(%rsp)              # 1-byte Folded Spill
	movq	2088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r11d
	movl	%r11d, 3680(%rsp)       # 4-byte Spill
	movl	%r11d, %r15d
	andl	$1, %r15d
	sete	5184(%rsp)              # 1-byte Folded Spill
	movq	1880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	movq	1888(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3456(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	movq	1896(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	movq	2064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vmovd	4128(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 4160(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$2, 3648(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3616(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	movq	1992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm1
	vpinsrd	$1, 3424(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	movq	1904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%esi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$2, 3456(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3408(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3392(%rsp)        # 4-byte Spill
	vmovd	%r8d, %xmm4
	vpinsrd	$1, %r9d, %xmm4, %xmm4
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%edi
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	vpinsrd	$2, %r12d, %xmm4, %xmm4
	vpinsrd	$3, %r10d, %xmm4, %xmm4
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	movq	2000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	movq	2008(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vmovd	%r11d, %xmm2
	vpbroadcastd	%xmm2, %xmm13
	vpsrad	$31, %xmm1, %xmm7
	vmovdqa	2112(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm7, %xmm7
	vpaddd	%xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm4, %xmm7
	vpand	%xmm10, %xmm7, %xmm7
	vpaddd	%xmm4, %xmm7, %xmm4
	vmovdqa	5120(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm0, %xmm7
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm5, %xmm5
	vmovdqa	5344(%rsp), %xmm2       # 16-byte Reload
	vpmaxsd	%xmm2, %xmm5, %xmm5
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm14, %xmm0
	vmovdqa	5296(%rsp), %xmm9       # 16-byte Reload
	vpsubd	%xmm1, %xmm9, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm5, %xmm0, %xmm0
	vmovdqa	5360(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4160(%rsp)       # 16-byte Spill
	vmovdqa	5104(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm4, %xmm14, %xmm3
	vpsubd	%xmm4, %xmm9, %xmm5
	vblendvps	%xmm3, %xmm4, %xmm5, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4128(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	movq	5248(%rsp), %r10        # 8-byte Reload
	orl	%r10d, %eax
	testb	$1, %al
	movq	1912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vmovdqa	3648(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm4
	vpbroadcastd	3328(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm2, %xmm5, %xmm5
	vpcmpgtd	%xmm3, %xmm14, %xmm6
	vpsubd	%xmm3, %xmm9, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm4, %xmm5, %xmm3, %xmm3
	vpmulld	%xmm12, %xmm3, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	sete	3456(%rsp)              # 1-byte Folded Spill
	movb	5184(%rsp), %r12b       # 1-byte Reload
	movb	4192(%rsp), %r9b        # 1-byte Reload
	andb	%r9b, %r12b
	vpaddd	%xmm1, %xmm8, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	movb	5152(%rsp), %r14b       # 1-byte Reload
	andb	%r14b, %r15b
	movl	%r15d, 5184(%rsp)       # 4-byte Spill
	movl	3840(%rsp), %r8d        # 4-byte Reload
	testl	%r11d, %r8d
	setne	3408(%rsp)              # 1-byte Folded Spill
	movq	2024(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %ecx
	movl	%ecx, %r11d
	andl	$1, %r11d
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	sete	%r15b
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	vmovd	3552(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3584(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3488(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3616(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3392(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3424(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3376(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3344(%rsp), %xmm4, %xmm7 # 4-byte Folded Reload
	movq	2016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm4
	vpinsrd	$1, %esi, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm4
	vpaddd	%xmm11, %xmm13, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm2, %xmm6, %xmm6
	vpcmpgtd	%xmm0, %xmm14, %xmm1
	vpsubd	%xmm0, %xmm9, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm6, %xmm0, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm4
	vpaddd	%xmm4, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	orl	%r10d, %eax
	testb	$1, %al
	sete	2544(%rsp)              # 1-byte Folded Spill
	andb	%r9b, %r15b
	movb	%r15b, 4192(%rsp)       # 1-byte Spill
	andb	%r14b, %r11b
	testl	%ecx, %r8d
	vmovd	%ecx, %xmm0
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm6
	vpsrad	$31, %xmm7, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm3
	vpsubd	%xmm1, %xmm9, %xmm7
	vblendvps	%xmm3, %xmm1, %xmm7, %xmm1
	vmovdqa	4944(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm3, %xmm3
	vpbroadcastd	3616(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm11, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm2, %xmm7, %xmm7
	vpaddd	%xmm2, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm2, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm7, %xmm1, %xmm1
	vpsrad	$31, %xmm5, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm9, %xmm7
	vblendvps	%xmm5, %xmm3, %xmm7, %xmm3
	vmovdqa	5040(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm5, %xmm5
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm2, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm2, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm1, %xmm1
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm2
	setne	%cl
	vmovq	%xmm2, %r15
	movq	%r15, 2304(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 2320(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm2, %rsi
	movq	%rsi, 2352(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vmovdqa	3648(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm2
	vmovq	%xmm2, %r8
	movq	%r8, 2368(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm2, %r13
	movq	%r13, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2432(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2464(%rsp)        # 8-byte Spill
	movq	2048(%rsp), %rdx        # 8-byte Reload
	movq	5216(%rsp), %rbx        # 8-byte Reload
	leal	(%rdx,%rbx), %edi
	movslq	%edi, %rdi
	movq	%rdi, %rdx
	orq	$4, %rdx
	movq	%rdx, 2448(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2480(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2512(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2496(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2528(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2608(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2592(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	movq	%rdi, %rdx
	orq	$6, %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	vmovdqa	4128(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2816(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 2832(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3120(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3104(%rsp)        # 8-byte Spill
	vmovdqa	4160(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm6, %xmm11
	vmovaps	%xmm11, 2576(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	movq	2032(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rbx), %r10d
	movq	2040(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rbx), %edx
	movl	%edx, 2288(%rsp)        # 4-byte Spill
	je	.LBB147_707
# BB#706:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_707:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	movzbl	3456(%rsp), %r12d       # 1-byte Folded Reload
	vmovd	%r12d, %xmm1
	movzbl	3408(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_709
# BB#708:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_709:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movl	5184(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm1
	movq	4816(%rsp), %r12        # 8-byte Reload
	je	.LBB147_711
# BB#710:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_711:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm0, 2192(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	je	.LBB147_713
# BB#712:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_713:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm0, 2208(%rsp)       # 16-byte Spill
	movzbl	4192(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_715
# BB#714:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_715:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm1, 2224(%rsp)       # 16-byte Spill
	movzbl	2544(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	je	.LBB147_717
# BB#716:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_717:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm2, 2240(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5152(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm1
	je	.LBB147_719
# BB#718:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_719:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm3, 2544(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2256(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_721
# BB#720:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_721:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm0, 2272(%rsp)       # 16-byte Spill
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movq	2656(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2688(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2672(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	movslq	%r10d, %rcx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm5
	vmovups	12328(%rdx,%rcx,4), %xmm6
	vmovups	12304(%rdx,%rcx,4), %xmm7
	vmovups	12320(%rdx,%rcx,4), %xmm0
	vshufps	$136, 12336(%rdx,%rcx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,2],mem[0,2]
	vmovaps	3808(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm4 # xmm4 = xmm5[1,3],xmm6[1,3]
	vmovaps	5408(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm4, %xmm4
	vmovaps	5440(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm4, %xmm6
	vmulps	%xmm14, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm1, %xmm7
	vsubps	%xmm2, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_723
# BB#722:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm11, 4160(%rsp)      # 16-byte Spill
.LBB147_723:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3808(%rsp), %xmm15, %xmm14 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_725
# BB#724:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	%xmm8, 4128(%rsp)       # 16-byte Spill
.LBB147_725:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	movq	2304(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2320(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2336(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2352(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rsi,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2368(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2384(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r13,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2288(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24592(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vmovups	24608(%rdx,%rax,4), %xmm13
	vmovups	24624(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
	vmovups	24600(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 2752(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovdqa	2576(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_727
# BB#726:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	2192(%rsp), %xmm2       # 16-byte Reload
.LBB147_727:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rbx, %rcx
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2432(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2464(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rsi
	movq	2448(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	movq	2480(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2512(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2496(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2528(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2560(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2608(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2592(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%rcx, %rdx
	movq	2640(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3248(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rdi,4), %xmm10
	vmovups	48(%rsi,%rdi,4), %xmm9
	vmovaps	%xmm9, 3200(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rdi,4), %xmm14
	vmovaps	%xmm14, 3232(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3744(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vmulps	5184(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	2736(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	vmovdqa	2544(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_729
# BB#728:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	2208(%rsp), %xmm6       # 16-byte Reload
.LBB147_729:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_731
# BB#730:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	2224(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5152(%rsp)       # 16-byte Spill
.LBB147_731:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	4128(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_733
# BB#732:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovaps	2240(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
.LBB147_733:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2784(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2832(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2768(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3008(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3216(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3104(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3152(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3280(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5184(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4160(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3456(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_735
# BB#734:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	2256(%rsp), %xmm13      # 16-byte Reload
.LBB147_735:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3328(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3200(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3584(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3296(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3408(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_737
# BB#736:                               # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vmovdqa	2272(%rsp), %xmm1       # 16-byte Reload
.LBB147_737:                            # %for gH.s0.v10.v10337
                                        #   in Loop: Header=BB147_705 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3680(%rsp), %rax        # 4-byte Folded Reload
	movq	2104(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r12,%rax,4)
	movq	5216(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	3536(%rsp), %r13        # 8-byte Reload
	addl	$8, %r13d
	movl	3712(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_705
.LBB147_738:                            # %end for gH.s0.v10.v10338
                                        #   in Loop: Header=BB147_649 Depth=3
	movl	1376(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 1376(%rsp)        # 4-byte Spill
	addq	$1, 5248(%rsp)          # 8-byte Folded Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	addl	%eax, 1304(%rsp)        # 4-byte Folded Spill
	addl	%eax, 1308(%rsp)        # 4-byte Folded Spill
	cmpl	1188(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_649
.LBB147_739:                            # %end for gH.s0.v11328
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	2176(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1364(%rsp)        # 4-byte Folded Reload
	jle	.LBB147_777
# BB#740:                               #   in Loop: Header=BB147_466 Depth=2
	movq	1616(%rsp), %rax        # 8-byte Reload
	movq	880(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rax), %eax
	imull	1768(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 2104(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_741:                            # %for gH.s0.v11341
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_743 Depth 4
	cmpl	$0, 2184(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_776
# BB#742:                               # %for gH.s0.v10.v10344.preheader
                                        #   in Loop: Header=BB147_741 Depth=3
	movl	2176(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %edi
	movl	%r8d, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	andl	$1, %edi
	movl	%edi, 4128(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1740(%rsp), %ebx        # 4-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1784(%rsp), %rdi        # 8-byte Reload
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%esi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movq	1744(%rsp), %rax        # 8-byte Reload
	cmpl	%r8d, %eax
	cmovgl	%ecx, %edx
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2192(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	imulq	1712(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2112(%rsp)        # 8-byte Spill
	movl	2184(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %r10        # 8-byte Reload
	movl	2104(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_743:                            # %for gH.s0.v10.v10344
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_741 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 4128(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r10d, %r14d
	andl	$1, %r14d
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	3080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4160(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3408(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	movq	3064(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movq	3072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	4160(%rsp), %xmm1       # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpinsrd	$1, 4192(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$2, 3712(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3680(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	%xmm1, 3456(%rsp)       # 16-byte Spill
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%r15d, %xmm0
	vpinsrd	$1, 3552(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpinsrd	$3, 3536(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r8d, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r13d, %xmm2, %xmm2
	vpinsrd	$2, %r11d, %xmm2, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, 3424(%rsp)        # 4-byte Spill
	vpinsrd	$3, %r9d, %xmm2, %xmm5
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3392(%rsp)        # 4-byte Spill
	leal	-3(%r10), %eax
	vmovd	%eax, %xmm11
	vmovd	%r10d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2192(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm0
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm9, %xmm7, %xmm7
	vmovdqa	5120(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm3
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm3, %xmm3
	vmovdqa	5072(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	5296(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vmovdqa	5344(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm6, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vmovdqa	5360(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm12, %xmm0, %xmm1
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm7, %xmm0
	vmovdqa	5104(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm1, %xmm7, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	4960(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm11, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	%xmm0, 4160(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	movl	%r10d, %eax
	movl	2176(%rsp), %r15d       # 4-byte Reload
	orl	%r15d, %eax
	testb	$1, %al
	movq	3088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vmovdqa	3456(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm9, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	5088(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vpbroadcastd	3712(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm12, %xmm3, %xmm1
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	sete	3536(%rsp)              # 1-byte Folded Spill
	movb	5216(%rsp), %r13b       # 1-byte Reload
	movb	5152(%rsp), %r9b        # 1-byte Reload
	andb	%r9b, %r13b
	vpaddd	%xmm1, %xmm7, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	movb	5184(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, %r14b
	movl	%r14d, 5216(%rsp)       # 4-byte Spill
	movl	4128(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3456(%rsp)              # 1-byte Folded Spill
	leal	1(%r10), %ebx
	movl	%ebx, %r12d
	andl	$1, %r12d
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	sete	%r14b
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	3408(%rsp)              # 4-byte Folded Reload
	vmovd	3616(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 3648(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 3584(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3680(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	3488(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 3552(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 3424(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 3392(%rsp), %xmm4, %xmm3 # 4-byte Folded Reload
	leal	-4(%r10), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %esi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm5
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	5056(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vmovdqa	4992(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm0, %xmm14, %xmm6
	vpsubd	%xmm0, %xmm8, %xmm1
	vblendvps	%xmm6, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm2, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm6
	vpaddd	%xmm6, %xmm7, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	movl	%ebx, %eax
	orl	%r15d, %eax
	testb	$1, %al
	sete	2576(%rsp)              # 1-byte Folded Spill
	andb	%r9b, %r14b
	movb	%r14b, 5152(%rsp)       # 1-byte Spill
	andb	%r11b, %r12b
	testl	%ebx, %r8d
	vmovd	%ebx, %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm11
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm0, %xmm0
	vmovdqa	4944(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	4784(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm7
	vpor	%xmm1, %xmm7, %xmm1
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	3680(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm15, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm1
	vpsrad	$31, %xmm5, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vpcmpgtd	%xmm0, %xmm14, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm0
	vmovdqa	5040(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm2
	vpor	%xmm5, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	5184(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm2, %xmm0, %xmm5, %xmm0
	vpmulld	%xmm12, %xmm0, %xmm0
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm1, %xmm3, %xmm2
	setne	%r11b
	vmovq	%xmm2, %rax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm2, %r13
	movq	%r13, 2336(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vpaddd	%xmm6, %xmm3, %xmm2
	vmovq	%xmm2, %r8
	movq	%r8, 2352(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 2368(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vmovdqa	3712(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovq	%xmm2, %rdx
	movq	%rdx, 2384(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	vpextrq	$1, %xmm2, %rcx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovq	%xmm1, %rcx
	movq	%rcx, 2432(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 2448(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rcx
	movq	%rcx, %rsi
	orq	$4, %rsi
	movq	%rsi, 2496(%rsp)        # 8-byte Spill
	vpaddd	%xmm6, %xmm2, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 2512(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 2544(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 2528(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 2560(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 2592(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 2640(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 2656(%rsp)        # 8-byte Spill
	movq	%rcx, %rsi
	orq	$6, %rsi
	movq	%rsi, 2672(%rsp)        # 8-byte Spill
	vmovdqa	4160(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3008(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3136(%rsp)        # 8-byte Spill
	vmovdqa	4192(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3200(%rsp)        # 8-byte Spill
	vpaddd	%xmm5, %xmm2, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rsi, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3680(%rsp)        # 8-byte Spill
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rsi
	movq	%rsi, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm2, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rsi
	movq	%rsi, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	movq	%rsi, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm11, %xmm11
	vmovaps	%xmm11, 2608(%rsp)      # 16-byte Spill
	vpxor	%xmm9, %xmm9, %xmm9
	cmpl	$1, 104(%rbp)
	movq	4864(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rdi), %r14d
	movq	4872(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rdi), %esi
	movl	%esi, 2304(%rsp)        # 4-byte Spill
	je	.LBB147_745
# BB#744:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm11, %xmm11, %xmm11
.LBB147_745:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	movzbl	3536(%rsp), %r15d       # 1-byte Folded Reload
	vmovd	%r15d, %xmm1
	movzbl	3456(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm3
	vmovaps	%xmm3, %xmm8
	je	.LBB147_747
# BB#746:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm8, %xmm8, %xmm8
.LBB147_747:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movl	5216(%rsp), %esi        # 4-byte Reload
	movzbl	%sil, %ebx
	vmovd	%ebx, %xmm1
	je	.LBB147_749
# BB#748:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_749:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm0, 2208(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	je	.LBB147_751
# BB#750:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_751:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm0, 2224(%rsp)       # 16-byte Spill
	movzbl	5152(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, %xmm1
	je	.LBB147_753
# BB#752:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_753:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm1, 2240(%rsp)       # 16-byte Spill
	movzbl	2576(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	movzbl	%r11b, %ebx
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	je	.LBB147_755
# BB#754:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_755:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm2, 2256(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	movzbl	%r12b, %ebx
	vmovd	%ebx, %xmm1
	je	.LBB147_757
# BB#756:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_757:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm3, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2272(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movq	4816(%rsp), %r11        # 8-byte Reload
	je	.LBB147_759
# BB#758:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_759:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm0, 2288(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	movq	5464(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3312(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3328(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdi,%rsi,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	movq	3232(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3248(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3264(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdi,%rsi,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	movq	2752(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2768(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2784(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdi,%rsi,4), %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],mem[0]
	movq	2688(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2736(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2704(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdi,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2720(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdi,%rsi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movslq	%r14d, %rbx
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	12312(%rsi,%rbx,4), %xmm5
	vmovups	12328(%rsi,%rbx,4), %xmm6
	vmovups	12304(%rsi,%rbx,4), %xmm7
	vmovups	12320(%rsi,%rbx,4), %xmm0
	vshufps	$136, 12336(%rsi,%rbx,4), %xmm0, %xmm15 # xmm15 = xmm0[0,2],mem[0,2]
	vmovaps	3840(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm12, %xmm10, %xmm12
	vshufps	$221, %xmm6, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm6[1,3]
	vmovaps	5408(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	5440(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vmulps	%xmm13, %xmm10, %xmm7
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm6, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm6[0,2]
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm0, %xmm5
	vminps	%xmm12, %xmm1, %xmm6
	vmulps	%xmm4, %xmm10, %xmm4
	vsubps	%xmm2, %xmm7, %xmm0
	vmulps	%xmm0, %xmm3, %xmm7
	vsubps	%xmm2, %xmm15, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_761
# BB#760:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm11, 4192(%rsp)      # 16-byte Spill
.LBB147_761:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmaxps	%xmm9, %xmm5, %xmm10
	vmaxps	%xmm9, %xmm6, %xmm0
	vmulps	3840(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm7, %xmm7
	je	.LBB147_763
# BB#762:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	%xmm8, 4160(%rsp)       # 16-byte Spill
.LBB147_763:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	movq	2320(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdi,%rbx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2336(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%r13,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2352(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdi,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2368(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%r9,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	movq	2384(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2400(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdi,%rax,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	movslq	2304(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24592(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vmovups	24608(%rsi,%rax,4), %xmm13
	vmovups	24624(%rsi,%rax,4), %xmm3
	vmovaps	%xmm3, 3248(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rax,4), %xmm2
	vmovaps	%xmm2, 3312(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm0, %xmm6
	vmovaps	%xmm6, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm14, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm7, %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm0
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm11, %xmm6, %xmm3
	vshufps	$136, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm15
	vbroadcastss	.LCPI147_18(%rip), %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovdqa	2608(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_765
# BB#764:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	2208(%rsp), %xmm2       # 16-byte Reload
.LBB147_765:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	movq	2432(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rdi, %rdx
	vmovss	(%rdx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2464(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2448(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2480(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	2496(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm6
	vmovaps	%xmm6, 3328(%rsp)       # 16-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	2544(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	2528(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	2560(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	2592(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2640(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2624(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2656(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	2672(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3280(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm10
	vmovups	48(%rsi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3232(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rcx,4), %xmm14
	vmovaps	%xmm14, 3264(%rsp)      # 16-byte Spill
	vfmsub213ps	%xmm1, %xmm11, %xmm15
	vmovaps	3776(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vshufps	$136, %xmm10, %xmm6, %xmm5 # xmm5 = xmm6[0,2],xmm10[0,2]
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$136, %xmm9, %xmm10, %xmm5 # xmm5 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm4, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vbroadcastss	.LCPI147_19(%rip), %xmm14
	vmovaps	2784(%rsp), %xmm3       # 16-byte Reload
	vmulps	5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm9, %xmm4, %xmm8
	vmovdqa	2576(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_767
# BB#766:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	2224(%rsp), %xmm6       # 16-byte Reload
.LBB147_767:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vsubps	%xmm0, %xmm15, %xmm0
	vpslld	$31, %xmm2, %xmm7
	vfmadd213ps	%xmm5, %xmm14, %xmm1
	vmaxps	%xmm9, %xmm3, %xmm4
	vpslld	$31, %xmm6, %xmm3
	vblendvps	%xmm3, %xmm8, %xmm9, %xmm3
	je	.LBB147_769
# BB#768:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	2240(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	%xmm2, 5184(%rsp)       # 16-byte Spill
.LBB147_769:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	4160(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm6
	vfmadd213ps	%xmm5, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm3, %xmm3
	vaddps	%xmm4, %xmm8, %xmm15
	je	.LBB147_771
# BB#770:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovaps	2256(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
.LBB147_771:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vblendvps	%xmm6, %xmm0, %xmm3, %xmm7
	movq	2816(%rsp), %rax        # 8-byte Reload
	cltq
	movq	2832(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3008(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2800(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3120(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vshufps	$221, 3248(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm13[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3152(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3136(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3184(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3312(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm11, %xmm4
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm0
	vmulps	5216(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	3536(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_773
# BB#772:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	2272(%rsp), %xmm13      # 16-byte Reload
.LBB147_773:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vblendvps	%xmm1, %xmm8, %xmm7, %xmm8
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$221, 3232(%rsp), %xmm10, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm10[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	5632(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3280(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3264(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovss	(%rdx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vfmsub213ps	%xmm4, %xmm5, %xmm11
	movq	3648(%rsp), %rax        # 8-byte Reload
	cltq
	movq	3616(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovaps	3328(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm10[1,3]
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm2, %xmm11, %xmm2
	vfmadd213ps	%xmm3, %xmm14, %xmm0
	vfmadd213ps	%xmm3, %xmm14, %xmm2
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm4
	vpslld	$31, %xmm13, %xmm5
	vmovdqa	3456(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_775
# BB#774:                               # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vmovdqa	2288(%rsp), %xmm1       # 16-byte Reload
.LBB147_775:                            # %for gH.s0.v10.v10344
                                        #   in Loop: Header=BB147_743 Depth=4
	vpslld	$31, %xmm1, %xmm6
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vblendvps	%xmm6, %xmm1, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r10d, %rax
	movq	2112(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r11,%rax,4)
	movq	5248(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r10d
	movl	3744(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_743
.LBB147_776:                            # %end for gH.s0.v10.v10345
                                        #   in Loop: Header=BB147_741 Depth=3
	movl	2176(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2176(%rsp)        # 4-byte Spill
	movl	2104(%rsp), %ecx        # 4-byte Reload
	addl	1768(%rsp), %ecx        # 4-byte Folded Reload
	movl	%ecx, 2104(%rsp)        # 4-byte Spill
	cmpl	1364(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_741
.LBB147_777:                            # %produce gV349
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1752(%rsp), %rax        # 8-byte Reload
	movq	1000(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %edx
	cmovgel	%eax, %edx
	addl	$2, %edx
	movl	1364(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	movl	%edx, 2576(%rsp)        # 4-byte Spill
	leal	3(%rcx), %ecx
	movl	%ecx, 2320(%rsp)        # 4-byte Spill
	movl	976(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	movl	1372(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r8d
	cmpl	%edx, %eax
	movl	2184(%rsp), %edx        # 4-byte Reload
	jl	.LBB147_778
	jmp	.LBB147_780
.LBB147_779:                            # %for gV.s0.v11351.end for gV.s0.v10.v10354_crit_edge
                                        #   in Loop: Header=BB147_778 Depth=3
	addl	$1, %r8d
	movl	%r8d, %eax
	jmp	.LBB147_841
	.align	16, 0x90
.LBB147_778:                            # %for gV.s0.v11351
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_823 Depth 4
	testl	%edx, %edx
	jle	.LBB147_779
# BB#822:                               # %for gV.s0.v10.v10353.preheader
                                        #   in Loop: Header=BB147_778 Depth=3
	movq	%r8, 2816(%rsp)         # 8-byte Spill
	movl	%r8d, %ebx
	movq	1752(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ebx
	leal	-1(%rbx), %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edi
	movl	%ecx, %r11d
	subl	%eax, %edi
	movq	1784(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %esi
	cmovgl	%r8d, %esi
	addl	$-1, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ecx
	cmovll	%edi, %esi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %ebx
	cmovgl	%r8d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %ecx
	cmovlel	%edx, %ebx
	movl	%r8d, %ecx
	subl	%r9d, %ecx
	cmovll	%edx, %ebx
	cmovlel	%edi, %esi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r12d, %edi
	addl	%edx, %edi
	movl	%r11d, %eax
	subl	%edi, %eax
	cmpl	%edi, %r10d
	cmovgl	%edi, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	leal	1(%r8), %edi
	movl	%edi, 2560(%rsp)        # 4-byte Spill
	cmpl	%edi, %r13d
	movl	%r13d, %edx
	cmovgl	%edi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%eax, %edx
	movl	%r8d, %edi
	andl	$1, %edi
	movl	%edi, 5184(%rsp)        # 4-byte Spill
	movslq	%ebx, %r11
	movq	1816(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2784(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rbx
	movq	%rbx, 5248(%rsp)        # 8-byte Spill
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5248(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r11), %r12
	movl	1796(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %edx
	subl	%esi, %edx
	cmpl	%esi, %r10d
	cmovgl	%esi, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	leal	2(%r8), %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %esi
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rax,%rdx), %r15
	subl	%ecx, %ebx
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	leal	-2(%r8), %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ebx, %ecx
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %ecx
	movslq	%ecx, %rcx
	imulq	%r14, %rcx
	leaq	(%rax,%rcx), %rbx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r15,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1712(%rsp), %rcx        # 8-byte Folded Reload
	movq	1512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	movq	1424(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2704(%rsp)        # 8-byte Spill
	subq	4712(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1616(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2640(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	xorl	%r8d, %r8d
	movl	2184(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_823:                            # %for gV.s0.v10.v10353
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_778 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 5184(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %r9d
	movl	%r9d, 3712(%rsp)        # 4-byte Spill
	movl	%r9d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r15d
	cltd
	idivl	%r15d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vmovd	%esi, %xmm0
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	vpinsrd	$1, %r10d, %xmm0, %xmm0
	vpinsrd	$2, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r14d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2784(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r9d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5328(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	movq	2816(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	sete	%r10b
	andb	5216(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5248(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5184(%rsp), %r9d        # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r12
	movq	%r12, %r13
	sarq	$32, %r13
	vpextrq	$1, %xmm2, %r11
	movq	%r11, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm0, %r9
	movq	%r9, 3184(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movq	2624(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %r14d
	movq	2592(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3120(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3200(%rsp)        # 4-byte Spill
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3216(%rsp)        # 4-byte Spill
	movq	2608(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3232(%rsp)        # 4-byte Spill
	je	.LBB147_825
# BB#824:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_825:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movzbl	%r10b, %r10d
	vmovd	%r10d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_827
# BB#826:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_827:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_829
# BB#828:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_829:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3008(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_831
# BB#830:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_831:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r14d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	movslq	3120(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r12d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r11d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5408(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2752(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_833
# BB#832:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovdqa	2848(%rsp), %xmm9       # 16-byte Reload
.LBB147_833:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5408(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4696(%rsp), %rcx        # 8-byte Reload
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3680(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3248(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_835
# BB#834:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovdqa	2832(%rsp), %xmm7       # 16-byte Reload
.LBB147_835:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	3360(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4160(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3232(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5152(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_837
# BB#836:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovdqa	3008(%rsp), %xmm6       # 16-byte Reload
.LBB147_837:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3776(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	je	.LBB147_839
# BB#838:                               # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovdqa	3104(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
.LBB147_839:                            # %for gV.s0.v10.v10353
                                        #   in Loop: Header=BB147_823 Depth=4
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3424(%rsp), %xmm11      # 16-byte Reload
	vmulps	4160(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4128(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3648(%rsp), %xmm7       # 16-byte Reload
	vmulps	3808(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3776(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vmulps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3552(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3536(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3584(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5248(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3712(%rsp), %rax        # 4-byte Folded Reload
	movq	2736(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	movl	3744(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_823
# BB#840:                               #   in Loop: Header=BB147_778 Depth=3
	movl	2560(%rsp), %eax        # 4-byte Reload
.LBB147_841:                            # %end for gV.s0.v10.v10354
                                        #   in Loop: Header=BB147_778 Depth=3
	movl	%eax, %r8d
	cmpl	2576(%rsp), %eax        # 4-byte Folded Reload
	movl	2184(%rsp), %edx        # 4-byte Reload
	jne	.LBB147_778
.LBB147_780:                            # %end for gV.s0.v11352
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	2800(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, 2576(%rsp)        # 4-byte Folded Reload
	movq	4696(%rsp), %rsi        # 8-byte Reload
	jge	.LBB147_862
# BB#781:                               #   in Loop: Header=BB147_466 Depth=2
	movl	892(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	jmp	.LBB147_782
.LBB147_821:                            # %end for gV.s0.v10.v10364.end for gV.s0.v10.v10368_crit_edge
                                        #   in Loop: Header=BB147_782 Depth=3
	movq	5184(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	jmp	.LBB147_861
	.align	16, 0x90
.LBB147_782:                            # %for gV.s0.v11357
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_784 Depth 4
                                        #         Child Loop BB147_803 Depth 4
                                        #         Child Loop BB147_843 Depth 4
	cmpl	$0, 1264(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_801
# BB#783:                               # %for gV.s0.v10.v10359.preheader
                                        #   in Loop: Header=BB147_782 Depth=3
	movq	5184(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	andl	$1, %eax
	movl	%eax, 5152(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2784(%rsp)       # 16-byte Spill
	movq	%r10, %rax
	movq	1816(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rbx
	imulq	%rbx, %rax
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rdi
	leaq	1(%r10), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rcx), %rdx
	leaq	-1(%r10), %rsi
	imulq	%rbx, %rsi
	leaq	(%rsi,%rcx), %rsi
	movq	1824(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rcx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rax), %r8
	leaq	2(%r10), %rdx
	imulq	%rbx, %rdx
	leaq	(%rsi,%rdx), %r9
	leaq	-2(%r10), %rdi
	imulq	%rbx, %rdi
	leaq	(%rsi,%rdi), %rbx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rdi,%rsi), %rsi
	vbroadcastss	(%rcx,%rbx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r9,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r8,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rsi,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	movl	%r10d, %ecx
	andl	$63, %ecx
	imulq	1712(%rsp), %rcx        # 8-byte Folded Reload
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %edi
	movq	1688(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %edi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %ebx
	imull	%esi, %ebx
	movq	%rbx, 2704(%rsp)        # 8-byte Spill
	subq	4712(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdx
	leal	(%rdx,%rdi), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	imull	%esi, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	imull	%esi, %eax
	movq	1472(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r10), %edi
	imull	%esi, %edi
	movq	%rdi, 2640(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	xorl	%r8d, %r8d
	movq	1128(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_784:                            # %for gV.s0.v10.v10359
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_782 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3712(%rsp)        # 4-byte Spill
	cmpl	$0, 5152(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %r14d
	movl	%r14d, 3680(%rsp)       # 4-byte Spill
	movl	%r14d, %r13d
	andl	$1, %r13d
	sete	%r12b
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r15d
	cltd
	idivl	%r15d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r11d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2784(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5328(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	5184(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	sete	%r11b
	andb	5216(%rsp), %r12b       # 1-byte Folded Reload
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5248(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5152(%rsp), %r14d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r15
	movq	%r15, %r9
	sarq	$32, %r9
	vpextrq	$1, %xmm2, %r12
	movq	%r12, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpextrq	$1, %xmm0, %r14
	movq	%r14, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movq	2624(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %r13d
	movq	2592(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3104(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3184(%rsp)        # 4-byte Spill
	movq	2672(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3200(%rsp)        # 4-byte Spill
	movq	2608(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r8), %ebx
	movl	%ebx, 3216(%rsp)        # 4-byte Spill
	je	.LBB147_786
# BB#785:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_786:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	movzbl	%r11b, %r11d
	vmovd	%r11d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_788
# BB#787:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_788:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_790
# BB#789:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_790:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3648(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	je	.LBB147_792
# BB#791:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_792:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3312(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3248(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3328(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3264(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r13d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	movslq	3104(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r15d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r12d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5408(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2752(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	4192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_794
# BB#793:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovdqa	2832(%rsp), %xmm9       # 16-byte Reload
.LBB147_794:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5408(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4696(%rsp), %rsi        # 8-byte Reload
	vmovaps	4160(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3648(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3232(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_796
# BB#795:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovdqa	2816(%rsp), %xmm7       # 16-byte Reload
.LBB147_796:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	3344(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3184(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4128(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	4192(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_798
# BB#797:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovdqa	2848(%rsp), %xmm6       # 16-byte Reload
.LBB147_798:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm5       # 16-byte Reload
	vmulps	3808(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3776(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3248(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3744(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3392(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	je	.LBB147_800
# BB#799:                               # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovdqa	3008(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
.LBB147_800:                            # %for gV.s0.v10.v10359
                                        #   in Loop: Header=BB147_784 Depth=4
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3408(%rsp), %xmm11      # 16-byte Reload
	vmulps	4128(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3264(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	3840(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vmulps	3776(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3744(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4160(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3328(%rsp), %xmm2       # 16-byte Reload
	vmulps	3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3808(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3536(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3488(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3648(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3552(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5248(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3680(%rsp), %rax        # 4-byte Folded Reload
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rsi,%rax,4)
	addl	$8, %r8d
	movl	3712(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	cmpl	$-1, %eax
	jne	.LBB147_784
.LBB147_801:                            # %end for gV.s0.v10.v10360
                                        #   in Loop: Header=BB147_782 Depth=3
	movl	1260(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1264(%rsp)        # 4-byte Folded Reload
	movq	4608(%rsp), %r8         # 8-byte Reload
	movq	4824(%rsp), %rdx        # 8-byte Reload
	jge	.LBB147_820
# BB#802:                               # %for gV.s0.v10.v10363.preheader
                                        #   in Loop: Header=BB147_782 Depth=3
	movq	5184(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %eax
	movq	%r14, %r11
	movq	1816(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rbx
	imulq	%rbx, %r11
	andl	$1, %eax
	movl	%eax, 3216(%rsp)        # 4-byte Spill
	leaq	1(%r14), %rcx
	imulq	%rbx, %rcx
	movq	1776(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r9
	leaq	-1(%r14), %rsi
	imulq	%rbx, %rsi
	leaq	(%rcx,%rdi), %rcx
	leaq	(%rsi,%rdi), %rsi
	movq	1824(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r9,4), %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	leaq	2(%r14), %rcx
	imulq	%rbx, %rcx
	movq	1808(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r9
	leaq	-2(%r14), %rsi
	imulq	%rbx, %rsi
	leaq	(%rdi,%rcx), %r10
	leaq	(%rdi,%rsi), %rbx
	movq	1800(%rsp), %rdi        # 8-byte Reload
	leaq	(%r11,%rdi), %r11
	leaq	(%rcx,%rdi), %rcx
	leaq	(%rsi,%rdi), %rsi
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r9,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r14d, %eax
	andl	$63, %eax
	imulq	1712(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %ecx
	movq	1688(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %ecx
	movq	2152(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rcx), %eax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	addl	$-8, %ecx
	movq	%rcx, 3136(%rsp)        # 8-byte Spill
	leal	(%rsi,%rax), %ecx
	movq	%rcx, 3008(%rsp)        # 8-byte Spill
	addl	$-8, %eax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	movq	2160(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	imull	%edi, %eax
	movq	1472(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r14), %ebx
	imull	%edi, %ebx
	movq	%rdx, %rdi
	leal	(%rax,%rcx), %eax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	addl	$-8, %ebx
	movq	%rbx, 2832(%rsp)        # 8-byte Spill
	movl	1048(%rsp), %ecx        # 4-byte Reload
	movl	1100(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r13d
	.align	16, 0x90
.LBB147_803:                            # %for gV.s0.v10.v10363
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_782 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	movl	3216(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	setne	%r9b
	sete	%r10b
	movq	3000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r15d
	movslq	%r15d, %r14
	movq	%r14, 3712(%rsp)        # 8-byte Spill
	andl	$1, %r15d
	sete	%al
	leaq	-2(%r14), %rsi
	imulq	%r8, %rsi
	leaq	(%rdi,%rsi), %r11
	movq	4248(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rsi), %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	movq	4688(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx), %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	movl	%r14d, %esi
	movq	%rdi, %rdx
	movq	5184(%rsp), %rdi        # 8-byte Reload
	orl	%edi, %esi
	testb	$1, %sil
	movq	%r8, %rsi
	sete	%r8b
	andb	%r9b, %al
	andb	%r10b, %r15b
	testl	%r14d, %r12d
	setne	%r10b
	leaq	-1(%r14), %rdi
	imulq	%rsi, %rdi
	leaq	(%rdx,%rdi), %r12
	leaq	(%rbx,%rdi), %r14
	leaq	(%rdi,%rcx), %rcx
	movq	%rcx, 3424(%rsp)        # 8-byte Spill
	movq	3136(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ebx
	movslq	%ebx, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	movq	3104(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ebx
	movslq	%ebx, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movq	2832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movq	2816(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movq	2768(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movq	3120(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3376(%rsp)        # 4-byte Spill
	movq	3008(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3392(%rsp)        # 4-byte Spill
	movq	2784(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r13), %edx
	movl	%edx, 3408(%rsp)        # 4-byte Spill
	je	.LBB147_805
# BB#804:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_805:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movzbl	%r8b, %ebx
	vmovd	%ebx, %xmm0
	movzbl	%r10b, %ebx
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_807
# BB#806:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_807:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movzbl	%r15b, %ebx
	vmovd	%ebx, %xmm0
	vmovaps	%xmm3, %xmm1
	movq	%rsi, %r8
	je	.LBB147_809
# BB#808:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_809:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_811
# BB#810:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_811:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r11,4), %rbx
	movq	4680(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rdx,4), %rbx
	vinsertps	$16, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rbx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rbx,4), %xmm15
	vmovups	12328(%rdx,%rbx,4), %xmm7
	cltq
	vmovups	12312(%rdx,%rax,4), %xmm4
	vmovups	12328(%rdx,%rax,4), %xmm9
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	movslq	%ecx, %rax
	vmovups	12312(%rdx,%rax,4), %xmm1
	vmovups	12328(%rdx,%rax,4), %xmm3
	vmovss	(%rdi,%r12,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r12,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovaps	3200(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm5, %xmm0
	vshufps	$136, %xmm7, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm7[0,2]
	vmovaps	5408(%rsp), %xmm10      # 16-byte Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmovaps	5440(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovaps	3184(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm5, %xmm0
	vshufps	$136, %xmm9, %xmm4, %xmm6 # xmm6 = xmm4[0,2],xmm9[0,2]
	vmovaps	%xmm4, %xmm2
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm8
	vmovss	(%rdi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r14,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vshufps	$221, %xmm7, %xmm15, %xmm4 # xmm4 = xmm15[1,3],xmm7[1,3]
	vmulps	%xmm13, %xmm12, %xmm6
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm6, %xmm4, %xmm15
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rax
	vinsertps	$16, (%rax,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$32, (%rax,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leaq	(%rax,%rsi,4), %rax
	vinsertps	$48, (%rax,%rsi,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm9[1,3]
	vmulps	%xmm13, %xmm14, %xmm6
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vshufps	$136, %xmm3, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm14, %xmm0, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm3, %xmm3
	vminps	%xmm14, %xmm8, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm8
	vmovaps	3168(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm5, %xmm5
	vsubps	%xmm10, %xmm7, %xmm0
	vmulps	%xmm0, %xmm11, %xmm7
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_813
# BB#812:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovdqa	3328(%rsp), %xmm6       # 16-byte Reload
.LBB147_813:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmulps	%xmm7, %xmm5, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm15, %xmm10
	vminps	%xmm14, %xmm2, %xmm9
	vsubps	5408(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm8, %xmm3, %xmm8
	movq	4824(%rsp), %rdi        # 8-byte Reload
	movq	4696(%rsp), %rsi        # 8-byte Reload
	movl	3744(%rsp), %ecx        # 4-byte Reload
	je	.LBB147_815
# BB#814:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_815:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	3312(%rsp), %xmm4       # 16-byte Reload
	vmulps	4192(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3376(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmulps	4160(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	4128(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	movslq	3408(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm15
	vfmsub213ps	%xmm1, %xmm15, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm10, %xmm4
	vmaxps	%xmm5, %xmm9, %xmm3
	vmulps	%xmm13, %xmm12, %xmm1
	vmulps	5440(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm13
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm9
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm8, %xmm7
	vbroadcastss	.LCPI147_19(%rip), %xmm6
	vmovdqa	3680(%rsp), %xmm12      # 16-byte Reload
	je	.LBB147_817
# BB#816:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovdqa	3232(%rsp), %xmm12      # 16-byte Reload
.LBB147_817:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vaddps	%xmm4, %xmm3, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm4       # 16-byte Reload
	vmulps	3840(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3584(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm2[0,2]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	3808(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm11
	movq	3344(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3344(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm11, %xmm5 # xmm5 = xmm11[0,2],xmm5[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmulps	3776(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm10
	vshufps	$136, %xmm10, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm10[0,2]
	vsubps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm3, %xmm3
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm0, %xmm4, %xmm4
	vfmsub213ps	%xmm3, %xmm15, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm0, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm8
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm6, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm7, %xmm6, %xmm8
	vpsrad	$31, %xmm13, %xmm1
	vmovdqa	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm9, %xmm0
	vmovdqa	%xmm0, 3616(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm12, %xmm1
	vpsrad	$31, %xmm1, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	je	.LBB147_819
# BB#818:                               # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_819:                            # %for gV.s0.v10.v10363
                                        #   in Loop: Header=BB147_803 Depth=4
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm1, %xmm9, %xmm1
	vmovaps	3424(%rsp), %xmm2       # 16-byte Reload
	vmulps	4160(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	4128(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vpxor	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm1, %xmm1
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vfmsub213ps	%xmm1, %xmm15, %xmm4
	vshufps	$221, 3344(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vmovaps	5632(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm1
	vmovaps	3488(%rsp), %xmm11      # 16-byte Reload
	vmulps	3808(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3280(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm10[1,3]
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	3776(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vfmsub213ps	%xmm1, %xmm15, %xmm5
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm1
	vmovaps	3680(%rsp), %xmm2       # 16-byte Reload
	vmulps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3536(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3584(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vsubps	%xmm4, %xmm5, %xmm4
	vfmadd213ps	%xmm2, %xmm6, %xmm1
	vfmadd213ps	%xmm2, %xmm6, %xmm4
	vmovdqa	5216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm0, %xmm7, %xmm12, %xmm2
	vmovaps	3456(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm8, %xmm2, %xmm2
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm5, %xmm12, %xmm3
	vblendvps	%xmm0, %xmm4, %xmm3, %xmm0
	vmovaps	3616(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, 5248(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm7, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3152(%rsp), %rax        # 8-byte Reload
	movq	3712(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rax), %rax
	vmovups	%ymm0, (%rsi,%rax,4)
	addl	$8, %r13d
	addl	$-1, %ecx
	jne	.LBB147_803
.LBB147_820:                            # %end for gV.s0.v10.v10364
                                        #   in Loop: Header=BB147_782 Depth=3
	movl	2184(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, 1260(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_821
# BB#842:                               # %for gV.s0.v10.v10367.preheader
                                        #   in Loop: Header=BB147_782 Depth=3
	movq	5184(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %edx
	movl	%r9d, %ecx
	movq	1624(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	leal	8(%rcx), %eax
	movq	1648(%rsp), %r11        # 8-byte Reload
	imull	%r11d, %eax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	andl	$1, %edx
	movl	%edx, 4128(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2752(%rsp)       # 16-byte Spill
	movq	%r9, %r8
	movq	1816(%rsp), %rbx        # 8-byte Reload
	imulq	%rbx, %r8
	leal	9(%rcx), %r14d
	imull	%r11d, %r14d
	movq	1776(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r8), %r10
	leaq	1(%r9), %rdx
	movq	%rdx, 2640(%rsp)        # 8-byte Spill
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rdx
	leaq	-1(%r9), %rsi
	imulq	%rbx, %rsi
	leaq	(%rsi,%rdi), %rsi
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r10,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	2(%r9), %rax
	imulq	%rbx, %rax
	movq	%r9, %rdx
	addq	$-2, %rdx
	imulq	%rbx, %rdx
	movq	1808(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdx), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	leaq	(%rbx,%rax), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	leaq	(%rbx,%r8), %rsi
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	leal	7(%rcx), %ebx
	imull	%r11d, %ebx
	movq	4864(%rsp), %rsi        # 8-byte Reload
	addl	%esi, %r14d
	movq	%r14, 2736(%rsp)        # 8-byte Spill
	addl	%esi, %ebx
	movq	%rbx, 2688(%rsp)        # 8-byte Spill
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	leal	10(%rcx), %edx
	imull	%r11d, %edx
	movq	%rdx, 2672(%rsp)        # 8-byte Spill
	addl	$6, %ecx
	imull	%r11d, %ecx
	movq	%rcx, 2784(%rsp)        # 8-byte Spill
	leaq	(%r8,%rsi), %rdx
	leaq	(%rax,%rsi), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	andl	$63, %r9d
	imulq	1712(%rsp), %r9         # 8-byte Folded Reload
	subq	4712(%rsp), %r9         # 8-byte Folded Reload
	movq	%r9, 2656(%rsp)         # 8-byte Spill
	movq	1064(%rsp), %rcx        # 8-byte Reload
	movq	4672(%rsp), %rdi        # 8-byte Reload
	.align	16, 0x90
.LBB147_843:                            # %for gV.s0.v10.v10367
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_782 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rcx, 5248(%rsp)        # 8-byte Spill
	cmpl	$0, 4128(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %r13        # 8-byte Reload
	leal	(%r13,%rcx,8), %r8d
	movl	%r8d, 3616(%rsp)        # 4-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	sete	4192(%rsp)              # 1-byte Folded Spill
	movl	%r8d, %ecx
	subl	%edi, %ecx
	leal	-1(%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r9d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	vmovd	%esi, %xmm0
	addl	$-2, %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r12d
	vpinsrd	$3, %r11d, %xmm0, %xmm15
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2752(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm8
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm3
	movq	5248(%rsp), %rax        # 8-byte Reload
	leal	-2(%r13,%rax,8), %eax
	vmovdqa	5328(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm13, %xmm4
	vmovdqa	5296(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm1, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm4, %xmm4
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm4, %xmm1, %xmm1
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm1, %xmm12, %xmm3
	vpextrq	$1, %xmm3, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm3, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm1, %xmm10, %xmm3
	vpextrq	$1, %xmm3, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vmovq	%xmm3, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	movl	%r8d, %eax
	movq	5184(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	5248(%rsp), %rax        # 8-byte Reload
	leal	-1(%r13,%rax,8), %eax
	vmovd	%eax, %xmm1
	sete	4160(%rsp)              # 1-byte Folded Spill
	movb	4192(%rsp), %al         # 1-byte Reload
	andb	5152(%rsp), %al         # 1-byte Folded Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm3
	andb	5216(%rsp), %r15b       # 1-byte Folded Reload
	vpsrad	$31, %xmm15, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm13, %xmm4
	vpsubd	%xmm2, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5120(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	testl	4128(%rsp), %r8d        # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm1
	setne	%r9b
	vmovq	%xmm1, %r13
	movq	%r13, %rsi
	sarq	$32, %rsi
	vpextrq	$1, %xmm1, %r11
	movq	%r11, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm10, %xmm1
	vmovq	%xmm1, %r12
	movq	%r12, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vpextrq	$1, %xmm1, %rdi
	movq	%rdi, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %r8
	movq	%r8, 3184(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	movq	2784(%rsp), %rax        # 8-byte Reload
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx,8), %r10d
	movq	4872(%rsp), %rbx        # 8-byte Reload
	leal	(%r10,%rbx), %eax
	movl	%eax, 3200(%rsp)        # 4-byte Spill
	movslq	%r10d, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orq	$6, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movq	2672(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	leal	(%rdx,%rbx), %eax
	movl	%eax, 3328(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	2768(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movq	4864(%rsp), %rax        # 8-byte Reload
	leal	(%rdx,%rax), %eax
	movl	%eax, 3104(%rsp)        # 4-byte Spill
	leal	(%rdx,%rbx), %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movslq	%edx, %r10
	movq	%r10, %rax
	orq	$6, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_845
# BB#844:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_845:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	movzbl	4160(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%r9b, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_847
# BB#846:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_847:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 4160(%rsp)       # 16-byte Spill
	movzbl	%r15b, %edx
	vmovd	%edx, %xmm0
	je	.LBB147_849
# BB#848:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_849:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	je	.LBB147_851
# BB#850:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_851:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	movq	2688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movslq	%edx, %rdx
	movq	5608(%rsp), %rax        # 8-byte Reload
	vmovups	12312(%rax,%rdx,4), %xmm1
	vmovups	12328(%rax,%rdx,4), %xmm6
	movq	2736(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rcx,8), %edx
	movslq	%edx, %rdx
	vmovups	12312(%rax,%rdx,4), %xmm8
	vmovups	12328(%rax,%rdx,4), %xmm10
	movq	3360(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3376(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3392(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	movslq	3104(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	12312(%rax,%rdx,4), %xmm13
	vmovups	12328(%rax,%rdx,4), %xmm3
	movq	%rax, %r9
	movslq	%r13d, %rdx
	vmovss	(%rbx,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm11, %xmm4
	vshufps	$136, %xmm6, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm6[0,2]
	vmovaps	5408(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm4, %xmm9
	vmovaps	2704(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm11, %xmm4
	vshufps	$136, %xmm10, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vmulps	%xmm15, %xmm0, %xmm6
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm1, %xmm1
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%r8,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm8, %xmm0 # xmm0 = xmm8[1,3],xmm10[1,3]
	vmulps	%xmm15, %xmm5, %xmm6
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vshufps	$136, %xmm3, %xmm13, %xmm6 # xmm6 = xmm13[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm13, %xmm10 # xmm10 = xmm13[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm9, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm3, %xmm8
	vminps	%xmm13, %xmm12, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm9
	vmulps	3840(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	cmpl	$0, 104(%rbp)
	je	.LBB147_853
# BB#852:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 4160(%rsp)       # 16-byte Spill
.LBB147_853:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmulps	%xmm5, %xmm3, %xmm2
	vmovaps	%xmm2, 3248(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm1, %xmm11
	vminps	%xmm13, %xmm0, %xmm12
	vsubps	5408(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
	vaddps	%xmm9, %xmm8, %xmm9
	movq	4672(%rsp), %rdi        # 8-byte Reload
	movl	2184(%rsp), %edx        # 4-byte Reload
	movq	4696(%rsp), %rsi        # 8-byte Reload
	vmovdqa	3344(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_855
# BB#854:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovdqa	2832(%rsp), %xmm8       # 16-byte Reload
.LBB147_855:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	3280(%rsp), %xmm4       # 16-byte Reload
	vmulps	3808(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	3776(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3328(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm6[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	3744(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	5216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%r9,%rax,4), %xmm6
	vmovaps	%xmm6, 3280(%rsp)       # 16-byte Spill
	vmovups	24616(%r9,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm6
	vminps	%xmm13, %xmm5, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm5
	vbroadcastss	.LCPI147_18(%rip), %xmm14
	vfmsub213ps	%xmm6, %xmm14, %xmm5
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmaxps	%xmm7, %xmm11, %xmm2
	vmaxps	%xmm7, %xmm12, %xmm5
	vmulps	3840(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
	vmovdqa	4160(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm15
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm8, %xmm0
	vmovdqa	%xmm0, 3216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_20(%rip), %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm9, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm10
	je	.LBB147_857
# BB#856:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	2848(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_857:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vaddps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm5       # 16-byte Reload
	vmulps	3712(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	40(%r9,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm2[0,2]
	vmovaps	5616(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm1, %xmm8
	vmulps	3680(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm11
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	40(%r9,%rax,4), %xmm12
	vshufps	$136, %xmm12, %xmm11, %xmm4 # xmm4 = xmm11[0,2],xmm12[0,2]
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3648(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%r9,%rax,4), %xmm5
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	vmovups	40(%r9,%r10,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm14, %xmm4
	vminps	%xmm13, %xmm8, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm9
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm10, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm10, %xmm9
	vpsrad	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3536(%rsp)       # 16-byte Spill
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_859
# BB#858:                               # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovdqa	3008(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_859:                            # %for gV.s0.v10.v10367
                                        #   in Loop: Header=BB147_843 Depth=4
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3392(%rsp), %xmm8       # 16-byte Reload
	vmulps	3776(%rsp), %xmm8, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3264(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	3744(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm3, %xmm3
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm14, %xmm3
	vshufps	$221, %xmm12, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm12[1,3]
	vxorps	%xmm12, %xmm12, %xmm12
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3584(%rsp), %xmm11      # 16-byte Reload
	vmulps	3680(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	3648(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm14, %xmm4
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	3808(%rsp), %xmm8, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vmulps	3344(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vmulps	3712(%rsp), %xmm11, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vsubps	%xmm3, %xmm4, %xmm3
	vfmadd213ps	%xmm2, %xmm10, %xmm0
	vfmadd213ps	%xmm2, %xmm10, %xmm3
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3488(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm2, %xmm7, %xmm12, %xmm4
	vmovaps	4192(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm9, %xmm4, %xmm4
	vmovaps	4160(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm5, %xmm12, %xmm6
	vblendvps	%xmm2, %xmm3, %xmm6, %xmm2
	vmovaps	3536(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 5216(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	3552(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, %xmm7, %xmm3, %xmm1
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3616(%rsp), %rax        # 4-byte Folded Reload
	movq	2656(%rsp), %rbx        # 8-byte Reload
	leaq	(%rax,%rbx), %rax
	vmovups	%ymm0, (%rsi,%rax,4)
	addq	$1, %rcx
	cmpl	%edx, %ecx
	jne	.LBB147_843
# BB#860:                               #   in Loop: Header=BB147_782 Depth=3
	movq	2640(%rsp), %rax        # 8-byte Reload
.LBB147_861:                            # %end for gV.s0.v10.v10368
                                        #   in Loop: Header=BB147_782 Depth=3
	movl	2576(%rsp), %edi        # 4-byte Reload
	addl	$1, %edi
	movl	%edi, 2576(%rsp)        # 4-byte Spill
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	movq	2800(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %edi
	jne	.LBB147_782
.LBB147_862:                            # %end for gV.s0.v11358
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	%rsi, 4696(%rsp)        # 8-byte Spill
	movq	2800(%rsp), %rax        # 8-byte Reload
	cmpl	1364(%rsp), %eax        # 4-byte Folded Reload
	movl	1228(%rsp), %eax        # 4-byte Reload
	movl	1212(%rsp), %edi        # 4-byte Reload
	movl	1208(%rsp), %esi        # 4-byte Reload
	movq	1216(%rsp), %r8         # 8-byte Reload
	jl	.LBB147_863
	jmp	.LBB147_885
.LBB147_864:                            # %for gV.s0.v11371.end for gV.s0.v10.v10375_crit_edge
                                        #   in Loop: Header=BB147_863 Depth=3
	movq	2800(%rsp), %rcx        # 8-byte Reload
	addl	$1, %ecx
	movl	%ecx, %ebx
	jmp	.LBB147_884
	.align	16, 0x90
.LBB147_863:                            # %for gV.s0.v11371
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_866 Depth 4
	testl	%edx, %edx
	jle	.LBB147_864
# BB#865:                               # %for gV.s0.v10.v10374.preheader
                                        #   in Loop: Header=BB147_863 Depth=3
	movq	2800(%rsp), %r8         # 8-byte Reload
	movl	%r8d, %ecx
	movq	1752(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %ecx
	leal	-1(%rcx), %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1744(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %edx
	cmovgl	%r8d, %edx
	addl	$-1, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	1796(%rsp), %edi        # 4-byte Reload
	movl	%edi, %esi
	movl	%edi, %r11d
	subl	%eax, %esi
	movq	1784(%rsp), %r10        # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %esi
	addl	%r9d, %esi
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	%r8d, %ebx
	cmovgel	%edx, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	cmpl	%r8d, %r13d
	movl	%r13d, %edx
	cmovgl	%r8d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	subl	%eax, %edi
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%r9d, %edi
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	cmpl	%r8d, %ebx
	cmovgl	%edx, %edi
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	leal	1(%r8), %ebx
	movl	%ebx, 2592(%rsp)        # 4-byte Spill
	cmpl	%ebx, %r13d
	movl	%r13d, %edx
	cmovgl	%ebx, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%r11d, %ebx
	subl	%eax, %ebx
	cmpl	%eax, %r10d
	cmovgl	%eax, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, %r13d
	cmovgl	%edx, %ebx
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2816(%rsp)       # 16-byte Spill
	movslq	%edi, %r11
	movq	1816(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r11
	movq	1776(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movslq	%ebx, %rax
	imulq	%r14, %rax
	leaq	(%rax,%rdi), %rax
	movslq	%esi, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%rdi), %rdx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%ebx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r12d, %ecx
	addl	%edx, %ecx
	movq	5248(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%r11), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	leal	2(%r8), %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	1796(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ebx
	subl	%esi, %ebx
	cmpl	%esi, %r10d
	cmovgl	%esi, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %ebx
	movslq	%ebx, %rdx
	imulq	%r14, %rdx
	leaq	(%r15,%rdx), %r12
	leal	-2(%r8), %ebx
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	subl	%ecx, %eax
	cmpl	%ecx, %r10d
	cmovgl	%ecx, %eax
	addl	%r9d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%ebx, %eax
	cltq
	imulq	%r14, %rax
	leaq	(%r15,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%r11,%rsi), %rbx
	leaq	(%rdx,%rsi), %rdx
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movq	5248(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r8d, %ecx
	andl	$63, %ecx
	imulq	1712(%rsp), %rcx        # 8-byte Folded Reload
	movq	1512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %edi
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %edi
	movq	%rdi, 2736(%rsp)        # 8-byte Spill
	movq	1424(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %ebx
	imull	%edx, %ebx
	movq	%rbx, 2720(%rsp)        # 8-byte Spill
	subq	4712(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 2752(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rsi
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edx, %eax
	movq	1616(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%r8), %edi
	imull	%edx, %edi
	movq	%rdi, 2656(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2640(%rsp)        # 8-byte Spill
	leal	(%rsi,%rdi), %eax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2608(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	movl	2184(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_866:                            # %for gV.s0.v10.v10374
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_863 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	cmpl	$0, 5184(%rsp)          # 4-byte Folded Reload
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r11d
	movl	%r11d, 3712(%rsp)       # 4-byte Spill
	movl	%r11d, %r13d
	andl	$1, %r13d
	sete	%r15b
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vmovd	%esi, %xmm0
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r9d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%edi
	vpinsrd	$3, %r10d, %xmm0, %xmm13
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ecx
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2816(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm3
	vmovdqa	5136(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	5328(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	movq	2800(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	sete	%r9b
	andb	5216(%rsp), %r15b       # 1-byte Folded Reload
	movzbl	%r15b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %r13b       # 1-byte Folded Reload
	movl	%r13d, 5248(%rsp)       # 4-byte Spill
	vpsrad	$31, %xmm13, %xmm4
	vpand	%xmm2, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm2, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm5, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	testl	5184(%rsp), %r11d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm2
	setne	%dl
	vmovq	%xmm2, %r8
	movq	%r8, %r14
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r13
	movq	%r13, %rax
	sarq	$32, %rax
	vpaddd	%xmm0, %xmm10, %xmm2
	vmovq	%xmm2, %rsi
	movq	%rsi, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %rdi
	movq	%rdi, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm0, %r11
	movq	%r11, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2672(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movq	2640(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %r10d
	movq	2608(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3120(%rsp)        # 4-byte Spill
	movq	2704(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3200(%rsp)        # 4-byte Spill
	movq	2688(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3216(%rsp)        # 4-byte Spill
	movq	2624(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%r12), %ebx
	movl	%ebx, 3232(%rsp)        # 4-byte Spill
	je	.LBB147_868
# BB#867:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_868:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	movzbl	%r9b, %r9d
	vmovd	%r9d, %xmm0
	movzbl	%dl, %edx
	vmovd	%edx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_870
# BB#869:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_870:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5248(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_872
# BB#871:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_872:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	%xmm3, 5248(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3008(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_874
# BB#873:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_874:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3328(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rdx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rdx
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rdx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%ecx, %rcx
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	12312(%rdx,%rcx,4), %xmm10
	vmovups	12328(%rdx,%rcx,4), %xmm5
	movslq	%r10d, %rcx
	vmovups	12312(%rdx,%rcx,4), %xmm8
	vmovups	12328(%rdx,%rcx,4), %xmm14
	movq	3376(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	movslq	3120(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	12312(%rdx,%rcx,4), %xmm2
	vmovups	12328(%rdx,%rcx,4), %xmm3
	movslq	%r8d, %rcx
	vmovss	(%rbx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r13d, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vshufps	$136, %xmm5, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm5[0,2]
	vmovaps	5408(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm1
	vmovaps	2768(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm4
	vshufps	$136, %xmm14, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm12
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	vshufps	$221, %xmm5, %xmm10, %xmm5 # xmm5 = xmm10[1,3],xmm5[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm6
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm9, %xmm7
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm3, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm3[0,2]
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vbroadcastss	.LCPI147_17(%rip), %xmm14
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm8
	vminps	%xmm14, %xmm12, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm12
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm10, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5248(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_876
# BB#875:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovdqa	2848(%rsp), %xmm9       # 16-byte Reload
.LBB147_876:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm13
	vminps	%xmm14, %xmm5, %xmm10
	vsubps	5408(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm8, %xmm8
	movq	4696(%rsp), %rcx        # 8-byte Reload
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	3680(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3248(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_878
# BB#877:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovdqa	2832(%rsp), %xmm7       # 16-byte Reload
.LBB147_878:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	3360(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4160(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	movslq	3232(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdx,%rax,4), %xmm3
	vmovaps	%xmm3, 3296(%rsp)       # 16-byte Spill
	vmovups	24616(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vminps	%xmm14, %xmm3, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm0
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 5248(%rsp)       # 16-byte Spill
	vmaxps	%xmm1, %xmm13, %xmm5
	vmaxps	%xmm1, %xmm10, %xmm1
	vmulps	5152(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmulps	5440(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vpslld	$31, %xmm9, %xmm9
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm14, %xmm3, %xmm15
	vpslld	$31, %xmm7, %xmm11
	vbroadcastss	.LCPI147_20(%rip), %xmm3
	vmovaps	%xmm3, 3360(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm3
	vbroadcastss	.LCPI147_19(%rip), %xmm13
	vmovdqa	%xmm12, %xmm6
	je	.LBB147_880
# BB#879:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovdqa	3008(%rsp), %xmm6       # 16-byte Reload
.LBB147_880:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vaddps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm4
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm7[0,2]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmulps	3776(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdx,%rax,4), %xmm5
	vmovaps	%xmm5, 3408(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rdx,%rax,4), %xmm8
	vshufps	$136, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm8[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vfmsub213ps	%xmm2, %xmm0, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm10
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm3, %xmm13, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vfmadd213ps	%xmm3, %xmm13, %xmm10
	vpsrad	$31, %xmm9, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm15, %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm6, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	je	.LBB147_882
# BB#881:                               # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovdqa	3104(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	%xmm1, 5216(%rsp)       # 16-byte Spill
.LBB147_882:                            # %for gV.s0.v10.v10374
                                        #   in Loop: Header=BB147_866 Depth=4
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmovaps	3424(%rsp), %xmm11      # 16-byte Reload
	vmulps	4160(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	4128(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm2, %xmm2
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vfmsub213ps	%xmm2, %xmm0, %xmm3
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm7[1,3]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	3648(%rsp), %xmm7       # 16-byte Reload
	vmulps	3808(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	3776(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vfmsub213ps	%xmm2, %xmm0, %xmm1
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vmulps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm3, %xmm1, %xmm1
	vfmadd213ps	%xmm2, %xmm13, %xmm0
	vfmadd213ps	%xmm2, %xmm13, %xmm1
	vmovdqa	5216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3552(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm2, %xmm6, %xmm9, %xmm3
	vmovaps	3536(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm10, %xmm3, %xmm3
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm4, %xmm9, %xmm5
	vblendvps	%xmm2, %xmm1, %xmm5, %xmm1
	vmovaps	3584(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 5248(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm6, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3712(%rsp), %rax        # 4-byte Folded Reload
	movq	2752(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r12d
	movl	3744(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_866
# BB#883:                               #   in Loop: Header=BB147_863 Depth=3
	movl	1228(%rsp), %eax        # 4-byte Reload
	movl	1212(%rsp), %edi        # 4-byte Reload
	movl	1208(%rsp), %esi        # 4-byte Reload
	movq	1216(%rsp), %r8         # 8-byte Reload
	movl	2592(%rsp), %ebx        # 4-byte Reload
.LBB147_884:                            # %end for gV.s0.v10.v10375
                                        #   in Loop: Header=BB147_863 Depth=3
	movl	%ebx, %ecx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	cmpl	1364(%rsp), %ebx        # 4-byte Folded Reload
	movl	2184(%rsp), %edx        # 4-byte Reload
	jne	.LBB147_863
.LBB147_885:                            # %produce dV379
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	%r8, 1216(%rsp)         # 8-byte Spill
	movl	%esi, 1208(%rsp)        # 4-byte Spill
	movl	%edi, 1212(%rsp)        # 4-byte Spill
	movl	%eax, 1228(%rsp)        # 4-byte Spill
	movq	1000(%rsp), %rcx        # 8-byte Reload
	leal	6(%rcx), %edx
	movl	%edx, 2528(%rsp)        # 4-byte Spill
	movl	1644(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	movl	1368(%rsp), %eax        # 4-byte Reload
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	movl	%esi, 2352(%rsp)        # 4-byte Spill
	leal	7(%rcx), %ecx
	movl	976(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %esi
	cmovgel	%esi, %ecx
	movq	%rcx, 2672(%rsp)        # 8-byte Spill
	movl	%edx, %r9d
	cmpl	%esi, %edx
	movl	2188(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_886
	jmp	.LBB147_888
.LBB147_887:                            # %for dV.s0.v11381.end for dV.s0.v10.v10384_crit_edge
                                        #   in Loop: Header=BB147_886 Depth=3
	addl	$1, %r9d
	movl	%r9d, %eax
	jmp	.LBB147_949
	.align	16, 0x90
.LBB147_886:                            # %for dV.s0.v11381
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_931 Depth 4
	testl	%eax, %eax
	jle	.LBB147_887
# BB#930:                               # %for dV.s0.v10.v10383.preheader
                                        #   in Loop: Header=BB147_886 Depth=3
	movq	%r9, 2688(%rsp)         # 8-byte Spill
	movl	%r9d, %edi
	movq	1752(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1760(%rsp), %r13        # 8-byte Reload
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %r15d
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r11d
	subl	%eax, %esi
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1740(%rsp), %r8d        # 4-byte Reload
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %r10d
	cmovgl	%r9d, %r10d
	addl	$-1, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	cmpl	%r9d, %ecx
	cmovll	%esi, %r10d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, %r8d
	movl	%r8d, %edi
	cmovgl	%r9d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r9d, %ecx
	cmovlel	%edx, %edi
	movl	%r9d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r10d
	leal	1(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%r11d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	leal	1(%r9), %eax
	movl	%eax, 2336(%rsp)        # 4-byte Spill
	cmpl	%eax, %r8d
	movl	%r8d, %r14d
	cmovgl	%eax, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r9d, %r8d
	cmovlel	%esi, %r14d
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movslq	%edi, %rdi
	movq	1816(%rsp), %r12        # 8-byte Reload
	imulq	%r12, %rdi
	movq	%rdi, 5248(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r13d
	movl	%edx, %r11d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %r15        # 8-byte Reload
	leaq	(%r15,%rdi), %rax
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	movl	%r11d, %edi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r13d
	sarl	$31, %edi
	movl	1772(%rsp), %eax        # 4-byte Reload
	andl	%eax, %edi
	addl	%r11d, %edi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	addl	%edx, %ecx
	movl	1796(%rsp), %eax        # 4-byte Reload
	subl	%edi, %eax
	movq	1784(%rsp), %r13        # 8-byte Reload
	cmpl	%edi, %r13d
	cmovgl	%edi, %eax
	addl	%ebx, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	2(%r9), %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r9d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%eax, %edx
	cmpl	%r9d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r11
	movq	1824(%rsp), %rax        # 8-byte Reload
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	imulq	%r12, %r11
	leaq	(%r15,%r11), %rdx
	movl	1796(%rsp), %edi        # 4-byte Reload
	subl	%ecx, %edi
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %edi
	addl	%ebx, %edi
	cmpl	%edi, %r8d
	cmovlel	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	leal	-2(%r9), %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%edi, %ecx
	cmpl	%r9d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%edi, %ecx
	movslq	%ecx, %rcx
	imulq	%r12, %rcx
	leaq	(%r15,%rcx), %rdi
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	cmpl	%r9d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r14d
	movslq	%r14d, %rdx
	imulq	%r12, %rdx
	leaq	(%rdx,%r15), %rdx
	movslq	%r10d, %rsi
	imulq	%r12, %rsi
	leaq	(%rsi,%r15), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r11), %r10
	leaq	(%rdi,%rcx), %rsi
	movq	5248(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1800(%rsp), %rbx        # 8-byte Reload
	leaq	(%r11,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	1512(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2512(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2480(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2464(%rsp)        # 8-byte Spill
	movq	1424(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2448(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2432(%rsp)        # 8-byte Spill
	movq	1616(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2416(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	2188(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_931:                            # %for dV.s0.v10.v10383
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_886 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5216(%rsp)          # 4-byte Folded Reload
	setne	3680(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movl	%r12d, 3776(%rsp)       # 4-byte Spill
	movl	%r12d, %r15d
	andl	$1, %r15d
	sete	%r13b
	movq	3872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	movq	3880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r11d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r15d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2656(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4928(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5296(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4120(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%r12d, %eax
	movq	2688(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4112(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	sete	3328(%rsp)              # 1-byte Folded Spill
	andb	3680(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_55(%rip), %xmm3, %xmm3
	vmovdqa	4752(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5216(%rsp), %r12d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r11
	movq	%r11, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r13
	movq	%r13, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3152(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r15d
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r10d
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r9d
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movq	2384(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3296(%rsp)        # 4-byte Spill
	movq	2432(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	je	.LBB147_933
# BB#932:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_933:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movzbl	3328(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	je	.LBB147_935
# BB#934:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_935:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_937
# BB#936:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_937:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	%xmm4, 3312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	je	.LBB147_939
# BB#938:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_939:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r15d, %rcx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2848(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2816(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3008(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movslq	%r12d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5408(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5440(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2624(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2784(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2800(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2576(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2768(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2848(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3008(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3248(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3312(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_941
# BB#940:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovdqa	2720(%rsp), %xmm10      # 16-byte Reload
.LBB147_941:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vandps	3360(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3280(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3264(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4704(%rsp), %rdx        # 8-byte Reload
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3328(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_943
# BB#942:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovdqa	2704(%rsp), %xmm11      # 16-byte Reload
.LBB147_943:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5152(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3536(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3344(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	movq	%rcx, %rax
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3264(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3680(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_945
# BB#944:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovdqa	2736(%rsp), %xmm5       # 16-byte Reload
.LBB147_945:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vmulps	4160(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4128(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm7
	vmovaps	%xmm7, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3840(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm14
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rax,%rcx,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3312(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmulps	3360(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmulps	3232(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	je	.LBB147_947
# BB#946:                               # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
.LBB147_947:                            # %for dV.s0.v10.v10383
                                        #   in Loop: Header=BB147_931 Depth=4
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	5184(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3536(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5152(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	4160(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4128(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3680(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3456(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3392(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r14d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_931
# BB#948:                               #   in Loop: Header=BB147_886 Depth=3
	movl	2336(%rsp), %eax        # 4-byte Reload
.LBB147_949:                            # %end for dV.s0.v10.v10384
                                        #   in Loop: Header=BB147_886 Depth=3
	movl	%eax, %r9d
	cmpl	2352(%rsp), %eax        # 4-byte Folded Reload
	movl	2188(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_886
.LBB147_888:                            # %end for dV.s0.v11382
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	2672(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, 2352(%rsp)        # 4-byte Folded Reload
	movq	4704(%rsp), %r13        # 8-byte Reload
	jge	.LBB147_970
# BB#889:                               #   in Loop: Header=BB147_466 Depth=2
	movl	896(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	jmp	.LBB147_890
.LBB147_929:                            # %end for dV.s0.v10.v10394.end for dV.s0.v10.v10398_crit_edge
                                        #   in Loop: Header=BB147_890 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	jmp	.LBB147_969
	.align	16, 0x90
.LBB147_890:                            # %for dV.s0.v11387
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_892 Depth 4
                                        #         Child Loop BB147_911 Depth 4
                                        #         Child Loop BB147_951 Depth 4
	cmpl	$0, 1256(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_909
# BB#891:                               # %for dV.s0.v10.v10389.preheader
                                        #   in Loop: Header=BB147_890 Depth=3
	movq	5248(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	movq	%r11, %r9
	movq	1816(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	imulq	%rax, %r9
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2656(%rsp)       # 16-byte Spill
	leaq	2(%r11), %r10
	imulq	%rax, %r10
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r9), %rdx
	leaq	(%rcx,%r10), %rsi
	movq	1824(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rdx
	imulq	%rax, %rdx
	leaq	(%rcx,%rdx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	leaq	-1(%r11), %rdi
	imulq	%rax, %rdi
	leaq	(%rdi,%rcx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %r8
	leaq	(%rax,%rdx), %rdi
	leaq	(%rax,%r9), %rsi
	movq	1800(%rsp), %rax        # 8-byte Reload
	leaq	(%r10,%rax), %rcx
	leaq	(%rdx,%rax), %rdx
	leaq	(%r9,%rax), %rax
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movl	%r11d, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	movq	1688(%rsp), %rdx        # 8-byte Reload
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2544(%rsp)        # 8-byte Spill
	movq	1480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	imull	%edx, %eax
	movq	1504(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%edx, %esi
	movq	%rsi, 2512(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2480(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2464(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %edi
	imull	%edx, %edi
	movq	%rdi, 2448(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2432(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%edx, %esi
	movq	%rsi, 2416(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	movl	1096(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_892:                            # %for dV.s0.v10.v10389
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_890 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	cmpl	$0, 5184(%rsp)          # 4-byte Folded Reload
	setne	3648(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r14d
	movl	%r14d, 3744(%rsp)       # 4-byte Spill
	movl	%r14d, %r11d
	andl	$1, %r11d
	sete	%r13b
	movq	3872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	movq	3880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r15d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r11d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2656(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4928(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5296(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4120(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4112(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	sete	3312(%rsp)              # 1-byte Folded Spill
	andb	3648(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5216(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_55(%rip), %xmm3, %xmm3
	vmovdqa	4752(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5184(%rsp), %r14d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r15
	movq	%r15, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r11
	movq	%r11, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3136(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movq	2448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2368(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r13d
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r10d
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r9d
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %r14d
	movq	2496(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	movq	2384(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3200(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3280(%rsp)        # 4-byte Spill
	movq	2432(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r12), %edx
	movl	%edx, 3328(%rsp)        # 4-byte Spill
	je	.LBB147_894
# BB#893:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_894:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	movzbl	3312(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	je	.LBB147_896
# BB#895:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_896:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	%xmm1, 2688(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_898
# BB#897:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_898:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	je	.LBB147_900
# BB#899:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_900:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r13d, %rcx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2816(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2800(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 2848(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3232(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5408(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5440(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2624(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3344(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2768(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2784(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2576(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2752(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	movq	3008(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2832(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2848(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3232(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3296(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_902
# BB#901:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovdqa	2704(%rsp), %xmm10      # 16-byte Reload
.LBB147_902:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vandps	3344(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3264(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3248(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4704(%rsp), %r13        # 8-byte Reload
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3312(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_904
# BB#903:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovdqa	2688(%rsp), %xmm11      # 16-byte Reload
.LBB147_904:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3200(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3536(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3280(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4160(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3328(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3648(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_906
# BB#905:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovdqa	2720(%rsp), %xmm5       # 16-byte Reload
.LBB147_906:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vmulps	4128(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3360(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm4
	vmovaps	%xmm4, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	3840(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm6
	vmovaps	%xmm6, 3280(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm7
	vmovaps	%xmm7, 3264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3808(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm14
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rdi,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3296(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmulps	3344(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmulps	3216(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	je	.LBB147_908
# BB#907:                               # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
.LBB147_908:                            # %for dV.s0.v10.v10389
                                        #   in Loop: Header=BB147_892 Depth=4
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3680(%rsp), %xmm2       # 16-byte Reload
	vmulps	5152(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	4192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4160(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3712(%rsp), %xmm7       # 16-byte Reload
	vmulps	4128(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3264(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	3840(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3808(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3408(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3552(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3424(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3376(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3744(%rsp), %rax        # 4-byte Folded Reload
	movq	2560(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addl	$8, %r12d
	movl	3776(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_892
.LBB147_909:                            # %end for dV.s0.v10.v10390
                                        #   in Loop: Header=BB147_890 Depth=3
	movl	1256(%rsp), %eax        # 4-byte Reload
	cmpl	1276(%rsp), %eax        # 4-byte Folded Reload
	movq	4824(%rsp), %r15        # 8-byte Reload
	jge	.LBB147_928
# BB#910:                               # %for dV.s0.v10.v10393.preheader
                                        #   in Loop: Header=BB147_890 Depth=3
	movq	5248(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 3104(%rsp)        # 4-byte Spill
	movq	%r11, %r9
	movq	1816(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rax
	imulq	%rax, %r9
	movq	1776(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r9), %rcx
	leaq	2(%r11), %r10
	imulq	%rax, %r10
	leaq	(%rdx,%r10), %rsi
	movq	1824(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rcx
	imulq	%rax, %rcx
	leaq	(%rdx,%rcx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rdx), %rsi
	leaq	-1(%r11), %rdi
	imulq	%rax, %rdi
	leaq	(%rdi,%rdx), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r10), %r8
	leaq	(%rax,%rcx), %rdi
	leaq	(%rax,%r9), %rsi
	movq	1800(%rsp), %rax        # 8-byte Reload
	leaq	(%r10,%rax), %rdx
	leaq	(%rcx,%rax), %rcx
	leaq	(%r9,%rax), %rax
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movl	%r11d, %edx
	andl	$63, %edx
	imulq	1720(%rsp), %rdx        # 8-byte Folded Reload
	movq	1488(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %eax
	movq	1688(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %eax
	movq	1480(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%esi, %ecx
	subq	4712(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2768(%rsp)        # 8-byte Spill
	movq	2160(%rsp), %rdx        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdx), %eax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	movq	1504(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %edi
	imull	%esi, %edi
	movq	2152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdi), %ecx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	movq	1496(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ebx
	imull	%esi, %ebx
	leal	(%rdx,%rdi), %ecx
	movq	%rcx, 2656(%rsp)        # 8-byte Spill
	addl	$-8, %edi
	movq	%rdi, 2720(%rsp)        # 8-byte Spill
	leal	(%rax,%rbx), %ecx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %ecx
	movq	%rcx, 2624(%rsp)        # 8-byte Spill
	movq	1472(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%esi, %ecx
	addl	$-8, %ebx
	movq	%rbx, 2688(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	leal	(%rdx,%rcx), %eax
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	addl	$-8, %ecx
	movq	%rcx, 2608(%rsp)        # 8-byte Spill
	movl	1088(%rsp), %ecx        # 4-byte Reload
	movl	1092(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r9d
	.align	16, 0x90
.LBB147_911:                            # %for dV.s0.v10.v10393
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_890 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	movl	3104(%rsp), %r13d       # 4-byte Reload
	testl	%r13d, %r13d
	setne	%r14b
	sete	%r10b
	movq	3000(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %r8d
	movslq	%r8d, %rdx
	movq	%rdx, 3712(%rsp)        # 8-byte Spill
	andl	$1, %r8d
	sete	%r12b
	leaq	-6(%rdx), %rdi
	movq	4608(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %rdi
	movq	4248(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdi), %r11
	leaq	(%r15,%rdi), %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	sete	%al
	andb	%r14b, %r12b
	andb	%r10b, %r8b
	testl	%edx, %r13d
	setne	%r14b
	leaq	-5(%rdx), %rcx
	imulq	%rsi, %rcx
	movq	4688(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi), %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rcx), %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	leaq	(%r15,%rcx), %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	leaq	(%rcx,%rsi), %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	movq	2608(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	movzbl	%r12b, %edx
	vmovd	%edx, %xmm0
	movq	2688(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %edx
	movslq	%edx, %r12
	movq	%r12, %rcx
	orq	$2, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2576(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ebx
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r13d
	movq	2624(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r15d
	movq	2752(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %edx
	movq	2736(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %r10d
	movq	2592(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3360(%rsp)        # 4-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3392(%rsp)        # 4-byte Spill
	movq	2640(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	je	.LBB147_913
# BB#912:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_913:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_915
# BB#914:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_915:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm4, %xmm1
	movq	5608(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_917
# BB#916:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_917:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	%xmm4, 3424(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_919
# BB#918:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_919:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%r11,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r11,4), %rax
	movq	4680(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm0, %xmm8 # xmm8 = xmm0[0,1,2],mem[0]
	movslq	%ebx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3200(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
	movslq	%r13d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	movslq	%r15d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3232(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm13
	vmovaps	%xmm13, 3264(%rsp)      # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	movslq	%r10d, %rax
	vmovups	12296(%rcx,%rax,4), %xmm11
	vmovups	12312(%rcx,%rax,4), %xmm12
	vmovaps	3008(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm7
	vshufps	$136, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0,2],xmm3[0,2]
	vmovaps	5408(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	5440(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vbroadcastss	.LCPI147_17(%rip), %xmm7
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	2848(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm8, %xmm6
	vmovaps	5216(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, %xmm1, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm1[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm8, %xmm4
	vmovaps	5184(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm2
	vmovaps	%xmm2, 3296(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm8, %xmm3
	vshufps	$136, %xmm10, %xmm13, %xmm4 # xmm4 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2800(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm10, %xmm8, %xmm4
	vshufps	$136, %xmm12, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm12[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm3, %xmm6 # xmm6 = xmm3[0,1,2],mem[0]
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	%xmm6, %xmm0, %xmm4
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm4, %xmm3, %xmm8
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3184(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmulps	%xmm6, %xmm14, %xmm4
	vxorps	%xmm14, %xmm14, %xmm14
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	movq	3344(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	vmovaps	3264(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm6, %xmm2, %xmm2
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm1, %xmm1
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm12[1,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovss	(%rsi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	leaq	(%rsi,%rax,4), %rax
	vinsertps	$16, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$32, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rdi,4), %rax
	vinsertps	$48, (%rax,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm4, 3488(%rsp)       # 16-byte Spill
	vmulps	%xmm6, %xmm10, %xmm4
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm4, %xmm2, %xmm6
	vbroadcastss	.LCPI147_21(%rip), %xmm11
	vminps	%xmm7, %xmm8, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm4
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm5
	vminps	%xmm7, %xmm1, %xmm3
	vminps	%xmm7, %xmm6, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3424(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_921
# BB#920:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovdqa	3136(%rsp), %xmm8       # 16-byte Reload
.LBB147_921:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vandps	3312(%rsp), %xmm11, %xmm6 # 16-byte Folded Reload
	vandps	3296(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm4, %xmm9
	vsubps	%xmm2, %xmm5, %xmm15
	vmaxps	%xmm14, %xmm3, %xmm2
	vmaxps	%xmm14, %xmm1, %xmm5
	vandps	3280(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	4704(%rsp), %r13        # 8-byte Reload
	vmovaps	2784(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_923
# BB#922:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	3120(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
.LBB147_923:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vaddps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm2, %xmm5, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmulps	%xmm10, %xmm13, %xmm0
	movslq	3360(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm1
	vmulps	4160(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm3
	vmulps	4128(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	movslq	3456(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm5
	vandps	%xmm11, %xmm9, %xmm2
	vandps	%xmm11, %xmm15, %xmm0
	vpslld	$31, %xmm8, %xmm4
	vmovdqa	%xmm4, 3280(%rsp)       # 16-byte Spill
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm6
	vminps	%xmm7, %xmm3, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vminps	%xmm7, %xmm5, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm3
	vbroadcastss	.LCPI147_18(%rip), %xmm9
	vfnmadd213ps	%xmm3, %xmm9, %xmm6
	vbroadcastss	.LCPI147_20(%rip), %xmm15
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3264(%rsp)       # 16-byte Spill
	vandps	%xmm11, %xmm6, %xmm3
	vaddps	5216(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovdqa	3680(%rsp), %xmm12      # 16-byte Reload
	je	.LBB147_925
# BB#924:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovdqa	3152(%rsp), %xmm12      # 16-byte Reload
.LBB147_925:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm6       # 16-byte Reload
	vmulps	3840(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 3616(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	5616(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vmulps	3808(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3584(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm4[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmulps	3776(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rcx,%r12,4), %xmm8
	vshufps	$136, %xmm8, %xmm4, %xmm13 # xmm13 = xmm4[0,2],xmm8[0,2]
	vsubps	%xmm1, %xmm13, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vfnmadd213ps	%xmm2, %xmm9, %xmm0
	vandps	%xmm11, %xmm0, %xmm0
	vaddps	5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vandps	3312(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 5216(%rsp)       # 16-byte Spill
	vmulps	3344(%rsp), %xmm15, %xmm13 # 16-byte Folded Reload
	vmovdqa	3264(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 3648(%rsp)       # 16-byte Spill
	vmulps	3248(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3536(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm12, %xmm6
	vpsrad	$31, %xmm6, %xmm14
	vmulps	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	je	.LBB147_927
# BB#926:                               # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_927:                            # %for dV.s0.v10.v10393
                                        #   in Loop: Header=BB147_911 Depth=4
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3488(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	4160(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	3456(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	4128(%rsp), %xmm4, %xmm12 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vminps	%xmm7, %xmm2, %xmm2
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm2, %xmm2
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm12
	vfnmadd213ps	%xmm2, %xmm9, %xmm12
	vmovaps	3552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3616(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm1, %xmm4
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vmulps	3840(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm10
	vmovaps	3584(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	3808(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmovaps	3328(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm2, %xmm5 # xmm5 = xmm2[1,3],xmm8[1,3]
	vmulps	3776(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vminps	%xmm7, %xmm10, %xmm2
	vminps	%xmm7, %xmm3, %xmm3
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm2, %xmm2
	vfnmadd213ps	%xmm3, %xmm9, %xmm2
	vmovdqa	5152(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vblendvps	%xmm1, %xmm13, %xmm6, %xmm3
	vblendvps	%xmm14, 3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	4192(%rsp), %xmm15, %xmm5 # 16-byte Folded Reload
	vblendvps	%xmm14, %xmm5, %xmm6, %xmm6
	vandps	%xmm11, %xmm2, %xmm2
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm2, %xmm6, %xmm1
	vmovaps	3648(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3536(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm13, %xmm2, %xmm2
	vandps	%xmm11, %xmm12, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2768(%rsp), %rax        # 8-byte Reload
	movq	3712(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addl	$8, %r9d
	movl	3744(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	movq	4824(%rsp), %r15        # 8-byte Reload
	jne	.LBB147_911
.LBB147_928:                            # %end for dV.s0.v10.v10394
                                        #   in Loop: Header=BB147_890 Depth=3
	movl	1276(%rsp), %eax        # 4-byte Reload
	cmpl	2188(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_929
# BB#950:                               # %for dV.s0.v10.v10397.preheader
                                        #   in Loop: Header=BB147_890 Depth=3
	movq	5248(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %r14d
	movq	1624(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r14d
	leal	8(%r14), %eax
	movq	1648(%rsp), %r10        # 8-byte Reload
	imull	%r10d, %eax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	movq	%rdx, %r8
	movq	1816(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %r8
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rdi
	leal	10(%r14), %esi
	imull	%r10d, %esi
	movq	%rsi, 2656(%rsp)        # 8-byte Spill
	leaq	2(%rdx), %r9
	imulq	%rax, %r9
	leaq	(%rcx,%r9), %rsi
	movq	1824(%rsp), %rbx        # 8-byte Reload
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	leal	6(%r14), %edi
	imull	%r10d, %edi
	movq	%rdi, 2624(%rsp)        # 8-byte Spill
	leaq	-2(%rdx), %r11
	imulq	%rax, %r11
	leaq	(%rcx,%r11), %rdi
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	leaq	1(%rdx), %rdi
	movq	%rdi, 2480(%rsp)        # 8-byte Spill
	movq	%rdx, %rsi
	addq	$-1, %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	%rdi, %rsi
	imulq	%rax, %rsi
	leaq	(%rsi,%rcx), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r8), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	leaq	(%rax,%r11), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	leaq	(%rax,%r9), %rsi
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%edx, %esi
	movq	1800(%rsp), %rcx        # 8-byte Reload
	leaq	(%r8,%rcx), %rax
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	leal	9(%r14), %edi
	imull	%r10d, %edi
	andl	$1, %esi
	movl	%esi, 3808(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2512(%rsp)       # 16-byte Spill
	movq	4864(%rsp), %rax        # 8-byte Reload
	addl	%eax, %edi
	movq	%rdi, 2544(%rsp)        # 8-byte Spill
	addl	$7, %r14d
	imull	%r10d, %r14d
	addl	%eax, %r14d
	movq	%r14, 2704(%rsp)        # 8-byte Spill
	leaq	(%r9,%rcx), %rax
	leaq	(%r11,%rcx), %rcx
	vbroadcastss	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	andl	$63, %edx
	imulq	1720(%rsp), %rdx        # 8-byte Folded Reload
	subq	4712(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2496(%rsp)        # 8-byte Spill
	movq	1056(%rsp), %r11        # 8-byte Reload
	.align	16, 0x90
.LBB147_951:                            # %for dV.s0.v10.v10397
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_890 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	cmpl	$0, 3808(%rsp)          # 4-byte Folded Reload
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r11,8), %r15d
	movl	%r15d, 3680(%rsp)       # 4-byte Spill
	movl	%r15d, %eax
	andl	$1, %eax
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %ecx
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ecx
	leal	-5(%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r14d
	addl	$-6, %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r14d, %xmm1, %xmm8
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r13d
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2512(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm13
	vmovdqa	4928(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm2, %xmm2
	leal	-6(%r12,%r11,8), %eax
	vmovdqa	5328(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm3
	vmovdqa	5296(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm1, %xmm0, %xmm5
	vblendvps	%xmm3, %xmm1, %xmm5, %xmm1
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm3, %xmm3
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm3, %xmm1, %xmm1
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm1, %xmm10, %xmm2
	vpextrq	$1, %xmm2, %r9
	movq	%r9, 3200(%rsp)         # 8-byte Spill
	vmovq	%xmm2, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm1, %xmm12, %xmm2
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vmovq	%xmm2, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	leal	-5(%r12,%r11,8), %eax
	vmovd	%eax, %xmm1
	sete	3296(%rsp)              # 1-byte Folded Spill
	movb	4192(%rsp), %al         # 1-byte Reload
	andb	5152(%rsp), %al         # 1-byte Folded Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm2
	movl	5184(%rsp), %eax        # 4-byte Reload
	andb	5216(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, %r12d
	vpsrad	$31, %xmm8, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpsubd	%xmm3, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vmovdqa	4912(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm4, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm6, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	testl	3808(%rsp), %r15d       # 4-byte Folded Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm10, %xmm1
	setne	%r13b
	vmovq	%xmm1, %r15
	movq	%r15, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm1, %rsi
	movq	%rsi, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpaddd	%xmm0, %xmm12, %xmm1
	vmovq	%xmm1, %r8
	movq	%r8, 3104(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm1, %r10
	movq	%r10, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	movq	2688(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %edx
	movq	4864(%rsp), %rbx        # 8-byte Reload
	leal	(%rdx,%rbx), %r14d
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leal	(%rdx,%rcx), %eax
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	2624(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %edi
	leal	(%rdi,%rbx), %edx
	leal	(%rdi,%rcx), %eax
	movl	%eax, 3264(%rsp)        # 4-byte Spill
	movslq	%edi, %rax
	movq	%rax, %rdi
	orq	$2, %rdi
	movq	%rdi, 3488(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	movq	2656(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	leal	(%rax,%rbx), %edi
	leal	(%rax,%rcx), %ecx
	movl	%ecx, 3328(%rsp)        # 4-byte Spill
	cltq
	movq	%rax, %rcx
	orq	$2, %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	je	.LBB147_953
# BB#952:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_953:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movzbl	3296(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_955
# BB#954:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_955:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm3, %xmm1
	movq	4704(%rsp), %r13        # 8-byte Reload
	je	.LBB147_957
# BB#956:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_957:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	je	.LBB147_959
# BB#958:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_959:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3184(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %rax
	movq	5608(%rsp), %rcx        # 8-byte Reload
	vmovups	12296(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 2800(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 2816(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	vmovups	12296(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 2784(%rsp)       # 16-byte Spill
	movslq	%edi, %rax
	vmovups	12296(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm14
	vmovaps	%xmm14, 2832(%rsp)      # 16-byte Spill
	movq	2704(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	vmovups	12296(%rcx,%rax,4), %xmm10
	vmovaps	%xmm10, 3152(%rsp)      # 16-byte Spill
	vmovups	12312(%rcx,%rax,4), %xmm9
	vmovaps	%xmm9, 3184(%rsp)       # 16-byte Spill
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	vmovups	12296(%rcx,%rax,4), %xmm6
	vmovups	12312(%rcx,%rax,4), %xmm12
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm4
	vshufps	$136, %xmm3, %xmm2, %xmm7 # xmm7 = xmm2[0,2],xmm3[0,2]
	vmovaps	5408(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm7, %xmm7
	vmovaps	5440(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm4, %xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm4, %xmm4
	vmovaps	2608(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm0, %xmm7
	vmovaps	5216(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm5[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 3312(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm2
	vmovaps	5184(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm14, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm14[0,2]
	vxorps	%xmm5, %xmm5, %xmm5
	vsubps	%xmm15, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm2, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 3200(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm2
	vshufps	$136, %xmm9, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm9[0,2]
	vxorps	%xmm7, %xmm7, %xmm7
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	2560(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm14, %xmm0, %xmm0
	vshufps	$136, %xmm12, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm12[0,2]
	vmovaps	%xmm6, %xmm9
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3008(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm10, %xmm1, %xmm2
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm11, %xmm7
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm7, %xmm2, %xmm7
	movq	3616(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	movq	3584(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm3, %xmm4
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm4, %xmm2, %xmm4
	vmovss	(%rbx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3152(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm10, %xmm5, %xmm5
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm5, %xmm1, %xmm6
	vmovss	(%rbx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm12[1,3]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rbx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm3, 5184(%rsp)       # 16-byte Spill
	vmulps	%xmm10, %xmm14, %xmm3
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm8, %xmm7, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm5
	vminps	%xmm8, %xmm6, %xmm6
	vminps	%xmm8, %xmm3, %xmm4
	cmpl	$0, 104(%rbp)
	vmovdqa	3280(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_961
# BB#960:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovdqa	2720(%rsp), %xmm11      # 16-byte Reload
.LBB147_961:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vandps	3312(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3200(%rsp), %xmm12, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm1, %xmm10
	vsubps	%xmm0, %xmm5, %xmm13
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm6, %xmm0
	vmaxps	%xmm1, %xmm4, %xmm6
	vandps	3344(%rsp), %xmm12, %xmm14 # 16-byte Folded Reload
	movl	2188(%rsp), %edx        # 4-byte Reload
	vmovdqa	3296(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_963
# BB#962:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovdqa	2736(%rsp), %xmm1       # 16-byte Reload
.LBB147_963:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vaddps	%xmm3, %xmm7, %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmulps	4160(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	movslq	3248(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3408(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3264(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3376(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	3840(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3328(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vandps	%xmm12, %xmm10, %xmm7
	vandps	%xmm12, %xmm13, %xmm6
	vpslld	$31, %xmm11, %xmm9
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm4
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm4
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm1, %xmm0
	vmovdqa	%xmm0, 3264(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm4, %xmm3
	vaddps	%xmm3, %xmm14, %xmm2
	je	.LBB147_965
# BB#964:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovdqa	2752(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_965:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vaddps	%xmm7, %xmm6, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vmulps	3776(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	3744(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	vmovaps	%xmm5, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm5[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm4, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	3712(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	movq	3552(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm15
	vshufps	$136, %xmm15, %xmm5, %xmm0 # xmm0 = xmm5[0,2],xmm15[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm8, %xmm6, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vminps	%xmm8, %xmm3, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vandps	%xmm12, %xmm1, %xmm0
	vaddps	%xmm0, %xmm14, %xmm1
	vandps	3296(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm9, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmulps	3344(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm2, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm14
	vmulps	%xmm11, %xmm1, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_967
# BB#966:                               # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_967:                            # %for dV.s0.v10.v10397
                                        #   in Loop: Header=BB147_951 Depth=4
	vmovaps	3408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	5184(%rsp), %xmm2       # 16-byte Reload
	vmulps	4160(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	4128(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm1
	vfnmadd213ps	%xmm3, %xmm10, %xmm1
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	5216(%rsp), %xmm7       # 16-byte Reload
	vmulps	3776(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	3744(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$221, %xmm15, %xmm5, %xmm4 # xmm4 = xmm5[1,3],xmm15[1,3]
	vmulps	3712(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm0, %xmm0
	vfnmadd213ps	%xmm3, %xmm10, %xmm0
	vmovdqa	5152(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	3536(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm2, %xmm5, %xmm9, %xmm3
	vblendvps	%xmm14, 4192(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	3584(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vblendvps	%xmm14, %xmm4, %xmm9, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm11, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm0, %xmm7, %xmm0
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3344(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm3, %xmm5, %xmm2, %xmm2
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm11, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm1, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3680(%rsp), %rax        # 4-byte Folded Reload
	movq	2496(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r13,%rax,4)
	addq	$1, %r11
	cmpl	%edx, %r11d
	jne	.LBB147_951
# BB#968:                               #   in Loop: Header=BB147_890 Depth=3
	movq	2480(%rsp), %rax        # 8-byte Reload
.LBB147_969:                            # %end for dV.s0.v10.v10398
                                        #   in Loop: Header=BB147_890 Depth=3
	movl	2352(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2352(%rsp)        # 4-byte Spill
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movq	2672(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	jne	.LBB147_890
.LBB147_970:                            # %end for dV.s0.v11388
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	%r13, 4704(%rsp)        # 8-byte Spill
	movq	2672(%rsp), %rax        # 8-byte Reload
	cmpl	1368(%rsp), %eax        # 4-byte Folded Reload
	movl	2188(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_971
	jmp	.LBB147_993
.LBB147_972:                            # %for dV.s0.v11401.end for dV.s0.v10.v10405_crit_edge
                                        #   in Loop: Header=BB147_971 Depth=3
	movq	2672(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movl	%eax, %ecx
	jmp	.LBB147_992
	.align	16, 0x90
.LBB147_971:                            # %for dV.s0.v11401
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_974 Depth 4
	testl	%eax, %eax
	jle	.LBB147_972
# BB#973:                               # %for dV.s0.v10.v10404.preheader
                                        #   in Loop: Header=BB147_971 Depth=3
	movq	2672(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %r10d
	movq	1752(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r10d
	leal	-1(%r10), %eax
	cltd
	movq	1760(%rsp), %r12        # 8-byte Reload
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r13d       # 4-byte Reload
	andl	%r13d, %eax
	addl	%edx, %eax
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r9d, %ecx
	movl	%ecx, %edx
	cmovgl	%r9d, %edx
	addl	$-1, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1796(%rsp), %esi        # 4-byte Reload
	movl	%esi, %r11d
	movl	%esi, %ebx
	subl	%eax, %r11d
	movq	1784(%rsp), %rsi        # 8-byte Reload
	cmpl	%eax, %esi
	movq	%rsi, %r14
	cmovgl	%eax, %r11d
	addl	%edi, %r11d
	movl	1740(%rsp), %r8d        # 4-byte Reload
	cmpl	%r11d, %r8d
	cmovlel	%r8d, %r11d
	cmpl	%edi, %r11d
	cmovll	%edi, %r11d
	cmpl	%r9d, %ecx
	cmovgel	%edx, %r11d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	cmpl	%r9d, %r8d
	movl	%r8d, %edx
	cmovgl	%r9d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%ebx, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%edi, %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r9d, %ecx
	cmovgl	%edx, %esi
	leal	1(%r10), %eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	leal	1(%r9), %ecx
	movl	%ecx, 2368(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r8d
	movl	%r8d, %edx
	cmovgl	%ecx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	subl	%eax, %ebx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ebx
	addl	%edi, %ebx
	cmpl	%ebx, %r8d
	cmovlel	%r8d, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	%r9d, %r8d
	cmovgl	%edx, %ebx
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movslq	%esi, %rax
	movq	1816(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2688(%rsp)       # 16-byte Spill
	movq	1776(%rsp), %r14        # 8-byte Reload
	leaq	(%r14,%rax), %rax
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	leal	2(%r10), %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andl	%r13d, %esi
	addl	$-2, %r10d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	addl	%ecx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r13d, %ecx
	addl	%edx, %ecx
	leal	2(%r9), %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	1796(%rsp), %r12d       # 4-byte Reload
	movl	%r12d, %edx
	subl	%esi, %edx
	movq	1784(%rsp), %r13        # 8-byte Reload
	cmpl	%esi, %r13d
	cmovgl	%esi, %edx
	addl	%edi, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r9d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%eax, %edx
	movslq	%edx, %r10
	movq	1824(%rsp), %rax        # 8-byte Reload
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	imulq	%r15, %r10
	leaq	(%r14,%r10), %rdx
	movq	%rdx, 5184(%rsp)        # 8-byte Spill
	leal	-2(%r9), %esi
	cmpl	%esi, %r8d
	cmovlel	%r8d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	subl	%ecx, %r12d
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r12d
	addl	%edi, %r12d
	cmpl	%r12d, %r8d
	cmovlel	%r8d, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	%r9d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %r12d
	movslq	%r12d, %rcx
	imulq	%r15, %rcx
	leaq	(%r14,%rcx), %rdx
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movq	5184(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movslq	%ebx, %rdx
	imulq	%r15, %rdx
	leaq	(%rdx,%r14), %rdx
	movslq	%r11d, %rsi
	imulq	%r15, %rsi
	leaq	(%rsi,%r14), %rsi
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r10), %r11
	leaq	(%rdi,%rcx), %rsi
	movq	5248(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	movq	1800(%rsp), %rbx        # 8-byte Reload
	leaq	(%r10,%rbx), %r8
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rbx
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2576(%rsp)        # 8-byte Spill
	movq	1608(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	1768(%rsp), %edx        # 4-byte Reload
	imull	%edx, %eax
	movq	4864(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %eax
	imull	%edx, %eax
	movq	1512(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2544(%rsp)        # 8-byte Spill
	leal	(%rax,%rcx), %eax
	movq	%rax, 2512(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %edi
	movq	%rdi, 2496(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %esi
	movq	%rsi, 2480(%rsp)        # 8-byte Spill
	movq	1424(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %edi
	imull	%edx, %edi
	movq	%rdi, 2464(%rsp)        # 8-byte Spill
	leal	(%rax,%rdi), %esi
	movq	%rsi, 2448(%rsp)        # 8-byte Spill
	movq	1616(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%edx, %esi
	movq	%rsi, 2432(%rsp)        # 8-byte Spill
	leal	(%rcx,%rdi), %edx
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 2400(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	2188(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_974:                            # %for dV.s0.v10.v10404
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_971 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	cmpl	$0, 5216(%rsp)          # 4-byte Folded Reload
	setne	3680(%rsp)              # 1-byte Folded Spill
	sete	5248(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movl	%r12d, 3776(%rsp)       # 4-byte Spill
	movl	%r12d, %r14d
	andl	$1, %r14d
	sete	%r13b
	movq	3872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	movq	3880(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %r11d, %xmm1, %xmm13
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movl	%r14d, %edi
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2688(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	4928(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	5328(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	5296(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	5312(%rsp), %xmm6       # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	4120(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm0, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm0, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	movl	%r12d, %eax
	movq	2672(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4112(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	sete	3328(%rsp)              # 1-byte Folded Spill
	andb	3680(%rsp), %r13b       # 1-byte Folded Reload
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	andb	5248(%rsp), %dil        # 1-byte Folded Reload
	vpsrad	$31, %xmm13, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm3
	vpsubd	%xmm2, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpxor	.LCPI147_55(%rip), %xmm3, %xmm3
	vmovdqa	4752(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	testl	5216(%rsp), %r12d       # 4-byte Folded Reload
	vpaddd	%xmm0, %xmm10, %xmm2
	setne	%cl
	vmovq	%xmm2, %rsi
	movq	%rsi, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm2, %r11
	movq	%r11, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpaddd	%xmm0, %xmm12, %xmm2
	vmovq	%xmm2, %r14
	movq	%r14, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm2, %r8
	movq	%r8, 3152(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %rbx
	movq	%rbx, 3168(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm1, %xmm3
	vpxor	%xmm11, %xmm11, %xmm11
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movq	2480(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r10d
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r9d
	movq	2560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r13d
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movq	2400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3216(%rsp)        # 4-byte Spill
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3296(%rsp)        # 4-byte Spill
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15), %edx
	movl	%edx, 3344(%rsp)        # 4-byte Spill
	je	.LBB147_976
# BB#975:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_976:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movzbl	3328(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	je	.LBB147_978
# BB#977:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_978:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	movzbl	%dil, %ecx
	vmovd	%ecx, %xmm0
	vmovaps	%xmm4, %xmm1
	je	.LBB147_980
# BB#979:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_980:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	%xmm4, 3312(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2736(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	je	.LBB147_982
# BB#981:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_982:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	movq	3232(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movslq	%r12d, %rcx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	12296(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2848(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	movslq	%r10d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2832(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 2816(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm9
	vmovaps	%xmm9, 3008(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movslq	%r13d, %rcx
	vmovups	12296(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	cltq
	vmovups	12296(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovups	12312(%rdi,%rax,4), %xmm12
	movq	%rdi, %rcx
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm1
	vshufps	$136, %xmm3, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm3[0,2]
	vmovaps	5408(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmovaps	5440(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmovaps	2640(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm4, %xmm3
	vshufps	$136, %xmm2, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm3
	vmovaps	2784(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm9, %xmm7 # xmm7 = xmm9[0,2],xmm6[0,2]
	vsubps	%xmm14, %xmm7, %xmm7
	vmulps	%xmm7, %xmm13, %xmm7
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm1
	vmovaps	2800(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm10, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm10[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2592(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm4, %xmm3
	vmovaps	2768(%rsp), %xmm10      # 16-byte Reload
	vshufps	$136, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovss	(%rdx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	movq	3104(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3120(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	vmovaps	2848(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm0, %xmm3
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm15, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vmovaps	3008(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	vmulps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm2, %xmm1, %xmm6
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	3136(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$221, 3248(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm7[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3152(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm12[1,3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	3200(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm9, %xmm5
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm5, %xmm0, %xmm1
	vbroadcastss	.LCPI147_21(%rip), %xmm12
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm5
	vminps	%xmm8, %xmm3, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm8, %xmm4, %xmm6
	vminps	%xmm8, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	3312(%rsp), %xmm10      # 16-byte Reload
	je	.LBB147_984
# BB#983:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovdqa	2720(%rsp), %xmm10      # 16-byte Reload
.LBB147_984:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vandps	3360(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vandps	3280(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm0, %xmm13
	vsubps	%xmm5, %xmm3, %xmm9
	vmaxps	%xmm15, %xmm6, %xmm3
	vmaxps	%xmm15, %xmm1, %xmm6
	vandps	3264(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	movq	4704(%rsp), %rdx        # 8-byte Reload
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3328(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_986
# BB#985:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovdqa	2704(%rsp), %xmm11      # 16-byte Reload
.LBB147_986:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vaddps	%xmm4, %xmm7, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	3216(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm4[0,2]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmulps	5152(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	3296(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3536(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	4192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movslq	3344(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	24600(%rcx,%rax,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm13, %xmm7
	vandps	%xmm12, %xmm9, %xmm3
	vpslld	$31, %xmm10, %xmm0
	vmovdqa	%xmm0, 3264(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vminps	%xmm8, %xmm4, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm10
	vfnmadd213ps	%xmm0, %xmm10, %xmm1
	vbroadcastss	.LCPI147_20(%rip), %xmm9
	vpslld	$31, %xmm11, %xmm0
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm2
	vmovdqa	3680(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_988
# BB#987:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovdqa	2736(%rsp), %xmm5       # 16-byte Reload
.LBB147_988:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vaddps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vmulps	4160(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm4[0,2]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	4128(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm7
	vmovaps	%xmm7, 3280(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vmulps	3840(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm14
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm15, %xmm4, %xmm4
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm10, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vaddps	%xmm0, %xmm2, %xmm4
	vandps	3312(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovdqa	3264(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmulps	3360(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmulps	3232(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm5, %xmm0
	vpsrad	$31, %xmm0, %xmm15
	vmulps	%xmm9, %xmm4, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	je	.LBB147_990
# BB#989:                               # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
.LBB147_990:                            # %for dV.s0.v10.v10404
                                        #   in Loop: Header=BB147_974 Depth=4
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm0, %xmm13, %xmm0
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	5184(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	3536(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	5152(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm13, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm8, %xmm1, %xmm1
	vpxor	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm13
	vfnmadd213ps	%xmm1, %xmm10, %xmm13
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	5616(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	4160(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4128(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vshufps	$221, %xmm11, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm11[1,3]
	vmulps	3840(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vminps	%xmm8, %xmm0, %xmm0
	vminps	%xmm8, %xmm1, %xmm1
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vfnmadd213ps	%xmm1, %xmm10, %xmm0
	vmovdqa	5248(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3424(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm1, %xmm4, %xmm5, %xmm2
	vblendvps	%xmm15, 3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3680(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm15, %xmm3, %xmm5, %xmm7
	vandps	%xmm12, %xmm0, %xmm0
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm9, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm0, %xmm7, %xmm0
	vmovaps	3456(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3392(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm4, %xmm1, %xmm1
	vandps	%xmm12, %xmm13, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2576(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r15d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_974
# BB#991:                               #   in Loop: Header=BB147_971 Depth=3
	movl	2368(%rsp), %ecx        # 4-byte Reload
.LBB147_992:                            # %end for dV.s0.v10.v10405
                                        #   in Loop: Header=BB147_971 Depth=3
	movl	%ecx, %eax
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	cmpl	1368(%rsp), %ecx        # 4-byte Folded Reload
	movl	2188(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_971
.LBB147_993:                            # %produce dh409
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1752(%rsp), %rax        # 8-byte Reload
	movl	2528(%rsp), %edx        # 4-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	movl	1368(%rsp), %ecx        # 4-byte Reload
	cmpl	%esi, %ecx
	movl	%ecx, %edi
	cmovgl	%esi, %edi
	movl	%edi, 2400(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	%esi, 2512(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	%ecx, 2368(%rsp)        # 4-byte Spill
	movl	948(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2448(%rsp)        # 4-byte Spill
	movl	944(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2432(%rsp)        # 4-byte Spill
	movl	940(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2416(%rsp)        # 4-byte Spill
	cmpl	%edi, %edx
	jge	.LBB147_1014
	.align	16, 0x90
.LBB147_994:                            # %for dh.s0.v11411
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_996 Depth 4
	cmpl	$0, 2188(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1013
# BB#995:                               # %for dh.s0.v10.v10413.preheader
                                        #   in Loop: Header=BB147_994 Depth=3
	movl	2528(%rsp), %edi        # 4-byte Reload
	movl	%edi, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	movl	1740(%rsp), %eax        # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	1744(%rsp), %rdx        # 8-byte Reload
	cmpl	%edi, %edx
	cmovlel	%ecx, %eax
	cmpl	%esi, %edi
	cmovll	%ecx, %eax
	movl	%edi, %ecx
	andl	$1, %ecx
	movl	%ecx, 2496(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5248(%rsp)       # 16-byte Spill
	cltq
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movl	%edi, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	movl	2188(%rsp), %eax        # 4-byte Reload
	movq	5288(%rsp), %r15        # 8-byte Reload
	movl	2448(%rsp), %r10d       # 4-byte Reload
	movl	2432(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, 3840(%rsp)        # 4-byte Spill
	movl	2416(%rsp), %r9d        # 4-byte Reload
	.align	16, 0x90
.LBB147_996:                            # %for dh.s0.v10.v10413
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_994 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%r9d, 3744(%rsp)        # 4-byte Spill
	movl	%r10d, 3776(%rsp)       # 4-byte Spill
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	movl	2496(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5216(%rsp)              # 1-byte Folded Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r11d
	andl	$1, %r11d
	sete	5152(%rsp)              # 1-byte Folded Spill
	movq	4080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r13d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4928(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5344(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovdqa	5360(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3536(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r13d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	4944(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4784(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	movq	4104(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3616(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	leal	-8(%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r13d
	sarq	$32, %rbx
	movq	%rbx, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3408(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	leal	-7(%r15), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2768(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4272(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3904(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3424(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4752(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3712(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3680(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3888(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	2528(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	2672(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %bl         # 1-byte Reload
	andb	5216(%rsp), %bl         # 1-byte Folded Reload
	andb	5184(%rsp), %r11b       # 1-byte Folded Reload
	movl	%r11d, 5184(%rsp)       # 4-byte Spill
	testl	%r15d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5216(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r13d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r15), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	4960(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, %r9
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r8
	movq	%r8, %r13
	sarq	$32, %r13
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3536(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r14
	movq	%r14, 2608(%rsp)        # 8-byte Spill
	sarq	$32, %r14
	vpextrq	$1, %xmm0, %r12
	movq	%r12, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vmovdqa	3424(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3488(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movslq	%r10d, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3280(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3264(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_998
# BB#997:                               # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_998:                            # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	2672(%rsp), %r10d       # 1-byte Folded Reload
	vmovd	%r10d, %xmm0
	movzbl	5216(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	je	.LBB147_1000
# BB#999:                               # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1000:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movl	5184(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1002
# BB#1001:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1002:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	je	.LBB147_1004
# BB#1003:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1004:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3584(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3408(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3456(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2768(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2752(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2800(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	3840(%rsp), %rbx        # 4-byte Folded Reload
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	%rsi, %r10
	movq	2832(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3120(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3104(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3136(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	movq	3008(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3168(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3184(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r11d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r8d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2480(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5184(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1006
# BB#1005:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	2560(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5216(%rsp)       # 16-byte Spill
.LBB147_1006:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4664(%rsp), %r8         # 8-byte Reload
	movl	3744(%rsp), %r9d        # 4-byte Reload
	vmovdqa	2672(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1008
# BB#1007:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovdqa	2544(%rsp), %xmm15      # 16-byte Reload
.LBB147_1008:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	movq	2608(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2624(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r12,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2640(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2656(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2688(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2720(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	%r9d, %rcx
	vmovups	24584(%r10,%rcx,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rcx,4), %xmm5
	vmovaps	%xmm5, 3152(%rsp)       # 16-byte Spill
	vmovups	24576(%r10,%rcx,4), %xmm10
	vmovaps	%xmm10, 3184(%rsp)      # 16-byte Spill
	vmovups	24592(%r10,%rcx,4), %xmm9
	vmovups	24608(%r10,%rcx,4), %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3616(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5216(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3136(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3120(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_1010
# BB#1009:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_1010:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm7
	movq	3296(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r10,%rcx,4), %xmm11
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r10,%rax,4), %xmm15
	vmovaps	%xmm15, 3360(%rsp)      # 16-byte Spill
	vmovups	32(%r10,%rax,4), %xmm8
	vmovaps	%xmm8, 3456(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vaddps	5184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3136(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmulps	3552(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovdqa	3120(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmulps	3104(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovdqa	4192(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4192(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	je	.LBB147_1012
# BB#1011:                              # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_1012:                           # %for dh.s0.v10.v10413
                                        #   in Loop: Header=BB147_996 Depth=4
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3680(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3712(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3648(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3408(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5216(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4128(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3456(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3536(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4192(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3424(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3584(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3616(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3488(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	2464(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r8,%rax,4)
	addl	$8, %r9d
	addl	$8, 3840(%rsp)          # 4-byte Folded Spill
	movl	3776(%rsp), %r10d       # 4-byte Reload
	addl	$8, %r10d
	addl	$8, %r15d
	movl	3808(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_996
.LBB147_1013:                           # %end for dh.s0.v10.v10414
                                        #   in Loop: Header=BB147_994 Depth=3
	movl	2528(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2528(%rsp)        # 4-byte Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	addl	%eax, 2416(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2432(%rsp)        # 4-byte Folded Spill
	addl	%eax, 2448(%rsp)        # 4-byte Folded Spill
	cmpl	2400(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_994
.LBB147_1014:                           # %end for dh.s0.v11412
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	2368(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 2400(%rsp)        # 4-byte Folded Reload
	jge	.LBB147_1074
# BB#1015:                              #   in Loop: Header=BB147_466 Depth=2
	movq	1608(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	movl	900(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %edx
	imull	1768(%rsp), %edx        # 4-byte Folded Reload
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	notl	%ecx
	movq	680(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movl	%eax, 2384(%rsp)        # 4-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1016:                           # %for dh.s0.v11417
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1018 Depth 4
                                        #         Child Loop BB147_1037 Depth 4
                                        #         Child Loop BB147_1056 Depth 4
	cmpl	$0, 1252(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1035
# BB#1017:                              # %for dh.s0.v10.v10419.preheader
                                        #   in Loop: Header=BB147_1016 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 2560(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1720(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2496(%rsp)        # 8-byte Spill
	movl	1080(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %r13        # 8-byte Reload
	movq	2416(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1018:                           # %for dh.s0.v10.v10419
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1016 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5184(%rsp)        # 8-byte Spill
	movl	%ecx, 4128(%rsp)        # 4-byte Spill
	movl	2560(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5152(%rsp)              # 1-byte Folded Spill
	sete	4192(%rsp)              # 1-byte Folded Spill
	movl	%r13d, %r14d
	andl	$1, %r14d
	sete	%r15b
	movq	4080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5216(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 3840(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r12d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4928(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5344(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vmovdqa	5360(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 3712(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r12d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	4944(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4784(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	movq	4104(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3744(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	leal	-8(%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r12d
	sarq	$32, %rbx
	movq	%rbx, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3552(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r13), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3808(%rsp)       # 16-byte Spill
	leal	-7(%r13), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4272(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3904(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3536(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	3840(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4752(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3808(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3808(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3888(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3840(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movl	%r13d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	andb	5152(%rsp), %r15b       # 1-byte Folded Reload
	andb	4192(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 5152(%rsp)       # 4-byte Spill
	testl	%r13d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	4192(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r12d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%r15b, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	4960(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r15
	movq	%r15, %r11
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r14
	movq	%r14, %r12
	sarq	$32, %r12
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	3712(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2640(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	3536(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3584(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	movq	5184(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3584(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	4864(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %r8d
	movq	4872(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %edx
	movl	%edx, 2688(%rsp)        # 4-byte Spill
	je	.LBB147_1020
# BB#1019:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1020:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movzbl	%bl, %ebx
	vmovd	%ebx, %xmm0
	movzbl	4192(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_1022
# BB#1021:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1022:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5152(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1024
# BB#1023:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1024:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2608(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3712(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	je	.LBB147_1026
# BB#1025:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1026:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3456(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3552(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2800(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3008(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2848(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3120(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r8d, %rbx
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	%rsi, %r8
	movq	3104(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3168(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3152(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3184(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	movq	3136(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3232(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3200(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3248(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r15d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r14d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r12,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2544(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm2
	vminps	%xmm2, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm2, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm13
	vmulps	3744(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm2, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	vmovdqa	5152(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1028
# BB#1027:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovdqa	2592(%rsp), %xmm15      # 16-byte Reload
.LBB147_1028:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vandps	%xmm13, %xmm1, %xmm12
	vandps	%xmm13, %xmm4, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovdqa	2720(%rsp), %xmm8       # 16-byte Reload
	je	.LBB147_1030
# BB#1029:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovdqa	2576(%rsp), %xmm8       # 16-byte Reload
.LBB147_1030:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	movq	2640(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2656(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2672(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2688(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3184(%rsp)       # 16-byte Spill
	vmovups	24576(%r8,%rcx,4), %xmm11
	vmovaps	%xmm11, 3232(%rsp)      # 16-byte Spill
	vmovups	24592(%r8,%rcx,4), %xmm9
	vmovups	24608(%r8,%rcx,4), %xmm10
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm10, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3744(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vandps	%xmm13, %xmm14, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vandps	3680(%rsp), %xmm13, %xmm10 # 16-byte Folded Reload
	vpslld	$31, %xmm15, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	vminps	%xmm2, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm5
	vfnmadd213ps	%xmm0, %xmm5, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm8, %xmm0
	vmovdqa	%xmm0, 3456(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm3, %xmm0
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vmovdqa	3712(%rsp), %xmm11      # 16-byte Reload
	je	.LBB147_1032
# BB#1031:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovdqa	2608(%rsp), %xmm11      # 16-byte Reload
.LBB147_1032:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm6
	vmovaps	%xmm6, 3328(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm7
	vmovaps	%xmm7, 3312(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm8
	vmovaps	%xmm8, 3680(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r8,%rax,4), %xmm14
	vmovaps	%xmm14, 3424(%rsp)      # 16-byte Spill
	vmovups	32(%r8,%rax,4), %xmm15
	vmovaps	%xmm15, 3536(%rsp)      # 16-byte Spill
	vaddps	3648(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	2528(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm4
	vshufps	$136, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm7[0,2]
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm10, %xmm0
	vshufps	$136, %xmm8, %xmm14, %xmm6 # xmm6 = xmm14[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm10, %xmm3
	vshufps	$136, %xmm15, %xmm8, %xmm6 # xmm6 = xmm8[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vfnmadd213ps	%xmm0, %xmm5, %xmm3
	vmovaps	%xmm5, 3712(%rsp)       # 16-byte Spill
	vandps	%xmm13, %xmm3, %xmm0
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3552(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5152(%rsp)       # 16-byte Spill
	vmulps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmovdqa	3456(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	3168(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm11, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	je	.LBB147_1034
# BB#1033:                              # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_1034:                           # %for dh.s0.v10.v10419
                                        #   in Loop: Header=BB147_1018 Depth=4
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3808(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm14, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3840(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3776(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm11, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3248(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm2, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm4
	vmovaps	3712(%rsp), %xmm9       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm9, %xmm4
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm14, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm10, %xmm1
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm8
	vpaddd	%xmm15, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm11, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3536(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm10, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm2, %xmm8, %xmm3
	vminps	%xmm2, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm9, %xmm1
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm5, %xmm2
	vmovaps	3488(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm6, %xmm8, %xmm5, %xmm6
	vandps	%xmm13, %xmm1, %xmm1
	vmovaps	3744(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3552(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	5152(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm13, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	2496(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4664(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	5184(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r13d
	movl	4128(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1018
.LBB147_1035:                           # %end for dh.s0.v10.v10420
                                        #   in Loop: Header=BB147_1016 Depth=3
	movl	1252(%rsp), %eax        # 4-byte Reload
	cmpl	1284(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1054
# BB#1036:                              # %for dh.s0.v10.v10423.preheader
                                        #   in Loop: Header=BB147_1016 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 3280(%rsp)        # 4-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1720(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 3248(%rsp)        # 8-byte Spill
	movl	1044(%rsp), %ecx        # 4-byte Reload
	movl	1084(%rsp), %eax        # 4-byte Reload
	movl	%eax, %edx
	movl	2384(%rsp), %eax        # 4-byte Reload
	movl	%eax, %r13d
	.align	16, 0x90
.LBB147_1037:                           # %for dh.s0.v10.v10423
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1016 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rdx, 3840(%rsp)        # 8-byte Spill
	movl	%ecx, 4128(%rsp)        # 4-byte Spill
	movl	3280(%rsp), %r10d       # 4-byte Reload
	testl	%r10d, %r10d
	setne	%r15b
	sete	5184(%rsp)              # 1-byte Folded Spill
	leal	-8(%rdx), %eax
	movslq	%eax, %r12
	movq	%r12, 3808(%rsp)        # 8-byte Spill
	andl	$1, %eax
	sete	%r14b
	leaq	-6(%r12), %rdx
	movq	4608(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rdx
	leaq	-4(%r12), %r8
	imulq	%r11, %r8
	leaq	-8(%r12), %rdi
	imulq	%r11, %rdi
	leaq	-5(%r12), %rcx
	imulq	%r11, %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	movq	4248(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%rdx), %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	leaq	-7(%r12), %r9
	imulq	%r11, %r9
	movq	%r9, 3776(%rsp)         # 8-byte Spill
	movl	%r12d, %esi
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %esi
	testb	$1, %sil
	sete	3392(%rsp)              # 1-byte Folded Spill
	andb	%r15b, %r14b
	andb	5184(%rsp), %al         # 1-byte Folded Reload
	movl	%eax, 5184(%rsp)        # 4-byte Spill
	testl	%r12d, %r10d
	setne	3744(%rsp)              # 1-byte Folded Spill
	leaq	-3(%r12), %r15
	imulq	%r11, %r15
	movq	%r15, 3344(%rsp)        # 8-byte Spill
	movq	4688(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rdx), %rcx
	movq	%rcx, 3488(%rsp)        # 8-byte Spill
	movq	4824(%rsp), %r10        # 8-byte Reload
	leaq	(%rdx,%r10), %rcx
	movq	%rcx, 3552(%rsp)        # 8-byte Spill
	leal	-8(%r13), %edx
	movslq	%edx, %r11
	movq	%r11, %rcx
	orq	$2, %rcx
	movq	%rcx, 3616(%rsp)        # 8-byte Spill
	movzbl	%r14b, %ecx
	vmovd	%ecx, %xmm0
	movq	%r11, %rcx
	orq	$6, %rcx
	movq	%rcx, 3536(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rdi), %r12
	leaq	(%rsi,%rdi), %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	movq	%r10, %rdx
	leaq	(%rdi,%rdx), %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	movq	%r11, %rcx
	orq	$4, %rcx
	movq	%rcx, 3648(%rsp)        # 8-byte Spill
	leaq	(%rbx,%r8), %r10
	leaq	(%rsi,%r8), %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	leaq	(%r8,%rdx), %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vxorps	%xmm13, %xmm13, %xmm13
	cmpl	$1, 104(%rbp)
	movq	2160(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %ecx
	movq	5216(%rsp), %rax        # 8-byte Reload
	leaq	(%rbx,%rax), %r8
	leaq	(%rbx,%r9), %r9
	leaq	(%rbx,%r15), %r15
	movq	2152(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %eax
	movl	%eax, 3424(%rsp)        # 4-byte Spill
	je	.LBB147_1039
# BB#1038:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1039:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movzbl	3392(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movzbl	3744(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1041
# BB#1040:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1041:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	movl	5184(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %edi
	vmovd	%edi, %xmm0
	vmovaps	%xmm3, %xmm2
	je	.LBB147_1043
# BB#1042:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1043:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovaps	%xmm3, 5184(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	movq	%r13, 3360(%rsp)        # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm10
	vmovdqa	%xmm10, %xmm0
	movq	5608(%rsp), %r14        # 8-byte Reload
	je	.LBB147_1045
# BB#1044:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vpxor	%xmm0, %xmm0, %xmm0
.LBB147_1045:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovdqa	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rdx        # 8-byte Reload
	movq	3408(%rsp), %r13        # 8-byte Reload
	leaq	(%rdx,%r13,4), %rdi
	movq	4680(%rsp), %rax        # 8-byte Reload
	leaq	(%rdi,%rax,4), %rbx
	leaq	(%rbx,%rax,4), %rsi
	vmovss	(%rdx,%r13,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	3264(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm0, %xmm11, %xmm1
	movslq	%ecx, %rbx
	vmovups	12296(%r14,%rbx,4), %xmm0
	vmovups	12312(%r14,%rbx,4), %xmm7
	vshufps	$136, %xmm7, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm7[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vbroadcastss	.LCPI147_17(%rip), %xmm5
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm2
	leaq	(%rdx,%r10,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdx,%r10,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm11, %xmm3
	vmovups	12304(%r14,%rbx,4), %xmm1
	vmovups	12320(%r14,%rbx,4), %xmm14
	vshufps	$136, %xmm14, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm12
	vbroadcastss	.LCPI147_21(%rip), %xmm15
	leaq	(%rdx,%r12,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	vmovss	(%rdx,%r12,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm11, %xmm3
	vmovups	12288(%r14,%rbx,4), %xmm6
	vshufps	$136, %xmm1, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm1[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	leaq	(%rdx,%r8,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r8,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm11, %xmm3
	vshufps	$221, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm3, %xmm0, %xmm3
	leaq	(%rdx,%r9,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r9,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm11, %xmm0
	vshufps	$221, %xmm1, %xmm6, %xmm4 # xmm4 = xmm6[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm7
	vsubps	%xmm0, %xmm7, %xmm3
	leaq	(%rdx,%r15,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	vmovss	(%rdx,%r15,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%rax, %rcx
	vmulps	%xmm4, %xmm11, %xmm4
	vshufps	$221, %xmm14, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	cmpl	$0, 104(%rbp)
	vmovdqa	5184(%rsp), %xmm14      # 16-byte Reload
	je	.LBB147_1047
# BB#1046:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovdqa	3328(%rsp), %xmm14      # 16-byte Reload
.LBB147_1047:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vandps	%xmm15, %xmm12, %xmm11
	vandps	%xmm15, %xmm2, %xmm8
	vsubps	%xmm7, %xmm0, %xmm9
	vsubps	%xmm7, %xmm1, %xmm12
	vandps	%xmm15, %xmm3, %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	movq	4688(%rsp), %r10        # 8-byte Reload
	movq	4824(%rsp), %r9         # 8-byte Reload
	je	.LBB147_1049
# BB#1048:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovdqa	3392(%rsp), %xmm10      # 16-byte Reload
.LBB147_1049:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	movq	%rdx, %rdi
	movq	3488(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rax
	movq	%rcx, %rbx
	leaq	(%rax,%rbx,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	movslq	3424(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24584(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	24600(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm2[0,2]
	vmovaps	5664(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	movq	3456(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r8
	vmovss	(%rdi,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r8,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm6, %xmm1
	vmovups	24576(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 3456(%rsp)       # 16-byte Spill
	vmovups	24592(%r14,%rax,4), %xmm7
	vshufps	$136, %xmm7, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm7[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	movq	3712(%rsp), %rsi        # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %r8
	vmovss	(%rdi,%rsi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r8,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vmovups	24608(%r14,%rax,4), %xmm6
	vmovaps	%xmm6, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm7, %xmm6 # xmm6 = xmm7[0,2],xmm6[0,2]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vaddps	%xmm8, %xmm11, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 3712(%rsp)      # 16-byte Spill
	vandps	%xmm15, %xmm9, %xmm3
	vandps	%xmm15, %xmm12, %xmm4
	vpslld	$31, %xmm14, %xmm0
	vmovdqa	%xmm0, 3312(%rsp)       # 16-byte Spill
	vminps	%xmm5, %xmm2, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm2
	vminps	%xmm5, %xmm1, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm5, %xmm6, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm12
	vfnmadd213ps	%xmm0, %xmm12, %xmm2
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm10, %xmm14
	vandps	%xmm15, %xmm2, %xmm2
	vaddps	5184(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	je	.LBB147_1051
# BB#1050:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
.LBB147_1051:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vaddps	%xmm4, %xmm3, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movq	%rdi, %rsi
	movq	3552(%rsp), %rdx        # 8-byte Reload
	leaq	(%rsi,%rdx,4), %rax
	movq	%rbx, %rdi
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rbx
	vmovss	(%rsi,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	4160(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm2, %xmm0, %xmm2
	movq	3616(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 3536(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm3[0,2]
	vmovaps	5616(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm2, %xmm8
	movq	3584(%rsp), %rbx        # 8-byte Reload
	leaq	(%rsi,%rbx,4), %rax
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rdx
	vmovss	(%rsi,%rbx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm0, %xmm4
	vmovups	(%r14,%r11,4), %xmm3
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	vmovups	(%r14,%rax,4), %xmm2
	vshufps	$136, %xmm2, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm2[0,2]
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	movq	3680(%rsp), %rbx        # 8-byte Reload
	leaq	(%rsi,%rbx,4), %rax
	leaq	(%rax,%rdi,4), %rcx
	leaq	(%rcx,%rdi,4), %rdx
	vmovss	(%rsi,%rbx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	%rdi, %rdx
	vmulps	%xmm6, %xmm0, %xmm6
	vmovups	32(%r14,%r11,4), %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm0[0,2]
	vsubps	%xmm1, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vaddps	%xmm3, %xmm4, %xmm3
	vminps	%xmm5, %xmm8, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vfnmadd213ps	%xmm3, %xmm12, %xmm4
	vandps	%xmm15, %xmm4, %xmm3
	vaddps	5184(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovdqa	3312(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmulps	3328(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vpsrad	$31, %xmm14, %xmm0
	vmovdqa	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm10, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovdqa	5152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpsrad	$31, %xmm4, %xmm10
	vmulps	%xmm11, %xmm3, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovdqa	3744(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1053
# BB#1052:                              # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovdqa	3296(%rsp), %xmm9       # 16-byte Reload
.LBB147_1053:                           # %for dh.s0.v10.v10423
                                        #   in Loop: Header=BB147_1037 Depth=4
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	movq	5216(%rsp), %r8         # 8-byte Reload
	leaq	(%r10,%r8), %rax
	movq	%rsi, %rdi
	leaq	(%rdi,%rax,4), %rcx
	movq	%rdx, %rbx
	leaq	(%rcx,%rbx,4), %rdx
	leaq	(%rdx,%rbx,4), %rsi
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	5664(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm3, %xmm3
	vmovaps	5696(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm3, %xmm14, %xmm3
	vmulps	%xmm4, %xmm3, %xmm4
	movq	3776(%rsp), %rsi        # 8-byte Reload
	leaq	(%r10,%rsi), %rax
	leaq	(%rdi,%rax,4), %rcx
	leaq	(%rcx,%rbx,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rdx,%rbx,4), %rax
	vinsertps	$32, (%rdx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rax,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm0, %xmm3
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm6 # xmm6 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm3, %xmm6, %xmm3
	vshufps	$221, 3392(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm7[1,3],mem[1,3]
	movq	3344(%rsp), %rdx        # 8-byte Reload
	leaq	(%r10,%rdx), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm0, %xmm7
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm5, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vminps	%xmm5, %xmm6, %xmm6
	vmaxps	%xmm13, %xmm6, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm8
	vfnmadd213ps	%xmm3, %xmm12, %xmm8
	vmovaps	3552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	leaq	(%r8,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	4160(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	5616(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	leaq	(%rsi,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, %xmm2, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm2[1,3]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm4, %xmm6, %xmm4
	vshufps	$221, 3376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	leaq	(%rdx,%r9), %rax
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%rdi,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	leaq	(%rcx,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm7, %xmm6
	vsubps	%xmm0, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vminps	%xmm5, %xmm3, %xmm3
	vminps	%xmm5, %xmm4, %xmm4
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm2, %xmm2
	vaddps	%xmm2, %xmm4, %xmm2
	vmaxps	%xmm13, %xmm3, %xmm3
	vfnmadd213ps	%xmm2, %xmm12, %xmm3
	vpslld	$31, %xmm9, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovaps	3648(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm1, %xmm7, %xmm13, %xmm2
	vblendvps	%xmm10, 5152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3488(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vblendvps	%xmm10, %xmm4, %xmm13, %xmm5
	vandps	%xmm15, %xmm3, %xmm3
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vaddps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm11, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm3, %xmm5, %xmm1
	vmovaps	3680(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm7, %xmm2, %xmm2
	vandps	%xmm15, %xmm8, %xmm3
	vaddps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm11, %xmm3, %xmm0
	vblendvps	%xmm5, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm4, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	3248(%rsp), %rax        # 8-byte Reload
	movq	3808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	4664(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	3360(%rsp), %r13        # 8-byte Reload
	addl	$8, %r13d
	movq	3840(%rsp), %rdx        # 8-byte Reload
	addl	$8, %edx
	movl	4128(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1037
.LBB147_1054:                           # %end for dh.s0.v10.v10424
                                        #   in Loop: Header=BB147_1016 Depth=3
	movl	1284(%rsp), %eax        # 4-byte Reload
	cmpl	2188(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1073
# BB#1055:                              # %for dh.s0.v10.v10427.preheader
                                        #   in Loop: Header=BB147_1016 Depth=3
	movq	5248(%rsp), %rax        # 8-byte Reload
	movq	%rax, %rdi
	andl	$1, %eax
	movl	%eax, 4160(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	%rdi, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	andl	$63, %edi
	imulq	1720(%rsp), %rdi        # 8-byte Folded Reload
	subq	4712(%rsp), %rdi        # 8-byte Folded Reload
	movq	%rdi, 2480(%rsp)        # 8-byte Spill
	movq	1024(%rsp), %rax        # 8-byte Reload
	movq	2416(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	movq	1016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	movq	1032(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	%rax, 2432(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1076(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1056:                           # %for dh.s0.v10.v10427
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1016 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3840(%rsp)        # 4-byte Spill
	cmpl	$0, 4160(%rsp)          # 4-byte Folded Reload
	setne	5184(%rsp)              # 1-byte Folded Spill
	sete	5152(%rsp)              # 1-byte Folded Spill
	movq	1984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r9d
	movl	%r9d, 3808(%rsp)        # 4-byte Spill
	movl	%r9d, %r14d
	andl	$1, %r14d
	sete	4192(%rsp)              # 1-byte Folded Spill
	movq	1832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r12d
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	movq	1840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vmovdqa	5216(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm8
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 3776(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovd	%r9d, %xmm0
	vpbroadcastd	%xmm0, %xmm2
	vmovdqa	%xmm2, 3712(%rsp)       # 16-byte Spill
	vmovdqa	4928(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm0, %xmm1
	vmovdqa	%xmm2, %xmm7
	movq	1936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vmovdqa	5312(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm4, %xmm4
	vmovdqa	5344(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vmovdqa	5328(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm13, %xmm5
	vmovdqa	5296(%rsp), %xmm10      # 16-byte Reload
	vpsubd	%xmm3, %xmm10, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	movq	1848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vmovd	%xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovdqa	5360(%rsp), %xmm11      # 16-byte Reload
	vpmulld	%xmm11, %xmm1, %xmm0
	vmovdqa	%xmm0, 3648(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	1856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpand	%xmm8, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm8, %xmm4, %xmm4
	vmovdqa	4944(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm2, %xmm5
	movq	1944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm9, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpcmpgtd	%xmm1, %xmm13, %xmm6
	vpsubd	%xmm1, %xmm10, %xmm2
	vblendvps	%xmm6, %xmm1, %xmm2, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm5, %xmm7, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	movq	1864(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5104(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm0, %xmm15, %xmm4
	vmovd	%xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrq	$1, %xmm4, %rbx
	movq	%rbx, 3680(%rsp)        # 8-byte Spill
	vmovq	%xmm4, %r15
	movq	%r15, 3584(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	movq	1952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vmovd	%esi, %xmm5
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r10d
	sarq	$32, %r15
	movq	%r15, 3424(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm5, %xmm2
	sarq	$32, %rbx
	movq	%rbx, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm15, %xmm6
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vmovq	%xmm6, %rcx
	movq	%rcx, 3456(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm2, %xmm5
	movq	1960(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	movq	1968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm8
	sarq	$32, %rcx
	movq	%rcx, 2816(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm6, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm13, %xmm6
	vpsubd	%xmm3, %xmm10, %xmm1
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vmovdqa	4272(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3712(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm3
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm4, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm3
	vpaddd	%xmm3, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm5, %xmm1
	vpand	5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm1, %xmm1
	vmovdqa	4912(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm4, %xmm4
	vpbroadcastd	%xmm7, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vmovdqa	3776(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm13, %xmm6
	vpsubd	%xmm0, %xmm10, %xmm7
	vblendvps	%xmm6, %xmm0, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm5, %xmm6, %xmm4
	vpmulld	%xmm11, %xmm4, %xmm4
	vmovdqa	%xmm4, 3744(%rsp)       # 16-byte Spill
	vpaddd	%xmm4, %xmm15, %xmm4
	vmovq	%xmm4, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm4, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm4, %xmm4
	vmovdqa	%xmm2, %xmm7
	vpbroadcastd	%xmm8, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm13, %xmm5
	vpsubd	%xmm1, %xmm10, %xmm6
	vblendvps	%xmm5, %xmm1, %xmm6, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm4, %xmm2, %xmm1, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	%xmm1, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	movq	5248(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movq	1872(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	sete	2688(%rsp)              # 1-byte Folded Spill
	movb	4192(%rsp), %bl         # 1-byte Reload
	andb	5184(%rsp), %bl         # 1-byte Folded Reload
	andb	5152(%rsp), %r14b       # 1-byte Folded Reload
	movl	%r14d, 5152(%rsp)       # 4-byte Spill
	testl	4160(%rsp), %r9d        # 4-byte Folded Reload
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	setne	5184(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r10d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	movq	1976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	movzbl	%bl, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm2
	vpsubd	%xmm1, %xmm10, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	4960(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm0, %xmm0
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, %r12
	sarq	$32, %r12
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, %r11
	sarq	$32, %r11
	vmovdqa	3648(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r8
	movq	%r8, 2608(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2640(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %r15
	movq	%r15, 2672(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vmovdqa	3616(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3184(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3168(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm2
	vmovaps	%xmm2, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %r14d
	movq	2464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r13), %edx
	movl	%edx, 2656(%rsp)        # 4-byte Spill
	je	.LBB147_1058
# BB#1057:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1058:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	2688(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	5184(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_1060
# BB#1059:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1060:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	movl	5152(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1062
# BB#1061:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1062:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	%xmm1, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3648(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	je	.LBB147_1064
# BB#1063:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1064:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3424(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3408(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3456(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2784(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2800(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movq	2768(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2832(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %rbx
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	12296(%rsi,%rbx,4), %xmm7
	vmovups	12312(%rsi,%rbx,4), %xmm0
	vmovups	12304(%rsi,%rbx,4), %xmm3
	vmovups	12320(%rsi,%rbx,4), %xmm14
	vmovups	12288(%rsi,%rbx,4), %xmm5
	movq	3120(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3248(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3232(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3264(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rbx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3280(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3312(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rbx,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r9d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%edi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2528(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm13, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm4, %xmm4
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm4, %xmm4
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm13, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5152(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm13, %xmm5, %xmm5
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1066
# BB#1065:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	2560(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5184(%rsp)       # 16-byte Spill
.LBB147_1066:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vandps	%xmm2, %xmm1, %xmm13
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4664(%rsp), %r9         # 8-byte Reload
	vmovdqa	2688(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1068
# BB#1067:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovdqa	2544(%rsp), %xmm15      # 16-byte Reload
.LBB147_1068:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	movq	2608(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2624(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2640(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2672(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2704(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2656(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3248(%rsp)       # 16-byte Spill
	vmovups	24576(%rsi,%rcx,4), %xmm10
	vmovaps	%xmm10, 3280(%rsp)      # 16-byte Spill
	vmovups	24592(%rsi,%rcx,4), %xmm9
	vmovups	24608(%rsi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3680(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 3680(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3456(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm11
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3424(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovdqa	3648(%rsp), %xmm13      # 16-byte Reload
	je	.LBB147_1070
# BB#1069:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovdqa	2576(%rsp), %xmm13      # 16-byte Reload
.LBB147_1070:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	movq	3008(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3152(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3136(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3200(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3184(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	movq	3168(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 3296(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %xmm7
	vmovaps	%xmm7, 3648(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3536(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%rsi,%rax,4), %xmm15
	vmovaps	%xmm15, 3376(%rsp)      # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 3488(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	2496(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm4, %xmm10, %xmm4
	vshufps	$136, %xmm6, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm6[0,2]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm10, %xmm0
	vshufps	$136, %xmm7, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm10, %xmm3
	vshufps	$136, %xmm8, %xmm7, %xmm6 # xmm6 = xmm7[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3456(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5152(%rsp)       # 16-byte Spill
	vmulps	3584(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovdqa	3424(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmulps	3232(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vpslld	$31, %xmm13, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmulps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	je	.LBB147_1072
# BB#1071:                              # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_1072:                           # %for dh.s0.v10.v10427
                                        #   in Loop: Header=BB147_1056 Depth=4
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4128(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3776(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3280(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3712(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm14, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3408(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm5, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm8
	vmovaps	5184(%rsp), %xmm13      # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm13, %xmm8
	vmovdqa	5424(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm15, %xmm4, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm10, %xmm1
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm7, %xmm4, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	3648(%rsp), %xmm7       # 16-byte Reload
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm14, %xmm4, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3488(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm7[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm10, %xmm7
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm12, %xmm3, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm13, %xmm1
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3552(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm0, %xmm7, %xmm9, %xmm2
	vmovaps	3456(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, 3424(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmulps	3616(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vblendvps	%xmm4, %xmm3, %xmm9, %xmm6
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm11, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, 3536(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmovaps	5152(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, %xmm7, %xmm1, %xmm1
	vandps	%xmm2, %xmm8, %xmm2
	vaddps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm11, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm6, %xmm3, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2480(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	addl	$8, %r13d
	movl	3840(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1056
.LBB147_1073:                           # %end for dh.s0.v10.v10428
                                        #   in Loop: Header=BB147_1016 Depth=3
	movl	2400(%rsp), %ecx        # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, 2400(%rsp)        # 4-byte Spill
	addq	$1, 5248(%rsp)          # 8-byte Folded Spill
	movl	1768(%rsp), %eax        # 4-byte Reload
	movq	2416(%rsp), %rdx        # 8-byte Reload
	addl	%eax, %edx
	movq	%rdx, 2416(%rsp)        # 8-byte Spill
	addl	%eax, 2384(%rsp)        # 4-byte Folded Spill
	cmpl	2368(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1016
.LBB147_1074:                           # %end for dh.s0.v11418
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	2512(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, 1368(%rsp)        # 4-byte Folded Reload
	jle	.LBB147_1096
# BB#1075:                              #   in Loop: Header=BB147_466 Depth=2
	movq	864(%rsp), %rax         # 8-byte Reload
	movq	904(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rax), %eax
	imull	1768(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 2480(%rsp)        # 4-byte Spill
	.align	16, 0x90
.LBB147_1076:                           # %for dh.s0.v11431
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1078 Depth 4
	cmpl	$0, 2188(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1095
# BB#1077:                              # %for dh.s0.v10.v10434.preheader
                                        #   in Loop: Header=BB147_1076 Depth=3
	movl	2512(%rsp), %r8d        # 4-byte Reload
	movl	%r8d, %edi
	movl	%r8d, %eax
	movq	1752(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	andl	$1, %edi
	movl	%edi, 2544(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1740(%rsp), %ebx        # 4-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r8d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1784(%rsp), %rdi        # 8-byte Reload
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%esi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movq	1744(%rsp), %rax        # 8-byte Reload
	cmpl	%r8d, %eax
	cmovgl	%ecx, %edx
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 5248(%rsp)       # 16-byte Spill
	movslq	%edx, %rax
	imulq	1816(%rsp), %rax        # 8-byte Folded Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1800(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	imulq	1720(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2496(%rsp)        # 8-byte Spill
	movl	2188(%rsp), %ecx        # 4-byte Reload
	movq	5288(%rsp), %r15        # 8-byte Reload
	movl	2480(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1078:                           # %for dh.s0.v10.v10434
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1076 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%rax, 5216(%rsp)        # 8-byte Spill
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	movl	2544(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	setne	5184(%rsp)              # 1-byte Folded Spill
	sete	5152(%rsp)              # 1-byte Folded Spill
	movl	%r15d, %r12d
	andl	$1, %r12d
	sete	%r13b
	movq	4080(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	movq	4088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %ebx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	5248(%rsp), %xmm3       # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm3, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 4192(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	4928(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm3
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	4768(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	5296(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm1, %xmm14, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	5344(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-6(%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4640(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovdqa	5360(%rsp), %xmm2       # 16-byte Reload
	vpmulld	%xmm2, %xmm1, %xmm11
	vmovdqa	%xmm2, %xmm6
	vmovdqa	%xmm11, 4160(%rsp)      # 16-byte Spill
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	movq	4096(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	4944(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vmovdqa	4784(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm1, %xmm0, %xmm5
	vpsubd	%xmm1, %xmm14, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	leal	-4(%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	4104(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovdqa	5104(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm11, %xmm10, %xmm5
	vpextrq	$1, %xmm5, %rbx
	movq	%rbx, 3680(%rsp)        # 8-byte Spill
	vmovd	%xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vmovq	%xmm5, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	leal	-8(%r15), %eax
	vmovd	%eax, %xmm5
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3424(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm4
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrq	$1, %xmm4, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edi, %xmm7, %xmm7
	vmovq	%xmm4, %rsi
	movq	%rsi, 3536(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm7, %xmm11
	leal	-5(%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3776(%rsp)       # 16-byte Spill
	leal	-7(%r15), %eax
	vmovd	%eax, %xmm9
	sarq	$32, %rsi
	movq	%rsi, 2816(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2800(%rsp)        # 8-byte Spill
	vpcmpgtd	%xmm3, %xmm0, %xmm1
	vpsubd	%xmm3, %xmm14, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovdqa	4272(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	3904(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpmulld	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 3488(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3104(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm11, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovdqa	4192(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vmovdqa	%xmm0, %xmm6
	vpsubd	%xmm3, %xmm14, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vmovdqa	4912(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vmovdqa	4752(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpbroadcastd	3776(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm2, %xmm5, %xmm2
	vmovdqa	%xmm4, %xmm5
	vpmulld	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 3744(%rsp)       # 16-byte Spill
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm10, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3136(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vmovdqa	4256(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm7, %xmm2, %xmm2
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vmovdqa	3888(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm7
	vpsubd	%xmm1, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm9, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm5, %xmm6
	vmovdqa	%xmm1, 3776(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm10, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3120(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3184(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	2512(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, %al
	movq	4632(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	andb	5184(%rsp), %r13b       # 1-byte Folded Reload
	andb	5152(%rsp), %r12b       # 1-byte Folded Reload
	movl	%r12d, 5152(%rsp)       # 4-byte Spill
	testl	%r15d, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	setne	5184(%rsp)              # 1-byte Folded Spill
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm2
	leal	-3(%r15), %eax
	vmovd	%eax, %xmm4
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm5
	vpsrad	$31, %xmm2, %xmm1
	vpand	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm7, %xmm2
	vpsubd	%xmm1, %xmm14, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vmovdqa	4960(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	4800(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vpbroadcastd	%xmm4, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 3712(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovq	%xmm0, %r13
	movq	%r13, %r11
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %r12
	movq	%r12, %r14
	sarq	$32, %r14
	vmovdqa	5376(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	4160(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %r9
	movq	%r9, 2624(%rsp)         # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2640(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vmovdqa	3488(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rcx
	movq	%rcx, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	vpextrq	$1, %xmm0, %rdi
	movq	%rdi, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vmovdqa	3552(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	vmovdqa	5424(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	movq	5216(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rax
	movq	%rax, %rdx
	orq	$2, %rdx
	movq	%rdx, 3312(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$6, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	vpaddd	%xmm4, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3360(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3376(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rdx, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm1, %xmm0
	vmovq	%xmm0, %rdx
	movq	%rdx, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	movq	4864(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %r8d
	movq	4872(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rsi), %edx
	movl	%edx, 2672(%rsp)        # 4-byte Spill
	je	.LBB147_1080
# BB#1079:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1080:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movzbl	%bl, %ebx
	vmovd	%ebx, %xmm0
	movzbl	5184(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 4192(%rsp)       # 16-byte Spill
	je	.LBB147_1082
# BB#1081:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1082:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	%xmm1, 2560(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	movl	5152(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %ebx
	vmovd	%ebx, %xmm0
	je	.LBB147_1084
# BB#1083:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1084:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	%xmm1, 2592(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	je	.LBB147_1086
# BB#1085:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1086:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %rbx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3680(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3424(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movq	3536(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2816(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3584(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2800(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	2784(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2848(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2832(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	movslq	%r8d, %rbx
	movq	5608(%rsp), %r8         # 8-byte Reload
	vmovups	12296(%r8,%rbx,4), %xmm7
	vmovups	12312(%r8,%rbx,4), %xmm0
	vmovups	12304(%r8,%rbx,4), %xmm3
	vmovups	12320(%r8,%rbx,4), %xmm14
	vmovups	12288(%r8,%rbx,4), %xmm5
	movq	3008(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3152(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3136(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3168(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm1, 5152(%rsp)       # 16-byte Spill
	movq	3120(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rsi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3184(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3232(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rsi,4), %xmm4, %xmm15 # xmm15 = xmm4[0,1,2],mem[0]
	movslq	%r13d, %rbx
	vmovss	(%rdx,%rbx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%r12d, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%r14,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vmovaps	2528(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm10, %xmm9, %xmm4
	vshufps	$136, %xmm0, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm0[0,2]
	vmovaps	5408(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmovaps	5440(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm4
	vmulps	%xmm12, %xmm9, %xmm2
	vshufps	$136, %xmm14, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm6, %xmm9, %xmm2
	vshufps	$136, %xmm3, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm10, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm13
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm4, %xmm4
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vminps	%xmm13, %xmm6, %xmm6
	vmaxps	%xmm12, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm4
	vshufps	$221, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm0[1,3]
	vbroadcastss	.LCPI147_21(%rip), %xmm2
	vmulps	5152(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm15, %xmm9, %xmm6
	vshufps	$221, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm13, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm6
	vsubps	%xmm5, %xmm6, %xmm0
	vmulps	%xmm11, %xmm9, %xmm7
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm7, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	cmpl	$0, 104(%rbp)
	je	.LBB147_1088
# BB#1087:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	2576(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm7, 5184(%rsp)       # 16-byte Spill
.LBB147_1088:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vandps	%xmm2, %xmm1, %xmm12
	vandps	%xmm2, %xmm4, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm14
	vsubps	%xmm6, %xmm3, %xmm11
	vandps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm8
	movq	4664(%rsp), %r11        # 8-byte Reload
	vmovdqa	2704(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1090
# BB#1089:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovdqa	2560(%rsp), %xmm15      # 16-byte Reload
.LBB147_1090:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	movq	2624(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2640(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r10,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	2656(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vmovss	(%rdx,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2688(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	2752(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movslq	2672(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24584(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3184(%rsp)       # 16-byte Spill
	vmovups	24576(%r8,%rcx,4), %xmm10
	vmovaps	%xmm10, 3232(%rsp)      # 16-byte Spill
	vmovups	24592(%r8,%rcx,4), %xmm9
	vmovups	24608(%r8,%rcx,4), %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vshufps	$136, %xmm5, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm5[0,2]
	vmovaps	5664(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm6, %xmm6
	vmovaps	5696(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm0, %xmm7, %xmm0
	vshufps	$136, %xmm9, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vshufps	$136, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vaddps	3680(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3680(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm2
	vandps	%xmm2, %xmm14, %xmm14
	vandps	%xmm2, %xmm11, %xmm10
	vmovdqa	5184(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3584(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm3, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_18(%rip), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vbroadcastss	.LCPI147_20(%rip), %xmm12
	vpslld	$31, %xmm15, %xmm0
	vmovdqa	%xmm0, 3168(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm3, %xmm0
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	je	.LBB147_1092
# BB#1091:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
.LBB147_1092:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3248(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm7
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vmovups	(%r8,%rcx,4), %xmm11
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %rdi
	vmovups	(%r8,%rax,4), %xmm15
	vmovaps	%xmm15, 3408(%rsp)      # 16-byte Spill
	vmovups	32(%r8,%rax,4), %xmm8
	vmovaps	%xmm8, 3536(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm14, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm7, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm7[0,2]
	vmovaps	%xmm7, %xmm14
	vmovaps	5616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm6, %xmm6
	vmovaps	5632(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm8, %xmm11, %xmm6 # xmm6 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vaddps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	3584(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 5152(%rsp)       # 16-byte Spill
	vmulps	3616(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmovdqa	3168(%rsp), %xmm1       # 16-byte Reload
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmulps	3152(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovdqa	4160(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vmovdqa	%xmm1, 4160(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	je	.LBB147_1094
# BB#1093:                              # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
.LBB147_1094:                           # %for dh.s0.v10.v10434
                                        #   in Loop: Header=BB147_1078 Depth=4
	vmovdqa	5376(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	3744(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm5, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3184(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	4128(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm4
	vmovdqa	3776(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm3, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovdqa	3712(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3424(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm9[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm4
	vmovaps	5184(%rsp), %xmm8       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm8, %xmm4
	vmovdqa	5424(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm5, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	3840(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	5616(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm1, %xmm0, %xmm3
	vpaddd	%xmm10, %xmm7, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	3408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpaddd	%xmm15, %xmm7, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vshufps	$221, 3536(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm7, %xmm1, %xmm1
	vminps	%xmm13, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm3, %xmm1
	vfnmadd213ps	%xmm0, %xmm8, %xmm1
	vmovdqa	4192(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vblendvps	%xmm0, %xmm3, %xmm9, %xmm2
	vmovaps	4160(%rsp), %xmm5       # 16-byte Reload
	vblendvps	%xmm5, 3488(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
	vmulps	3648(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vblendvps	%xmm5, %xmm8, %xmm9, %xmm6
	vmovaps	3456(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm1, %xmm1
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm1, %xmm6, %xmm0
	vmovaps	3616(%rsp), %xmm7       # 16-byte Reload
	vblendvps	%xmm7, 3552(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
	vmovaps	5152(%rsp), %xmm6       # 16-byte Reload
	vblendvps	%xmm6, %xmm3, %xmm1, %xmm1
	vandps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm8, %xmm0, %xmm0
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	2496(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vmovups	%ymm0, (%r11,%rax,4)
	movq	5216(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$8, %r15d
	movl	3808(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB147_1078
.LBB147_1095:                           # %end for dh.s0.v10.v10435
                                        #   in Loop: Header=BB147_1076 Depth=3
	movl	2512(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 2512(%rsp)        # 4-byte Spill
	movl	2480(%rsp), %ecx        # 4-byte Reload
	addl	1768(%rsp), %ecx        # 4-byte Folded Reload
	movl	%ecx, 2480(%rsp)        # 4-byte Spill
	cmpl	1368(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1076
.LBB147_1096:                           # %for f4.s0.v11441.preheader
                                        #   in Loop: Header=BB147_466 Depth=2
	movl	960(%rsp), %r9d         # 4-byte Reload
	movl	1372(%rsp), %eax        # 4-byte Reload
	movl	2184(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	movq	808(%rsp), %r15         # 8-byte Reload
	movq	4664(%rsp), %rbx        # 8-byte Reload
	vmovaps	736(%rsp), %ymm2        # 32-byte Reload
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vmovdqu	.LCPI147_22(%rip), %xmm3
	jle	.LBB147_1097
	.align	16, 0x90
.LBB147_1104:                           # %for f4.s0.v10.v10444.preheader.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1100 Depth 4
                                        #           Child Loop BB147_1101 Depth 5
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	movl	%eax, %r11d
	andl	$63, %r11d
	movl	%r11d, %r10d
	movq	1168(%rsp), %rax        # 8-byte Reload
	imull	%eax, %r10d
	imulq	1200(%rsp), %r11        # 8-byte Folded Reload
	subq	4712(%rsp), %r11        # 8-byte Folded Reload
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB147_1100:                           # %for f4.s0.v10.v10444.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1104 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB147_1101 Depth 5
	leal	(,%r14,8), %r13d
	vxorps	%ymm0, %ymm0, %ymm0
	movl	$9, %ecx
	movl	%r9d, %eax
	movq	4704(%rsp), %r8         # 8-byte Reload
	.align	16, 0x90
.LBB147_1101:                           # %for sum.s1.r4$y449.us
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1104 Depth=3
                                        #         Parent Loop BB147_1100 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	movl	%eax, %esi
	andl	$63, %esi
	imull	%r15d, %esi
	leal	(%r13,%rsi), %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$1, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$2, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$3, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$4, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$5, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	movl	%edx, %edi
	orl	$6, %edi
	movslq	%edi, %rdi
	vmovups	(%r8,%rdi,4), %ymm1
	vcmpnltps	(%rbx,%rdi,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	orl	$7, %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	leal	8(%r13,%rsi), %edx
	movslq	%edx, %rdx
	vmovups	(%r8,%rdx,4), %ymm1
	vcmpnltps	(%rbx,%rdx,4), %ymm1, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	addl	$1, %eax
	addl	$-1, %ecx
	jne	.LBB147_1101
# BB#1102:                              # %consume sum455.us
                                        #   in Loop: Header=BB147_1100 Depth=4
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%r13,%rax), %eax
	addl	%r10d, %r13d
	vpbroadcastd	%xmm3, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm0
	movslq	%r13d, %rcx
	movq	4816(%rsp), %rdx        # 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm1
	movq	4696(%rsp), %rdx        # 8-byte Reload
	vblendvps	%ymm0, (%rdx,%rcx,4), %ymm1, %ymm0
	cltq
	leaq	(%rax,%r11), %rax
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r14
	cmpl	%r12d, %r14d
	jne	.LBB147_1100
# BB#1103:                              # %end for f4.s0.v10.v10445.us
                                        #   in Loop: Header=BB147_1104 Depth=3
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %eax
	addl	$1, %r9d
	cmpl	2320(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1104
.LBB147_1097:                           # %produce f7457
                                        #   in Loop: Header=BB147_466 Depth=2
	notl	972(%rsp)               # 4-byte Folded Spill
	movl	1644(%rsp), %eax        # 4-byte Reload
	movq	1000(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %esi
	cmovgel	%eax, %esi
	movl	1372(%rsp), %eax        # 4-byte Reload
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	movq	%rsi, 2112(%rsp)        # 8-byte Spill
	leal	1(%rcx), %edx
	movl	976(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%eax, %edx
	addl	$1, %edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movq	%rdx, 2368(%rsp)        # 8-byte Spill
	movl	%ecx, %r8d
	cmpl	%esi, %ecx
	movl	1736(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_1098
	jmp	.LBB147_1141
.LBB147_1099:                           # %for f7.s0.v11459.end for f7.s0.v10.v10462_crit_edge
                                        #   in Loop: Header=BB147_1098 Depth=3
	addl	$1, %r8d
	movl	%r8d, %ecx
	jmp	.LBB147_1140
	.align	16, 0x90
.LBB147_1098:                           # %for f7.s0.v11459
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1106 Depth 4
	testl	%eax, %eax
	jle	.LBB147_1099
# BB#1105:                              # %for f7.s0.v10.v10461.preheader
                                        #   in Loop: Header=BB147_1098 Depth=3
	movq	%r8, 2400(%rsp)         # 8-byte Spill
	movl	%r8d, %edi
	movq	1752(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %edi
	leal	-1(%rdi), %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %esi
	movl	%ecx, %r10d
	subl	%eax, %esi
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r9
	cmovgl	%eax, %esi
	addl	%ebx, %esi
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	movq	1744(%rsp), %rcx        # 8-byte Reload
	cmpl	%r8d, %ecx
	movl	%ecx, %r14d
	cmovgl	%r8d, %r14d
	addl	$-1, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	cmpl	%r8d, %ecx
	cmovll	%esi, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r9d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, %r13d
	movl	%r13d, %edi
	cmovgl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	%r8d, %ecx
	cmovlel	%edx, %edi
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	cmovll	%edx, %edi
	cmovlel	%esi, %r14d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r10d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	leal	1(%r8), %eax
	movl	%eax, 2192(%rsp)        # 4-byte Spill
	cmpl	%eax, %r13d
	movl	%r13d, %r9d
	cmovgl	%eax, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	cmpl	%r8d, %r13d
	cmovlel	%r11d, %r9d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3104(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2384(%rsp)       # 16-byte Spill
	movslq	%edi, %r10
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2416(%rsp)        # 8-byte Spill
	leal	2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	andl	%r12d, %esi
	addl	$-2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	addl	%edi, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movq	1816(%rsp), %r15        # 8-byte Reload
	imulq	%r15, %r10
	movq	1800(%rsp), %r12        # 8-byte Reload
	leaq	(%r10,%r12), %rcx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%esi, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%esi, %edx
	cmovgl	%esi, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	2(%r8), %r10d
	cmpl	%r10d, %r13d
	movl	%r13d, %edx
	cmovgl	%r10d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r15, %rcx
	leaq	(%rcx,%r12), %rcx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	movq	1784(%rsp), %rdx        # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	leal	-2(%r8), %esi
	cmpl	%esi, %r13d
	movl	%r13d, %eax
	cmovgl	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %eax
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %eax
	cltq
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movslq	%r14d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%r11d, %r9d
	movslq	%r9d, %rax
	imulq	%r15, %rax
	leaq	(%rax,%r12), %rax
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movq	2416(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1728(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	movl	2192(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	movq	1416(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1768(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2288(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	movq	1408(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2256(%rsp)        # 8-byte Spill
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	movq	1400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2240(%rsp)        # 8-byte Spill
	andl	$63, %r10d
	imull	%eax, %r10d
	movq	%r10, 2352(%rsp)        # 8-byte Spill
	movq	1392(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2224(%rsp)        # 8-byte Spill
	movq	1528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2208(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2416(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1106:                           # %for f7.s0.v10.v10461
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1098 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3008(%rsp)        # 4-byte Spill
	cmpl	$0, 3104(%rsp)          # 4-byte Folded Reload
	sete	3776(%rsp)              # 1-byte Folded Spill
	setne	3680(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 5152(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3744(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r14d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2384(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5152(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5056(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4992(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5296(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5312(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3744(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r15d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3712(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4896(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4736(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r15d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r14d
	movl	%r14d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2736(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2784(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r15d
	vmovq	%xmm1, %rax
	movq	%rax, 2720(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2752(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5040(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	movb	3680(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5216(%rsp)       # 1-byte Folded Spill
	movl	5152(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2400(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	movq	2208(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r14
	sete	3200(%rsp)              # 1-byte Folded Spill
	movl	3104(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3168(%rsp)              # 1-byte Folded Spill
	movb	3776(%rsp), %r10b       # 1-byte Reload
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r12d
	movzbl	3200(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r15b
	movq	2224(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movslq	%ecx, %r10
	movq	2240(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movslq	%edi, %rcx
	movq	%r10, %rsi
	orq	$6, %rsi
	movq	%rsi, 3280(%rsp)        # 8-byte Spill
	movq	%rcx, %r12
	movq	%rcx, %rsi
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2416(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2768(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movq	2336(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2672(%rsp)        # 4-byte Spill
	movq	2272(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3184(%rsp)        # 4-byte Spill
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3200(%rsp)        # 4-byte Spill
	je	.LBB147_1108
# BB#1107:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1108:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1110
# BB#1109:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1110:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm1, 2432(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	movzbl	3168(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_1112
# BB#1111:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1112:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3120(%rsp)       # 16-byte Spill
	je	.LBB147_1114
# BB#1113:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1114:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm1, 2448(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1116
# BB#1115:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1116:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	je	.LBB147_1118
# BB#1117:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1118:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, %r9
	je	.LBB147_1120
# BB#1119:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1120:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2816(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2848(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	je	.LBB147_1122
# BB#1121:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1122:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2736(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2624(%rsp)       # 16-byte Spill
	movq	2720(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2752(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3232(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	32(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovups	48(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	8(%rcx,%r8,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovups	24(%rcx,%r8,4), %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4160(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rdx,%r10,4), %xmm14
	vmovups	48(%rdx,%r10,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3744(%rsp)       # 16-byte Spill
	vmovaps	4128(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rdx,%r9,4), %xmm6
	vmovups	48(%rdx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovups	40(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovups	56(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	movq	%rdx, %rbx
	movq	%rcx, %rdi
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2736(%rsp)      # 16-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	movslq	2672(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	3712(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	48(%rbx,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm14
	vmovaps	%xmm14, 3712(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3776(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3808(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3328(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovups	48(%rbx,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm3
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	movq	3216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	movslq	2768(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5184(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3184(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	movslq	3200(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2608(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4192(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3264(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1124
# BB#1123:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	2544(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
.LBB147_1124:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vsubps	3408(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2656(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vsubps	3424(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3456(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vmulps	5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 3248(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3216(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3488(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2768(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3328(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3744(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2544(%rsp)       # 16-byte Spill
	je	.LBB147_1126
# BB#1125:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2432(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3120(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1126:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3360(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2592(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2576(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3280(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vmulps	4160(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rbx,%r12,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4192(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2560(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vaddps	3376(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3584(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3296(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1128
# BB#1127:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovdqa	2448(%rsp), %xmm6       # 16-byte Reload
.LBB147_1128:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2592(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vsubps	2576(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	3232(%rsp), %xmm3       # 16-byte Reload
	vmulps	3808(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3360(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3216(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3408(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2816(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1130
# BB#1129:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovdqa	2464(%rsp), %xmm4       # 16-byte Reload
.LBB147_1130:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3120(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3264(%rsp), %xmm4       # 16-byte Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2848(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1132
# BB#1131:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	2480(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
.LBB147_1132:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4192(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm4       # 16-byte Reload
	vmulps	5184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3216(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3136(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1134
# BB#1133:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	2496(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3152(%rsp)       # 16-byte Spill
.LBB147_1134:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3776(%rsp), %xmm4       # 16-byte Reload
	vmulps	4160(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	4128(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2656(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2768(%rsp), %xmm4       # 16-byte Reload
	vaddps	3376(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3744(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5424(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1136
# BB#1135:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovdqa	2512(%rsp), %xmm15      # 16-byte Reload
.LBB147_1136:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vaddps	3488(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2752(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3616(%rsp), %xmm6       # 16-byte Reload
	vmulps	3808(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5248(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3552(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3840(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vaddps	3264(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3168(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	movq	4656(%rsp), %rcx        # 8-byte Reload
	vmovdqa	2832(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1138
# BB#1137:                              # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovdqa	2528(%rsp), %xmm0       # 16-byte Reload
.LBB147_1138:                           # %for f7.s0.v10.v10461
                                        #   in Loop: Header=BB147_1106 Depth=4
	vmovaps	3392(%rsp), %xmm7       # 16-byte Reload
	vaddps	3424(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5152(%rsp), %rax        # 4-byte Folded Reload
	movq	2320(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	3008(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1106
# BB#1139:                              #   in Loop: Header=BB147_1098 Depth=3
	movl	2192(%rsp), %ecx        # 4-byte Reload
.LBB147_1140:                           # %end for f7.s0.v10.v10462
                                        #   in Loop: Header=BB147_1098 Depth=3
	movl	%ecx, %r8d
	movq	2112(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	movl	1736(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_1098
.LBB147_1141:                           # %end for f7.s0.v11460
                                        #   in Loop: Header=BB147_466 Depth=2
	movslq	972(%rsp), %rax         # 4-byte Folded Reload
	movq	%rax, 4160(%rsp)        # 8-byte Spill
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	movq	2112(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %eax
	movq	%rax, 2104(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	cmpl	%eax, %ecx
	jl	.LBB147_1142
	jmp	.LBB147_1234
.LBB147_1197:                           # %end for f7.s0.v10.v10472.end for f7.s0.v10.v10476_crit_edge
                                        #   in Loop: Header=BB147_1142 Depth=3
	movq	2104(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2104(%rsp)        # 8-byte Spill
	movq	4192(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	vmovaps	5424(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB147_1233
	.align	16, 0x90
.LBB147_1142:                           # %for f7.s0.v11465
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1144 Depth 4
                                        #         Child Loop BB147_1179 Depth 4
                                        #         Child Loop BB147_1199 Depth 4
	cmpl	$0, 3100(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1177
# BB#1143:                              # %for f7.s0.v10.v10467.preheader
                                        #   in Loop: Header=BB147_1142 Depth=3
	movq	4192(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %eax
	andl	$1, %eax
	movl	%eax, 3104(%rsp)        # 4-byte Spill
	movl	%edx, %r10d
	andl	$63, %r10d
	movl	%r10d, %eax
	movq	1288(%rsp), %r8         # 8-byte Reload
	imull	%r8d, %eax
	movq	%rax, 2416(%rsp)        # 8-byte Spill
	movl	%edx, %ebx
	movq	1624(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ebx
	leal	8(%rbx), %eax
	movq	1648(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2352(%rsp)       # 16-byte Spill
	movq	%rdx, %rax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %rax
	movq	1800(%rsp), %r14        # 8-byte Reload
	leaq	(%rax,%r14), %rax
	leal	10(%rbx), %ecx
	imull	%edi, %ecx
	movq	%rcx, 2336(%rsp)        # 8-byte Spill
	movq	1824(%rsp), %r9         # 8-byte Reload
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	leaq	2(%rdx), %rcx
	movq	%rcx, %rax
	imulq	%rsi, %rax
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	leal	6(%rbx), %ecx
	imull	%edi, %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	leaq	(%rax,%r14), %rax
	leaq	-2(%rdx), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	leal	62(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leal	7(%rbx), %eax
	imull	%edi, %eax
	movq	%rax, 2272(%rsp)        # 8-byte Spill
	leaq	-1(%rdx), %rax
	imulq	%rsi, %rax
	leal	63(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2256(%rsp)        # 8-byte Spill
	addl	$9, %ebx
	imull	%edi, %ebx
	movq	%rbx, 2400(%rsp)        # 8-byte Spill
	leaq	(%rax,%r14), %rax
	leaq	1(%rdx), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	movq	2104(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2240(%rsp)        # 8-byte Spill
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	imulq	1728(%rsp), %r10        # 8-byte Folded Reload
	subq	4712(%rsp), %r10        # 8-byte Folded Reload
	movq	%r10, 2432(%rsp)        # 8-byte Spill
	xorl	%r10d, %r10d
	.align	16, 0x90
.LBB147_1144:                           # %for f7.s0.v10.v10467
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1142 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	cmpl	$0, 3104(%rsp)          # 4-byte Folded Reload
	sete	3712(%rsp)              # 1-byte Folded Spill
	setne	3680(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r10,8), %ecx
	movl	%ecx, 3744(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movl	%ecx, %r8d
	movq	4672(%rsp), %rax        # 8-byte Reload
	subl	%eax, %r8d
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3376(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 5152(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	leal	2(%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%ecx, %r9d
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%esi, %ecx
	idivl	%ecx
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %esi
	movl	%edx, %r12d
	leal	-2(%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	%r9d, %ebx
	idivl	%ebx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	vmovd	%r14d, %xmm0
	leal	-1(%r8), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vpinsrd	$1, %r13d, %xmm0, %xmm0
	vpinsrd	$2, 5152(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vpinsrd	$3, 3648(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vmovd	%r15d, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpinsrd	$1, 3616(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, 3584(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r13d
	movl	%edx, %esi
	vpinsrd	$3, %r12d, %xmm2, %xmm1
	vmovdqa	%xmm1, 5152(%rsp)       # 16-byte Spill
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	2(%r12,%r10,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovd	%r9d, %xmm4
	leal	1(%r8), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vpinsrd	$1, 3552(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, %r11d, %xmm4, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, 3536(%rsp), %xmm4, %xmm6 # 4-byte Folded Reload
	leal	-2(%r12,%r10,8), %eax
	vmovd	%eax, %xmm5
	vmovd	%r14d, %xmm4
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2352(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3744(%rsp), %r11d       # 4-byte Reload
	vmovd	%r11d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5328(%rsp), %xmm13      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm13, %xmm1
	vmovdqa	5296(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm7, %xmm8, %xmm2
	vblendvps	%xmm1, %xmm7, %xmm2, %xmm1
	vmovdqa	5056(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	4992(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vmovdqa	5344(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm12, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm10, %xmm0, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vmovdqa	5360(%rsp), %xmm11      # 16-byte Reload
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	5424(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm1, %xmm15, %xmm1
	vpinsrd	$1, 3488(%rsp), %xmm4, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r15d, %xmm2, %xmm2
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%edi
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3616(%rsp)        # 8-byte Spill
	vpinsrd	$3, %esi, %xmm2, %xmm2
	leal	-1(%r12,%r10,8), %eax
	vmovd	%eax, %xmm4
	vmovaps	%xmm4, 3584(%rsp)       # 16-byte Spill
	vmovq	%xmm1, %rsi
	movq	%rsi, 3184(%rsp)        # 8-byte Spill
	vmovdqa	5152(%rsp), %xmm4       # 16-byte Reload
	vpsrad	$31, %xmm4, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm7
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm7, %xmm1, %xmm4, %xmm1
	vmovdqa	4896(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	4736(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	3648(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm6, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm6, %xmm4, %xmm4
	vpcmpgtd	%xmm4, %xmm13, %xmm6
	vpsubd	%xmm4, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vmovdqa	5136(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vmovdqa	5088(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vmovdqa	%xmm10, %xmm7
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm6, %xmm4, %xmm5, %xmm4
	vpsrad	$31, %xmm2, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm2
	vpcmpgtd	%xmm2, %xmm13, %xmm5
	vpsubd	%xmm2, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5120(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	.LCPI147_55(%rip), %xmm5, %xmm5
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%ecx, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %r9d, %xmm6, %xmm3
	sarq	$32, %rsi
	movq	%rsi, 2768(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r13d
	sarq	$32, %rbx
	movq	%rbx, 2752(%rsp)        # 8-byte Spill
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vmovq	%xmm1, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3168(%rsp)        # 8-byte Spill
	vpmulld	%xmm11, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpaddd	%xmm12, %xmm2, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	3584(%rsp), %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	5040(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm10, %xmm2, %xmm2
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm13, %xmm3
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	leal	1(%r12,%r10,8), %r9d
	vmovd	%r9d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	movb	3680(%rsp), %r15b       # 1-byte Reload
	andb	%r15b, 5216(%rsp)       # 1-byte Folded Spill
	movl	%r11d, %ecx
	movl	%ecx, %eax
	movq	4192(%rsp), %r11        # 8-byte Reload
	orl	%r11d, %eax
	testb	$1, %al
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %rdx
	movq	%rdx, 3200(%rsp)        # 8-byte Spill
	sete	3152(%rsp)              # 1-byte Folded Spill
	movl	3104(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3264(%rsp)              # 1-byte Folded Spill
	movb	3712(%rsp), %bl         # 1-byte Reload
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	%bl, %al
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movq	%rdx, %rax
	orq	$6, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3584(%rsp)        # 8-byte Spill
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	cltq
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movl	%r9d, %r13d
	andl	$1, %r13d
	sete	%r14b
	addl	$3, %r8d
	vmovd	%r8d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3392(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3376(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	3(%rax,%r10,8), %eax
	vmovd	%eax, %xmm2
	andb	%r15b, %r14b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm9, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm3
	vpsubd	%xmm1, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm10, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovq	%xmm0, %r8
	movq	%r8, 3376(%rsp)         # 8-byte Spill
	sarq	$32, %r8
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movl	%r9d, %eax
	orl	%r11d, %eax
	testb	$1, %al
	sete	%al
	testl	%r9d, %r12d
	movzbl	3152(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%bl, %r13b
	movq	2336(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %ecx
	movslq	%ecx, %r15
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %edi
	movslq	%edi, %r9
	movq	%r15, %r11
	orq	$6, %r11
	movq	%r9, %r12
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm5
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%xmm5, %xmm0
	cmpl	$1, 104(%rbp)
	je	.LBB147_1146
# BB#1145:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1146:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	movl	5248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %edi
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm4
	vmovaps	%xmm4, %xmm1
	je	.LBB147_1148
# BB#1147:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1148:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	movzbl	3264(%rsp), %edi        # 1-byte Folded Reload
	vmovd	%edi, %xmm0
	je	.LBB147_1150
# BB#1149:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1150:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3120(%rsp)       # 16-byte Spill
	je	.LBB147_1152
# BB#1151:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1152:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm1, 2448(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	je	.LBB147_1154
# BB#1153:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1154:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 2816(%rsp)       # 16-byte Spill
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm4
	je	.LBB147_1156
# BB#1155:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm4, %xmm4, %xmm4
.LBB147_1156:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm4, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm4
	vmovaps	%xmm4, 3152(%rsp)       # 16-byte Spill
	movzbl	%dl, %eax
	vmovd	%eax, %xmm0
	movq	5032(%rsp), %rdi        # 8-byte Reload
	movq	5608(%rsp), %rdx        # 8-byte Reload
	je	.LBB147_1158
# BB#1157:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm4, %xmm4, %xmm4
.LBB147_1158:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 3008(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	je	.LBB147_1160
# BB#1159:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1160:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	3184(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2768(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2752(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2768(%rsp)       # 16-byte Spill
	movq	2736(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2784(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3168(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3712(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm13
	vmovaps	5184(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm6, %xmm3
	movq	3200(%rsp), %rbx        # 8-byte Reload
	vmovups	32(%rdx,%rbx,4), %xmm15
	vmovups	48(%rdx,%rbx,4), %xmm14
	vshufps	$136, %xmm14, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm14[0,2]
	vmovaps	5616(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	5632(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movq	%rdx, %rsi
	movslq	%eax, %r14
	vmovups	8(%rdi,%r14,4), %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r14,4), %xmm3
	vmovaps	%xmm3, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm3[0,2]
	vmovaps	4128(%rsp), %xmm11      # 16-byte Reload
	vmovaps	%xmm6, %xmm3
	vmulps	%xmm11, %xmm3, %xmm6
	vmovups	32(%rsi,%r15,4), %xmm10
	vmovups	48(%rsi,%r15,4), %xmm9
	vshufps	$136, %xmm9, %xmm10, %xmm7 # xmm7 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm0, %xmm7, %xmm7
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm8
	vminps	%xmm8, %xmm6, %xmm6
	vmaxps	%xmm2, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmovups	32(%rsi,%r9,4), %xmm3
	vmovups	48(%rsi,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm7[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rbx,4), %xmm6
	vmovaps	%xmm6, 3616(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%rbx,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm13 # xmm13 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm13, 2736(%rsp)      # 16-byte Spill
	movq	3216(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm12, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movslq	%eax, %r13
	vmovups	8(%rdi,%r13,4), %xmm1
	vmovups	24(%rdi,%r13,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm6, 3344(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm7[1,3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3296(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm12, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm14[1,3]
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm10, %xmm1 # xmm1 = xmm10[1,3],xmm9[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm11, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rsi,%rax,4), %xmm15
	vmovups	48(%rsi,%rax,4), %xmm14
	vshufps	$136, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3808(%rsp), %xmm6       # 16-byte Reload
	vmovaps	2768(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rax,4), %xmm7
	vmovaps	%xmm7, 3648(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	3712(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm6, %xmm11, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm2, %xmm1
	movq	3408(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rsi,%rax,4), %xmm2
	vmovups	48(%rsi,%rax,4), %xmm4
	vshufps	$136, %xmm4, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rax,4), %xmm10
	vmovaps	%xmm10, 3584(%rsp)      # 16-byte Spill
	vmovups	56(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm10, %xmm3 # xmm3 = xmm10[0,2],xmm1[0,2]
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm9, %xmm11, %xmm1
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$221, %xmm4, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm4[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm9, %xmm12, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm6, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm10[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm9, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm0, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm6, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2592(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	5152(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	cltq
	vmovaps	5248(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm2, %xmm6
	vmovaps	5216(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmovaps	%xmm2, 3376(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm10
	vmovaps	3296(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm4
	vmulps	5184(%rsp), %xmm12, %xmm11 # 16-byte Folded Reload
	vmovaps	3280(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm14
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbx
	vinsertps	$32, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	2256(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r10,8), %ebx
	movslq	%ebx, %rbx
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovaps	3184(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rsi        # 8-byte Reload
	vinsertps	$48, (%rcx,%rsi,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	movq	2240(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10,8), %esi
	movslq	%esi, %rsi
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm3, %xmm15
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm3
	vmovups	16(%rdi,%rax,4), %xmm7
	vmovaps	%xmm7, 5152(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm9
	vmovaps	%xmm9, 3248(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm0
	vmovups	24(%rdi,%rsi,4), %xmm12
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rbx,4), %xmm2
	vmovups	24(%rdi,%rbx,4), %xmm1
	vmovups	16(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3184(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm13, %xmm5 # xmm5 = xmm13[0,2],xmm3[0,2]
	vmovaps	%xmm5, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[1,3],xmm7[1,3]
	je	.LBB147_1162
# BB#1161:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	2608(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3136(%rsp)       # 16-byte Spill
.LBB147_1162:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vsubps	3344(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm5, 3344(%rsp)       # 16-byte Spill
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2704(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovaps	%xmm5, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm7, %xmm10, %xmm5
	vmovaps	%xmm5, 3312(%rsp)       # 16-byte Spill
	vsubps	3216(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm13, %xmm3 # xmm3 = xmm13[1,3],xmm3[1,3]
	vmovaps	%xmm3, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm14, %xmm11, %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	3200(%rsp), %xmm4       # 16-byte Reload
	vmulps	5184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm4, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	3376(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm3, %xmm4, %xmm13
	vmovaps	2624(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm3, %xmm4, %xmm4
	vmovaps	3408(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 3216(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2624(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2608(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmovaps	%xmm5, 2592(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm11
	vmovaps	2560(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm10
	vmovaps	3392(%rsp), %xmm5       # 16-byte Reload
	vmaxps	%xmm3, %xmm5, %xmm5
	vmovaps	%xmm5, 3376(%rsp)       # 16-byte Spill
	vmaxps	%xmm3, %xmm15, %xmm5
	vmovaps	%xmm5, 2704(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm7, %xmm5
	vmovaps	%xmm5, 2560(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	3424(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vmovaps	%xmm9, 2752(%rsp)       # 16-byte Spill
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3296(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm3[0,2],mem[0,2]
	vshufps	$136, %xmm1, %xmm2, %xmm6 # xmm6 = xmm2[0,2],xmm1[0,2]
	vmovaps	5248(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm3, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm3[0,2],mem[0,2]
	vshufps	$136, %xmm12, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm12[0,2]
	vmovaps	5216(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3456(%rsp), %xmm3, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm3[0,2],mem[0,2]
	vaddps	3680(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	je	.LBB147_1164
# BB#1163:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3120(%rsp)       # 16-byte Spill
.LBB147_1164:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vsubps	%xmm15, %xmm13, %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vsubps	3360(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm2, %xmm0 # xmm0 = xmm2[1,3],xmm1[1,3]
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm11, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm14, %xmm10, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rdx,%r11,4), %xmm0
	vmovups	40(%rdx,%r15,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vmulps	4128(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r14,4), %xmm1
	vmovups	16(%rdi,%r14,4), %xmm5
	vmovaps	%xmm5, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rdx,%r12,4), %xmm2
	vmovups	40(%rdx,%r9,4), %xmm5
	vmovaps	%xmm5, 2736(%rsp)       # 16-byte Spill
	movq	%rdx, %rax
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm14
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%r13,4), %xmm2
	vmovups	16(%rdi,%r13,4), %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	2560(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm4
	vaddps	3344(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmovaps	3216(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm7
	vmovaps	2624(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm12
	vmovaps	2608(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm3
	vmovaps	2592(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm2
	vmovaps	3232(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm15 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm15, %xmm10
	vmovaps	3168(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3184(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm11
	vmovdqa	3264(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1166
# BB#1165:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovdqa	2448(%rsp), %xmm6       # 16-byte Reload
.LBB147_1166:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vsubps	%xmm0, %xmm4, %xmm0
	vsubps	2576(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3264(%rsp)       # 16-byte Spill
	vsubps	2544(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3232(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm2, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vmovaps	3200(%rsp), %xmm4       # 16-byte Reload
	vmulps	3776(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	3168(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm8, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	3808(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm14, %xmm3
	vmovaps	%xmm14, %xmm12
	vxorps	%xmm14, %xmm14, %xmm14
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	3328(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm10, %xmm3, %xmm7
	vmovaps	%xmm3, %xmm10
	vpslld	$31, %xmm6, %xmm3
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm11, %xmm1, %xmm6
	vmovdqa	2816(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1168
# BB#1167:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovdqa	2464(%rsp), %xmm4       # 16-byte Reload
.LBB147_1168:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vaddps	%xmm0, %xmm2, %xmm1
	vbroadcastss	.LCPI147_23(%rip), %xmm9
	vmovdqa	3120(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vmulps	%xmm11, %xmm7, %xmm7
	vpslld	$31, %xmm4, %xmm4
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vaddps	3264(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm2
	vmovaps	%xmm2, 3200(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm0, %xmm14, %xmm0
	vblendvps	%xmm3, %xmm6, %xmm0, %xmm3
	je	.LBB147_1170
# BB#1169:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	2480(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
.LBB147_1170:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vblendvps	%xmm5, %xmm7, %xmm3, %xmm3
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[1,3],mem[1,3]
	vmovaps	3536(%rsp), %xmm2       # 16-byte Reload
	vmulps	5184(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm0, %xmm4, %xmm0
	vaddps	%xmm10, %xmm15, %xmm4
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm4, %xmm4
	vmovdqa	3136(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vmulps	%xmm9, %xmm1, %xmm1
	vmovdqa	2832(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1172
# BB#1171:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovdqa	2496(%rsp), %xmm15      # 16-byte Reload
.LBB147_1172:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vblendvps	%xmm5, %xmm1, %xmm3, %xmm1
	vaddps	3312(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vmulps	4128(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rax,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rax,%r9,4), %xmm2, %xmm4 # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmovaps	%xmm12, %xmm6
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2640(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r13,4), %xmm2, %xmm4 # xmm4 = xmm2[0,2],mem[0,2]
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2752(%rsp), %xmm2       # 16-byte Reload
	vaddps	3344(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vaddps	3680(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3328(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm0, %xmm3
	vmovdqa	3008(%rsp), %xmm2       # 16-byte Reload
	je	.LBB147_1174
# BB#1173:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovdqa	2512(%rsp), %xmm2       # 16-byte Reload
.LBB147_1174:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vaddps	3424(%rsp), %xmm1, %xmm10 # 16-byte Folded Reload
	vmulps	%xmm9, %xmm5, %xmm9
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3584(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm6, %xmm1
	vmulps	%xmm4, %xmm1, %xmm4
	vmovaps	3536(%rsp), %xmm6       # 16-byte Reload
	vmulps	3776(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3808(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3216(%rsp), %xmm1       # 16-byte Reload
	vaddps	3232(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3248(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm11, %xmm3, %xmm5
	vmulps	%xmm11, %xmm0, %xmm6
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm3
	vpslld	$31, %xmm15, %xmm4
	vpslld	$31, %xmm2, %xmm7
	movq	4656(%rsp), %rcx        # 8-byte Reload
	vmovdqa	2848(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_1176
# BB#1175:                              # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovdqa	2528(%rsp), %xmm1       # 16-byte Reload
.LBB147_1176:                           # %for f7.s0.v10.v10467
                                        #   in Loop: Header=BB147_1144 Depth=4
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vaddps	3392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	3200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpslld	$31, %xmm1, %xmm1
	vblendvps	%xmm1, %xmm0, %xmm14, %xmm0
	vblendvps	%xmm7, %xmm6, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm3, %xmm9, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm10, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3744(%rsp), %rax        # 4-byte Folded Reload
	movq	2432(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r10
	cmpl	3100(%rsp), %r10d       # 4-byte Folded Reload
	jne	.LBB147_1144
.LBB147_1177:                           # %end for f7.s0.v10.v10468
                                        #   in Loop: Header=BB147_1142 Depth=3
	movl	3100(%rsp), %eax        # 4-byte Reload
	cmpl	1388(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1196
# BB#1178:                              # %for f7.s0.v10.v10471.preheader
                                        #   in Loop: Header=BB147_1142 Depth=3
	movq	4192(%rsp), %r12        # 8-byte Reload
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 2416(%rsp)        # 4-byte Spill
	movl	%r12d, %r10d
	andl	$63, %r10d
	movq	%r12, %rax
	movq	1816(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	imulq	%rsi, %rax
	movq	1800(%rsp), %rdi        # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	movq	1824(%rsp), %rdx        # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	leaq	2(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	-2(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	leaq	-1(%r12), %rcx
	imulq	%rsi, %rcx
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	leaq	(%rcx,%rdi), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	leaq	1(%r12), %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rdi), %rax
	movq	%r10, %r8
	movq	2104(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %r15d
	andl	$63, %r15d
	movl	1700(%rsp), %r14d       # 4-byte Reload
	imull	%r14d, %r15d
	movq	1464(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movq	1688(%rsp), %r13        # 8-byte Reload
	imull	%r13d, %ecx
	movq	%rcx, 4128(%rsp)        # 8-byte Spill
	imulq	1728(%rsp), %r8         # 8-byte Folded Reload
	movb	%r12b, %cl
	addb	$63, %cl
	movzbl	%cl, %r9d
	andl	$63, %r9d
	imull	%r14d, %r9d
	movq	1456(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r12), %r11d
	imull	%r13d, %r11d
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	movb	%r12b, %al
	addb	$62, %al
	movzbl	%al, %edi
	andl	$63, %edi
	imull	%r14d, %edi
	movq	1448(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %ebx
	imull	%r13d, %ebx
	leal	2(%r12), %esi
	andl	$63, %esi
	imull	%r14d, %esi
	movq	1440(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %edx
	imull	%r13d, %edx
	movq	1432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	imull	%r13d, %eax
	subq	4712(%rsp), %r8         # 8-byte Folded Reload
	movq	%r8, 2400(%rsp)         # 8-byte Spill
	movq	4128(%rsp), %r12        # 8-byte Reload
	imull	%r14d, %r10d
	movq	%rax, %r14
	movl	1248(%rsp), %r8d        # 4-byte Reload
	movq	5288(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1179:                           # %for f7.s0.v10.v10471
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1142 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r11, 3680(%rsp)        # 8-byte Spill
	movq	%r12, 4128(%rsp)        # 8-byte Spill
	movq	%rsi, 3584(%rsp)        # 8-byte Spill
	movq	%rdi, 3648(%rsp)        # 8-byte Spill
	movq	%r9, 3712(%rsp)         # 8-byte Spill
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	movl	%r8d, 2832(%rsp)        # 4-byte Spill
	movq	%r14, 2848(%rsp)        # 8-byte Spill
	movq	%rdx, 3008(%rsp)        # 8-byte Spill
	movq	%rbx, 3104(%rsp)        # 8-byte Spill
	movq	%r15, 3616(%rsp)        # 8-byte Spill
	movq	%r10, 3120(%rsp)        # 8-byte Spill
	movl	2416(%rsp), %r13d       # 4-byte Reload
	testl	%r13d, %r13d
	sete	5216(%rsp)              # 1-byte Folded Spill
	setne	%r12b
	movq	2136(%rsp), %r15        # 8-byte Reload
	movq	%rbx, %r8
	movq	%rdx, %rsi
	leal	(%r15,%rax), %r11d
	movslq	%r11d, %r9
	movq	%r9, 2736(%rsp)         # 8-byte Spill
	andl	$1, %r11d
	sete	%cl
	movq	%r9, %rdx
	movq	4608(%rsp), %rbx        # 8-byte Reload
	imulq	%rbx, %rdx
	movq	4824(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	leaq	2(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	leaq	-2(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 5248(%rsp)        # 8-byte Spill
	leaq	-1(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	leaq	1(%r9), %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	andb	%r12b, %cl
	movb	%cl, 3536(%rsp)         # 1-byte Spill
	movl	%r9d, %ecx
	movq	4192(%rsp), %rdx        # 8-byte Reload
	orl	%edx, %ecx
	testb	$1, %cl
	sete	%cl
	leal	(%r15,%r14), %edx
	movslq	%edx, %r14
	leal	(%r15,%rsi), %edx
	movslq	%edx, %rax
	leal	(%r15,%r8), %edx
	movslq	%edx, %rdx
	testl	%r9d, %r13d
	movq	%rax, %r13
	setne	3456(%rsp)              # 1-byte Folded Spill
	andb	5216(%rsp), %r11b       # 1-byte Folded Reload
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %eax
	movslq	%eax, %r12
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	movq	4128(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %ecx
	movslq	%ecx, %r8
	movq	%r8, %rcx
	orq	$6, %rcx
	movq	%rcx, 3360(%rsp)        # 8-byte Spill
	leaq	3(%r9), %rsi
	imulq	%rbx, %rsi
	movq	%r12, %rcx
	orq	$6, %rcx
	movq	%rcx, 3376(%rsp)        # 8-byte Spill
	leaq	(%rsi,%rdi), %rcx
	movq	%rcx, 3392(%rsp)        # 8-byte Spill
	movq	%r13, %rcx
	orq	$6, %rcx
	movq	%rcx, 2704(%rsp)        # 8-byte Spill
	movq	%rdx, %rcx
	movq	%rdx, %rsi
	orq	$6, %rcx
	movq	%rcx, 2720(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	leal	(%r15,%r10), %r9d
	movq	3584(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %r10d
	movq	3648(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	leal	(%r15,%rax), %ecx
	movl	%ecx, 5216(%rsp)        # 4-byte Spill
	movq	3616(%rsp), %rcx        # 8-byte Reload
	leal	(%r15,%rcx), %ecx
	movl	%ecx, 3408(%rsp)        # 4-byte Spill
	movq	3616(%rsp), %r15        # 8-byte Reload
	je	.LBB147_1181
# BB#1180:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1181:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	3536(%rsp), %ebx        # 1-byte Folded Reload
	vmovd	%ebx, %xmm0
	movzbl	%r11b, %edi
	vmovd	%edi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	je	.LBB147_1183
# BB#1182:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1183:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm1, 2464(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	movzbl	3456(%rsp), %edx        # 1-byte Folded Reload
	vmovd	%edx, %xmm0
	movq	%rsi, %rax
	je	.LBB147_1185
# BB#1184:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1185:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm1, 2432(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	je	.LBB147_1187
# BB#1186:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1187:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm0, 2448(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rdi        # 8-byte Reload
	movq	3552(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rdi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rcx,4), %rdx
	movq	4680(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm2, 2656(%rsp)       # 16-byte Spill
	vmovaps	5184(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm2, %xmm0
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	32(%rsi,%r14,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovups	48(%rsi,%r14,4), %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	5616(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm1
	vmovaps	5632(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	movslq	%r10d, %r10
	movq	5032(%rsp), %rbx        # 8-byte Reload
	vmovups	8(%rbx,%r10,4), %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmovaps	3840(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm13, %xmm2, %xmm3
	vmovaps	%xmm2, %xmm1
	movq	%r13, 3328(%rsp)        # 8-byte Spill
	vmovups	32(%rsi,%r13,4), %xmm2
	vmovaps	%xmm2, 3200(%rsp)       # 16-byte Spill
	vmovups	48(%rsi,%r13,4), %xmm15
	vshufps	$136, %xmm15, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm15[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vbroadcastss	.LCPI147_17(%rip), %xmm10
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	movslq	3488(%rsp), %r11        # 4-byte Folded Reload
	vmovups	8(%rbx,%r11,4), %xmm9
	vmovups	24(%rbx,%r11,4), %xmm7
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm5
	vmovups	32(%rsi,%rax,4), %xmm4
	vmovups	48(%rsi,%rax,4), %xmm3
	movq	%rax, %r13
	vshufps	$136, %xmm3, %xmm4, %xmm0 # xmm0 = xmm4[0,2],xmm3[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vshufps	$136, %xmm7, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm7[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	movq	5152(%rsp), %rax        # 8-byte Reload
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%rax,4), %rdx
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm11, 2768(%rsp)      # 16-byte Spill
	vmovups	40(%rsi,%r14,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r14,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm12, %xmm11, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	movslq	%r9d, %r9
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm14
	vmovups	16(%rbx,%r9,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm5[0,2]
	vsubps	%xmm5, %xmm14, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rax        # 8-byte Reload
	leaq	(%rdi,%rax,4), %rdx
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm14, 2608(%rsp)      # 16-byte Spill
	movq	3312(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 3264(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm12, %xmm14, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm9, %xmm0 # xmm0 = xmm9[1,3],xmm7[1,3]
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm4, %xmm0 # xmm0 = xmm4[1,3],xmm3[1,3]
	movq	3424(%rsp), %r14        # 8-byte Reload
	leaq	(%rdi,%r14,4), %rdx
	vmovss	(%rdi,%r14,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm2, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	3184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3168(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	3200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm15[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm13, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r12,4), %xmm12
	vmovups	48(%rsi,%r12,4), %xmm15
	vshufps	$136, %xmm15, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm15[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3776(%rsp), %xmm4       # 16-byte Reload
	vmovaps	2656(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm4, %xmm2, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%r12,4), %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r12,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm11, %xmm7
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vmovaps	3744(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm2
	vmovups	32(%rsi,%r8,4), %xmm13
	vmovups	48(%rsi,%r8,4), %xmm1
	vshufps	$136, %xmm1, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm1[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%r8,4), %xmm7
	vmovaps	%xmm7, 3424(%rsp)       # 16-byte Spill
	vmovups	56(%rsi,%r8,4), %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm11, %xmm2
	vmulps	%xmm0, %xmm2, %xmm2
	vshufps	$221, %xmm1, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm15, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm15[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm7[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm5, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	3376(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movq	5248(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdi,%rdx,4), %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %r14        # 8-byte Reload
	leaq	(%rdi,%r14,4), %rax
	vmovss	(%rdi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm5
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vmulps	5184(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm8
	vmovaps	3184(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm13
	movslq	5216(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movslq	3408(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	2512(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm6
	vminps	%xmm10, %xmm2, %xmm2
	cmpl	$0, 104(%rbp)
	vmovups	8(%rbx,%r9,4), %xmm12
	vmovups	24(%rbx,%r9,4), %xmm0
	vmovups	(%rbx,%r9,4), %xmm15
	vmovups	8(%rbx,%rcx,4), %xmm7
	vmovups	24(%rbx,%rcx,4), %xmm3
	vmovups	16(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	8(%rbx,%rax,4), %xmm11
	vmovups	24(%rbx,%rax,4), %xmm14
	vmovups	16(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm1 # xmm1 = xmm12[0,2],xmm0[0,2]
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, 5152(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[1,3],mem[1,3]
	je	.LBB147_1189
# BB#1188:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm2, 3392(%rsp)       # 16-byte Spill
	vmovaps	2480(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 3216(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
.LBB147_1189:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vsubps	%xmm1, %xmm5, %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vsubps	3344(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm0[1,3]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmulps	%xmm8, %xmm9, %xmm12
	vsubps	3248(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3200(%rsp), %xmm1       # 16-byte Reload
	vmulps	5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2592(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vmovaps	2544(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2544(%rsp)       # 16-byte Spill
	vmovaps	2528(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	2496(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vxorps	%xmm4, %xmm4, %xmm4
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm4, %xmm1, %xmm9
	vmovaps	2624(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm4, %xmm1, %xmm13
	vmaxps	%xmm4, %xmm6, %xmm8
	vmaxps	%xmm4, %xmm2, %xmm1
	vmovaps	%xmm1, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, 5152(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[0,2],mem[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm5
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm11, %xmm4 # xmm4 = xmm11[0,2],xmm14[0,2]
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3312(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm3, %xmm7, %xmm0 # xmm0 = xmm7[0,2],xmm3[0,2]
	vmovaps	5216(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, 3296(%rsp), %xmm6, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm6[0,2],mem[0,2]
	movq	4656(%rsp), %rcx        # 8-byte Reload
	movq	4128(%rsp), %rdx        # 8-byte Reload
	movq	2848(%rsp), %r14        # 8-byte Reload
	movl	2832(%rsp), %r8d        # 4-byte Reload
	je	.LBB147_1191
# BB#1190:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	%xmm12, %xmm6
	vmovaps	2464(%rsp), %xmm12      # 16-byte Reload
	vmovaps	%xmm12, 3136(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, %xmm12
.LBB147_1191:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vshufps	$221, %xmm3, %xmm7, %xmm3 # xmm3 = xmm7[1,3],xmm3[1,3]
	vmovaps	%xmm3, 2464(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[1,3],xmm14[1,3]
	vaddps	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm2, 2528(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm9, %xmm2
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm13, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vsubps	%xmm0, %xmm8, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	movq	2704(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovups	40(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm0, %xmm15, %xmm0
	vmovaps	2608(%rsp), %xmm2       # 16-byte Reload
	vmulps	3840(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rbx,%r10,4), %xmm1
	vmovups	16(%rbx,%r10,4), %xmm5
	vmovaps	%xmm5, 2592(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3808(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	movq	2720(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm2
	vmovups	40(%rsi,%r13,4), %xmm5
	vmovaps	%xmm5, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm15, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rbx,%r11,4), %xmm2
	vmovups	16(%rbx,%r11,4), %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	2480(%rsp), %xmm1       # 16-byte Reload
	vmaxps	%xmm6, %xmm1, %xmm1
	vmovaps	2544(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm3
	vmovaps	2512(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm5
	vmovaps	2496(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm6, %xmm2, %xmm4
	vmovdqa	3216(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 2512(%rsp)       # 16-byte Spill
	vmovdqa	3136(%rsp), %xmm7       # 16-byte Reload
	vpslld	$31, %xmm7, %xmm2
	vmovdqa	%xmm2, 2496(%rsp)       # 16-byte Spill
	vminps	%xmm10, %xmm12, %xmm7
	vmaxps	%xmm6, %xmm7, %xmm7
	vsubps	3392(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm6, 3136(%rsp)       # 16-byte Spill
	vaddps	3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm6, %xmm0
	vaddps	3344(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3184(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm6, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm6[1,3],mem[1,3]
	vbroadcastss	.LCPI147_23(%rip), %xmm6
	vmovaps	%xmm6, 3216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm2
	je	.LBB147_1193
# BB#1192:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovaps	2432(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3152(%rsp)       # 16-byte Spill
.LBB147_1193:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vsubps	2464(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm9
	vsubps	%xmm0, %xmm5, %xmm11
	vsubps	%xmm7, %xmm4, %xmm12
	vmovaps	3200(%rsp), %xmm4       # 16-byte Reload
	vmulps	3744(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovaps	2560(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3424(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	3168(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	3776(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm4       # 16-byte Reload
	vmovaps	2576(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm4, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmovaps	%xmm15, %xmm6
	vmulps	%xmm5, %xmm3, %xmm3
	vmovaps	3184(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vaddps	3280(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm0, %xmm5
	vmovdqa	2512(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3184(%rsp)       # 16-byte Spill
	vmovaps	2528(%rsp), %xmm0       # 16-byte Reload
	vmulps	3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vmovdqa	2496(%rsp), %xmm0       # 16-byte Reload
	vpsrad	$31, %xmm0, %xmm0
	vmovdqa	%xmm0, 3200(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm3
	vmovaps	%xmm3, 2544(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm8, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm8
	vaddps	3248(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vaddps	3232(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm2, %xmm5
	vmovdqa	2800(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1195
# BB#1194:                              # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vmovdqa	2448(%rsp), %xmm0       # 16-byte Reload
.LBB147_1195:                           # %for f7.s0.v10.v10471
                                        #   in Loop: Header=BB147_1179 Depth=4
	vpslld	$31, %xmm0, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm15
	vaddps	%xmm14, %xmm15, %xmm0
	vaddps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vbroadcastss	.LCPI147_19(%rip), %xmm9
	vmulps	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm11, %xmm7
	vxorps	%xmm11, %xmm11, %xmm11
	vblendvps	%xmm2, %xmm0, %xmm11, %xmm0
	vblendvps	%xmm8, %xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3424(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2640(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	2656(%rsp), %xmm12      # 16-byte Reload
	vmulps	3744(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	5248(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vshufps	$221, 2672(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm4[1,3],mem[1,3]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	3776(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm10, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vsubps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm3, %xmm15, %xmm4
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vaddps	%xmm1, %xmm14, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	3232(%rsp), %xmm1       # 16-byte Reload
	vaddps	3264(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm9, %xmm1, %xmm1
	vblendvps	%xmm8, %xmm1, %xmm11, %xmm1
	vmovaps	2544(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm8
	vmovaps	3200(%rsp), %xmm9       # 16-byte Reload
	vmovaps	3152(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm9, 2576(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovaps	3184(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm0, 3168(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovaps	2704(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rsi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	2768(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rbx,%r10,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm11, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rsi,%r13,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vmulps	3808(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rbx,%r11,4), %xmm3, %xmm5 # xmm5 = xmm3[0,2],mem[0,2]
	vminps	%xmm10, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vsubps	%xmm5, %xmm4, %xmm4
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vaddps	3536(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3552(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3456(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm2, %xmm8, %xmm0
	vmovaps	5152(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2752(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	5184(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	3136(%rsp), %xmm3       # 16-byte Reload
	vaddps	3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm3, %xmm2
	vaddps	3376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vblendvps	%xmm9, %xmm2, %xmm0, %xmm0
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2400(%rsp), %rax        # 8-byte Reload
	movq	2736(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	addl	$8, %edx
	movq	3712(%rsp), %r9         # 8-byte Reload
	addl	$8, %r9d
	movq	3680(%rsp), %r11        # 8-byte Reload
	addl	$8, %r11d
	movq	3648(%rsp), %rdi        # 8-byte Reload
	addl	$8, %edi
	movq	3104(%rsp), %rbx        # 8-byte Reload
	addl	$8, %ebx
	movq	3584(%rsp), %rsi        # 8-byte Reload
	addl	$8, %esi
	movq	%rdx, %r12
	movq	3008(%rsp), %rdx        # 8-byte Reload
	addl	$8, %edx
	addl	$8, %r14d
	movq	3120(%rsp), %r10        # 8-byte Reload
	addl	$8, %r10d
	movq	2816(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	addl	$-1, %r8d
	jne	.LBB147_1179
.LBB147_1196:                           # %end for f7.s0.v10.v10472
                                        #   in Loop: Header=BB147_1142 Depth=3
	movl	1388(%rsp), %eax        # 4-byte Reload
	cmpl	1736(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1197
# BB#1198:                              # %for f7.s0.v10.v10475.preheader
                                        #   in Loop: Header=BB147_1142 Depth=3
	movq	4192(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 3008(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$63, %r8d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2352(%rsp)       # 16-byte Spill
	movq	%r9, %rcx
	movq	1816(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rdi
	imulq	%rdi, %rcx
	movq	1800(%rsp), %rbx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	1824(%rsp), %rsi        # 8-byte Reload
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	leaq	2(%r9), %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	leaq	-2(%r9), %rdx
	imulq	%rdi, %rdx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	-1(%r9), %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	leaq	1(%r9), %rcx
	movq	%rcx, 1376(%rsp)        # 8-byte Spill
	imulq	%rdi, %rcx
	leaq	(%rcx,%rbx), %rcx
	movq	2104(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2104(%rsp)        # 8-byte Spill
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movq	%r8, %rsi
	imulq	1728(%rsp), %rsi        # 8-byte Folded Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	movq	1464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r9), %edx
	movq	1688(%rsp), %rax        # 8-byte Reload
	imull	%eax, %edx
	subq	4712(%rsp), %rsi        # 8-byte Folded Reload
	movq	%rsi, 2336(%rsp)        # 8-byte Spill
	movq	1232(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$63, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1456(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%eax, %esi
	leal	(%rdx,%rbx), %edx
	movq	%rdx, 2304(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%eax, %edx
	leal	(%rsi,%rbx), %esi
	movq	%rsi, 2272(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %ecx
	movq	%rcx, 2256(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %ecx
	movq	%rcx, 2240(%rsp)        # 8-byte Spill
	leal	2(%r9), %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	1440(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%eax, %edx
	movq	1432(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r9), %esi
	imull	%eax, %esi
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2224(%rsp)        # 8-byte Spill
	imull	%edi, %r8d
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2208(%rsp)        # 8-byte Spill
	leal	(%rsi,%rbx), %eax
	movq	%rax, 2192(%rsp)        # 8-byte Spill
	leal	(%r8,%rbx), %eax
	movq	%rax, 2176(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1244(%rsp), %eax        # 4-byte Reload
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	.align	16, 0x90
.LBB147_1199:                           # %for f7.s0.v10.v10475
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1142 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	cmpl	$0, 3008(%rsp)          # 4-byte Folded Reload
	sete	3744(%rsp)              # 1-byte Folded Spill
	setne	%r10b
	movb	%r10b, 3248(%rsp)       # 1-byte Spill
	movq	2992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %ebx
	movl	%ebx, 2832(%rsp)        # 4-byte Spill
	movl	%ebx, %r13d
	andl	$1, %r13d
	sete	%r9b
	movq	2928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm2       # 16-byte Reload
	vpextrd	$1, %xmm2, %ecx
	movl	%ecx, 3296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %esi
	movl	%esi, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%esi, %r12d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r8d
	movl	%r8d, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovd	%esi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r11d
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2352(%rsp), %xmm13      # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm2
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %xmm8
	vmovdqa	5056(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm1
	vpaddd	%xmm11, %xmm8, %xmm3
	vmovdqa	5312(%rsp), %xmm9       # 16-byte Reload
	vpminsd	%xmm9, %xmm3, %xmm3
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vmovdqa	5328(%rsp), %xmm10      # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm10, %xmm4
	vmovdqa	5296(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm2, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	movq	2936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpminsd	%xmm9, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vmovd	%xmm4, %eax
	cltd
	idivl	%r12d
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vmovd	%edx, %xmm2
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r8d
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edx, %xmm2, %xmm2
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	4896(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	movq	2952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm10, %xmm5
	vpsubd	%xmm3, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	movq	2904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpaddd	%xmm7, %xmm3, %xmm3
	vpminsd	%xmm9, %xmm3, %xmm3
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm4, %xmm3, %xmm2
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm3
	movq	2960(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vpcmpgtd	%xmm4, %xmm10, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm9, %xmm5, %xmm5
	movq	2912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpmaxsd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovd	%xmm6, %eax
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vblendvps	%xmm3, %xmm5, %xmm4, %xmm3
	vmovd	%esi, %xmm4
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm10, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5120(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm5, %xmm5
	movq	2968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm9, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm4, %xmm4
	vpminsd	%xmm9, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm5, %xmm6, %xmm4, %xmm4
	movq	2920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r14d
	vmovdqa	5360(%rsp), %xmm6       # 16-byte Reload
	vpmulld	%xmm6, %xmm1, %xmm1
	vmovd	%xmm5, %eax
	cltd
	idivl	%r12d
	movl	%edx, %r12d
	vpaddd	%xmm1, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vmovq	%xmm1, %rsi
	movq	%rsi, 2752(%rsp)        # 8-byte Spill
	vpextrd	$3, %xmm5, %eax
	vmovd	%r12d, %xmm1
	cltd
	idivl	%r11d
	sarq	$32, %rsi
	movq	%rsi, 2720(%rsp)        # 8-byte Spill
	vpinsrd	$1, %r14d, %xmm1, %xmm1
	vpmulld	%xmm6, %xmm2, %xmm2
	sarq	$32, %rcx
	movq	%rcx, 2736(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vmovq	%xmm2, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm1, %xmm1
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 2768(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm3, %xmm2
	vpaddd	%xmm2, %xmm12, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm4, %xmm3
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm12, %xmm2
	vmovq	%xmm2, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm2, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm1, %xmm2
	vpand	%xmm13, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovdqa	5040(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm10, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	movq	2976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	vmovd	%r12d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpminsd	%xmm9, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm9, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm3, %xmm1, %xmm1
	vpmulld	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	andb	%r10b, %r9b
	movb	%r9b, 5216(%rsp)        # 1-byte Spill
	movl	%ebx, %eax
	movq	4192(%rsp), %r9         # 8-byte Reload
	orl	%r9d, %eax
	testb	$1, %al
	movq	2944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	movq	2192(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, 2672(%rsp)        # 8-byte Spill
	sete	%r14b
	movl	3008(%rsp), %r10d       # 4-byte Reload
	testl	%ebx, %r10d
	setne	3152(%rsp)              # 1-byte Folded Spill
	movb	3744(%rsp), %r8b        # 1-byte Reload
	andb	%r8b, %r13b
	movl	%r13d, 5248(%rsp)       # 4-byte Spill
	orq	$6, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3312(%rsp)        # 8-byte Spill
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%r12d, %r13d
	andl	$1, %r13d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	sete	%bl
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	2984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	3248(%rsp), %bl         # 1-byte Folded Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm10, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm9, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm9, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rsi
	movq	%rsi, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm0, %rdx
	movq	%rdx, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	movl	%r12d, %eax
	orl	%r9d, %eax
	testb	$1, %al
	sete	%r11b
	testl	%r12d, %r10d
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm0
	setne	%r14b
	andb	%r8b, %r13b
	movq	2208(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rcx
	movq	2240(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r9
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3248(%rsp)        # 8-byte Spill
	movq	%r9, %rax
	orq	$6, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	movq	2224(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r10d
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3184(%rsp)        # 4-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3200(%rsp)        # 4-byte Spill
	je	.LBB147_1201
# BB#1200:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1201:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5248(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1203
# BB#1202:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1203:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm1, 2384(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
	movzbl	3152(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1205
# BB#1204:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1205:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_1207
# BB#1206:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1207:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm1, 2400(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2416(%rsp)       # 16-byte Spill
	movzbl	%r11b, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1209
# BB#1208:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1209:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm0, 2432(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2800(%rsp)       # 16-byte Spill
	movq	%rcx, %r11
	je	.LBB147_1211
# BB#1210:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1211:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm1, 2448(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
	movzbl	%r14b, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1213
# BB#1212:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1213:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2784(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2464(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2816(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	movq	5032(%rsp), %rdi        # 8-byte Reload
	movq	5608(%rsp), %rbx        # 8-byte Reload
	je	.LBB147_1215
# BB#1214:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1215:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movq	2752(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rcx        # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2720(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2736(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2736(%rsp)       # 16-byte Spill
	movq	2704(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3216(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2768(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	2672(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vmovups	48(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	movslq	%r12d, %r8
	vmovups	8(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vmovups	24(%rdi,%r8,4), %xmm1
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4128(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rbx,%r11,4), %xmm14
	vmovups	48(%rbx,%r11,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rbx,%r9,4), %xmm6
	vmovups	48(%rbx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3408(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2720(%rsp)      # 16-byte Spill
	movq	2688(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movslq	%r10d, %r10
	vmovups	8(%rdi,%r10,4), %xmm1
	vmovups	24(%rdi,%r10,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vmovaps	3216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	3312(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm13
	vmovups	48(%rbx,%rax,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2736(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm14
	vmovaps	%xmm14, 3680(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3328(%rsp), %rax        # 8-byte Reload
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovups	48(%rbx,%rax,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rax,4), %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	5152(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rcx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	movslq	3168(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5184(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3360(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3184(%rsp), %rdx        # 4-byte Folded Reload
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	movslq	3200(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2576(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3232(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rdx,4), %xmm5
	vmovups	24(%rdi,%rdx,4), %xmm6
	vmovups	16(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1217
# BB#1216:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	2496(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
.LBB147_1217:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vsubps	2624(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2640(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3216(%rsp), %xmm1       # 16-byte Reload
	vmulps	5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	2512(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2592(%rsp)       # 16-byte Spill
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2576(%rsp)       # 16-byte Spill
	vmovaps	2528(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2560(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3392(%rsp)       # 16-byte Spill
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3456(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2704(%rsp)       # 16-byte Spill
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3312(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3712(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2496(%rsp)       # 16-byte Spill
	movq	4656(%rsp), %rcx        # 8-byte Reload
	je	.LBB147_1219
# BB#1218:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2384(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1219:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vsubps	2656(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3200(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2544(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2528(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3248(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r11,4), %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2720(%rsp), %xmm2       # 16-byte Reload
	vmulps	4128(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	movq	3264(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%r10,4), %xmm2
	vmovups	16(%rdi,%r10,4), %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5152(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2512(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2496(%rsp), %xmm0       # 16-byte Reload
	vaddps	3360(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2560(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3168(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3280(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1221
# BB#1220:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovdqa	2400(%rsp), %xmm6       # 16-byte Reload
.LBB147_1221:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2544(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vsubps	2528(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	3216(%rsp), %xmm3       # 16-byte Reload
	vmulps	3776(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3168(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3808(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3344(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3200(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2784(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1223
# BB#1222:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovdqa	2416(%rsp), %xmm4       # 16-byte Reload
.LBB147_1223:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3104(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3248(%rsp), %xmm4       # 16-byte Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3232(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2816(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1225
# BB#1224:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	2432(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
.LBB147_1225:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2752(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3584(%rsp), %xmm4       # 16-byte Reload
	vmulps	5184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3200(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3120(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1227
# BB#1226:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	2448(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3136(%rsp)       # 16-byte Spill
.LBB147_1227:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vmulps	4128(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2624(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2704(%rsp), %xmm4       # 16-byte Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3712(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5424(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1229
# BB#1228:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovdqa	2464(%rsp), %xmm15      # 16-byte Reload
.LBB147_1229:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vaddps	3456(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2736(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vmulps	3776(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5248(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3808(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vaddps	3248(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3136(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	vmovdqa	2800(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1231
# BB#1230:                              # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovdqa	2480(%rsp), %xmm0       # 16-byte Reload
.LBB147_1231:                           # %for f7.s0.v10.v10475
                                        #   in Loop: Header=BB147_1199 Depth=4
	vmovaps	3376(%rsp), %xmm7       # 16-byte Reload
	vaddps	3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	2832(%rsp), %rax        # 4-byte Folded Reload
	movq	2336(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	2848(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1199
# BB#1232:                              #   in Loop: Header=BB147_1142 Depth=3
	movq	1376(%rsp), %rax        # 8-byte Reload
.LBB147_1233:                           # %end for f7.s0.v10.v10476
                                        #   in Loop: Header=BB147_1142 Depth=3
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	movq	2104(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	jne	.LBB147_1142
.LBB147_1234:                           # %end for f7.s0.v11466
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	2368(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r8d
	cmpl	1372(%rsp), %eax        # 4-byte Folded Reload
	movq	4672(%rsp), %r9         # 8-byte Reload
	movq	4664(%rsp), %rax        # 8-byte Reload
	jl	.LBB147_1235
	jmp	.LBB147_1273
.LBB147_1236:                           # %for f7.s0.v11479.end for f7.s0.v10.v10483_crit_edge
                                        #   in Loop: Header=BB147_1235 Depth=3
	addl	$1, %r8d
	movl	%r8d, %ecx
	jmp	.LBB147_1272
	.align	16, 0x90
.LBB147_1235:                           # %for f7.s0.v11479
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1238 Depth 4
	cmpl	$0, 1736(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1236
# BB#1237:                              # %for f7.s0.v10.v10482.preheader
                                        #   in Loop: Header=BB147_1235 Depth=3
	movq	%r8, 2384(%rsp)         # 8-byte Spill
	movl	%r8d, %r11d
	movq	1752(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r11d
	leal	-1(%r11), %eax
	cltd
	movq	1760(%rsp), %r13        # 8-byte Reload
	idivl	%r13d
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	movl	1772(%rsp), %eax        # 4-byte Reload
	andl	%eax, %esi
	movl	%eax, %ebx
	leal	1(%r11), %eax
	cltd
	idivl	%r13d
	addl	%ecx, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	addl	%edx, %ecx
	movq	1744(%rsp), %r15        # 8-byte Reload
	cmpl	%r8d, %r15d
	movl	%r15d, %eax
	cmovgl	%r8d, %eax
	addl	$-1, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	1796(%rsp), %r14d       # 4-byte Reload
	movl	%r14d, %r10d
	subl	%esi, %r10d
	movq	1784(%rsp), %r9         # 8-byte Reload
	cmpl	%esi, %r9d
	cmovgl	%esi, %r10d
	addl	%edi, %r10d
	movl	1740(%rsp), %ebx        # 4-byte Reload
	cmpl	%r10d, %ebx
	cmovlel	%ebx, %r10d
	cmpl	%edi, %r10d
	cmovll	%edi, %r10d
	leal	1(%r8), %esi
	movl	%esi, 2176(%rsp)        # 4-byte Spill
	cmpl	%esi, %ebx
	movl	%ebx, %edx
	cmovgl	%esi, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r14d, %r12d
	subl	%ecx, %r12d
	cmpl	%ecx, %r9d
	cmovgl	%ecx, %r12d
	addl	%edi, %r12d
	cmpl	%r12d, %ebx
	cmovlel	%ebx, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	%r8d, %ebx
	movl	%ebx, %esi
	cmovgl	%r8d, %esi
	cmovgl	%edx, %r12d
	cmpl	%r8d, %r15d
	cmovgel	%eax, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1772(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	subl	%eax, %r14d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r14d
	addl	%edi, %r14d
	cmpl	%r14d, %ebx
	cmovlel	%ebx, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	cmpl	%r8d, %r15d
	cmovgl	%esi, %r14d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 3008(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2352(%rsp)       # 16-byte Spill
	movl	%r8d, %esi
	leal	2(%r11), %eax
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	andl	$63, %esi
	movq	%rsi, 2400(%rsp)        # 8-byte Spill
	movl	%r9d, %esi
	sarl	$31, %esi
	addl	$-2, %r11d
	movl	%r11d, %eax
	cltd
	idivl	%r13d
	movl	1772(%rsp), %r11d       # 4-byte Reload
	andl	%r11d, %esi
	addl	%r9d, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movslq	%r14d, %rcx
	movq	1816(%rsp), %r9         # 8-byte Reload
	imulq	%r9, %rcx
	movq	1800(%rsp), %r14        # 8-byte Reload
	leaq	(%rcx,%r14), %rcx
	movq	1824(%rsp), %r11        # 8-byte Reload
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	leal	2(%r8), %r15d
	cmpl	%r15d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%esi, %edx
	movq	1784(%rsp), %r13        # 8-byte Reload
	cmpl	%esi, %r13d
	cmovgl	%esi, %edx
	addl	%edi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%r9, %rcx
	leaq	(%rcx,%r14), %rcx
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	leal	-2(%r8), %r13d
	cmpl	%r13d, %ebx
	movl	%ebx, %ecx
	cmovgl	%r13d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	1784(%rsp), %rsi        # 8-byte Reload
	cmpl	%eax, %esi
	cmovgl	%eax, %edx
	addl	%edi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rax
	imulq	%r9, %rax
	leaq	(%rax,%r14), %rax
	movslq	%r10d, %rcx
	imulq	%r9, %rcx
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	leaq	(%rcx,%r14), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movslq	%r12d, %rax
	imulq	%r9, %rax
	leaq	(%rax,%r14), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	movq	2400(%rsp), %rdi        # 8-byte Reload
	movq	%rdi, %rax
	imulq	1728(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2304(%rsp)        # 8-byte Spill
	movl	2176(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	movq	1416(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %edx
	movl	1768(%rsp), %ecx        # 4-byte Reload
	imull	%ecx, %edx
	movq	%rdx, 2272(%rsp)        # 8-byte Spill
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 2256(%rsp)        # 8-byte Spill
	movq	1408(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2240(%rsp)        # 8-byte Spill
	andl	$63, %r13d
	imull	%eax, %r13d
	movq	%r13, 2320(%rsp)        # 8-byte Spill
	movq	1400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2224(%rsp)        # 8-byte Spill
	andl	$63, %r15d
	imull	%eax, %r15d
	movq	%r15, 2336(%rsp)        # 8-byte Spill
	movq	1392(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2208(%rsp)        # 8-byte Spill
	movq	1528(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%ecx, %edx
	movq	%rdx, 2192(%rsp)        # 8-byte Spill
	movq	%rdi, %rcx
	imull	%eax, %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	xorl	%r13d, %r13d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1238:                           # %for f7.s0.v10.v10482
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1235 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 2848(%rsp)        # 4-byte Spill
	cmpl	$0, 3008(%rsp)          # 4-byte Folded Reload
	sete	3744(%rsp)              # 1-byte Folded Spill
	setne	3648(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 5152(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	sete	5216(%rsp)              # 1-byte Folded Spill
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3264(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3280(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 4192(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %esi
	vpinsrd	$2, 4192(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 3712(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r14d
	movl	%edx, %edi
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	2352(%rsp), %xmm15      # 16-byte Reload
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	movl	5152(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5056(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm11, %xmm11, %xmm11
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4992(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	5328(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vmovdqa	5296(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	5344(%rsp), %xmm7       # 16-byte Reload
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	5312(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm0, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	5360(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovd	%r12d, %xmm3
	vpaddd	%xmm2, %xmm12, %xmm2
	vpinsrd	$1, %r10d, %xmm3, %xmm3
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 3712(%rsp)        # 8-byte Spill
	vpinsrd	$2, %r15d, %xmm3, %xmm3
	vpinsrd	$3, %r8d, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vmovq	%xmm2, %r8
	movq	%r8, 3680(%rsp)         # 8-byte Spill
	vpsrad	$31, %xmm3, %xmm2
	vpand	%xmm15, %xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	4896(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4736(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	5088(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm2, %xmm9, %xmm4
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovd	%esi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r15d
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm15, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm3
	vmovdqa	5120(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	5072(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vpor	%xmm4, %xmm5, %xmm4
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vmovd	%xmm6, %eax
	cltd
	idivl	%r11d
	vpextrd	$2, %xmm6, %eax
	vpextrd	$3, %xmm6, %esi
	vmovd	%edx, %xmm6
	cltd
	idivl	%r14d
	movl	%r14d, %edi
	movq	%r8, %rbx
	sarq	$32, %rbx
	movq	%rbx, 2720(%rsp)        # 8-byte Spill
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpmulld	%xmm8, %xmm1, %xmm1
	sarq	$32, %r10
	movq	%r10, 2768(%rsp)        # 8-byte Spill
	vpaddd	%xmm1, %xmm12, %xmm1
	vpinsrd	$2, %edx, %xmm6, %xmm6
	movl	%esi, %eax
	cltd
	idivl	%r15d
	vmovq	%xmm1, %rax
	movq	%rax, 2704(%rsp)        # 8-byte Spill
	vpinsrd	$3, %edx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2736(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3216(%rsp)        # 8-byte Spill
	vpmulld	%xmm8, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 4192(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	vpaddd	%xmm7, %xmm3, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm1, %xmm2, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm6, %xmm1
	vpand	%xmm15, %xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	5040(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	%xmm11, %xmm2, %xmm2
	vmovdqa	5008(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %r8d
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm12, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movb	3648(%rsp), %r11b       # 1-byte Reload
	andb	%r11b, 5216(%rsp)       # 1-byte Folded Spill
	movl	5152(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	2384(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	movq	2192(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r14
	sete	3184(%rsp)              # 1-byte Folded Spill
	movl	3008(%rsp), %r12d       # 4-byte Reload
	testl	%ecx, %r12d
	setne	3152(%rsp)              # 1-byte Folded Spill
	movb	3744(%rsp), %r10b       # 1-byte Reload
	movl	5248(%rsp), %eax        # 4-byte Reload
	andb	%r10b, %al
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movq	%r14, %rax
	orq	$6, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	movq	2240(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %rcx
	movq	%rcx, 3296(%rsp)        # 8-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	orq	$6, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	movq	%rcx, %rax
	orq	$6, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	movl	%r8d, %r15d
	andl	$1, %r15d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r9b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3248(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	andb	%r11b, %r9b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm9, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm11, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm8, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3200(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 3232(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 3248(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	orl	%ebx, %ecx
	testb	$1, %cl
	sete	%r11b
	testl	%r8d, %r12d
	movzbl	3184(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	setne	%dl
	andb	%r10b, %r15b
	movq	2208(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movslq	%ecx, %r10
	movq	2224(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movslq	%edi, %rcx
	movq	%r10, %rsi
	orq	$6, %rsi
	movq	%rsi, 3264(%rsp)        # 8-byte Spill
	movq	%rcx, %r12
	movq	%rcx, %rsi
	orq	$6, %r12
	vbroadcastss	%xmm0, %xmm4
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm4, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2752(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %edi
	movq	2320(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 2656(%rsp)        # 4-byte Spill
	movq	2256(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3168(%rsp)        # 4-byte Spill
	movq	2288(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movl	%ecx, 3184(%rsp)        # 4-byte Spill
	je	.LBB147_1240
# BB#1239:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1240:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	5216(%rsp), %r8d        # 1-byte Folded Reload
	vmovd	%r8d, %xmm0
	movl	5248(%rsp), %ecx        # 4-byte Reload
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm3
	vmovaps	%xmm3, %xmm1
	je	.LBB147_1242
# BB#1241:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1242:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm1, 2416(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
	movzbl	3152(%rsp), %ecx        # 1-byte Folded Reload
	vmovd	%ecx, %xmm0
	je	.LBB147_1244
# BB#1243:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1244:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_1246
# BB#1245:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1246:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm1, 2432(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2448(%rsp)       # 16-byte Spill
	movzbl	%r11b, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1248
# BB#1247:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1248:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 2816(%rsp)       # 16-byte Spill
	je	.LBB147_1250
# BB#1249:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1250:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, %r9
	je	.LBB147_1252
# BB#1251:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1252:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm4, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2800(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2832(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	je	.LBB147_1254
# BB#1253:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1254:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2720(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3712(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2768(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 2608(%rsp)       # 16-byte Spill
	movq	2704(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	2736(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3216(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm2
	vmovaps	%xmm0, %xmm10
	movq	5608(%rsp), %rdx        # 8-byte Reload
	vmovups	32(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3680(%rsp)       # 16-byte Spill
	vmovups	48(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm1[0,2]
	vmovaps	5616(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	5632(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	8(%rcx,%r8,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vmovups	24(%rcx,%r8,4), %xmm1
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm1[0,2]
	vmovaps	4128(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm6
	vmovups	32(%rdx,%r10,4), %xmm14
	vmovups	48(%rdx,%r10,4), %xmm9
	vshufps	$136, %xmm9, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vbroadcastss	.LCPI147_17(%rip), %xmm12
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm4, %xmm6, %xmm1
	vmovaps	%xmm1, 3712(%rsp)       # 16-byte Spill
	vmovaps	3840(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm4
	vmovups	32(%rdx,%r9,4), %xmm6
	vmovups	48(%rdx,%r9,4), %xmm7
	vshufps	$136, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm7[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	vmovups	40(%rdx,%r14,4), %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vmovups	56(%rdx,%r14,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	movq	%rdx, %rbx
	movq	%rcx, %rdi
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3616(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3392(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3424(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm15, 2720(%rsp)      # 16-byte Spill
	movq	2688(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm10, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movslq	2656(%rsp), %rdx        # 4-byte Folded Reload
	vmovups	8(%rdi,%rdx,4), %xmm1
	vmovups	24(%rdi,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm4[0,2]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm7[1,3]
	movq	3344(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	3328(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	3360(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1,2],mem[0]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm3, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	3680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[1,3],xmm9[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm13, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movq	3296(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	48(%rbx,%rcx,4), %xmm10
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3808(%rsp), %xmm2       # 16-byte Reload
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm14
	vmovaps	%xmm14, 3680(%rsp)      # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmovaps	3744(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm3, %xmm0
	movq	3312(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %xmm1
	vmovups	48(%rbx,%rcx,4), %xmm4
	vshufps	$136, %xmm4, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	vmovups	40(%rbx,%rcx,4), %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovups	56(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm7, %xmm6
	vmulps	%xmm0, %xmm6, %xmm6
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm10[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm8, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm9, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm2, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	4192(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3536(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3584(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	movq	3200(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3232(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	movslq	2752(%rsp), %rax        # 4-byte Folded Reload
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3376(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmulps	5184(%rsp), %xmm8, %xmm15 # 16-byte Folded Reload
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm11
	movslq	3168(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movslq	3184(%rsp), %rsi        # 4-byte Folded Reload
	vmovaps	2592(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 3376(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm6, %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	8(%rdi,%rax,4), %xmm13
	vmovups	24(%rdi,%rax,4), %xmm10
	vmovups	16(%rdi,%rax,4), %xmm3
	vmovaps	%xmm3, 4192(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rax,4), %xmm1
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 3248(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rsi,4), %xmm2
	vmovups	24(%rdi,%rsi,4), %xmm7
	vmovups	16(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3488(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rsi,4), %xmm1
	vmovaps	%xmm1, 3168(%rsp)       # 16-byte Spill
	vmovups	8(%rdi,%rcx,4), %xmm5
	vmovups	24(%rdi,%rcx,4), %xmm6
	vmovups	16(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 5248(%rsp)       # 16-byte Spill
	vmovups	32(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovups	(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	je	.LBB147_1256
# BB#1255:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	2528(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
.LBB147_1256:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vsubps	3392(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2640(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm10[1,3]
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm9
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	5616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	5632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	3216(%rsp), %xmm1       # 16-byte Reload
	vmulps	5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3232(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm15
	vmaxps	%xmm1, %xmm11, %xmm11
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2560(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 2592(%rsp)       # 16-byte Spill
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm13
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm10
	vmovaps	3376(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 3376(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vminps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	3456(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2752(%rsp)       # 16-byte Spill
	vmovaps	4192(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3312(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm6, %xmm5, %xmm8 # xmm8 = xmm5[0,2],xmm6[0,2]
	vmovaps	5248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm7, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm7[0,2]
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vaddps	3712(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2528(%rsp)       # 16-byte Spill
	je	.LBB147_1258
# BB#1257:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	%xmm9, %xmm4
	vmovaps	2416(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm9, 3104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm9
.LBB147_1258:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vsubps	%xmm14, %xmm15, %xmm4
	vmovaps	%xmm4, 3344(%rsp)       # 16-byte Spill
	vsubps	3392(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 3200(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm7[1,3]
	vmovaps	%xmm2, 2576(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm6[1,3]
	vmovaps	%xmm2, 2560(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm13, %xmm2
	vmovaps	%xmm2, 3424(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm10, %xmm2
	vmovaps	%xmm2, 3408(%rsp)       # 16-byte Spill
	vmovaps	3376(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3392(%rsp)       # 16-byte Spill
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3264(%rsp), %rax        # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm0
	vmovups	40(%rbx,%r10,4), %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	5616(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	5632(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	2720(%rsp), %xmm2       # 16-byte Reload
	vmulps	4128(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovups	(%rdi,%r8,4), %xmm1
	vmovups	16(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	(%rbx,%r12,4), %xmm2
	vmovups	40(%rbx,%r9,4), %xmm5
	vmovaps	%xmm5, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm3, %xmm15
	vmulps	%xmm2, %xmm1, %xmm1
	vmovups	(%rdi,%rdx,4), %xmm2
	vmovups	16(%rdi,%rdx,4), %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 4192(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	2544(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm1
	vmovaps	2528(%rsp), %xmm0       # 16-byte Reload
	vaddps	3360(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm4
	vmovaps	2624(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm7
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm14
	vmovaps	2592(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm0
	vminps	%xmm12, %xmm9, %xmm5
	vmaxps	%xmm6, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm9 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm9, %xmm13
	vmovaps	3168(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vbroadcastss	.LCPI147_24(%rip), %xmm10
	vmovdqa	3280(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1260
# BB#1259:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovdqa	2432(%rsp), %xmm6       # 16-byte Reload
.LBB147_1260:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vsubps	%xmm2, %xmm1, %xmm2
	vsubps	2576(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vsubps	2560(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm14, %xmm1
	vmovaps	%xmm1, 3264(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vmovaps	3216(%rsp), %xmm3       # 16-byte Reload
	vmulps	3776(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	2656(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm15, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	3168(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	3808(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm15, %xmm7
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	3184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm3
	vaddps	3344(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vaddps	3200(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vpslld	$31, %xmm6, %xmm0
	vaddps	3392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm10, %xmm3, %xmm6
	vmovdqa	2800(%rsp), %xmm4       # 16-byte Reload
	je	.LBB147_1262
# BB#1261:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovdqa	2448(%rsp), %xmm4       # 16-byte Reload
.LBB147_1262:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vaddps	%xmm2, %xmm1, %xmm2
	vbroadcastss	.LCPI147_23(%rip), %xmm13
	vmovdqa	3104(%rsp), %xmm1       # 16-byte Reload
	vpslld	$31, %xmm1, %xmm3
	vmulps	%xmm10, %xmm5, %xmm5
	vpslld	$31, %xmm4, %xmm1
	vmovaps	3248(%rsp), %xmm4       # 16-byte Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3232(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm8
	vmulps	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm4, %xmm14, %xmm1
	vblendvps	%xmm0, %xmm6, %xmm1, %xmm0
	vmovdqa	2832(%rsp), %xmm15      # 16-byte Reload
	je	.LBB147_1264
# BB#1263:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	2464(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
.LBB147_1264:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vblendvps	%xmm3, %xmm5, %xmm0, %xmm0
	vmovaps	4192(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3584(%rsp), %xmm4       # 16-byte Reload
	vmulps	5184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	3200(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmovdqa	3120(%rsp), %xmm3       # 16-byte Reload
	vpslld	$31, %xmm3, %xmm3
	vmulps	%xmm13, %xmm2, %xmm2
	je	.LBB147_1266
# BB#1265:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	2480(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm4, 3136(%rsp)       # 16-byte Spill
.LBB147_1266:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vblendvps	%xmm3, %xmm2, %xmm0, %xmm0
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	3744(%rsp), %xmm4       # 16-byte Reload
	vmulps	4128(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	3840(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 56(%rbx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmovaps	%xmm11, %xmm6
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm5
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	2640(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rdi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	2752(%rsp), %xmm4       # 16-byte Reload
	vaddps	3360(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3712(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	5424(%rsp), %xmm9       # 16-byte Reload
	je	.LBB147_1268
# BB#1267:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovdqa	2496(%rsp), %xmm15      # 16-byte Reload
.LBB147_1268:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vaddps	3456(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmulps	%xmm13, %xmm1, %xmm1
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3488(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 2736(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm6, %xmm7
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm0
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	3584(%rsp), %xmm6       # 16-byte Reload
	vmulps	3776(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	5248(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3536(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 2784(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	3808(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vsubps	%xmm4, %xmm5, %xmm4
	vmovaps	3232(%rsp), %xmm0       # 16-byte Reload
	vaddps	3248(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm5, %xmm4
	vaddps	3264(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm10, %xmm2, %xmm3
	vmulps	%xmm10, %xmm4, %xmm5
	vmovdqa	3152(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm2
	vmovdqa	3136(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm4
	vpslld	$31, %xmm15, %xmm6
	movq	4656(%rsp), %rcx        # 8-byte Reload
	vmovdqa	2816(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1270
# BB#1269:                              # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovdqa	2512(%rsp), %xmm0       # 16-byte Reload
.LBB147_1270:                           # %for f7.s0.v10.v10482
                                        #   in Loop: Header=BB147_1238 Depth=4
	vmovaps	3376(%rsp), %xmm7       # 16-byte Reload
	vaddps	3408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm7, %xmm7
	vpslld	$31, %xmm0, %xmm0
	vblendvps	%xmm0, %xmm7, %xmm14, %xmm0
	vblendvps	%xmm6, %xmm5, %xmm0, %xmm0
	vblendvps	%xmm4, %xmm3, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	5152(%rsp), %rax        # 4-byte Folded Reload
	movq	2304(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	movl	2848(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovaps	%xmm9, %xmm12
	jne	.LBB147_1238
# BB#1271:                              #   in Loop: Header=BB147_1235 Depth=3
	movq	4672(%rsp), %r9         # 8-byte Reload
	movq	4664(%rsp), %rax        # 8-byte Reload
	movl	2176(%rsp), %ecx        # 4-byte Reload
.LBB147_1272:                           # %end for f7.s0.v10.v10483
                                        #   in Loop: Header=BB147_1235 Depth=3
	movl	%ecx, %r8d
	cmpl	1372(%rsp), %ecx        # 4-byte Folded Reload
	jne	.LBB147_1235
.LBB147_1273:                           # %produce f8487
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1000(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	movq	2112(%rsp), %rdx        # 8-byte Reload
	cmpl	%edx, %ecx
	jl	.LBB147_1274
	jmp	.LBB147_1312
.LBB147_1275:                           # %for f8.s0.v11489.end for f8.s0.v10.v10492_crit_edge
                                        #   in Loop: Header=BB147_1274 Depth=3
	addl	$1, %r8d
	movl	%r8d, %edx
	jmp	.LBB147_1311
	.align	16, 0x90
.LBB147_1274:                           # %for f8.s0.v11489
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1277 Depth 4
	cmpl	$0, 1736(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1275
# BB#1276:                              # %for f8.s0.v10.v10491.preheader
                                        #   in Loop: Header=BB147_1274 Depth=3
	movq	%r8, 2448(%rsp)         # 8-byte Spill
	movl	%r8d, %eax
	movq	1752(%rsp), %r11        # 8-byte Reload
	subl	%r11d, %eax
	addl	$-1, %eax
	cltd
	movq	1760(%rsp), %r15        # 8-byte Reload
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	movl	%ecx, %esi
	addl	%edx, %eax
	movl	1796(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	movl	%ecx, %r10d
	subl	%eax, %edx
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	%rcx, %r14
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	movl	1740(%rsp), %r13d       # 4-byte Reload
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movq	1744(%rsp), %rbx        # 8-byte Reload
	cmpl	%r8d, %ebx
	movl	%ebx, %r9d
	cmovgl	%r8d, %r9d
	addl	$-1, %r9d
	cmpl	%r11d, %r9d
	cmovll	%r11d, %r9d
	cmpl	%r8d, %ebx
	cmovll	%edx, %r9d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovlel	%edx, %r9d
	leal	1(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r10d, %esi
	subl	%edi, %esi
	cmpl	%edi, %r14d
	cmovgl	%edi, %esi
	addl	%r11d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	leal	1(%r8), %ecx
	movl	%ecx, 2224(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r13d
	movl	%r13d, %edi
	cmovgl	%ecx, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, %r13d
	cmovlel	%esi, %edi
	movl	%r13d, %r10d
	cmovgl	%r8d, %r10d
	cmpl	%r11d, %r10d
	cmovll	%r11d, %r10d
	cmpl	%r8d, %ebx
	cmovlel	%edx, %r10d
	movl	%r8d, %ecx
	subl	%r11d, %ecx
	cmovll	%edx, %r10d
	movl	%r8d, %r12d
	andl	$1, %r12d
	movl	%r12d, 2240(%rsp)       # 4-byte Spill
	leal	-2(%rcx), %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2432(%rsp)       # 16-byte Spill
	movl	%r8d, %eax
	andl	$63, %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	movl	%r14d, %ebx
	addl	$2, %ecx
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	sarl	$31, %ebx
	movl	1772(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %ebx
	addl	%r14d, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	cmpl	%r8d, 1640(%rsp)        # 4-byte Folded Reload
	cmovgl	%esi, %edi
	movslq	%edi, %rcx
	movq	1816(%rsp), %rdi        # 8-byte Reload
	imulq	%rdi, %rcx
	movq	1808(%rsp), %rsi        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	1824(%rsp), %r14        # 8-byte Reload
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movslq	%r9d, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %ecx        # 4-byte Reload
	subl	%ebx, %ecx
	movq	1784(%rsp), %r15        # 8-byte Reload
	cmpl	%ebx, %r15d
	cmovgl	%ebx, %ecx
	addl	%r11d, %ecx
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	leal	-2(%r8), %r9d
	cmpl	%r9d, %r13d
	movl	%r13d, %edx
	cmovgl	%r9d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovlel	%ecx, %edx
	cmpl	%r8d, 1644(%rsp)        # 4-byte Folded Reload
	cmovgl	%ecx, %edx
	movslq	%edx, %rcx
	imulq	%rdi, %rcx
	leaq	(%rcx,%rsi), %rcx
	movslq	%r10d, %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rsi), %rdx
	vbroadcastss	(%r14,%rdx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	movl	1796(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	leal	2(%r8), %ebx
	cmpl	%ebx, %r13d
	movl	%r13d, %eax
	cmovgl	%ebx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovlel	%edx, %eax
	cmpl	%r8d, 1636(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %eax
	cltq
	imulq	%rdi, %rax
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r14,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	2464(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1728(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1700(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2400(%rsp)        # 8-byte Spill
	movq	1392(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1768(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	subq	4712(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2384(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2352(%rsp)        # 8-byte Spill
	movq	1528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r9d
	imull	%esi, %r9d
	movq	%r9, 2416(%rsp)         # 8-byte Spill
	movq	1400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2336(%rsp)        # 8-byte Spill
	movq	1408(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2304(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2288(%rsp)        # 8-byte Spill
	movl	2224(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2272(%rsp)        # 8-byte Spill
	movq	1416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2256(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1277:                           # %for f8.s0.v10.v10491
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1274 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3152(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	sete	5152(%rsp)              # 1-byte Folded Spill
	setne	3744(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3456(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3424(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3552(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3712(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3680(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3648(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3616(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3584(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3552(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2432(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3776(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5328(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5296(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5040(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5008(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5344(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5360(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5376(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3712(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3680(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3648(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5088(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3104(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3008(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3616(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5056(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4992(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4896(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4736(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2784(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movb	3744(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5184(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2448(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	2240(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3264(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %r14b       # 1-byte Reload
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	%r14b, %al
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3488(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3536(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3424(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movl	%r11d, %r12d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r14b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3424(%rsp)        # 4-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2816(%rsp)        # 4-byte Spill
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %edi
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %esi
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2768(%rsp)        # 4-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3744(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r14d
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3248(%rsp)        # 4-byte Spill
	movq	2400(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r11d
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3280(%rsp)        # 4-byte Spill
	je	.LBB147_1279
# BB#1278:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1279:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5216(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
	je	.LBB147_1281
# BB#1280:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1281:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3184(%rsp)       # 16-byte Spill
	movzbl	3264(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1283
# BB#1282:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1283:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	je	.LBB147_1285
# BB#1284:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1285:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	je	.LBB147_1287
# BB#1286:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1287:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1289
# BB#1288:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1289:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm2, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3216(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1291
# BB#1290:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1291:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm3, 3264(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	je	.LBB147_1293
# BB#1292:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1293:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3680(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3104(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3008(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2832(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2784(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3008(%rsp)      # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5032(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4128(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2768(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3712(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2720(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3680(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2688(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2736(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3376(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3104(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movslq	2816(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	movslq	3744(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	movslq	3248(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	movslq	3280(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3584(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3552(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3536(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3488(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movslq	3424(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	vmovaps	3408(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3344(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3328(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3312(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3616(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovaps	3248(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3456(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3408(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_1295
# BB#1294:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	2480(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3184(%rsp)       # 16-byte Spill
.LBB147_1295:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vsubps	3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	vsubps	2672(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vsubps	2688(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	vsubps	3392(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2656(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3552(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3536(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3408(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3280(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1297
# BB#1296:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	2496(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3168(%rsp)       # 16-byte Spill
.LBB147_1297:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	3552(%rsp), %xmm0       # 16-byte Reload
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5664(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3376(%rsp), %xmm3       # 16-byte Reload
	vmulps	4192(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3536(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4128(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vmulps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2656(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2640(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2624(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2608(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3312(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vaddps	3488(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3344(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3536(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5152(%rsp)       # 16-byte Spill
	vmovdqa	3264(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1299
# BB#1298:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovdqa	2512(%rsp), %xmm7       # 16-byte Reload
.LBB147_1299:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3248(%rsp)       # 16-byte Spill
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3328(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3008(%rsp), %xmm5       # 16-byte Reload
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3392(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3536(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3552(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3120(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1301
# BB#1300:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovdqa	2528(%rsp), %xmm0       # 16-byte Reload
.LBB147_1301:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3264(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3376(%rsp)      # 16-byte Spill
	vaddps	3328(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3248(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3168(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5152(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1303
# BB#1302:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovdqa	2560(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3216(%rsp)       # 16-byte Spill
.LBB147_1303:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3296(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1305
# BB#1304:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3200(%rsp)       # 16-byte Spill
.LBB147_1305:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3360(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2800(%rsp), %xmm15      # 16-byte Reload
	vmulps	4192(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4128(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3248(%rsp), %xmm3       # 16-byte Reload
	vaddps	3424(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3392(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_1307
# BB#1306:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
.LBB147_1307:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vaddps	3264(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3104(%rsp), %xmm5       # 16-byte Reload
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2752(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3328(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3376(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3136(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1309
# BB#1308:                              # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovdqa	2592(%rsp), %xmm5       # 16-byte Reload
.LBB147_1309:                           # %for f8.s0.v10.v10491
                                        #   in Loop: Header=BB147_1277 Depth=4
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3008(%rsp), %xmm2       # 16-byte Reload
	vaddps	3552(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3776(%rsp), %rax        # 4-byte Folded Reload
	movq	2384(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	3152(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1277
# BB#1310:                              #   in Loop: Header=BB147_1274 Depth=3
	movq	4672(%rsp), %r9         # 8-byte Reload
	movq	4664(%rsp), %rax        # 8-byte Reload
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	movl	2224(%rsp), %edx        # 4-byte Reload
.LBB147_1311:                           # %end for f8.s0.v10.v10492
                                        #   in Loop: Header=BB147_1274 Depth=3
	movl	%edx, %r8d
	movq	2112(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %edx
	jne	.LBB147_1274
.LBB147_1312:                           # %end for f8.s0.v11490
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	movq	2112(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	jl	.LBB147_1313
	jmp	.LBB147_1405
.LBB147_1368:                           # %end for f8.s0.v10.v10502.end for f8.s0.v10.v10506_crit_edge
                                        #   in Loop: Header=BB147_1313 Depth=3
	movq	2112(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movq	%rax, 2112(%rsp)        # 8-byte Spill
	movq	4160(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB147_1404
	.align	16, 0x90
.LBB147_1313:                           # %for f8.s0.v11495
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1315 Depth 4
                                        #         Child Loop BB147_1350 Depth 4
                                        #         Child Loop BB147_1370 Depth 4
	cmpl	$0, 3100(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1348
# BB#1314:                              # %for f8.s0.v10.v10497.preheader
                                        #   in Loop: Header=BB147_1313 Depth=3
	movq	4160(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %r14d
	andl	$63, %r14d
	movl	%r14d, %eax
	movq	1288(%rsp), %r8         # 8-byte Reload
	imull	%r8d, %eax
	movq	%rax, 2432(%rsp)        # 8-byte Spill
	movl	%edx, %eax
	andl	$1, %eax
	movl	%eax, 3152(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2416(%rsp)       # 16-byte Spill
	movl	%edx, %ebx
	movq	1624(%rsp), %rax        # 8-byte Reload
	subl	%eax, %ebx
	leal	9(%rbx), %edi
	movq	1648(%rsp), %r12        # 8-byte Reload
	imull	%r12d, %edi
	leaq	1(%rdx), %rax
	movq	1816(%rsp), %rsi        # 8-byte Reload
	movq	%rsi, %r10
	imulq	%r10, %rax
	movq	2112(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rsi        # 8-byte Reload
	addl	%esi, %edi
	movq	%rdi, 2384(%rsp)        # 8-byte Spill
	movq	1808(%rsp), %r15        # 8-byte Reload
	leaq	(%rax,%r15), %rax
	movq	1824(%rsp), %r11        # 8-byte Reload
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	leal	7(%rbx), %eax
	imull	%r12d, %eax
	addl	%esi, %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	leaq	-1(%rdx), %rax
	imulq	%r10, %rax
	leal	63(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	leal	6(%rbx), %ecx
	imull	%r12d, %ecx
	leaq	(%rax,%r15), %rax
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	addl	%esi, %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	leaq	-2(%rdx), %rax
	imulq	%r10, %rax
	leaq	(%rax,%r15), %rax
	leal	62(%rdx), %ecx
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	leal	8(%rbx), %ecx
	imull	%r12d, %ecx
	addl	%esi, %ecx
	movq	%rcx, 2272(%rsp)        # 8-byte Spill
	movq	%rdx, %rcx
	imulq	%r10, %rcx
	leaq	(%rcx,%r15), %rcx
	addl	$10, %ebx
	imull	%r12d, %ebx
	vbroadcastss	(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	addl	%esi, %ebx
	movq	%rbx, 2400(%rsp)        # 8-byte Spill
	leaq	2(%rdx), %rcx
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movq	%rcx, %rax
	imulq	%r10, %rax
	leaq	(%rax,%r15), %rax
	andl	$63, %ecx
	imull	%r8d, %ecx
	movq	%rcx, 2256(%rsp)        # 8-byte Spill
	vbroadcastss	(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	imulq	1728(%rsp), %r14        # 8-byte Folded Reload
	subq	4712(%rsp), %r14        # 8-byte Folded Reload
	movq	%r14, 2448(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	.align	16, 0x90
.LBB147_1315:                           # %for f8.s0.v10.v10497
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1313 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movq	%r15, 2768(%rsp)        # 8-byte Spill
	cmpl	$0, 3152(%rsp)          # 4-byte Folded Reload
	sete	3776(%rsp)              # 1-byte Folded Spill
	setne	3744(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %r12        # 8-byte Reload
	leal	(%r12,%r15,8), %ecx
	movl	%ecx, 3808(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movl	%ecx, %r10d
	subl	%r9d, %r10d
	leal	1(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %r9d
	movl	%r9d, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 3232(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	movl	%edi, 3216(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edx, %r15d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebx
	movl	%ebx, 3200(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	leal	-1(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%esi, %ecx
	idivl	%ecx
	movl	%edx, %r13d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %esi
	leal	-2(%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 3536(%rsp)        # 4-byte Spill
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%edx, 3488(%rsp)        # 4-byte Spill
	vmovd	%r8d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vpinsrd	$2, %r15d, %xmm0, %xmm0
	vpinsrd	$3, %r14d, %xmm0, %xmm0
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r11d
	movl	%edx, %r14d
	vmovd	%r13d, %xmm2
	vpinsrd	$1, 3712(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	vpinsrd	$2, 3680(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %esi, %xmm1, %xmm2
	movq	2768(%rsp), %r13        # 8-byte Reload
	leal	-1(%r12,%r13,8), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	leal	2(%r10), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vmovd	3616(%rsp), %xmm3       # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 3648(%rsp), %xmm3, %xmm3 # 4-byte Folded Reload
	vmovd	%xmm5, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3584(%rsp), %xmm3, %xmm3 # 4-byte Folded Reload
	vpinsrd	$3, 3536(%rsp), %xmm3, %xmm4 # 4-byte Folded Reload
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	leal	-2(%r12,%r13,8), %eax
	vmovd	%eax, %xmm3
	vmovd	%r8d, %xmm6
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, 3488(%rsp), %xmm6, %xmm5 # 4-byte Folded Reload
	vpinsrd	$2, %r14d, %xmm5, %xmm5
	vpsrad	$31, %xmm0, %xmm6
	vmovdqa	2416(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm0, %xmm6, %xmm6
	movl	3808(%rsp), %r8d        # 4-byte Reload
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5328(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm11, %xmm7
	vmovdqa	5296(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm6, %xmm15, %xmm1
	vblendvps	%xmm7, %xmm6, %xmm1, %xmm1
	vmovdqa	5040(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vmovdqa	5008(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vmovdqa	5344(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	%r12, %rbx
	leal	1(%rbx,%r13,8), %r9d
	movq	%r13, %r12
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vmovdqa	5360(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vpinsrd	$3, %r15d, %xmm5, %xmm5
	vmovdqa	5376(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovd	%edi, %xmm6
	vpextrq	$1, %xmm1, %rdi
	movq	%rdi, 3712(%rsp)        # 8-byte Spill
	vpinsrd	$1, %esi, %xmm6, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpxor	.LCPI147_55(%rip), %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3680(%rsp), %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm2, %xmm1, %xmm7, %xmm1
	vpsrad	$31, %xmm4, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm4
	vpsubd	%xmm2, %xmm15, %xmm7
	vblendvps	%xmm4, %xmm2, %xmm7, %xmm2
	vmovdqa	5136(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpxor	.LCPI147_55(%rip), %xmm4, %xmm4
	vmovdqa	5088(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpinsrd	$2, %ecx, %xmm6, %xmm6
	sarq	$32, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm4, %xmm2, %xmm3, %xmm2
	vpsrad	$31, %xmm5, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm4
	vpsubd	%xmm3, %xmm15, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vmovdqa	5056(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	4992(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm14, %xmm0, %xmm5
	vpminsd	%xmm12, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpinsrd	$3, %edx, %xmm6, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	4896(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	4736(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	leal	2(%rbx,%r12,8), %eax
	vmovd	%eax, %xmm6
	sarq	$32, %rdi
	movq	%rdi, 3248(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3296(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3312(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3280(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpmulld	%xmm9, %xmm3, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm4, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm9, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movb	3744(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5184(%rsp)       # 1-byte Folded Spill
	movl	%r8d, %ecx
	movl	%ecx, %eax
	movq	4160(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	sete	3168(%rsp)              # 1-byte Folded Spill
	movl	3152(%rsp), %r14d       # 4-byte Reload
	testl	%ecx, %r14d
	setne	3184(%rsp)              # 1-byte Folded Spill
	movb	3776(%rsp), %r11b       # 1-byte Reload
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	%r11b, %al
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	sete	%r15b
	addl	$3, %r10d
	vmovd	%r10d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3232(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3200(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	%r12, %rcx
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	3(%rax,%rcx,8), %eax
	movq	%rcx, %rdi
	vmovd	%eax, %xmm2
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm9, %xmm0, %xmm0
	andb	%r13b, %r15b
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %r10
	movq	%r10, 3552(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	vpextrq	$1, %xmm0, %r13
	movq	%r13, %r12
	sarq	$32, %r12
	movl	%r9d, %eax
	orl	%ebx, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r14d
	movzbl	3168(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	setne	%al
	andb	%r11b, %r8b
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	je	.LBB147_1317
# BB#1316:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1317:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_1319
# BB#1318:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1319:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm1, 2464(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3184(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	movq	4672(%rsp), %r9         # 8-byte Reload
	je	.LBB147_1321
# BB#1320:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1321:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1323
# BB#1322:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1323:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2496(%rsp)       # 16-byte Spill
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1325
# BB#1324:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1325:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%r15b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%r8b, %ecx
	vmovd	%ecx, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm3
	je	.LBB147_1327
# BB#1326:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1327:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm3, 2512(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
	movzbl	%al, %eax
	vmovd	%eax, %xmm0
	movq	%rdi, %r15
	je	.LBB147_1329
# BB#1328:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1329:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm3, 2544(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3120(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_1331
# BB#1330:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1331:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movq	3344(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3328(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3248(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5184(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	3296(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3312(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3280(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 2848(%rsp)      # 16-byte Spill
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm0
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %rcx
	movq	5608(%rsp), %rbx        # 8-byte Reload
	vmovups	24608(%rbx,%rcx,4), %xmm9
	vmovups	24624(%rbx,%rcx,4), %xmm14
	vshufps	$221, %xmm14, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm14[1,3]
	vmovaps	5664(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %r11
	movq	5032(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r11,4), %xmm11
	vmovaps	4192(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm1
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	cltq
	vmovups	24608(%rbx,%rax,4), %xmm12
	vmovups	24624(%rbx,%rax,4), %xmm0
	vshufps	$221, %xmm0, %xmm12, %xmm6 # xmm6 = xmm12[1,3],xmm0[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm1
	vmovups	24600(%rbx,%rcx,4), %xmm6
	vmovaps	%xmm6, 3264(%rsp)       # 16-byte Spill
	vmovups	24616(%rbx,%rcx,4), %xmm5
	vmovaps	%xmm5, 3744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm15, %xmm1
	vmovups	24600(%rbx,%rax,4), %xmm7
	vmovaps	%xmm7, 2736(%rsp)       # 16-byte Spill
	vmovups	24616(%rbx,%rax,4), %xmm6
	vmovaps	%xmm6, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm6[1,3]
	vsubps	%xmm8, %xmm7, %xmm7
	vmulps	%xmm7, %xmm4, %xmm7
	vmulps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm1, 2704(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r11,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm7 # xmm7 = xmm11[1,3],xmm1[1,3]
	vmovaps	%xmm7, 3296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vmovss	(%rdx,%rdi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3536(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3360(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3376(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm10, 3248(%rsp)      # 16-byte Spill
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movq	3392(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3408(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3424(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm13, 3008(%rsp)      # 16-byte Spill
	vmovups	24632(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %r14
	vmovups	8(%rsi,%r14,4), %xmm0
	vmovups	24(%rsi,%r14,4), %xmm1
	vshufps	$221, %xmm1, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm1[1,3]
	vmovaps	%xmm2, 3536(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm9, %xmm0 # xmm0 = xmm9[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	vmovups	24632(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm5, %xmm0 # xmm0 = xmm5[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovups	24616(%rbx,%rax,4), %xmm12
	vmovaps	%xmm12, 3776(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	movq	2304(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15,8), %ecx
	movslq	%ecx, %r8
	vmovups	24608(%rbx,%r8,4), %xmm15
	vmovups	24624(%rbx,%r8,4), %xmm14
	vshufps	$221, %xmm14, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	4128(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm9, %xmm1, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r15,8), %ecx
	movslq	%ecx, %rcx
	vmovups	24608(%rbx,%rcx,4), %xmm7
	vmovups	24624(%rbx,%rcx,4), %xmm6
	vshufps	$221, %xmm6, %xmm7, %xmm3 # xmm3 = xmm7[1,3],xmm6[1,3]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	3840(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm5
	vmulps	%xmm3, %xmm5, %xmm5
	vmovups	24608(%rbx,%rax,4), %xmm2
	vmovups	24624(%rbx,%rax,4), %xmm1
	vshufps	$221, %xmm1, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm1[1,3]
	vshufps	$136, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm1[0,2]
	vmovaps	%xmm1, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm6[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm15, %xmm1 # xmm1 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm9, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vmovups	24632(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm1 # xmm1 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm11, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3584(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r10,4), %xmm1, %xmm2 # xmm2 = xmm1[0],mem[0],xmm1[2,3]
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	movslq	%eax, %rdi
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm1
	vmovaps	3344(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm15
	vmovaps	3328(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmovaps	%xmm6, 3616(%rsp)       # 16-byte Spill
	vmovaps	3488(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm6
	vmovaps	%xmm6, 3584(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm7
	vmovaps	3424(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm0, %xmm6, %xmm14
	vmovaps	3408(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	movslq	%r13d, %rax
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15,8), %eax
	vmovaps	3360(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	vinsertps	$48, (%rdx,%r12,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movq	2256(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r15,8), %edx
	cltq
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm2
	vsubps	%xmm8, %xmm3, %xmm8
	movslq	%edx, %rdx
	vminps	%xmm4, %xmm5, %xmm0
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r11,4), %xmm3
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r11,4), %xmm5
	vmovaps	%xmm5, 3488(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r11,4), %xmm6
	vmovaps	%xmm6, 2720(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r14,4), %xmm3
	vmovaps	%xmm3, 3552(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r14,4), %xmm12
	vmovups	32(%rsi,%r14,4), %xmm3
	vmovaps	%xmm3, 3408(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rdi,4), %xmm3
	vmovaps	%xmm3, 3344(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rdi,4), %xmm3
	vmovaps	%xmm3, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdi,4), %xmm3
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rdi,4), %xmm3
	vmovaps	%xmm3, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rdi,4), %xmm3
	vmovaps	%xmm3, 3680(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm5, %xmm6 # xmm6 = xmm5[0,2],xmm6[0,2]
	vmovups	8(%rsi,%rax,4), %xmm10
	vmovups	24(%rsi,%rax,4), %xmm9
	vmovups	8(%rsi,%rdx,4), %xmm13
	vmovups	24(%rsi,%rdx,4), %xmm11
	je	.LBB147_1333
# BB#1332:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	2608(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
.LBB147_1333:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vsubps	3536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3536(%rsp)       # 16-byte Spill
	vsubps	3296(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm1       # 16-byte Reload
	vsubps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vsubps	%xmm6, %xmm7, %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vsubps	3392(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3312(%rsp)       # 16-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	3616(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm5
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm7
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2592(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2576(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm1, %xmm3, %xmm6
	vmaxps	%xmm1, %xmm2, %xmm2
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5184(%rsp)       # 16-byte Spill
	vmulps	5696(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 3424(%rsp)      # 16-byte Spill
	vmovaps	3552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm12, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm12[1,3]
	vshufps	$136, 3408(%rsp), %xmm12, %xmm15 # 16-byte Folded Reload
                                        # xmm15 = xmm12[0,2],mem[0,2]
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
                                        # xmm14 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm9, %xmm10, %xmm8 # xmm8 = xmm10[1,3],xmm9[1,3]
	vshufps	$221, %xmm11, %xmm13, %xmm12 # xmm12 = xmm13[1,3],xmm11[1,3]
	je	.LBB147_1335
# BB#1334:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	2464(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
.LBB147_1335:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vsubps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm11, %xmm13, %xmm11 # xmm11 = xmm13[0,2],xmm11[0,2]
	vshufps	$136, %xmm9, %xmm10, %xmm10 # xmm10 = xmm10[0,2],xmm9[0,2]
	vsubps	%xmm14, %xmm6, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vsubps	%xmm8, %xmm2, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmulps	5184(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5664(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	3376(%rsp), %xmm5       # 16-byte Reload
	vmulps	5152(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm8
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	4192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm7       # 16-byte Reload
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm7, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm7[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm8, %xmm5
	vshufps	$221, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[1,3],xmm7[1,3]
	vmovaps	2608(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm6, %xmm0, %xmm2
	vmovaps	3248(%rsp), %xmm0       # 16-byte Reload
	vmulps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2672(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm3, %xmm14
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm8
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm9
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm6, %xmm3, %xmm12
	vaddps	3312(%rsp), %xmm5, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	3456(%rsp), %xmm5       # 16-byte Reload
	vaddps	3536(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vmovaps	3344(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[1,3],mem[1,3]
	vmovaps	%xmm6, 3552(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm6
	vmovaps	%xmm6, 5184(%rsp)       # 16-byte Spill
	je	.LBB147_1337
# BB#1336:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	2480(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3168(%rsp)       # 16-byte Spill
.LBB147_1337:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm2, 3248(%rsp)       # 16-byte Spill
	vmovaps	3344(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3328(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm8, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm9, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm12, %xmm7
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vmulps	3840(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rbx,%rcx,4), %xmm3
	vmovups	24616(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	%xmm14, %xmm10
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4128(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rbx,%r8,4), %xmm3
	vmovups	24616(%rbx,%r8,4), %xmm6
	vmovaps	%xmm6, 2848(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm6[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm14
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rax,4), %xmm3
	vmovups	16(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	3392(%rsp), %xmm13, %xmm12 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vaddps	2624(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm9
	vmovdqa	3168(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovaps	2656(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vsubps	3552(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vaddps	3616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	3584(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	5184(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovdqa	3104(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1339
# BB#1338:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovdqa	2496(%rsp), %xmm0       # 16-byte Reload
.LBB147_1339:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	2752(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[0,2],mem[0,2]
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vpslld	$31, %xmm0, %xmm5
	vmovaps	3280(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm2, %xmm1, %xmm1
	vsubps	3264(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm7, 3376(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm1
	vmovaps	%xmm1, 3280(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm0, %xmm0
	vblendvps	%xmm5, %xmm0, %xmm2, %xmm0
	vblendvps	%xmm6, %xmm3, %xmm0, %xmm3
	vaddps	3248(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5184(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	je	.LBB147_1341
# BB#1340:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovdqa	2528(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
.LBB147_1341:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm5, %xmm2
	vblendvps	%xmm6, %xmm1, %xmm3, %xmm3
	vaddps	3296(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1343
# BB#1342:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	2512(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
.LBB147_1343:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vblendvps	%xmm0, %xmm2, %xmm3, %xmm13
	vaddps	3360(%rsp), %xmm6, %xmm15 # 16-byte Folded Reload
	vmovaps	3424(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	2784(%rsp), %xmm9       # 16-byte Reload
	vmulps	5152(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm5, %xmm1, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2720(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm9, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm7, %xmm5, %xmm5
	vsubps	%xmm1, %xmm5, %xmm1
	vmovaps	3248(%rsp), %xmm5       # 16-byte Reload
	vaddps	3456(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm8
	vmovdqa	3120(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1345
# BB#1344:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovdqa	2544(%rsp), %xmm6       # 16-byte Reload
.LBB147_1345:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vaddps	3264(%rsp), %xmm13, %xmm13 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm15, %xmm15
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmovaps	3008(%rsp), %xmm3       # 16-byte Reload
	vmulps	3840(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2848(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 24632(%rbx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	2672(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rax,4), %xmm2, %xmm3 # xmm3 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	3328(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vaddps	3344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm8, %xmm10
	vmulps	%xmm1, %xmm0, %xmm11
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm8
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm5
	vpslld	$31, %xmm6, %xmm0
	vmovdqa	3136(%rsp), %xmm3       # 16-byte Reload
	je	.LBB147_1347
# BB#1346:                              # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovdqa	2560(%rsp), %xmm3       # 16-byte Reload
.LBB147_1347:                           # %for f8.s0.v10.v10497
                                        #   in Loop: Header=BB147_1315 Depth=4
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3680(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vsubps	5664(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	3168(%rsp), %xmm2       # 16-byte Reload
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	3280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpslld	$31, %xmm3, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vblendvps	%xmm0, %xmm11, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm10, %xmm0, %xmm0
	vblendvps	%xmm8, %xmm15, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2448(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r15
	cmpl	3100(%rsp), %r15d       # 4-byte Folded Reload
	jne	.LBB147_1315
.LBB147_1348:                           # %end for f8.s0.v10.v10498
                                        #   in Loop: Header=BB147_1313 Depth=3
	movl	3100(%rsp), %eax        # 4-byte Reload
	cmpl	1388(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB147_1367
# BB#1349:                              # %for f8.s0.v10.v10501.preheader
                                        #   in Loop: Header=BB147_1313 Depth=3
	movq	4160(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %eax
	andl	$1, %eax
	movl	%eax, 2608(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$63, %r8d
	leaq	1(%r9), %rcx
	movq	1816(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rbx
	imulq	%rbx, %rcx
	movq	1808(%rsp), %rax        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	1824(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	leaq	-1(%r9), %rcx
	imulq	%rbx, %rcx
	leaq	(%rcx,%rax), %rcx
	leaq	-2(%r9), %rdx
	imulq	%rbx, %rdx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rax), %rcx
	movq	%r9, %rdx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rax), %rdx
	leaq	2(%r9), %rsi
	imulq	%rbx, %rsi
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leaq	(%rsi,%rax), %rdx
	vbroadcastss	(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	movq	%r8, %rax
	imulq	1728(%rsp), %rax        # 8-byte Folded Reload
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	leal	2(%r9), %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %eax        # 4-byte Reload
	imull	%eax, %ecx
	movq	1440(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	movq	1688(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %edx
	movq	2136(%rsp), %rsi        # 8-byte Reload
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2576(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rdi        # 8-byte Reload
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	movq	1432(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2544(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2528(%rsp)        # 8-byte Spill
	movq	1448(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r9), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2512(%rsp)        # 8-byte Spill
	movb	%r9b, %cl
	addb	$63, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	movq	1456(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2496(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2480(%rsp)        # 8-byte Spill
	movq	2112(%rsp), %rcx        # 8-byte Reload
	leal	1(%rcx), %ecx
	andl	$63, %ecx
	imull	%eax, %ecx
	movq	1464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r9), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	imull	%eax, %r8d
	leal	(%rdx,%rdi), %ecx
	leal	(%rcx,%rsi), %eax
	movq	%rax, 2448(%rsp)        # 8-byte Spill
	leal	(%r8,%rsi), %eax
	movq	%rax, 2432(%rsp)        # 8-byte Spill
	xorl	%r10d, %r10d
	movl	1248(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1350:                           # %for f8.s0.v10.v10501
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1313 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3264(%rsp)        # 4-byte Spill
	movl	2608(%rsp), %r12d       # 4-byte Reload
	testl	%r12d, %r12d
	sete	%al
	setne	%bl
	movq	2056(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r10), %esi
	movslq	%esi, %r8
	andl	$1, %esi
	sete	%cl
	movq	2432(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	movl	%edx, 5184(%rsp)        # 4-byte Spill
	leaq	1(%r8), %rdx
	movq	4608(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rdx
	movq	4688(%rsp), %r15        # 8-byte Reload
	leaq	(%rdx,%r15), %r14
	leaq	-1(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %r13
	leaq	-2(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3552(%rsp)        # 8-byte Spill
	movq	%r8, %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3616(%rsp)        # 8-byte Spill
	leaq	2(%r8), %rdx
	imulq	%r11, %rdx
	leaq	(%rdx,%r15), %rdx
	movq	%rdx, 3648(%rsp)        # 8-byte Spill
	andb	%bl, %cl
	movl	%r8d, %edx
	movq	4160(%rsp), %rdi        # 8-byte Reload
	orl	%edi, %edx
	testb	$1, %dl
	sete	%bl
	movq	2464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %r9d
	movq	2448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edi
	movq	2496(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	movl	%edx, 5216(%rsp)        # 4-byte Spill
	movq	2480(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r10), %edx
	testl	%r8d, %r12d
	setne	%r12b
	andb	%al, %sil
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	leaq	3(%r8), %rax
	imulq	%r11, %rax
	leaq	(%rax,%r15), %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
	cmpl	$1, 104(%rbp)
	movq	2544(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3392(%rsp)        # 4-byte Spill
	movq	2528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3488(%rsp)        # 4-byte Spill
	movq	2512(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3408(%rsp)        # 4-byte Spill
	movq	2576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3536(%rsp)        # 4-byte Spill
	movq	2560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 3424(%rsp)        # 4-byte Spill
	je	.LBB147_1352
# BB#1351:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1352:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	movzbl	%sil, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1354
# BB#1353:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1354:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	%xmm2, 2624(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
	movzbl	%r12b, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1356
# BB#1355:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1356:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	%xmm2, 2640(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3248(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	je	.LBB147_1358
# BB#1357:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1358:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	movq	5464(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%r14,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r14,4), %rax
	movq	4680(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm1
	vmovss	(%rsi,%r13,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r13,4), %rax
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm14, 3200(%rsp)      # 16-byte Spill
	movslq	%r9d, %r9
	movq	5032(%rsp), %r15        # 8-byte Reload
	vmovups	8(%r15,%r9,4), %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vmovups	24(%r15,%r9,4), %xmm2
	vmovaps	%xmm2, 3216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm2[1,3]
	vmovaps	5152(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm2
	movslq	%edi, %r11
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%r11,4), %xmm3
	vmovaps	%xmm3, 3184(%rsp)       # 16-byte Spill
	vmovups	24624(%rdi,%r11,4), %xmm4
	vmovaps	%xmm4, 3168(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm4[1,3]
	vmovaps	5664(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmovaps	5696(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vbroadcastss	.LCPI147_17(%rip), %xmm11
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movslq	5216(%rsp), %r14        # 4-byte Folded Reload
	vmovups	8(%r15,%r14,4), %xmm12
	vmovups	24(%r15,%r14,4), %xmm10
	vshufps	$221, %xmm10, %xmm12, %xmm3 # xmm3 = xmm12[1,3],xmm10[1,3]
	vmovaps	4192(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm0
	movslq	%edx, %rcx
	vmovups	24608(%rdi,%rcx,4), %xmm15
	vmovups	24624(%rdi,%rcx,4), %xmm9
	vshufps	$221, %xmm9, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm9[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3776(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%r11,4), %xmm0
	vmovaps	%xmm0, 3104(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%r11,4), %xmm8
	vmovaps	%xmm8, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm8, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm8[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm5, %xmm14, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vmovups	(%r15,%r9,4), %xmm0
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%r9,4), %xmm2
	vmovaps	%xmm2, 5216(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm3 # xmm3 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm3, %xmm1, %xmm0
	vmovaps	%xmm0, 3744(%rsp)       # 16-byte Spill
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm2
	vmovaps	%xmm2, 3680(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm4, %xmm14, %xmm3
	vmulps	%xmm1, %xmm3, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm10[0,2]
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm9, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm9[0,2]
	movq	3616(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax,4), %rdx
	vmovss	(%rsi,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$32, (%rdx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$48, (%rdx,%rbx,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm10, 2832(%rsp)      # 16-byte Spill
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm4, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3296(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax,4), %rdx
	vmovss	(%rsi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$32, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%rbx,4), %rdx
	vinsertps	$48, (%rdx,%rbx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm3, 3648(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm4, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3152(%rsp)       # 16-byte Spill
	vmovaps	3312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vmovaps	3184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3168(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm5, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3136(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%r11,4), %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm8, %xmm1 # xmm1 = xmm8[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm5, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movslq	3392(%rsp), %rax        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3120(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rax,4), %xmm13
	vmovaps	%xmm13, 3616(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm13, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm13[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	5248(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movslq	3408(%rsp), %r11        # 4-byte Folded Reload
	vmovups	24608(%rdi,%r11,4), %xmm15
	vmovups	24624(%rdi,%r11,4), %xmm14
	vshufps	$221, %xmm14, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm14[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	4128(%rsp), %xmm8       # 16-byte Reload
	vmovaps	3584(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm8, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm0
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	movslq	3424(%rsp), %r13        # 4-byte Folded Reload
	vmovups	24608(%rdi,%r13,4), %xmm0
	vmovups	24624(%rdi,%r13,4), %xmm5
	vshufps	$221, %xmm5, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm5[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm2, %xmm1, %xmm4
	vmovups	24608(%rdi,%rax,4), %xmm1
	vmovups	24624(%rdi,%rax,4), %xmm3
	vshufps	$221, %xmm3, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm3[1,3]
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm9, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm14, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm8, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 3168(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm0[0,2]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	3648(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	movq	3552(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rax
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	movq	3456(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rax
	vmovss	(%rsi,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$32, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%rbx,4), %rax
	vinsertps	$48, (%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	movslq	5184(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	3328(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm3
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm14
	vmovaps	3152(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm9
	vmovaps	3136(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmovaps	2800(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmovaps	3392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm8
	movslq	3488(%rsp), %rdx        # 4-byte Folded Reload
	vmovaps	3408(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm7
	vsubps	%xmm6, %xmm2, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movslq	3536(%rsp), %rax        # 4-byte Folded Reload
	vminps	%xmm11, %xmm4, %xmm0
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	cmpl	$0, 104(%rbp)
	vmovups	(%r15,%r14,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%r14,4), %xmm0
	vmovaps	%xmm0, 3552(%rsp)       # 16-byte Spill
	vmovups	32(%r15,%r14,4), %xmm1
	vmovaps	%xmm1, 3152(%rsp)       # 16-byte Spill
	movq	%r15, %rsi
	vmovups	32(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3136(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 2800(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 3328(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 3296(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 5184(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rcx,4), %xmm4
	vmovaps	%xmm4, 3408(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm5, %xmm10 # xmm10 = xmm5[1,3],xmm0[1,3]
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm0, %xmm13 # xmm13 = xmm0[0,2],xmm2[0,2]
	vmovups	8(%rsi,%rdx,4), %xmm5
	vmovups	24(%rsi,%rdx,4), %xmm12
	vmovups	8(%rsi,%rax,4), %xmm15
	vmovups	24(%rsi,%rax,4), %xmm0
	je	.LBB147_1360
# BB#1359:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	2720(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm2, 3360(%rsp)       # 16-byte Spill
.LBB147_1360:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vsubps	%xmm10, %xmm3, %xmm2
	vmovaps	%xmm2, 3536(%rsp)       # 16-byte Spill
	vsubps	3376(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm9, %xmm1
	vmovaps	%xmm1, 3456(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 2720(%rsp)       # 16-byte Spill
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm10
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm14
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vsubps	2816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 5184(%rsp), %xmm1, %xmm9 # 16-byte Folded Reload
                                        # xmm9 = xmm1[1,3],mem[1,3]
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm8, %xmm8
	vmaxps	%xmm2, %xmm7, %xmm3
	vmovaps	3584(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vmulps	5696(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	2672(%rsp), %xmm4       # 16-byte Reload
	vmaxps	%xmm2, %xmm4, %xmm2
	vmovaps	2704(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm4, 3584(%rsp)       # 16-byte Spill
	vmovaps	3776(%rsp), %xmm4       # 16-byte Reload
	vaddps	3808(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3744(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2816(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm13 # xmm13 = xmm5[1,3],xmm12[1,3]
	vshufps	$221, %xmm0, %xmm15, %xmm4 # xmm4 = xmm15[1,3],xmm0[1,3]
	je	.LBB147_1362
# BB#1361:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	%xmm10, 3392(%rsp)      # 16-byte Spill
	vmovaps	%xmm14, %xmm10
	vmovaps	2624(%rsp), %xmm14      # 16-byte Reload
	vmovaps	%xmm14, 3344(%rsp)      # 16-byte Spill
	vmovaps	%xmm10, %xmm14
	vmovaps	3392(%rsp), %xmm10      # 16-byte Reload
.LBB147_1362:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vshufps	$136, %xmm0, %xmm15, %xmm15 # xmm15 = xmm15[0,2],xmm0[0,2]
	vshufps	$136, %xmm12, %xmm5, %xmm12 # xmm12 = xmm5[0,2],xmm12[0,2]
	vsubps	%xmm9, %xmm8, %xmm0
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	vsubps	%xmm13, %xmm3, %xmm13
	vmulps	%xmm1, %xmm7, %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm2, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vmovaps	3104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	5696(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm1
	vmovaps	3312(%rsp), %xmm3       # 16-byte Reload
	vmulps	5152(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	3008(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	2848(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	4192(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 3552(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vmovaps	2832(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	2720(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm8
	vmaxps	%xmm4, %xmm10, %xmm5
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm4, %xmm0, %xmm9
	vaddps	3488(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm0
	vmovaps	5184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmaxps	%xmm4, %xmm14, %xmm2
	vmovdqa	3360(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vmovdqa	%xmm4, 2832(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm4       # 16-byte Reload
	vaddps	3536(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm4, 2816(%rsp)       # 16-byte Spill
	vmovdqa	3344(%rsp), %xmm4       # 16-byte Reload
	vpslld	$31, %xmm4, %xmm4
	vmovdqa	%xmm4, 2688(%rsp)       # 16-byte Spill
	vaddps	3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	3584(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vmovaps	2800(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3328(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[1,3],mem[1,3]
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm14
	movq	%rdi, %rcx
	je	.LBB147_1364
# BB#1363:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	2640(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3280(%rsp)       # 16-byte Spill
.LBB147_1364:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vshufps	$136, 3328(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,2],mem[0,2]
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vmulps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm5, %xmm15
	vsubps	%xmm12, %xmm9, %xmm0
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	3200(%rsp), %xmm3       # 16-byte Reload
	vmulps	3840(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rcx,%r13,4), %xmm4
	vmovups	24616(%rcx,%r13,4), %xmm1
	vmovaps	%xmm1, 2768(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm1[1,3]
	vmovaps	%xmm6, %xmm12
	vsubps	%xmm12, %xmm4, %xmm4
	vmovaps	%xmm8, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmulps	4128(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rcx,%r11,4), %xmm4
	vmovups	24616(%rcx,%r11,4), %xmm3
	vmovaps	%xmm3, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm3[1,3]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	%xmm5, %xmm7
	vmulps	%xmm4, %xmm2, %xmm2
	vmovups	(%rsi,%rdx,4), %xmm5
	vmovups	16(%rsi,%rdx,4), %xmm4
	vmovaps	%xmm4, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm4[1,3]
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovdqa	2832(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 2832(%rsp)       # 16-byte Spill
	vmovaps	2816(%rsp), %xmm2       # 16-byte Reload
	vmulps	3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 2816(%rsp)       # 16-byte Spill
	vmovdqa	2688(%rsp), %xmm2       # 16-byte Reload
	vpsrad	$31, %xmm2, %xmm2
	vmovdqa	%xmm2, 3200(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 3104(%rsp)      # 16-byte Spill
	vmulps	%xmm14, %xmm10, %xmm2
	vmovaps	%xmm2, 2800(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm2       # 16-byte Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm4
	vmovaps	2784(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	3360(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 2784(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm13, %xmm0
	vmovaps	%xmm13, 3280(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	3376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm14, %xmm0, %xmm0
	vmovdqa	3248(%rsp), %xmm1       # 16-byte Reload
	je	.LBB147_1366
# BB#1365:                              # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovdqa	2656(%rsp), %xmm1       # 16-byte Reload
.LBB147_1366:                           # %for f8.s0.v10.v10501
                                        #   in Loop: Header=BB147_1350 Depth=4
	vmovaps	5248(%rsp), %xmm9       # 16-byte Reload
	vmulps	3312(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vmovaps	3120(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 3616(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	5184(%rsp), %xmm13      # 16-byte Reload
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm13[0,2]
	vminps	%xmm11, %xmm2, %xmm2
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	3328(%rsp), %xmm3, %xmm10 # 16-byte Folded Reload
	vaddps	%xmm15, %xmm10, %xmm3
	vmovaps	2848(%rsp), %xmm6       # 16-byte Reload
	vaddps	%xmm3, %xmm6, %xmm3
	vmovaps	3008(%rsp), %xmm8       # 16-byte Reload
	vaddps	%xmm8, %xmm3, %xmm3
	vaddps	%xmm3, %xmm2, %xmm3
	vpslld	$31, %xmm1, %xmm2
	vpsrad	$31, %xmm2, %xmm1
	vmovdqa	%xmm1, 3312(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 3296(%rsp)      # 16-byte Spill
	vbroadcastss	.LCPI147_23(%rip), %xmm15
	vmulps	%xmm15, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm3, %xmm14, %xmm3
	vmovdqa	%xmm4, %xmm2
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm3
	vmovaps	2768(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	3648(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	2752(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vmovaps	2736(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 24632(%rcx,%r11,4), %xmm4, %xmm5 # xmm5 = xmm4[0,2],mem[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vmovaps	2720(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vaddps	%xmm10, %xmm6, %xmm4
	vaddps	3296(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm8, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufps	$221, 3408(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm13[1,3],mem[1,3]
	vmovaps	3616(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3168(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	3184(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm8, %xmm9, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm11, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm1, %xmm4, %xmm1
	vmovaps	2784(%rsp), %xmm4       # 16-byte Reload
	vaddps	3376(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	3280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	3392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm15, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm1, %xmm14, %xmm1
	vmovaps	3104(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	3312(%rsp), %xmm2       # 16-byte Reload
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm10
	vmovaps	3200(%rsp), %xmm9       # 16-byte Reload
	vblendvps	%xmm9, 2800(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	2832(%rsp), %xmm0       # 16-byte Reload
	vblendvps	%xmm0, 2816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 3136(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3216(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	5152(%rsp), %xmm8, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 3152(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	3680(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 3232(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	4192(%rsp), %xmm8, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm11, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vmovaps	3536(%rsp), %xmm4       # 16-byte Reload
	vaddps	3776(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	3744(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	3808(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm6, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm2, %xmm10, %xmm0
	vmovaps	3584(%rsp), %xmm2       # 16-byte Reload
	vaddps	3456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3488(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vblendvps	%xmm9, %xmm2, %xmm0, %xmm0
	vaddps	3328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	2592(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r10d
	movl	3264(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1350
.LBB147_1367:                           # %end for f8.s0.v10.v10502
                                        #   in Loop: Header=BB147_1313 Depth=3
	movl	1388(%rsp), %eax        # 4-byte Reload
	cmpl	1736(%rsp), %eax        # 4-byte Folded Reload
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	jge	.LBB147_1368
# BB#1369:                              # %for f8.s0.v10.v10505.preheader
                                        #   in Loop: Header=BB147_1313 Depth=3
	movq	4160(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 3184(%rsp)        # 4-byte Spill
	movl	%r11d, %r8d
	andl	$63, %r8d
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2448(%rsp)       # 16-byte Spill
	leaq	1(%r11), %rcx
	movq	%rcx, 2240(%rsp)        # 8-byte Spill
	movq	1816(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rdi
	imulq	%rdi, %rcx
	movq	1808(%rsp), %rbx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	2112(%rsp), %r10        # 8-byte Reload
	addl	$1, %r10d
	movq	%r10, 2112(%rsp)        # 8-byte Spill
	leaq	-1(%r11), %rdx
	imulq	%rdi, %rdx
	movq	1824(%rsp), %rsi        # 8-byte Reload
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	leaq	(%rdx,%rbx), %rcx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	leaq	-2(%r11), %rcx
	imulq	%rdi, %rcx
	movq	%r11, %rdx
	imulq	%rdi, %rdx
	leaq	(%rcx,%rbx), %rcx
	leaq	(%rdx,%rbx), %rdx
	vbroadcastss	(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leaq	2(%r11), %rdx
	imulq	%rdi, %rdx
	leaq	(%rdx,%rbx), %rdx
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 3808(%rsp)       # 16-byte Spill
	movq	%r8, %rsi
	imulq	1728(%rsp), %rsi        # 8-byte Folded Reload
	leal	2(%r11), %ecx
	andl	$63, %ecx
	movl	1700(%rsp), %r9d        # 4-byte Reload
	imull	%r9d, %ecx
	movq	1440(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %edx
	movl	1768(%rsp), %ebx        # 4-byte Reload
	imull	%ebx, %edx
	subq	4712(%rsp), %rsi        # 8-byte Folded Reload
	movq	%rsi, 2432(%rsp)        # 8-byte Spill
	movq	1232(%rsp), %rdi        # 8-byte Reload
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2416(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rax        # 8-byte Reload
	leal	(%rdx,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2400(%rsp)        # 8-byte Spill
	movq	1432(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r11), %ecx
	imull	%ebx, %ecx
	leal	(%rcx,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2384(%rsp)        # 8-byte Spill
	movb	%r11b, %cl
	addb	$62, %cl
	movzbl	%cl, %ecx
	andl	$63, %ecx
	imull	%r9d, %ecx
	movq	1448(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r11), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2352(%rsp)        # 8-byte Spill
	leal	(%rdx,%rax), %ecx
	movb	%r11b, %dl
	addb	$63, %dl
	movzbl	%dl, %edx
	andl	$63, %edx
	imull	%r9d, %edx
	movq	1456(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%r11), %esi
	imull	%ebx, %esi
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2336(%rsp)        # 8-byte Spill
	leal	(%rdx,%rdi), %ecx
	movq	%rcx, 2320(%rsp)        # 8-byte Spill
	leal	(%rsi,%rax), %ecx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2304(%rsp)        # 8-byte Spill
	movl	%r10d, %ecx
	andl	$63, %ecx
	imull	%r9d, %ecx
	movq	1464(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r11), %edx
	imull	%ebx, %edx
	leal	(%rcx,%rdi), %ecx
	movq	%rcx, 2288(%rsp)        # 8-byte Spill
	imull	%r9d, %r8d
	leal	(%rdx,%rax), %ecx
	leal	(%rcx,%rdi), %eax
	movq	%rax, 2272(%rsp)        # 8-byte Spill
	leal	(%r8,%rdi), %eax
	movq	%rax, 2256(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movl	1244(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1370:                           # %for f8.s0.v10.v10505
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1313 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3168(%rsp)        # 4-byte Spill
	cmpl	$0, 3184(%rsp)          # 4-byte Folded Reload
	sete	5152(%rsp)              # 1-byte Folded Spill
	setne	%r15b
	movb	%r15b, 3312(%rsp)       # 1-byte Spill
	movq	2992(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r10d
	movl	%r10d, 3152(%rsp)       # 4-byte Spill
	movl	%r10d, %r12d
	andl	$1, %r12d
	sete	%r13b
	movq	2920(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3328(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	movl	%r8d, 3344(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r9d
	movl	%r9d, 3360(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	2448(%rsp), %xmm14      # 16-byte Reload
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm9
	vmovdqa	5040(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm1
	movq	2976(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vmovdqa	5312(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm2, %xmm2
	vmovdqa	5344(%rsp), %xmm8       # 16-byte Reload
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vmovdqa	5328(%rsp), %xmm12      # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm12, %xmm4
	movq	2912(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vmovdqa	5296(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm3, %xmm0, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovd	%xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpaddd	%xmm8, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm2, %xmm3, %xmm1
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm14, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm4
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm2, %xmm2
	movq	2968(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm8, %xmm3, %xmm3
	vpcmpgtd	%xmm4, %xmm12, %xmm5
	vpsubd	%xmm4, %xmm0, %xmm6
	movq	2904(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm13, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpaddd	%xmm8, %xmm4, %xmm4
	vmovd	%xmm7, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vblendvps	%xmm2, %xmm3, %xmm4, %xmm2
	vmovd	%esi, %xmm3
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %edi, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm4
	vpand	%xmm14, %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	5136(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm3
	movq	2960(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vpcmpgtd	%xmm4, %xmm12, %xmm6
	vpsubd	%xmm4, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	movq	2928(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm13, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpaddd	%xmm8, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovd	%xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm5, %xmm4, %xmm3
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm5
	vmovdqa	5056(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm4, %xmm4
	vpcmpgtd	%xmm5, %xmm12, %xmm6
	vpsubd	%xmm5, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vpaddd	%xmm13, %xmm9, %xmm6
	vpminsd	%xmm10, %xmm6, %xmm6
	movq	2936(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm13, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpmaxsd	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm8, %xmm5, %xmm5
	vmovd	%xmm7, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm8, %xmm5, %xmm5
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vblendvps	%xmm4, %xmm6, %xmm5, %xmm4
	vmovd	%esi, %xmm5
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpinsrd	$2, %edi, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm6
	vpand	%xmm14, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm5, %xmm12, %xmm6
	vpsubd	%xmm5, %xmm0, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vmovdqa	5360(%rsp), %xmm6       # 16-byte Reload
	vpmulld	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %rcx
	movq	%rcx, 2784(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	%rcx, 2832(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	movq	2952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vmovq	%xmm1, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3712(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3264(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3744(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm3, %xmm1
	vpaddd	%xmm1, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpmulld	%xmm6, %xmm4, %xmm3
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	vpaddd	%xmm3, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vmovdqa	4896(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm3
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm8, %xmm5, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vpmulld	%xmm6, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm11, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	andb	%r15b, %r13b
	movb	%r13b, 5184(%rsp)       # 1-byte Spill
	movl	%r10d, %eax
	movq	4160(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %eax
	testb	$1, %al
	movq	2944(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	sete	%r13b
	movl	3184(%rsp), %r8d        # 4-byte Reload
	testl	%r10d, %r8d
	setne	3280(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %r10b       # 1-byte Reload
	andb	%r10b, %r12b
	movl	%r12d, 5216(%rsp)       # 4-byte Spill
	movl	3776(%rsp), %r15d       # 4-byte Reload
	movl	%r15d, %r9d
	andl	$1, %r9d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	sete	%r12b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3328(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	2984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	andb	3312(%rsp), %r12b       # 1-byte Folded Reload
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm14, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm12, %xmm3
	vpsubd	%xmm1, %xmm0, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm8, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovq	%xmm0, %r11
	movq	%r11, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	vpextrq	$1, %xmm0, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	movl	%r15d, %eax
	orl	%ebx, %eax
	testb	$1, %al
	sete	%dil
	testl	%r15d, %r8d
	movzbl	%r13b, %eax
	vmovd	%eax, %xmm0
	setne	%cl
	andb	%r10b, %r9b
	vbroadcastss	%xmm0, %xmm5
	vmovaps	%xmm5, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2256(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 3312(%rsp)        # 4-byte Spill
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movl	%eax, 2768(%rsp)        # 4-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %eax
	movq	2320(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r10d
	movq	2304(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %ebx
	movq	2384(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3776(%rsp)        # 4-byte Spill
	movq	2352(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r15d
	movq	2336(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 2816(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %r8d
	movq	2400(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r14), %edx
	movl	%edx, 3296(%rsp)        # 4-byte Spill
	je	.LBB147_1372
# BB#1371:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1372:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm0, 2464(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %r13d       # 1-byte Folded Reload
	vmovd	%r13d, %xmm0
	movl	5216(%rsp), %edx        # 4-byte Reload
	movzbl	%dl, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3104(%rsp)       # 16-byte Spill
	je	.LBB147_1374
# BB#1373:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1374:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm1, 2480(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3216(%rsp)       # 16-byte Spill
	movzbl	3280(%rsp), %esi        # 1-byte Folded Reload
	vmovd	%esi, %xmm0
	je	.LBB147_1376
# BB#1375:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1376:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3200(%rsp)       # 16-byte Spill
	je	.LBB147_1378
# BB#1377:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1378:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2512(%rsp)       # 16-byte Spill
	movzbl	%dil, %esi
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, %xmm0
	je	.LBB147_1380
# BB#1379:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1380:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm0, 2544(%rsp)       # 16-byte Spill
	movzbl	%r12b, %esi
	vmovd	%esi, %xmm0
	movzbl	%r9b, %esi
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm4
	je	.LBB147_1382
# BB#1381:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm4, %xmm4, %xmm4
.LBB147_1382:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, 3248(%rsp)       # 16-byte Spill
	movzbl	%cl, %ecx
	vmovd	%ecx, %xmm0
	je	.LBB147_1384
# BB#1383:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm3, %xmm3, %xmm3
.LBB147_1384:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm5, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 2528(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 2560(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 3120(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3232(%rsp)       # 16-byte Spill
	je	.LBB147_1386
# BB#1385:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1386:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	%xmm0, 2576(%rsp)       # 16-byte Spill
	movq	2784(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2832(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3008(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2848(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2800(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3712(%rsp), %rcx        # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3264(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3744(%rsp), %rcx        # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm13, 2848(%rsp)      # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	cltq
	movq	5608(%rsp), %rsi        # 8-byte Reload
	vmovups	24608(%rsi,%rax,4), %xmm15
	vmovups	24624(%rsi,%rax,4), %xmm8
	vshufps	$221, %xmm8, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm8[1,3]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movslq	%r10d, %r12
	movq	5032(%rsp), %rdi        # 8-byte Reload
	vmovups	8(%rdi,%r12,4), %xmm9
	vmovaps	4128(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	%ebx, %rcx
	movq	%rdi, %rbx
	vmovups	24608(%rsi,%rcx,4), %xmm11
	vmovups	24624(%rsi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm13, %xmm2
	vmovups	24600(%rsi,%rax,4), %xmm5
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rax,4), %xmm12
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2752(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm13, %xmm2
	vmovups	24600(%rsi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2720(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r12,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2688(%rsp)       # 16-byte Spill
	movq	3536(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3376(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3392(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2736(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	movq	3408(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vmovss	(%rdx,%rdi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3488(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3424(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rdi
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3456(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3008(%rsp)      # 16-byte Spill
	vmovups	24632(%rsi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3488(%rsp)       # 16-byte Spill
	movslq	2768(%rsp), %rdi        # 4-byte Folded Reload
	vmovups	8(%rbx,%rdi,4), %xmm1
	vmovups	24(%rbx,%rdi,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3424(%rsp)       # 16-byte Spill
	movslq	3776(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2768(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3776(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movslq	2816(%rsp), %r9         # 4-byte Folded Reload
	vmovups	24608(%rsi,%r9,4), %xmm13
	vmovups	24624(%rsi,%r9,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	3840(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	movslq	3296(%rsp), %r10        # 4-byte Folded Reload
	vmovups	24608(%rsi,%r10,4), %xmm8
	vmovups	24624(%rsi,%r10,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3808(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rsi,%rcx,4), %xmm6
	vmovups	24624(%rsi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	vmovups	24632(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3648(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3584(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3616(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3328(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3360(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	movslq	3312(%rsp), %rax        # 4-byte Folded Reload
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm12
	vmovaps	2752(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovaps	3552(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3456(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3424(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3376(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r15d, %rcx
	vmovaps	2640(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	movslq	%r8d, %rdx
	vminps	%xmm4, %xmm5, %xmm11
	cmpl	$0, 104(%rbp)
	vmovups	(%rbx,%r12,4), %xmm1
	vmovaps	%xmm1, 2640(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%r12,4), %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%r12,4), %xmm5
	vmovaps	%xmm5, 2752(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rdi,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	8(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rax,4), %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rbx,%rcx,4), %xmm2
	vmovups	24(%rbx,%rcx,4), %xmm8
	vmovups	8(%rbx,%rdx,4), %xmm1
	vmovups	24(%rbx,%rdx,4), %xmm13
	je	.LBB147_1388
# BB#1387:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	2464(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3216(%rsp)       # 16-byte Spill
.LBB147_1388:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vsubps	3536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vsubps	2672(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vsubps	2688(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm12
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2688(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2656(%rsp), %xmm3       # 16-byte Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2672(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	2592(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2592(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm11, %xmm14
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3552(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3424(%rsp), %xmm3, %xmm11 # 16-byte Folded Reload
                                        # xmm11 = xmm3[0,2],mem[0,2]
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1390
# BB#1389:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	2480(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3200(%rsp)       # 16-byte Spill
.LBB147_1390:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vsubps	%xmm9, %xmm12, %xmm6
	vmovaps	%xmm6, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm12 # xmm12 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5664(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmulps	4192(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2720(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4128(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vmovaps	2640(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2688(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vmulps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2672(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2624(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm11
	vmovaps	2608(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2592(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3328(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vaddps	3536(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3552(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5152(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1392
# BB#1391:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	2496(%rsp), %xmm7       # 16-byte Reload
.LBB147_1392:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm11, %xmm11
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rdx,4), %xmm1
	vmovups	16(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	2848(%rsp), %xmm5       # 16-byte Reload
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%r10,4), %xmm3
	vmovups	24616(%rsi,%r10,4), %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm12
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rsi,%r9,4), %xmm3
	vmovups	24616(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rbx,%rcx,4), %xmm3
	vmovups	16(%rbx,%rcx,4), %xmm5
	vmovaps	%xmm5, 2672(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3408(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2656(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2848(%rsp)       # 16-byte Spill
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3104(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1394
# BB#1393:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	2512(%rsp), %xmm0       # 16-byte Reload
.LBB147_1394:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2768(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm12, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2640(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3280(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm11, %xmm13, %xmm5
	vmovaps	%xmm11, 3392(%rsp)      # 16-byte Spill
	vaddps	3344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3264(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm11
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5152(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1396
# BB#1395:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	2544(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3248(%rsp)       # 16-byte Spill
.LBB147_1396:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm11, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3312(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1398
# BB#1397:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	2528(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3232(%rsp)       # 16-byte Spill
.LBB147_1398:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3376(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2800(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2784(%rsp), %xmm15      # 16-byte Reload
	vmulps	4192(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2752(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4128(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmovaps	%xmm10, %xmm1
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vaddps	3456(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3536(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm10
	vmovdqa	3120(%rsp), %xmm6       # 16-byte Reload
	je	.LBB147_1400
# BB#1399:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	2560(%rsp), %xmm6       # 16-byte Reload
.LBB147_1400:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vaddps	3280(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm11, %xmm14, %xmm14
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm1, %xmm3
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	%xmm0, %xmm12, %xmm0
	vmovaps	3008(%rsp), %xmm5       # 16-byte Reload
	vmulps	3808(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2736(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2688(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm3, %xmm5, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rbx,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3344(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm10, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm13
	vpslld	$31, %xmm6, %xmm0
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	3136(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1402
# BB#1401:                              # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovdqa	2576(%rsp), %xmm5       # 16-byte Reload
.LBB147_1402:                           # %for f8.s0.v10.v10505
                                        #   in Loop: Header=BB147_1370 Depth=4
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	2848(%rsp), %xmm2       # 16-byte Reload
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm13, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3152(%rsp), %rax        # 4-byte Folded Reload
	movq	2432(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	movl	3168(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	vmovdqa	5376(%rsp), %xmm11      # 16-byte Reload
	jne	.LBB147_1370
# BB#1403:                              #   in Loop: Header=BB147_1313 Depth=3
	movq	2240(%rsp), %rax        # 8-byte Reload
.LBB147_1404:                           # %end for f8.s0.v10.v10506
                                        #   in Loop: Header=BB147_1313 Depth=3
	movq	%rax, 4160(%rsp)        # 8-byte Spill
	movq	2368(%rsp), %rax        # 8-byte Reload
	movq	2112(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	movq	4672(%rsp), %r9         # 8-byte Reload
	jne	.LBB147_1313
.LBB147_1405:                           # %end for f8.s0.v11496
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	2368(%rsp), %rax        # 8-byte Reload
	cmpl	1372(%rsp), %eax        # 4-byte Folded Reload
	movl	1736(%rsp), %eax        # 4-byte Reload
	jl	.LBB147_1406
	jmp	.LBB147_1444
.LBB147_1407:                           # %for f8.s0.v11509.end for f8.s0.v10.v10513_crit_edge
                                        #   in Loop: Header=BB147_1406 Depth=3
	movq	2368(%rsp), %rax        # 8-byte Reload
	addl	$1, %eax
	movl	%eax, %ecx
	jmp	.LBB147_1443
	.align	16, 0x90
.LBB147_1406:                           # %for f8.s0.v11509
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1409 Depth 4
	testl	%eax, %eax
	jle	.LBB147_1407
# BB#1408:                              # %for f8.s0.v10.v10512.preheader
                                        #   in Loop: Header=BB147_1406 Depth=3
	movq	2368(%rsp), %r8         # 8-byte Reload
	movl	%r8d, %r11d
	movq	1752(%rsp), %rdi        # 8-byte Reload
	subl	%edi, %r11d
	leal	1(%r11), %eax
	cltd
	movq	1760(%rsp), %rcx        # 8-byte Reload
	idivl	%ecx
	movq	%rcx, %rbx
	movl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	leal	-1(%r11), %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	movl	1772(%rsp), %r13d       # 4-byte Reload
	andl	%r13d, %esi
	addl	%ecx, %esi
	movl	%r9d, %ecx
	sarl	$31, %ecx
	movl	%r11d, %eax
	cltd
	idivl	%ebx
	andl	%r13d, %ecx
	addl	%r9d, %ecx
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%r13d, %ebx
	addl	%edx, %ebx
	leal	1(%r8), %eax
	movl	%eax, 2256(%rsp)        # 4-byte Spill
	movl	1740(%rsp), %r12d       # 4-byte Reload
	cmpl	%eax, %r12d
	movl	%r12d, %edx
	cmovgl	%eax, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1796(%rsp), %r15d       # 4-byte Reload
	movl	%r15d, %eax
	subl	%esi, %eax
	movq	1784(%rsp), %r9         # 8-byte Reload
	cmpl	%esi, %r9d
	cmovgl	%esi, %eax
	addl	%edi, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r8d, %r12d
	cmovgl	%edx, %eax
	movl	%r12d, %edx
	cmovgl	%r8d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r15d, %r10d
	subl	%ebx, %r10d
	cmpl	%ebx, %r9d
	movq	%r9, %r14
	cmovgl	%ebx, %r10d
	addl	%edi, %r10d
	cmpl	%r10d, %r12d
	cmovlel	%r12d, %r10d
	cmpl	%edi, %r10d
	cmovll	%edi, %r10d
	movq	1744(%rsp), %r9         # 8-byte Reload
	cmpl	%r8d, %r9d
	movl	%r9d, %esi
	cmovgl	%r8d, %esi
	cmovgl	%edx, %r10d
	addl	$-1, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	movl	%r15d, %ebx
	subl	%ecx, %ebx
	cmpl	%ecx, %r14d
	cmovgl	%ecx, %ebx
	addl	%edi, %ebx
	cmpl	%ebx, %r12d
	cmovlel	%r12d, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	%r8d, %r9d
	cmovgel	%esi, %ebx
	movl	%r8d, %ecx
	andl	$1, %ecx
	movl	%ecx, 3168(%rsp)        # 4-byte Spill
	vpabsd	5392(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%xmm0, 2448(%rsp)       # 16-byte Spill
	movslq	%eax, %r9
	movl	%r8d, %ecx
	leal	-2(%r11), %eax
	cltd
	movq	1760(%rsp), %rsi        # 8-byte Reload
	idivl	%esi
	movl	%edx, %r14d
	andl	$63, %ecx
	movq	%rcx, 2464(%rsp)        # 8-byte Spill
	movl	%r14d, %ecx
	sarl	$31, %ecx
	addl	$2, %r11d
	movl	%r11d, %eax
	cltd
	idivl	%esi
	andl	%r13d, %ecx
	addl	%r14d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	movq	1816(%rsp), %r14        # 8-byte Reload
	imulq	%r14, %r9
	movq	1808(%rsp), %r15        # 8-byte Reload
	leaq	(%r9,%r15), %rdx
	movq	1824(%rsp), %r9         # 8-byte Reload
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 4192(%rsp)       # 16-byte Spill
	movslq	%ebx, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%r15), %rdx
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 4160(%rsp)       # 16-byte Spill
	leal	-2(%r8), %r11d
	cmpl	%r11d, %r12d
	movl	%r12d, %edx
	cmovgl	%r11d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	1796(%rsp), %r13d       # 4-byte Reload
	movl	%r13d, %esi
	subl	%ecx, %esi
	movq	1784(%rsp), %rbx        # 8-byte Reload
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %esi
	addl	%edi, %esi
	cmpl	%esi, %r12d
	cmovlel	%r12d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r8d, 1704(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rcx
	imulq	%r14, %rcx
	leaq	(%rcx,%r15), %rcx
	movq	%rcx, 5216(%rsp)        # 8-byte Spill
	movslq	%r10d, %rdx
	imulq	%r14, %rdx
	leaq	(%rdx,%r15), %rdx
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 5248(%rsp)       # 16-byte Spill
	leal	2(%r8), %ebx
	cmpl	%ebx, %r12d
	movl	%r12d, %edx
	cmovgl	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%r13d, %esi
	subl	%eax, %esi
	movq	1784(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %esi
	addl	%edi, %esi
	cmpl	%esi, %r12d
	cmovlel	%r12d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r8d, 1708(%rsp)        # 4-byte Folded Reload
	cmovgl	%edx, %esi
	movslq	%esi, %rax
	imulq	%r14, %rax
	leaq	(%rax,%r15), %rax
	movq	5216(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%r9,%rcx,4), %xmm0
	vmovaps	%xmm0, 4128(%rsp)       # 16-byte Spill
	movq	2464(%rsp), %r10        # 8-byte Reload
	movq	%r10, %rdx
	imulq	1728(%rsp), %rdx        # 8-byte Folded Reload
	andl	$63, %ebx
	movl	1700(%rsp), %esi        # 4-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 2416(%rsp)        # 8-byte Spill
	movq	1392(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	1768(%rsp), %edi        # 4-byte Reload
	imull	%edi, %ecx
	vbroadcastss	(%r9,%rax,4), %xmm0
	vmovaps	%xmm0, 3840(%rsp)       # 16-byte Spill
	subq	4712(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 2400(%rsp)        # 8-byte Spill
	movq	4872(%rsp), %rbx        # 8-byte Reload
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	movq	1528(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	andl	$63, %r11d
	imull	%esi, %r11d
	movq	%r11, 2432(%rsp)        # 8-byte Spill
	movq	1400(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	imull	%edi, %ecx
	leal	63(%r8), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 2352(%rsp)        # 8-byte Spill
	movq	1408(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%r8), %edx
	imull	%edi, %edx
	leal	(%rax,%rbx), %eax
	movq	%rax, 2336(%rsp)        # 8-byte Spill
	leal	(%rcx,%rbx), %eax
	movq	%rax, 2320(%rsp)        # 8-byte Spill
	leal	(%rdx,%rbx), %eax
	movq	%rax, 2304(%rsp)        # 8-byte Spill
	movl	2256(%rsp), %eax        # 4-byte Reload
	andl	$63, %eax
	imull	%esi, %eax
	movq	%rax, 2288(%rsp)        # 8-byte Spill
	movq	1416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r8), %eax
	imull	%edi, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 2272(%rsp)        # 8-byte Spill
	movq	%r10, %rax
	imull	%esi, %eax
	movq	%rax, 2464(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	movl	1736(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1409:                           # %for f8.s0.v10.v10512
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1406 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%eax, 3152(%rsp)        # 4-byte Spill
	cmpl	$0, 3168(%rsp)          # 4-byte Folded Reload
	sete	5152(%rsp)              # 1-byte Folded Spill
	setne	3776(%rsp)              # 1-byte Folded Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3808(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	sete	5184(%rsp)              # 1-byte Folded Spill
	movq	4560(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI147_16(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	5392(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	movl	%ecx, 3488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %edi
	movl	%edx, %r11d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 3536(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 3552(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %r13d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 3312(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %esi
	movl	%edx, 3744(%rsp)        # 4-byte Spill
	movq	4832(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3712(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	%ebx, %ecx
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 3680(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, 3648(%rsp)        # 4-byte Spill
	movq	4840(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 3616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	%r8d, %ebx
	idivl	%ebx
	movl	%edx, 3584(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %r9d
	movq	4584(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%r14d, %xmm0
	vpinsrd	$1, %r11d, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	vpinsrd	$2, %r13d, %xmm0, %xmm0
	vpinsrd	$3, 3744(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	vmovd	%r12d, %xmm2
	vpinsrd	$1, 3712(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%esi, %r11d
	movl	%edx, %r12d
	vpinsrd	$2, 3680(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, 3648(%rsp), %xmm1, %xmm2 # 4-byte Folded Reload
	movq	4848(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vmovd	%r10d, %xmm4
	vpinsrd	$1, 3616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vmovd	%xmm3, %eax
	cltd
	idivl	%ecx
	movl	%edx, %edi
	vpinsrd	$2, 3584(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, %r9d, %xmm4, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movq	4856(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm6
	vpsrad	$31, %xmm0, %xmm7
	vmovdqa	2448(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm7
	movl	3808(%rsp), %r10d       # 4-byte Reload
	vmovd	%r10d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	5328(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm11, %xmm1
	vmovdqa	5296(%rsp), %xmm15      # 16-byte Reload
	vpsubd	%xmm7, %xmm15, %xmm4
	vblendvps	%xmm1, %xmm7, %xmm4, %xmm1
	vmovdqa	5040(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	5008(%rsp), %xmm7       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm7, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vmovdqa	5344(%rsp), %xmm10      # 16-byte Reload
	vpaddd	%xmm10, %xmm1, %xmm1
	vmovdqa	5312(%rsp), %xmm12      # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	movq	4592(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r9d
	vmovd	%r9d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm1, %xmm7, %xmm1
	vmovdqa	5360(%rsp), %xmm7       # 16-byte Reload
	vpmulld	%xmm7, %xmm1, %xmm1
	vpinsrd	$1, %r8d, %xmm6, %xmm4
	vmovdqa	5376(%rsp), %xmm13      # 16-byte Reload
	vpaddd	%xmm1, %xmm13, %xmm1
	vpinsrd	$2, %r13d, %xmm4, %xmm4
	vpextrq	$1, %xmm1, %rbx
	movq	%rbx, 3744(%rsp)        # 8-byte Spill
	vpinsrd	$3, %r12d, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	movq	%rcx, 3712(%rsp)        # 8-byte Spill
	vpsrad	$31, %xmm2, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm2
	vpsubd	%xmm1, %xmm15, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vmovdqa	5120(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm2, %xmm2
	vmovdqa	5072(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	3680(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm1
	vpsrad	$31, %xmm5, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm11, %xmm5
	vpsubd	%xmm2, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vmovdqa	5136(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	5088(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovd	%edi, %xmm6
	vpextrd	$3, %xmm3, %eax
	vpinsrd	$1, %esi, %xmm6, %xmm3
	sarq	$32, %rcx
	movq	%rcx, 3264(%rsp)        # 8-byte Spill
	vpinsrd	$2, %edx, %xmm3, %xmm3
	cltd
	idivl	%r11d
	sarq	$32, %rbx
	movq	%rbx, 3104(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	3648(%rsp), %xmm6 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm11, %xmm5
	vpsubd	%xmm4, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vmovdqa	5056(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4992(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm14, %xmm0, %xmm6
	vpminsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm3
	vpcmpgtd	%xmm3, %xmm11, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vmovdqa	4896(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vmovdqa	4736(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	movq	4600(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vmovq	%xmm1, %rax
	movq	%rax, 2848(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2800(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 2816(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm2, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3616(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3680(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3584(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3648(%rsp)        # 8-byte Spill
	vpmulld	%xmm7, %xmm4, %xmm1
	vpaddd	%xmm1, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	vpaddd	%xmm10, %xmm3, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpbroadcastd	%xmm6, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpminsd	%xmm12, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vblendvps	%xmm5, %xmm2, %xmm3, %xmm2
	vpmulld	%xmm7, %xmm2, %xmm2
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3328(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3344(%rsp)        # 8-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm1
	vmovq	%xmm1, %rax
	movq	%rax, 3360(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3408(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %rax
	movq	%rax, 3376(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3392(%rsp)        # 8-byte Spill
	movb	3776(%rsp), %r13b       # 1-byte Reload
	andb	%r13b, 5184(%rsp)       # 1-byte Folded Spill
	movl	%r10d, %ecx
	movl	%ecx, %eax
	movq	2368(%rsp), %r12        # 8-byte Reload
	orl	%r12d, %eax
	testb	$1, %al
	movq	4568(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	sete	%bl
	movl	3168(%rsp), %r11d       # 4-byte Reload
	testl	%ecx, %r11d
	setne	3280(%rsp)              # 1-byte Folded Spill
	movb	5152(%rsp), %r14b       # 1-byte Reload
	movl	5216(%rsp), %eax        # 4-byte Reload
	andb	%r14b, %al
	movl	%eax, 5216(%rsp)        # 4-byte Spill
	movl	%r9d, %r8d
	andl	$1, %r8d
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	sete	%r10b
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	3488(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	3536(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	3552(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	3312(%rsp)              # 4-byte Folded Reload
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	movq	4576(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	andb	%r13b, %r10b
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm11, %xmm3
	vpsubd	%xmm1, %xmm15, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vmovdqa	4880(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpxor	%xmm9, %xmm3, %xmm3
	vmovdqa	4720(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm0
	vpor	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm10, %xmm1, %xmm1
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm12, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovq	%xmm0, %rax
	movq	%rax, 3488(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 3552(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r13
	movq	%r13, 3536(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movl	%r9d, %eax
	orl	%r12d, %eax
	testb	$1, %al
	sete	%cl
	testl	%r9d, %r11d
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	setne	%bl
	andb	%r14b, %r8b
	vbroadcastss	%xmm0, %xmm3
	vmovaps	%xmm3, %xmm0
	cmpl	$1, 104(%rbp)
	movq	2464(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movq	2288(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2832(%rsp)        # 4-byte Spill
	movq	2272(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %edi
	movq	2352(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %esi
	movq	2304(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 2784(%rsp)        # 4-byte Spill
	movq	2336(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3776(%rsp)        # 4-byte Spill
	movq	2432(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r14d
	movq	2320(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3296(%rsp)        # 4-byte Spill
	movq	2416(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r11d
	movq	2384(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 3312(%rsp)        # 4-byte Spill
	je	.LBB147_1411
# BB#1410:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1411:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm0, 2480(%rsp)       # 16-byte Spill
	movzbl	5184(%rsp), %r9d        # 1-byte Folded Reload
	vmovd	%r9d, %xmm0
	movl	5216(%rsp), %eax        # 4-byte Reload
	movzbl	%al, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 3120(%rsp)       # 16-byte Spill
	je	.LBB147_1413
# BB#1412:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1413:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm1, 2496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm1
	vmovaps	%xmm1, 3200(%rsp)       # 16-byte Spill
	movzbl	3280(%rsp), %eax        # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	je	.LBB147_1415
# BB#1414:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1415:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3184(%rsp)       # 16-byte Spill
	je	.LBB147_1417
# BB#1416:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1417:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm1, 2512(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 2528(%rsp)       # 16-byte Spill
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
	je	.LBB147_1419
# BB#1418:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1419:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm0, 2560(%rsp)       # 16-byte Spill
	movzbl	%r10b, %eax
	vmovd	%eax, %xmm0
	movzbl	%r8b, %eax
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, %xmm2
	je	.LBB147_1421
# BB#1420:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1421:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm2, 2544(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm2
	vmovaps	%xmm2, 3232(%rsp)       # 16-byte Spill
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	je	.LBB147_1423
# BB#1422:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm2, %xmm2, %xmm2
.LBB147_1423:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm3, 3280(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 2576(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 3136(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 3216(%rsp)       # 16-byte Spill
	je	.LBB147_1425
# BB#1424:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB147_1425:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	%xmm0, 2592(%rsp)       # 16-byte Spill
	movq	3712(%rsp), %rax        # 8-byte Reload
	cltq
	movq	5464(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3264(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3744(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3104(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 5152(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm5
	movq	2848(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	2800(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3008(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	2816(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm15, 3008(%rsp)      # 16-byte Spill
	vmovaps	4192(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm1
	movslq	%edi, %rbx
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vmovups	24608(%rdi,%rbx,4), %xmm13
	vmovups	24624(%rdi,%rbx,4), %xmm8
	vshufps	$221, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm8[1,3]
	vmovaps	5664(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	5696(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 5216(%rsp)       # 16-byte Spill
	movslq	%esi, %r9
	movq	5032(%rsp), %rsi        # 8-byte Reload
	vmovups	8(%rsi,%r9,4), %xmm9
	vmovaps	4160(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm2
	movslq	2784(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rcx,4), %xmm11
	vmovups	24624(%rdi,%rcx,4), %xmm1
	vshufps	$221, %xmm1, %xmm11, %xmm6 # xmm6 = xmm11[1,3],xmm1[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 5184(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm15, %xmm2
	vmovups	24600(%rdi,%rbx,4), %xmm5
	vmovaps	%xmm5, 3264(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rbx,4), %xmm12
	vmovaps	%xmm12, 3744(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm5, %xmm6 # xmm6 = xmm5[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2768(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm15, %xmm2
	vmovups	24600(%rdi,%rcx,4), %xmm6
	vmovaps	%xmm6, 2736(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm5
	vmovaps	%xmm5, 3712(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm5[1,3]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm2, 2720(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r9,4), %xmm2
	vshufps	$221, %xmm2, %xmm9, %xmm6 # xmm6 = xmm9[1,3],xmm2[1,3]
	vmovaps	%xmm6, 2688(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm2[0,2]
	vmovaps	%xmm2, 2704(%rsp)       # 16-byte Spill
	movq	3424(%rsp), %rax        # 8-byte Reload
	cltq
	vshufps	$136, %xmm1, %xmm11, %xmm1 # xmm1 = xmm11[0,2],xmm1[0,2]
	vmovss	(%rdx,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movq	3456(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	3328(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	3344(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm2, %xmm10 # xmm10 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm10, 2752(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm10, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	movq	3360(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movq	3408(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	3376(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	3392(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm14, 3104(%rsp)      # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm1
	vmovaps	%xmm1, 2848(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	movslq	2832(%rsp), %r10        # 4-byte Folded Reload
	vmovups	8(%rsi,%r10,4), %xmm1
	vmovups	24(%rsi,%r10,4), %xmm2
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 3408(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm10, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2816(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	movslq	3776(%rsp), %rcx        # 4-byte Folded Reload
	vmovups	24600(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2784(%rsp)       # 16-byte Spill
	vmovups	24616(%rdi,%rcx,4), %xmm12
	vmovaps	%xmm12, 3776(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm12[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	5248(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm15, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	movslq	3296(%rsp), %r8         # 4-byte Folded Reload
	vmovups	24608(%rdi,%r8,4), %xmm13
	vmovups	24624(%rdi,%r8,4), %xmm15
	vshufps	$221, %xmm15, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	4128(%rsp), %xmm9       # 16-byte Reload
	vmovaps	5152(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm9, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	movslq	3312(%rsp), %rbx        # 4-byte Folded Reload
	vmovups	24608(%rdi,%rbx,4), %xmm8
	vmovups	24624(%rdi,%rbx,4), %xmm3
	vshufps	$221, %xmm3, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	3840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm5
	vmulps	%xmm2, %xmm5, %xmm5
	vmovups	24608(%rdi,%rcx,4), %xmm6
	vmovups	24624(%rdi,%rcx,4), %xmm0
	vshufps	$221, %xmm0, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm0[1,3]
	vshufps	$136, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm0[0,2]
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm1, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2640(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm9, %xmm10, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2624(%rsp)       # 16-byte Spill
	vmovups	24632(%rdi,%rcx,4), %xmm0
	vmovaps	%xmm0, 2832(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm11, %xmm14, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 2608(%rsp)       # 16-byte Spill
	movq	3616(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3680(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3584(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	3648(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 3392(%rsp)       # 16-byte Spill
	movq	3488(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movq	3552(%rsp), %rax        # 8-byte Reload
	vinsertps	$16, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	3536(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 2800(%rsp)       # 16-byte Spill
	movslq	%r12d, %rax
	vbroadcastss	.LCPI147_17(%rip), %xmm4
	vmovaps	5216(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm11
	vmovaps	2768(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 3616(%rsp)       # 16-byte Spill
	vmovaps	3424(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm15
	vmovaps	3376(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm6
	vmovaps	3360(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm9
	vmovaps	3344(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm3
	vmovaps	3328(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm1, 3648(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovaps	3296(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm14
	vsubps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, 3584(%rsp)       # 16-byte Spill
	movslq	%r11d, %rdx
	vminps	%xmm4, %xmm5, %xmm12
	cmpl	$0, 104(%rbp)
	vmovups	(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 2656(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r9,4), %xmm2
	vmovaps	%xmm2, 3488(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r9,4), %xmm5
	vmovaps	%xmm5, 2768(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3552(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 5184(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%r10,4), %xmm1
	vmovaps	%xmm1, 3424(%rsp)       # 16-byte Spill
	vmovups	8(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3360(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3344(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3296(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 5216(%rsp)       # 16-byte Spill
	vmovups	32(%rsi,%rax,4), %xmm1
	vmovaps	%xmm1, 3680(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm2, %xmm10 # xmm10 = xmm2[0,2],xmm5[0,2]
	vmovups	8(%rsi,%rcx,4), %xmm2
	vmovups	24(%rsi,%rcx,4), %xmm8
	vmovups	8(%rsi,%rdx,4), %xmm1
	vmovups	24(%rsi,%rdx,4), %xmm13
	je	.LBB147_1427
# BB#1426:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	2480(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm5, 3200(%rsp)       # 16-byte Spill
.LBB147_1427:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vsubps	3456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3536(%rsp)       # 16-byte Spill
	vsubps	2688(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3456(%rsp)       # 16-byte Spill
	vsubps	2704(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3376(%rsp)       # 16-byte Spill
	vsubps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm0, 3312(%rsp)       # 16-byte Spill
	vsubps	3408(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3328(%rsp)       # 16-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	3616(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm5, %xmm0, %xmm11
	vmovaps	2720(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 2704(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm3, %xmm0
	vmovaps	2672(%rsp), %xmm3       # 16-byte Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 2688(%rsp)       # 16-byte Spill
	vmovaps	2640(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2640(%rsp)       # 16-byte Spill
	vmovaps	2624(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2624(%rsp)       # 16-byte Spill
	vmovaps	2608(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm3, 2608(%rsp)       # 16-byte Spill
	vmovaps	3648(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm5, %xmm3, %xmm10
	vmaxps	%xmm5, %xmm14, %xmm7
	vmovaps	5152(%rsp), %xmm3       # 16-byte Reload
	vmulps	5248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 5152(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm3       # 16-byte Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 3584(%rsp)       # 16-byte Spill
	vmaxps	%xmm5, %xmm12, %xmm14
	vmovaps	5184(%rsp), %xmm3       # 16-byte Reload
	vmovaps	3552(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm5, %xmm9 # xmm9 = xmm5[1,3],xmm3[1,3]
	vshufps	$136, 3424(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm3[0,2],mem[0,2]
	vmovaps	3296(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 5216(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vshufps	$221, %xmm8, %xmm2, %xmm15 # xmm15 = xmm2[1,3],xmm8[1,3]
	vshufps	$221, %xmm13, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm13[1,3]
	je	.LBB147_1429
# BB#1428:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	2496(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm6, 3184(%rsp)       # 16-byte Spill
.LBB147_1429:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vsubps	%xmm9, %xmm11, %xmm6
	vmovaps	%xmm6, 3408(%rsp)       # 16-byte Spill
	vsubps	%xmm12, %xmm0, %xmm0
	vmovaps	%xmm0, 2720(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm11 # xmm11 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm8, %xmm2, %xmm13 # xmm13 = xmm2[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm10, %xmm0
	vmovaps	%xmm0, 3648(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm7, %xmm0
	vmovaps	%xmm0, 3616(%rsp)       # 16-byte Spill
	vmovaps	3584(%rsp), %xmm0       # 16-byte Reload
	vmulps	5152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 2672(%rsp)       # 16-byte Spill
	vsubps	%xmm3, %xmm14, %xmm0
	vmovaps	%xmm0, 3584(%rsp)       # 16-byte Spill
	vmovaps	3264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 3744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	5664(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm0
	vmovaps	5696(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	3392(%rsp), %xmm3       # 16-byte Reload
	vmulps	4192(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	3552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 5184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	2736(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	4160(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	3488(%rsp), %xmm3       # 16-byte Reload
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	3376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	3312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm1
	vshufps	$221, %xmm3, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm3[1,3]
	vmovaps	2704(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	2752(%rsp), %xmm0       # 16-byte Reload
	vmulps	5248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	2688(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	2640(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm12
	vmovaps	2624(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm8
	vmovaps	2608(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm7, %xmm6, %xmm10
	vaddps	3328(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	3456(%rsp), %xmm1       # 16-byte Reload
	vaddps	3536(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	3360(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, 3344(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[1,3],mem[1,3]
	vmovaps	%xmm7, 3552(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI147_24(%rip), %xmm7
	vmovaps	%xmm7, 5152(%rsp)       # 16-byte Spill
	vmovdqa	3280(%rsp), %xmm7       # 16-byte Reload
	je	.LBB147_1431
# BB#1430:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovdqa	2512(%rsp), %xmm7       # 16-byte Reload
.LBB147_1431:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 3264(%rsp)       # 16-byte Spill
	vmovaps	3360(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 3344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	%xmm2, 3280(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 2656(%rsp)       # 16-byte Spill
	vsubps	%xmm11, %xmm12, %xmm12
	vsubps	%xmm13, %xmm8, %xmm0
	vmovaps	%xmm0, 3344(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm0, 3360(%rsp)       # 16-byte Spill
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovups	16(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 2752(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vmovaps	3008(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%rbx,4), %xmm3
	vmovups	24616(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 2736(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm15, %xmm11
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmulps	4128(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovups	24600(%rdi,%r8,4), %xmm3
	vmovups	24616(%rdi,%r8,4), %xmm5
	vmovaps	%xmm5, 2704(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vsubps	%xmm9, %xmm3, %xmm3
	vmovaps	%xmm9, %xmm10
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmovups	(%rsi,%rcx,4), %xmm3
	vmovups	16(%rsi,%rcx,4), %xmm5
	vmovaps	%xmm5, 2688(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm5[1,3]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	3648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm2
	vaddps	3408(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vmovaps	2720(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm6, %xmm3, %xmm9
	vpslld	$31, %xmm7, %xmm6
	vmovaps	2672(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm4, %xmm1, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vsubps	3552(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 3008(%rsp)       # 16-byte Spill
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm2
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	5152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	3120(%rsp), %xmm0       # 16-byte Reload
	je	.LBB147_1433
# BB#1432:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovdqa	2528(%rsp), %xmm0       # 16-byte Reload
.LBB147_1433:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	3392(%rsp), %xmm1       # 16-byte Reload
	vmulps	5248(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmovaps	2784(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 3776(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vsubps	%xmm10, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vpslld	$31, %xmm0, %xmm7
	vmovaps	3296(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 5216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	2656(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	3280(%rsp), %xmm5, %xmm13 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm13, %xmm5
	vmovaps	%xmm12, 3392(%rsp)      # 16-byte Spill
	vaddps	3344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3360(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm0
	vbroadcastss	.LCPI147_23(%rip), %xmm8
	vmulps	%xmm8, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm2, %xmm0, %xmm2
	vaddps	3264(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vbroadcastss	.LCPI147_19(%rip), %xmm12
	vmovdqa	3184(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmulps	5152(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	je	.LBB147_1435
# BB#1434:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovdqa	2560(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	%xmm0, 3232(%rsp)       # 16-byte Spill
.LBB147_1435:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovdqa	3200(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmulps	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm6, %xmm5, %xmm2, %xmm2
	vaddps	3312(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vaddps	3328(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
	je	.LBB147_1437
# BB#1436:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	2544(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm3, 3216(%rsp)       # 16-byte Spill
.LBB147_1437:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm9
	vaddps	3376(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vmovaps	5184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 3424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	3744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2816(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	2800(%rsp), %xmm15      # 16-byte Reload
	vmulps	4192(%rsp), %xmm15, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	3488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2768(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[1,3],mem[1,3]
	vmovaps	3712(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 2848(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[1,3],mem[1,3]
	vmulps	4160(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm3, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vmovaps	3264(%rsp), %xmm3       # 16-byte Reload
	vaddps	3456(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm6, %xmm5
	vaddps	3408(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	3536(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm0, %xmm6
	je	.LBB147_1439
# BB#1438:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	2576(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, 3248(%rsp)       # 16-byte Spill
.LBB147_1439:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vaddps	3280(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vmulps	%xmm12, %xmm14, %xmm14
	vmovaps	2736(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmovaps	3104(%rsp), %xmm5       # 16-byte Reload
	vmulps	3840(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	2752(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	4128(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	2704(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%rdi,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	2688(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,2],mem[0,2]
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	3344(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
	vaddps	3392(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	3360(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	5152(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm11
	vmulps	%xmm1, %xmm0, %xmm7
	vmovdqa	3232(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm10
	vmovdqa	3216(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm6
	vmovdqa	3248(%rsp), %xmm0       # 16-byte Reload
	vpslld	$31, %xmm0, %xmm0
	vmovdqa	3136(%rsp), %xmm5       # 16-byte Reload
	je	.LBB147_1441
# BB#1440:                              # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovdqa	2592(%rsp), %xmm5       # 16-byte Reload
.LBB147_1441:                           # %for f8.s0.v10.v10512
                                        #   in Loop: Header=BB147_1409 Depth=4
	vmovaps	5216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 3680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	3776(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 2832(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[1,3],mem[1,3]
	vmulps	5248(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vsubps	5664(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	5696(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	3008(%rsp), %xmm2       # 16-byte Reload
	vaddps	3584(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	3616(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	3648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm8, %xmm1, %xmm1
	vpslld	$31, %xmm5, %xmm2
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vblendvps	%xmm0, %xmm7, %xmm1, %xmm0
	vblendvps	%xmm6, %xmm11, %xmm0, %xmm0
	vblendvps	%xmm10, %xmm14, %xmm0, %xmm0
	vaddps	3552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	.LCPI147_14(%rip), %ymm1 # ymm1 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm1, %ymm0
	vmovaps	.LCPI147_15(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	3808(%rsp), %rax        # 4-byte Folded Reload
	movq	2400(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	movl	3152(%rsp), %eax        # 4-byte Reload
	addl	$-1, %eax
	jne	.LBB147_1409
# BB#1442:                              #   in Loop: Header=BB147_1406 Depth=3
	vmovdqa	5424(%rsp), %xmm12      # 16-byte Reload
	movl	2256(%rsp), %ecx        # 4-byte Reload
.LBB147_1443:                           # %end for f8.s0.v10.v10513
                                        #   in Loop: Header=BB147_1406 Depth=3
	movl	%ecx, %eax
	movq	%rax, 2368(%rsp)        # 8-byte Spill
	cmpl	1372(%rsp), %ecx        # 4-byte Folded Reload
	movl	1736(%rsp), %eax        # 4-byte Reload
	jne	.LBB147_1406
.LBB147_1444:                           # %consume f8516
                                        #   in Loop: Header=BB147_466 Depth=2
	vmovdqa	%xmm12, 5424(%rsp)      # 16-byte Spill
	movq	824(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_1467
# BB#1445:                              # %for f0.s0.v10.v10517.preheader
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	1000(%rsp), %rax        # 8-byte Reload
	cltq
	movq	840(%rsp), %rcx         # 8-byte Reload
	movq	792(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	8(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3840(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3808(%rsp)       # 32-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3776(%rsp)       # 32-byte Spill
	movq	832(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	8(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3744(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3712(%rsp)       # 32-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3680(%rsp)       # 32-byte Spill
	movq	784(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3648(%rsp)       # 32-byte Spill
	movq	776(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 3616(%rsp)       # 32-byte Spill
	movq	952(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %edi
	andl	$63, %edi
	movq	%rdi, %rbx
	movq	1728(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rbx
	movq	%rbx, 3536(%rsp)        # 8-byte Spill
	leaq	1(%rsi), %r12
	andl	$63, %r12d
	movq	%r12, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, 3488(%rsp)        # 8-byte Spill
	movq	800(%rsp), %rdx         # 8-byte Reload
	vpbroadcastd	(%rcx,%rdx,4), %ymm0
	vmovdqa	%ymm0, 3584(%rsp)       # 32-byte Spill
	negq	%rax
	leaq	1(%rsi,%rax), %r14
	shlq	$5, %r14
	leaq	8(%r14), %rax
	movq	%rax, 3456(%rsp)        # 8-byte Spill
	leaq	16(%r14), %rax
	movq	%rax, 3424(%rsp)        # 8-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rdi
	movq	%rdi, 3552(%rsp)        # 8-byte Spill
	leaq	24(%r14), %rcx
	movq	%rcx, 3408(%rsp)        # 8-byte Spill
	imulq	%rax, %r12
	movq	5288(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r13d
	xorl	%r11d, %r11d
	.align	16, 0x90
.LBB147_1446:                           # %for f0.s0.v10.v10517
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1447 Depth 4
                                        #           Child Loop BB147_1448 Depth 5
	movl	%r11d, %eax
	shll	$5, %eax
	movq	5288(%rsp), %rcx        # 8-byte Reload
	addl	%ecx, %eax
	cltq
	movq	1160(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rdx
	movq	3552(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdx,%rdi), %rsi
	movq	5032(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rcx,%rsi,4), %ymm0
	leaq	(%rdx,%r12), %rsi
	vmovups	(%rcx,%rsi,4), %ymm1
	leaq	8(%rdi,%rdx), %rsi
	vmovdqu	(%rcx,%rsi,4), %ymm2
	leaq	8(%r12,%rdx), %rsi
	vmovdqu	(%rcx,%rsi,4), %ymm3
	leaq	16(%rdi,%rdx), %rsi
	vmovdqu	(%rcx,%rsi,4), %ymm4
	leaq	16(%r12,%rdx), %rsi
	vmovups	(%rcx,%rsi,4), %ymm5
	leaq	24(%rdi,%rdx), %rsi
	vmovups	(%rcx,%rsi,4), %ymm6
	subq	4712(%rsp), %rax        # 8-byte Folded Reload
	leaq	24(%r12,%rdx), %rdx
	vmovdqu	(%rcx,%rdx,4), %ymm7
	movq	3536(%rsp), %rdi        # 8-byte Reload
	leaq	(%rax,%rdi), %rdx
	movq	4656(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm8
	vmovaps	%ymm8, 5728(%rsp)
	movq	4648(%rsp), %rsi        # 8-byte Reload
	vmovdqu	(%rsi,%rdx,4), %ymm8
	movq	3488(%rsp), %rbx        # 8-byte Reload
	leaq	(%rax,%rbx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm9
	vmovaps	%ymm9, 5728(%rsp,%r14,4)
	vmovups	(%rsi,%rdx,4), %ymm9
	leaq	8(%rdi,%rax), %rdx
	vmovups	(%rcx,%rdx,4), %ymm10
	vmovaps	%ymm10, 5760(%rsp)
	vmovups	(%rsi,%rdx,4), %ymm10
	leaq	8(%rax,%rbx), %r8
	vmovups	(%rcx,%r8,4), %ymm11
	movq	3456(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm11, 5728(%rsp,%rdx,4)
	vmovups	(%rsi,%r8,4), %ymm11
	leaq	16(%rax,%rdi), %rdx
	vmovups	(%rcx,%rdx,4), %ymm12
	vmovaps	%ymm12, 5792(%rsp)
	vmovdqu	(%rsi,%rdx,4), %ymm12
	leaq	16(%rax,%rbx), %r8
	vmovups	(%rcx,%r8,4), %ymm13
	movq	3424(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm13, 5728(%rsp,%rdx,4)
	vmovdqu	(%rsi,%r8,4), %ymm13
	leaq	24(%rax,%rdi), %rdx
	vmovups	(%rcx,%rdx,4), %ymm14
	vmovaps	%ymm14, 5824(%rsp)
	leaq	24(%rax,%rbx), %rax
	vmovups	(%rcx,%rax,4), %ymm14
	movq	3408(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm14, 5728(%rsp,%rcx,4)
	vmovaps	%ymm0, 5984(%rsp)
	vmovaps	%ymm1, 5984(%rsp,%r14,4)
	vmovdqa	%ymm2, 6016(%rsp)
	vmovdqa	%ymm3, 6016(%rsp,%r14,4)
	vmovdqa	%ymm4, 6048(%rsp)
	vmovaps	%ymm5, 6048(%rsp,%r14,4)
	vmovaps	%ymm6, 6080(%rsp)
	vmovdqa	%ymm7, 6080(%rsp,%r14,4)
	vmovdqa	%ymm8, 6240(%rsp)
	vmovaps	%ymm9, 6240(%rsp,%r14,4)
	vmovaps	%ymm10, 6272(%rsp)
	vmovaps	%ymm11, 6272(%rsp,%r14,4)
	vmovdqa	%ymm12, 6304(%rsp)
	vmovdqa	%ymm13, 6304(%rsp,%r14,4)
	vmovups	(%rsi,%rdx,4), %ymm0
	movslq	%r13d, %rdx
	movq	1192(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdx,%rcx), %r8
	vmovaps	%ymm0, 6336(%rsp)
	vmovdqu	(%rsi,%rax,4), %ymm0
	vmovdqa	%ymm0, 6336(%rsp,%r14,4)
	xorl	%eax, %eax
	.align	16, 0x90
.LBB147_1447:                           # %for f0.s0.v11.v13.yii541
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1446 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB147_1448 Depth 5
	movq	%rax, %rdx
	shlq	$7, %rdx
	vmovaps	5728(%rsp,%rdx), %ymm14
	vmovaps	%ymm14, 4128(%rsp)      # 32-byte Spill
	vmovaps	5760(%rsp,%rdx), %ymm1
	vmovaps	%ymm1, 4160(%rsp)       # 32-byte Spill
	vmovaps	5792(%rsp,%rdx), %ymm0
	vmovaps	%ymm0, 4192(%rsp)       # 32-byte Spill
	vmovaps	5824(%rsp,%rdx), %ymm8
	vmovaps	5984(%rsp,%rdx), %ymm3
	vmovaps	6016(%rsp,%rdx), %ymm7
	vmovaps	6048(%rsp,%rdx), %ymm12
	vmovaps	6080(%rsp,%rdx), %ymm4
	vmovaps	6240(%rsp,%rdx), %ymm9
	vmovaps	6272(%rsp,%rdx), %ymm10
	vmovaps	6304(%rsp,%rdx), %ymm11
	vmovaps	3776(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm0, %ymm0
	vmulps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm5, %ymm14, %ymm2
	vmulps	%ymm5, %ymm8, %ymm5
	vmovaps	3808(%rsp), %ymm13      # 32-byte Reload
	vmovaps	%ymm13, %ymm6
	vfmadd213ps	%ymm5, %ymm4, %ymm6
	vmovaps	%ymm13, %ymm5
	vfmadd213ps	%ymm2, %ymm3, %ymm5
	vmovaps	%ymm13, %ymm2
	vfmadd213ps	%ymm1, %ymm7, %ymm2
	vmovaps	%ymm13, %ymm1
	vfmadd213ps	%ymm0, %ymm12, %ymm1
	vmovaps	3840(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm13
	vfmadd213ps	%ymm1, %ymm11, %ymm13
	vmovaps	%ymm13, 5248(%rsp)      # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm2, %ymm10, %ymm1
	vmovaps	%ymm1, 5216(%rsp)       # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm5, %ymm9, %ymm1
	vmovaps	%ymm1, 5184(%rsp)       # 32-byte Spill
	vmovaps	6336(%rsp,%rdx), %ymm15
	vfmadd213ps	%ymm6, %ymm15, %ymm0
	vmovaps	%ymm0, 5152(%rsp)       # 32-byte Spill
	vmovaps	3680(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm8, %ymm0
	vmovaps	3712(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm13
	vfmadd213ps	%ymm0, %ymm4, %ymm13
	vmulps	%ymm5, %ymm14, %ymm0
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm0, %ymm3, %ymm6
	vmulps	4160(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm0, %ymm7, %ymm1
	vmulps	4192(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm5
	vfmadd213ps	%ymm0, %ymm12, %ymm5
	vmovaps	3744(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm14
	vfmadd213ps	%ymm5, %ymm11, %ymm14
	vmovaps	%ymm0, %ymm5
	vfmadd213ps	%ymm1, %ymm10, %ymm5
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm6, %ymm9, %ymm1
	vmovaps	%ymm0, %ymm6
	vfmadd213ps	%ymm13, %ymm15, %ymm6
	vmovaps	3584(%rsp), %ymm2       # 32-byte Reload
	vmulps	%ymm2, %ymm8, %ymm8
	vmovaps	3616(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm4
	vmulps	4128(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm3
	vmulps	4160(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm7
	vmulps	4192(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm12
	vmovaps	3648(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm0, %ymm11
	vfmadd213ps	%ymm7, %ymm0, %ymm10
	vfmadd213ps	%ymm3, %ymm0, %ymm9
	vfmadd213ps	%ymm4, %ymm0, %ymm15
	xorl	%edx, %edx
	movl	$3, %ebx
	movq	%r8, %rsi
	.align	16, 0x90
.LBB147_1448:                           # %for f0.s0.v12544
                                        #   Parent Loop BB147_195 Depth=1
                                        #     Parent Loop BB147_466 Depth=2
                                        #       Parent Loop BB147_1446 Depth=3
                                        #         Parent Loop BB147_1447 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovaps	%ymm1, %ymm3
	cmpl	$1, %edx
	je	.LBB147_1450
# BB#1449:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	5184(%rsp), %ymm3       # 32-byte Reload
.LBB147_1450:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm5, %ymm4
	je	.LBB147_1452
# BB#1451:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	5216(%rsp), %ymm4       # 32-byte Reload
.LBB147_1452:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm14, %ymm8
	je	.LBB147_1454
# BB#1453:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	5248(%rsp), %ymm8       # 32-byte Reload
.LBB147_1454:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm6, %ymm12
	je	.LBB147_1456
# BB#1455:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	5152(%rsp), %ymm12      # 32-byte Reload
.LBB147_1456:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm15, %ymm7
	testl	%edx, %edx
	je	.LBB147_1458
# BB#1457:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm12, %ymm7
.LBB147_1458:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm11, %ymm12
	je	.LBB147_1460
# BB#1459:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm8, %ymm12
.LBB147_1460:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm10, %ymm8
	je	.LBB147_1462
# BB#1461:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm4, %ymm8
.LBB147_1462:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm9, %ymm4
	je	.LBB147_1464
# BB#1463:                              # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vmovaps	%ymm3, %ymm4
.LBB147_1464:                           # %for f0.s0.v12544
                                        #   in Loop: Header=BB147_1448 Depth=5
	vbroadcastss	.LCPI147_25(%rip), %ymm3
	vminps	%ymm3, %ymm4, %ymm4
	vminps	%ymm3, %ymm8, %ymm8
	vminps	%ymm3, %ymm12, %ymm12
	vminps	%ymm3, %ymm7, %ymm3
	vxorps	%ymm0, %ymm0, %ymm0
	vmaxps	%ymm0, %ymm4, %ymm4
	vmaxps	%ymm0, %ymm8, %ymm7
	vmaxps	%ymm0, %ymm12, %ymm8
	vmaxps	%ymm0, %ymm3, %ymm3
	vcvttps2dq	%ymm4, %ymm4
	vmovdqa	.LCPI147_7(%rip), %ymm0 # ymm0 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm0, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vcvttps2dq	%ymm7, %ymm7
	vpshufb	%ymm0, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vcvttps2dq	%ymm8, %ymm8
	vpshufb	%ymm0, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vcvttps2dq	%ymm3, %ymm3
	vpshufb	%ymm0, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm8, %ymm8    # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero,xmm8[4],zero,xmm8[5],zero,xmm8[6],zero,xmm8[7],zero
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vmovdqa	5536(%rsp), %ymm0       # 32-byte Reload
	vpmulld	%ymm0, %ymm4, %ymm12
	vpmulld	%ymm0, %ymm7, %ymm7
	vpmulld	%ymm0, %ymm8, %ymm8
	vpmulld	%ymm0, %ymm3, %ymm3
	vmovd	%edx, %xmm4
	vpsubd	5472(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpbroadcastd	%xmm4, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm4
	vpaddd	%ymm8, %ymm13, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	%ymm12, %ymm13, %ymm3
	vmovq	%xmm8, %r9
	movslq	%r9d, %r10
	movq	5568(%rsp), %r15        # 8-byte Reload
	movzbl	(%r15,%r10), %ecx
	vmovd	%ecx, %xmm2
	vpextrq	$1, %xmm8, %rcx
	sarq	$32, %r9
	vpinsrb	$1, (%r15,%r9), %xmm2, %xmm2
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vextracti128	$1, %ymm8, %xmm0
	vpinsrb	$2, (%r15,%rdi), %xmm2, %xmm2
	vmovq	%xmm0, %rdi
	vpinsrb	$3, (%r15,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$4, (%r15,%rcx), %xmm2, %xmm2
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rdi
	vpinsrb	$5, (%r15,%rdi), %xmm2, %xmm0
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vpinsrb	$6, (%r15,%rdi), %xmm0, %xmm0
	vmovq	%xmm4, %rdi
	vpinsrb	$7, (%r15,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$8, (%r15,%rcx), %xmm0, %xmm0
	vpextrq	$1, %xmm4, %rcx
	sarq	$32, %rdi
	vpinsrb	$9, (%r15,%rdi), %xmm0, %xmm0
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vextracti128	$1, %ymm4, %xmm2
	vpinsrb	$10, (%r15,%rdi), %xmm0, %xmm0
	vmovq	%xmm2, %rdi
	vpinsrb	$11, (%r15,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	vpinsrb	$12, (%r15,%rcx), %xmm0, %xmm0
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rdi
	vpinsrb	$13, (%r15,%rdi), %xmm0, %xmm0
	movslq	%ecx, %rdi
	vpinsrb	$14, (%r15,%rdi), %xmm0, %xmm0
	vmovq	%xmm3, %rdi
	sarq	$32, %rcx
	vpinsrb	$15, (%r15,%rcx), %xmm0, %xmm0
	movslq	%edi, %rcx
	movzbl	(%r15,%rcx), %ecx
	vmovd	%ecx, %xmm2
	vpextrq	$1, %xmm3, %rcx
	sarq	$32, %rdi
	vpinsrb	$1, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vextracti128	$1, %ymm3, %xmm3
	vpinsrb	$2, (%r15,%rdi), %xmm2, %xmm2
	vmovq	%xmm3, %rdi
	vpinsrb	$3, (%r15,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$4, (%r15,%rcx), %xmm2, %xmm2
	vpextrq	$1, %xmm3, %rcx
	sarq	$32, %rdi
	vpinsrb	$5, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vpinsrb	$6, (%r15,%rdi), %xmm2, %xmm2
	vmovq	%xmm7, %rdi
	vpinsrb	$7, (%r15,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$8, (%r15,%rcx), %xmm2, %xmm2
	vpextrq	$1, %xmm7, %rcx
	sarq	$32, %rdi
	vpinsrb	$9, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vextracti128	$1, %ymm7, %xmm3
	vpinsrb	$10, (%r15,%rdi), %xmm2, %xmm2
	vmovq	%xmm3, %rdi
	vpinsrb	$11, (%r15,%rcx), %xmm2, %xmm2
	movslq	%edi, %rcx
	vpinsrb	$12, (%r15,%rcx), %xmm2, %xmm2
	vpextrq	$1, %xmm3, %rcx
	sarq	$32, %rdi
	vpinsrb	$13, (%r15,%rdi), %xmm2, %xmm2
	movslq	%ecx, %rdi
	vpinsrb	$14, (%r15,%rdi), %xmm2, %xmm2
	sarq	$32, %rcx
	vpinsrb	$15, (%r15,%rcx), %xmm2, %xmm2
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vmovdqu	%ymm0, (%rsi)
	addq	5528(%rsp), %rsi        # 8-byte Folded Reload
	addl	$1, %edx
	addq	$-1, %rbx
	jne	.LBB147_1448
# BB#1465:                              # %end for f0.s0.v12545
                                        #   in Loop: Header=BB147_1447 Depth=4
	addq	$1, %rax
	addq	2144(%rsp), %r8         # 8-byte Folded Reload
	cmpq	$2, %rax
	jne	.LBB147_1447
# BB#1466:                              # %end for f0.s0.v11.v13.yii542
                                        #   in Loop: Header=BB147_1446 Depth=3
	addq	$1, %r11
	addl	$32, %r13d
	cmpl	1156(%rsp), %r11d       # 4-byte Folded Reload
	jne	.LBB147_1446
.LBB147_1467:                           # %end for f0.s0.v10.v10518
                                        #   in Loop: Header=BB147_466 Depth=2
	movq	920(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rsi
	addq	$1, %rsi
	movq	%rsi, 920(%rsp)         # 8-byte Spill
	movq	1216(%rsp), %r12        # 8-byte Reload
	addl	$1, %r12d
	movl	1228(%rsp), %r10d       # 4-byte Reload
	addl	$2, %r10d
	movl	1208(%rsp), %r15d       # 4-byte Reload
	addl	$-2, %r15d
	movl	1212(%rsp), %r14d       # 4-byte Reload
	addl	$-2, %r14d
	movl	984(%rsp), %edi         # 4-byte Reload
	addl	$2, %edi
	movl	996(%rsp), %r9d         # 4-byte Reload
	addl	$2, %r9d
	movl	980(%rsp), %r11d        # 4-byte Reload
	addl	$-2, %r11d
	movl	988(%rsp), %r13d        # 4-byte Reload
	addl	$2, %r13d
	movl	992(%rsp), %r8d         # 4-byte Reload
	addl	$-2, %r8d
	addq	$2, 952(%rsp)           # 8-byte Folded Spill
	movl	772(%rsp), %eax         # 4-byte Reload
	addl	%eax, 928(%rsp)         # 4-byte Folded Spill
	addl	%eax, 932(%rsp)         # 4-byte Folded Spill
	addl	%eax, 936(%rsp)         # 4-byte Folded Spill
	movl	964(%rsp), %ecx         # 4-byte Reload
	addl	$2, %ecx
	addl	%eax, 940(%rsp)         # 4-byte Folded Spill
	addl	%eax, 944(%rsp)         # 4-byte Folded Spill
	addl	%eax, 948(%rsp)         # 4-byte Folded Spill
	movl	968(%rsp), %eax         # 4-byte Reload
	addl	$2, %eax
	addl	$2, 960(%rsp)           # 4-byte Folded Spill
	movq	1192(%rsp), %rdx        # 8-byte Reload
	addq	672(%rsp), %rdx         # 8-byte Folded Reload
	movq	%rdx, 1192(%rsp)        # 8-byte Spill
	cmpq	$16, %rsi
	movq	5608(%rsp), %rsi        # 8-byte Reload
	jne	.LBB147_466
# BB#1468:                              # %call_destructor.exit1064
                                        #   in Loop: Header=BB147_195 Depth=1
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4816(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4696(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4704(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4664(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	5032(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4656(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	4648(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	movq	648(%rsp), %rcx         # 8-byte Reload
	leal	1(%rcx), %eax
	movl	644(%rsp), %edx         # 4-byte Reload
	addl	$-32, %edx
	cmpl	616(%rsp), %ecx         # 4-byte Folded Reload
	jne	.LBB147_195
.LBB147_1469:                           # %consume f0
	movq	408(%rsp), %rax         # 8-byte Reload
	leal	-8(%rax), %r15d
	movl	%r15d, 4752(%rsp)       # 4-byte Spill
	movq	728(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %r15d
	cmovgl	%eax, %r15d
	movq	440(%rsp), %rcx         # 8-byte Reload
	orl	$7, %ecx
	movq	%rcx, 440(%rsp)         # 8-byte Spill
	leal	(%rcx,%rax), %ecx
	movl	%ecx, 4512(%rsp)        # 4-byte Spill
	movl	404(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	movl	%eax, %r14d
	cmovgl	%ecx, %r14d
	movq	720(%rsp), %rax         # 8-byte Reload
	cmpl	$7, %eax
	movl	$8, %r12d
	cmovgl	%eax, %r12d
	movq	%rax, %r13
	movl	%r14d, %edi
	subl	%r15d, %edi
	addl	$1, %edi
	movl	%r12d, %eax
	leaq	(,%rdi,4), %rdx
	movl	%edx, %ecx
	imulq	%rax, %rdx
	leaq	(%rdx,%rdx,2), %rbx
	movl	%edx, %edx
	cmpq	$2147483647, %rbx       # imm = 0x7FFFFFFF
	ja	.LBB147_1471
# BB#1470:                              # %consume f0
	imulq	%rax, %rcx
	movq	%rdi, %rsi
	shrq	$30, %rsi
	imulq	%rax, %rsi
	shrq	$32, %rcx
	addq	%rcx, %rsi
	leaq	(%rdx,%rdx,2), %rax
	shrq	$32, %rax
	leaq	(%rsi,%rsi,2), %rcx
	addq	%rax, %rcx
	orq	%rsi, %rcx
	shrq	$32, %rcx
	jne	.LBB147_1471
# BB#1474:                              # %assert succeeded548
	movq	%rdi, 5696(%rsp)        # 8-byte Spill
	leaq	4(%rbx), %rsi
	xorl	%eax, %eax
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %rdx
	addq	$4, %rbx
	je	.LBB147_1476
# BB#1475:                              # %assert succeeded548
	testq	%rdx, %rdx
	je	.LBB147_198
.LBB147_1476:                           # %for transpose.s0.v12.preheader
	movl	%r14d, 5664(%rsp)       # 4-byte Spill
	vmovss	72(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	movq	1008(%rsp), %rdi        # 8-byte Reload
	movq	368(%rsp), %rax         # 8-byte Reload
	imull	%edi, %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	movq	5696(%rsp), %rax        # 8-byte Reload
	imull	%r12d, %eax
	movq	%rax, 5696(%rsp)        # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, 4552(%rsp)        # 8-byte Spill
	leaq	-1(%rax), %rsi
	movq	%rsi, 4448(%rsp)        # 8-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm0, %xmm1
	vmovd	%xmm0, %ecx
	movl	$1065353216, %eax       # imm = 0x3F800000
	cmovbl	%ecx, %eax
	movl	$0, %r9d
	testq	%rsi, %rsi
	cmovnsq	%rsi, %r9
	vucomiss	%xmm0, %xmm1
	seta	%cl
	movzbl	%cl, %ecx
	movq	448(%rsp), %r8          # 8-byte Reload
	jae	.LBB147_1477
# BB#1478:                              # %for transpose.s0.v12.preheader
	movl	%r12d, 4560(%rsp)       # 4-byte Spill
	movq	%rdx, 5408(%rsp)        # 8-byte Spill
	movl	%eax, %ecx
	andl	$-2139095041, %ecx      # imm = 0xFFFFFFFF807FFFFF
	movl	%ecx, %edx
	sarl	$22, %edx
	movl	$127, %esi
	subl	%edx, %esi
	sarl	$23, %eax
	subl	%esi, %eax
	shll	$23, %esi
	orl	%ecx, %esi
	vmovd	%esi, %xmm0
	vaddss	.LCPI147_28(%rip), %xmm0, %xmm0
	vmulss	%xmm0, %xmm0, %xmm1
	vcvtsi2ssl	%eax, %xmm0, %xmm2
	vmovss	.LCPI147_29(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_30(%rip), %xmm1, %xmm3
	vfmadd213ss	.LCPI147_31(%rip), %xmm1, %xmm3
	vfmadd213ss	.LCPI147_32(%rip), %xmm1, %xmm3
	vmovss	.LCPI147_33(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_34(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_35(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_36(%rip), %xmm1, %xmm4
	vfmadd213ss	.LCPI147_17(%rip), %xmm1, %xmm4
	vmulss	%xmm4, %xmm0, %xmm0
	vfmadd213ss	%xmm0, %xmm1, %xmm3
	vmovss	.LCPI147_37(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm3, %xmm2, %xmm0
	jmp	.LBB147_1479
.LBB147_1477:
	movl	%r12d, 4560(%rsp)       # 4-byte Spill
	movq	%rdx, 5408(%rsp)        # 8-byte Spill
	leaq	.LCPI147_38(%rip), %rax
	vmovss	(%rax,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
.LBB147_1479:                           # %for transpose.s0.v12.preheader
	movq	816(%rsp), %rax         # 8-byte Reload
	sarl	$3, %eax
	movq	%rax, 816(%rsp)         # 8-byte Spill
	movl	%r15d, %r10d
	sarl	$31, %r10d
	andl	%r15d, %r10d
	movq	712(%rsp), %r12         # 8-byte Reload
	movq	408(%rsp), %rcx         # 8-byte Reload
	cmpl	%r12d, %ecx
	cmovll	%r12d, %ecx
	addl	$-1, %ecx
	leal	-2(%r12), %eax
	movl	%eax, 4960(%rsp)        # 4-byte Spill
	movl	388(%rsp), %edx         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	testl	%edx, %edx
	movl	$0, %eax
	cmovnsl	%edx, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	$1, %r14d
	subl	%r10d, %r14d
	addl	%eax, %r14d
	movslq	460(%rsp), %rcx         # 4-byte Folded Reload
	movslq	356(%rsp), %rdx         # 4-byte Folded Reload
	movq	%rcx, %rax
	sarq	$63, %rax
	andq	%rcx, %rax
	movq	%rdx, %rsi
	sarq	$63, %rsi
	andq	%rdx, %rsi
	movq	%rsi, 4600(%rsp)        # 8-byte Spill
	leaq	-7(%r9), %rcx
	movq	%rcx, %rdx
	sarq	$63, %rdx
	andq	%rcx, %rdx
	movq	%rdx, 4416(%rsp)        # 8-byte Spill
	movslq	%r15d, %rdx
	movq	%rdx, 4608(%rsp)        # 8-byte Spill
	movq	%rax, %rcx
	imulq	%rdi, %rcx
	shlq	$5, %r14
	movq	%r14, 4880(%rsp)        # 8-byte Spill
	addq	%rsi, %rcx
	movq	%rcx, 4592(%rsp)        # 8-byte Spill
	movq	%rdx, %rcx
	sarq	$63, %rcx
	andq	%rdx, %rcx
	leaq	(,%rcx,8), %rdx
	negq	%rdx
	movq	%rdx, 4872(%rsp)        # 8-byte Spill
	leal	7(%r8), %edx
	sarl	$3, %edx
	movl	%edx, 5424(%rsp)        # 4-byte Spill
	movl	%r12d, %edx
	subl	%r10d, %edx
	movl	$7, %esi
	subl	%r13d, %esi
	cmpl	$1, %r13d
	movl	$6, %ebx
	cmovgl	%esi, %ebx
	movl	%ebx, 4856(%rsp)        # 4-byte Spill
	movl	$1, %esi
	subq	%rax, %rsi
	movslq	%r12d, %rax
	movq	%rax, 5632(%rsp)        # 8-byte Spill
	subq	%rcx, %rax
	movl	$1, %ebx
	subq	%rcx, %rbx
	andl	$1, 916(%rsp)           # 4-byte Folded Spill
	imulq	%rdi, %rsi
	movq	%rsi, 4664(%rsp)        # 8-byte Spill
	shlq	$3, %rbx
	movq	%rbx, 4648(%rsp)        # 8-byte Spill
	addl	$-1, %edx
	movslq	%edx, %rcx
	shlq	$3, %rcx
	movq	%rcx, 4640(%rsp)        # 8-byte Spill
	shlq	$3, %rax
	movq	%rax, 4656(%rsp)        # 8-byte Spill
	testl	%r13d, %r13d
	movq	464(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %esi
	notl	%esi
	movl	%esi, 4864(%rsp)        # 4-byte Spill
	movl	$1, %r15d
	cmovgl	%r13d, %r15d
	movl	$7, %eax
	subl	%r15d, %eax
	movl	%eax, 4848(%rsp)        # 4-byte Spill
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	notl	%eax
	movslq	%eax, %r11
	notq	%r11
	cmpq	$-2, %r11
	movq	$-1, %rax
	cmovleq	%rax, %r11
	movq	824(%rsp), %rsi         # 8-byte Reload
	shll	$5, %esi
	movq	%rsi, 824(%rsp)         # 8-byte Spill
	movq	1136(%rsp), %rax        # 8-byte Reload
	leal	-1(%rdx,%rax), %eax
	movq	5288(%rsp), %rdx        # 8-byte Reload
	leal	31(%rdx,%rsi), %edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	376(%rsp), %eax         # 4-byte Reload
	notl	%eax
	movl	%eax, 376(%rsp)         # 4-byte Spill
	negl	%r15d
	cmpl	%eax, %r15d
	cmovll	%eax, %r15d
	notl	%r15d
	cmpl	%r15d, %edx
	cmovgel	%edx, %r15d
	leal	1(%r15), %ecx
	movq	%r9, 5616(%rsp)         # 8-byte Spill
	movl	364(%rsp), %r9d         # 4-byte Reload
	subl	%r9d, %ecx
	testl	%r12d, %r12d
	movl	$1, %eax
	cmovgl	%r12d, %eax
	addl	$-1, %eax
	movq	%r8, %rdi
	movq	728(%rsp), %r8          # 8-byte Reload
	leal	(%r8,%rdi), %esi
	cmpl	%esi, %r12d
	cmovgel	%r12d, %esi
	testl	%esi, %esi
	movl	$1, %edx
	cmovlel	%edx, %esi
	movl	$-32, %edx
	movq	632(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %edx
	movl	616(%rsp), %ebx         # 4-byte Reload
	shll	$5, %ebx
	subl	%ebx, %edx
	negl	%esi
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	%r8d, %ebx
	negl	%ebx
	subl	%edi, %ebx
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%r8,%rax), %esi
	notl	%esi
	movl	%edi, %r12d
	negl	%r12d
	subl	%r8d, %r12d
	cmpl	%esi, %r12d
	cmovgel	%r12d, %esi
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	addl	$1, %esi
	subl	704(%rsp), %esi         # 4-byte Folded Reload
	imull	%ecx, %esi
	subl	%r9d, %r15d
	movl	%r8d, %r9d
	notl	%r9d
	movl	%r9d, 4840(%rsp)        # 4-byte Spill
	movl	$31, %edx
	subl	%r8d, %edx
	subl	%edi, %edx
	cmpl	%r9d, %edx
	movl	360(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmovll	%r9d, %edx
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	notl	%edx
	movslq	%edx, %rax
	notq	%rax
	cmpq	$-2, %rax
	movq	$-1, %rcx
	cmovleq	%rcx, %rax
	movl	$7, %edx
	subl	%r8d, %edx
	subl	%edi, %edx
	movl	%edx, 5008(%rsp)        # 4-byte Spill
	cmpl	%r9d, %edx
	movl	%r9d, %edi
	cmovgel	%edx, %edi
	movl	%edi, %edx
	notl	%edx
	movslq	%edx, %r9
	movq	%r9, %rdx
	notq	%rdx
	cmpq	$-2, %rdx
	cmovleq	%rcx, %rdx
	movq	%rdx, 5032(%rsp)        # 8-byte Spill
	leaq	1(%rax), %rcx
	movq	%rcx, 4704(%rsp)        # 8-byte Spill
	addq	$2, %rax
	movq	%rax, 4712(%rsp)        # 8-byte Spill
	negl	%r10d
	movl	%r10d, 4720(%rsp)       # 4-byte Spill
	movq	5632(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	shlq	$5, %rax
	movq	%rax, 4688(%rsp)        # 8-byte Spill
	movq	%r9, %rax
	negq	%rax
	movq	%rax, 4912(%rsp)        # 8-byte Spill
	cmpl	$7, %r13d
	movl	$8, %edx
	cmovgl	%r13d, %edx
	addl	$1, %edi
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	(%r8,%rax), %eax
	notl	%eax
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	subl	%ebx, %edi
	imull	%edx, %edi
	movl	$6, %ecx
	subq	5616(%rsp), %rcx        # 8-byte Folded Reload
	notl	%ebx
	movslq	%ebx, %rdx
	addq	$1, %rdx
	subq	%r9, %rdx
	movq	%rdx, 4824(%rsp)        # 8-byte Spill
	cmpq	$-2, %rcx
	movq	$-1, %rdx
	cmovleq	%rdx, %rcx
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	notl	%eax
	movslq	%eax, %rdx
	addq	$1, %rdx
	subq	%r9, %rdx
	movq	368(%rsp), %rax         # 8-byte Reload
	cltq
	movq	%rax, 4616(%rsp)        # 8-byte Spill
	movq	5696(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rax, 4568(%rsp)        # 8-byte Spill
	vmulss	.LCPI147_19(%rip), %xmm0, %xmm1
	vmovss	%xmm1, 4384(%rsp)       # 4-byte Spill
	movslq	5664(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 4320(%rsp)        # 8-byte Spill
	vmulss	.LCPI147_39(%rip), %xmm1, %xmm0
	vmovss	%xmm0, 4352(%rsp)       # 4-byte Spill
	movq	1176(%rsp), %rax        # 8-byte Reload
	leaq	1(%r11,%rax), %rax
	movq	%rax, 4736(%rsp)        # 8-byte Spill
	movslq	%esi, %rax
	movq	%rax, 4584(%rsp)        # 8-byte Spill
	movslq	%r15d, %rax
	leaq	2(%rax,%rax), %r13
	movq	%r13, 4816(%rsp)        # 8-byte Spill
	leaq	1(%rax), %rax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 4576(%rsp)        # 8-byte Spill
	shlq	$3, %rdx
	movq	%rdx, 4992(%rsp)        # 8-byte Spill
	leaq	2(%rcx), %rax
	movq	%rax, 4800(%rsp)        # 8-byte Spill
	addq	$1, %rcx
	movq	%rcx, 4832(%rsp)        # 8-byte Spill
	vroundss	$1, %xmm0, %xmm0, %xmm2
	vmovss	%xmm2, 4784(%rsp)       # 4-byte Spill
	vmovss	.LCPI147_40(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm2, %xmm0
	vmovss	.LCPI147_41(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm0, %xmm2, %xmm1
	vmulss	%xmm1, %xmm1, %xmm0
	vmovss	.LCPI147_42(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_43(%rip), %xmm0, %xmm2
	vfmadd213ss	.LCPI147_44(%rip), %xmm0, %xmm2
	vmovss	.LCPI147_17(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI147_45(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI147_46(%rip), %xmm0, %xmm4
	vfmadd213ss	%xmm3, %xmm0, %xmm2
	vfmadd213ss	.LCPI147_47(%rip), %xmm0, %xmm4
	vfmadd213ss	%xmm3, %xmm0, %xmm4
	vfmadd213ss	%xmm2, %xmm1, %xmm4
	vmovss	%xmm4, 4632(%rsp)       # 4-byte Spill
	leaq	4(%r14), %rax
	movq	%rax, 4768(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1480:                           # %for transpose.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1482 Depth 2
                                        #       Child Loop BB147_1495 Depth 3
                                        #       Child Loop BB147_1500 Depth 3
                                        #       Child Loop BB147_1503 Depth 3
                                        #         Child Loop BB147_1504 Depth 4
	movq	816(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB147_1507
# BB#1481:                              # %for transpose.s0.v11.v132.preheader
                                        #   in Loop: Header=BB147_1480 Depth=1
	movq	4624(%rsp), %rax        # 8-byte Reload
	imulq	4616(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, %rcx
	subq	4592(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 4896(%rsp)        # 8-byte Spill
	subq	4600(%rsp), %rax        # 8-byte Folded Reload
	movq	%rax, 4672(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
	movl	$-1, %edx
	.align	16, 0x90
.LBB147_1482:                           # %for transpose.s0.v11.v132
                                        #   Parent Loop BB147_1480 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1495 Depth 3
                                        #       Child Loop BB147_1500 Depth 3
                                        #       Child Loop BB147_1503 Depth 3
                                        #         Child Loop BB147_1504 Depth 4
	movq	%rcx, 4928(%rsp)        # 8-byte Spill
	movl	%edx, 4944(%rsp)        # 4-byte Spill
	movl	4848(%rsp), %r14d       # 4-byte Reload
	cmpl	%r14d, %edx
	movl	%r14d, %r15d
	cmovgel	%edx, %r15d
	cmpl	%r14d, %edx
	cmovgel	%edx, %r14d
	movl	4856(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %edx
	movl	%eax, %ebx
	cmovgel	%edx, %ebx
	leal	(,%rcx,8), %r12d
	movl	664(%rsp), %eax         # 4-byte Reload
	cmpl	%r12d, %eax
	cmovlel	%eax, %r12d
	testq	$-2147483648, 4880(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_1483
# BB#1484:                              # %assert succeeded552
                                        #   in Loop: Header=BB147_1482 Depth=2
	xorl	%edi, %edi
	movq	4768(%rsp), %rsi        # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, %r11
	testq	%r11, %r11
	je	.LBB147_1564
# BB#1485:                              # %for blur.s1.v10.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	notl	%ebx
	movslq	%ebx, %rax
	movslq	%r12d, %r8
	movq	4872(%rsp), %rdx        # 8-byte Reload
	subq	%r8, %rdx
	movq	4896(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rsi
	leaq	(%rdx,%rax), %rdi
	movq	1176(%rsp), %rbx        # 8-byte Reload
	movzbl	(%rbx,%rsi), %esi
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	1(%rcx,%rax), %rsi
	movzbl	(%rbx,%rsi), %esi
	vcvtsi2ssl	%esi, %xmm0, %xmm1
	vmovss	%xmm0, (%r11,%rdi,4)
	leaq	1(%rdx,%rax), %rsi
	vmovss	%xmm1, (%r11,%rsi,4)
	leaq	2(%rcx,%rax), %rsi
	movzbl	(%rbx,%rsi), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	2(%rdx,%rax), %rsi
	leaq	3(%rcx,%rax), %rdi
	movzbl	(%rbx,%rdi), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%r11,%rsi,4)
	leaq	3(%rdx,%rax), %rsi
	vmovss	%xmm1, (%r11,%rsi,4)
	leaq	4(%rcx,%rax), %rsi
	movzbl	(%rbx,%rsi), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	4(%rdx,%rax), %rsi
	leaq	5(%rcx,%rax), %rdi
	movzbl	(%rbx,%rdi), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%r11,%rsi,4)
	leaq	5(%rdx,%rax), %rsi
	vmovss	%xmm1, (%r11,%rsi,4)
	leaq	6(%rcx,%rax), %rsi
	movzbl	(%rbx,%rsi), %esi
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%esi, %xmm0, %xmm0
	leaq	6(%rdx,%rax), %rsi
	leaq	7(%rcx,%rax), %rdi
	movzbl	(%rbx,%rdi), %edi
	vcvtsi2ssl	%edi, %xmm0, %xmm1
	vmovss	%xmm0, (%r11,%rsi,4)
	leaq	7(%rdx,%rax), %rax
	vmovss	%xmm1, (%r11,%rax,4)
	movq	712(%rsp), %r9          # 8-byte Reload
	cmpl	$1, %r9d
	jle	.LBB147_1501
# BB#1486:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	vcvttss2si	4784(%rsp), %eax # 4-byte Folded Reload
	leal	127(%rax), %edx
	cmpl	$255, %edx
	jl	.LBB147_1487
# BB#1488:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	vmovss	.LCPI147_48(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB147_1489
	.align	16, 0x90
.LBB147_1487:                           #   in Loop: Header=BB147_1482 Depth=2
	shll	$23, %edx
	vmovd	%edx, %xmm0
	vmulss	4632(%rsp), %xmm0, %xmm1 # 4-byte Folded Reload
.LBB147_1489:                           # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	cmpl	$-127, %eax
	jg	.LBB147_1491
# BB#1490:                              # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1491:                           # %for blur.s2.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	vmovss	.LCPI147_17(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %edx
	cmpl	$0, 916(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1493
# BB#1492:                              # %for blur.s2.r76$x.prol
                                        #   in Loop: Header=BB147_1482 Depth=2
	movq	4672(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %rax
	addq	4664(%rsp), %rax        # 8-byte Folded Reload
	vpmovzxbd	(%rbx,%rax), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	movq	4648(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r11,%rax,4), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%r11,%rax,4)
	movl	$2, %edx
.LBB147_1493:                           # %for blur.s2.r76$x.preheader.split
                                        #   in Loop: Header=BB147_1482 Depth=2
	cmpl	$0, 4960(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1496
# BB#1494:                              # %for blur.s2.r76$x.preheader.split.split
                                        #   in Loop: Header=BB147_1482 Depth=2
	notl	%r14d
	movslq	%r14d, %r8
	movl	%r9d, %eax
	movq	4704(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	4680(%rsp), %rsi        # 8-byte Reload
	imulq	%rsi, %rcx
	subl	%edx, %eax
	addq	%r8, %rcx
	movq	4736(%rsp), %r10        # 8-byte Reload
	leaq	(%rcx,%r10), %rcx
	movq	5032(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%rdx), %rdi
	shlq	$5, %rdi
	movq	4712(%rsp), %rbx        # 8-byte Reload
	leaq	(%rdx,%rbx), %rbx
	imulq	%rsi, %rbx
	leaq	(%rdi,%r11), %rdx
	addq	%r8, %rbx
	leaq	(%rbx,%r10), %rsi
	xorl	%edi, %edi
	.align	16, 0x90
.LBB147_1495:                           # %for blur.s2.r76$x
                                        #   Parent Loop BB147_1480 Depth=1
                                        #     Parent Loop BB147_1482 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vpmovzxbd	(%rcx,%rdi), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vmovaps	(%rdx), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rdx)
	vpmovzxbd	(%rsi,%rdi), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rdx)
	addq	$64, %rdx
	addq	%r13, %rdi
	addl	$-2, %eax
	jne	.LBB147_1495
.LBB147_1496:                           # %for blur.s3.r76$x.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	movl	$1, %r8d
	cmpl	$0, 916(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1498
# BB#1497:                              # %for blur.s3.r76$x.prol
                                        #   in Loop: Header=BB147_1482 Depth=2
	movq	4640(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%r11,%rax,4), %ymm2
	vmulps	(%r11,%rax,4), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	movq	4656(%rsp), %rax        # 8-byte Reload
	vmovaps	%ymm2, -64(%r11,%rax,4)
	movl	$2, %r8d
.LBB147_1498:                           # %for blur.s3.r76$x.preheader.split
                                        #   in Loop: Header=BB147_1482 Depth=2
	cmpl	$0, 4960(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1501
# BB#1499:                              # %for blur.s3.r76$x.preheader.split.split
                                        #   in Loop: Header=BB147_1482 Depth=2
	movl	4720(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %edx
	subl	%r8d, %edx
	movq	4688(%rsp), %rax        # 8-byte Reload
	leaq	(%r11,%rax), %rax
	movq	4696(%rsp), %rsi        # 8-byte Reload
	subq	%r8, %rsi
	movq	%r8, %rdi
	shlq	$5, %rdi
	subq	%rdi, %rax
	shlq	$5, %rsi
	leaq	-32(%r11,%rsi), %rsi
	leal	1(%r8), %ebx
	movl	%ecx, %edi
	subl	%ebx, %edi
	movl	%r9d, %ebx
	.align	16, 0x90
.LBB147_1500:                           # %for blur.s3.r76$x
                                        #   Parent Loop BB147_1480 Depth=1
                                        #     Parent Loop BB147_1482 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leal	(%rdx,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r11), %ymm2
	vmulps	(%rcx,%r11), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rax)
	leal	(%rdi,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r11), %ymm2
	vmulps	(%rcx,%r11), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	addl	$-2, %ebx
	addq	$-64, %rax
	addq	$-64, %rsi
	cmpl	%ebx, %r8d
	jne	.LBB147_1500
.LBB147_1501:                           # %consume blur
                                        #   in Loop: Header=BB147_1482 Depth=2
	movq	%r11, 4984(%rsp)        # 8-byte Spill
	cmpl	$0, 5424(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1506
# BB#1502:                              # %for transpose.s0.v10.v130.preheader
                                        #   in Loop: Header=BB147_1482 Depth=2
	notl	%r15d
	movslq	%r15d, %rax
	movq	4800(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	4832(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	4824(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rcx
	imulq	%rdx, %rax
	movq	4912(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	%rcx, 5392(%rsp)        # 8-byte Spill
	movq	4984(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	subq	$-128, %rsi
	movq	%rsi, 5376(%rsp)        # 8-byte Spill
	leaq	260(%rcx), %rsi
	movq	%rsi, 5360(%rsp)        # 8-byte Spill
	leaq	228(%rcx), %rsi
	movq	%rsi, 5344(%rsp)        # 8-byte Spill
	leaq	196(%rcx), %rsi
	movq	%rsi, 5328(%rsp)        # 8-byte Spill
	leaq	164(%rcx), %rsi
	movq	%rsi, 5312(%rsp)        # 8-byte Spill
	leaq	132(%rcx), %rsi
	movq	%rsi, 5296(%rsp)        # 8-byte Spill
	leaq	100(%rcx), %rsi
	movq	%rsi, 5248(%rsp)        # 8-byte Spill
	leaq	68(%rcx), %rsi
	movq	%rsi, 5216(%rsp)        # 8-byte Spill
	leaq	36(%rcx), %rsi
	movq	%rsi, 5184(%rsp)        # 8-byte Spill
	leaq	256(%rcx), %rsi
	movq	%rsi, 5152(%rsp)        # 8-byte Spill
	leaq	224(%rcx), %rsi
	movq	%rsi, 5136(%rsp)        # 8-byte Spill
	leaq	192(%rcx), %rsi
	movq	%rsi, 5120(%rsp)        # 8-byte Spill
	leaq	(%rax,%rdx), %rax
	movq	%rax, 5104(%rsp)        # 8-byte Spill
	leaq	160(%rcx), %rax
	movq	%rax, 5088(%rsp)        # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, 5072(%rsp)        # 8-byte Spill
	leaq	64(%rcx), %rax
	movq	%rax, 5056(%rsp)        # 8-byte Spill
	leaq	32(%rcx), %rax
	movq	%rax, 5040(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	4840(%rsp), %ecx        # 4-byte Reload
	.align	16, 0x90
.LBB147_1503:                           # %for transpose.s0.v10.v130
                                        #   Parent Loop BB147_1480 Depth=1
                                        #     Parent Loop BB147_1482 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1504 Depth 4
	movl	%ecx, 5440(%rsp)        # 4-byte Spill
	movl	%eax, 5464(%rsp)        # 4-byte Spill
	movl	5008(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	movslq	%eax, %rcx
	movq	5032(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	shlq	$5, %r8
	movq	5392(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rcx), %rax
	movq	%rax, 5696(%rsp)        # 8-byte Spill
	movq	5104(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rax
	movq	%rax, 5664(%rsp)        # 8-byte Spill
	movq	5360(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5632(%rsp)        # 8-byte Spill
	movq	5344(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5616(%rsp)        # 8-byte Spill
	movq	5328(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5568(%rsp)        # 8-byte Spill
	movq	5296(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5536(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5528(%rsp)        # 8-byte Spill
	movq	5216(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rax
	movq	%rax, 5472(%rsp)        # 8-byte Spill
	movq	5184(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r14
	movq	5152(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r15
	movq	5136(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %r11
	movq	5120(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	5088(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r8), %rsi
	movq	5376(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r8), %rdx
	movq	5072(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r8), %rdi
	movq	5056(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r8), %r12
	movq	5040(%rsp), %rax        # 8-byte Reload
	leaq	(%r8,%rax), %r13
	movq	5408(%rsp), %r8         # 8-byte Reload
	xorl	%r9d, %r9d
	movq	4992(%rsp), %rax        # 8-byte Reload
	.align	16, 0x90
.LBB147_1504:                           # %for transpose.s0.v11.v11
                                        #   Parent Loop BB147_1480 Depth=1
                                        #     Parent Loop BB147_1482 Depth=2
                                        #       Parent Loop BB147_1503 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	(%rsi,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r11,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r13,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5664(%rsp), %rbx        # 8-byte Reload
	vmovups	%ymm0, (%r8,%rbx,4)
	movq	5568(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	movq	5608(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	5616(%rsp), %rbx        # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	5632(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r14,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	movq	5472(%rsp), %rbx        # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	5528(%rsp), %rbx        # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	5536(%rsp), %rbx        # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5696(%rsp), %r10        # 8-byte Reload
	vmovups	%ymm0, (%r8,%r10,4)
	addq	$8, %r9
	addq	%rax, %r8
	cmpq	$32, %r9
	jne	.LBB147_1504
# BB#1505:                              # %end for transpose.s0.v11.v11
                                        #   in Loop: Header=BB147_1503 Depth=3
	movl	5464(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5440(%rsp), %ecx        # 4-byte Reload
	addl	$-8, %ecx
	cmpl	5424(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1503
.LBB147_1506:                           # %call_destructor.exit777
                                        #   in Loop: Header=BB147_1482 Depth=2
	xorl	%edi, %edi
	movq	4984(%rsp), %rsi        # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	4928(%rsp), %rsi        # 8-byte Reload
	leal	1(%rsi), %eax
	movl	4944(%rsp), %edx        # 4-byte Reload
	addl	$-8, %edx
	movq	816(%rsp), %rcx         # 8-byte Reload
	cmpl	%ecx, %esi
	movl	%eax, %ecx
	movq	4816(%rsp), %r13        # 8-byte Reload
	jne	.LBB147_1482
.LBB147_1507:                           # %end for transpose.s0.v11.v132
                                        #   in Loop: Header=BB147_1480 Depth=1
	movq	4624(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	movq	4584(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4736(%rsp)        # 8-byte Folded Spill
	movq	4576(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4912(%rsp)        # 8-byte Folded Spill
	cmpl	$3, %eax
	jne	.LBB147_1480
# BB#1508:                              # %consume transpose
	movq	728(%rsp), %rax         # 8-byte Reload
	movl	380(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movq	416(%rsp), %rbx         # 8-byte Reload
	movl	4512(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	movl	404(%rsp), %eax         # 4-byte Reload
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movq	424(%rsp), %rax         # 8-byte Reload
	leal	-8(%rax), %r15d
	movq	464(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %r15d
	cmovgl	%eax, %r15d
	subl	%ecx, %ebx
	addl	$1, %ebx
	movq	1136(%rsp), %rax        # 8-byte Reload
	cmpl	$7, %eax
	movl	$8, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 4480(%rsp)        # 4-byte Spill
	movq	%rax, %r12
	movl	%ecx, %eax
	leaq	(,%rax,4), %rdx
	movl	%edx, %ecx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdx,2), %r14
	movl	%edx, %edx
	cmpq	$2147483647, %r14       # imm = 0x7FFFFFFF
	ja	.LBB147_1510
# BB#1509:                              # %consume transpose
	imulq	%rbx, %rcx
	shrq	$30, %rax
	imulq	%rbx, %rax
	shrq	$32, %rcx
	addq	%rcx, %rax
	leaq	(%rdx,%rdx,2), %rcx
	shrq	$32, %rcx
	leaq	(%rax,%rax,2), %rdx
	addq	%rcx, %rdx
	orq	%rax, %rdx
	shrq	$32, %rdx
	jne	.LBB147_1510
# BB#1513:                              # %assert succeeded556
	leaq	4(%r14), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	addq	$4, %r14
	je	.LBB147_1515
# BB#1514:                              # %assert succeeded556
	testq	%rax, %rax
	je	.LBB147_1564
.LBB147_1515:                           # %for transpose$1.s0.v12.preheader
	movq	%rax, 5472(%rsp)        # 8-byte Spill
	imull	4480(%rsp), %ebx        # 4-byte Folded Reload
	movq	%rbx, 416(%rsp)         # 8-byte Spill
	movl	%r15d, %r9d
	sarl	$31, %r9d
	andl	%r15d, %r9d
	movq	720(%rsp), %r8          # 8-byte Reload
	movq	424(%rsp), %rax         # 8-byte Reload
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	addl	$-1, %eax
	leal	-2(%r8), %edx
	movl	%edx, 4944(%rsp)        # 4-byte Spill
	movl	384(%rsp), %ecx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	testl	%ecx, %ecx
	movl	$0, %edx
	cmovsl	%edx, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	$1, %esi
	subl	%r9d, %esi
	addl	%ecx, %esi
	movslq	%r15d, %rax
	movq	%rax, %rdx
	sarq	$63, %rdx
	andq	%rax, %rdx
	shlq	$5, %rsi
	movq	%rsi, 4856(%rsp)        # 8-byte Spill
	movq	4320(%rsp), %r15        # 8-byte Reload
	subq	4608(%rsp), %r15        # 8-byte Folded Reload
	addq	$1, %r15
	leal	7(%r12), %eax
	movq	%r12, %rsi
	sarl	$3, %eax
	movl	%eax, 5392(%rsp)        # 4-byte Spill
	movl	%r8d, %edi
	subl	%r9d, %edi
	movq	728(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r12d
	notl	%r12d
	movl	%r12d, 4896(%rsp)       # 4-byte Spill
	movl	$7, %r10d
	subl	%eax, %r10d
	movq	%rax, %rbx
	movq	448(%rsp), %rax         # 8-byte Reload
	subl	%eax, %r10d
	movq	%rax, %r13
	movl	%r10d, 4848(%rsp)       # 4-byte Spill
	movl	$7, %ecx
	movq	464(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ecx
	subl	%esi, %ecx
	movl	%ecx, 4992(%rsp)        # 4-byte Spill
	movl	4864(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	movslq	%eax, %r14
	movq	%r14, %rax
	sarq	$63, %rax
	andq	%r14, %rax
	shlq	$5, %rax
	xorq	$-32, %rax
	addq	$32, %rax
	movq	%rax, 4840(%rsp)        # 8-byte Spill
	movl	%ebx, %r11d
	negl	%r11d
	subl	%r13d, %r11d
	movl	4512(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %r11d
	cmovgel	%r11d, %ecx
	cmpl	%r12d, %r10d
	movl	%r12d, %esi
	cmovgel	%r10d, %esi
	leal	1(%rsi), %r10d
	movl	%r10d, %eax
	subl	%ecx, %eax
	movl	4560(%rsp), %ebx        # 4-byte Reload
	imull	%eax, %ebx
	movl	%ebx, 4560(%rsp)        # 4-byte Spill
	notl	%ecx
	movslq	%ecx, %rcx
	addq	$1, %rcx
	notl	%esi
	movslq	%esi, %rsi
	subq	%rsi, %rcx
	movq	4416(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %rcx
	movl	$1, %r13d
	subq	%rax, %r13
	imulq	%r15, %r13
	movq	%r13, 4656(%rsp)        # 8-byte Spill
	movq	4552(%rsp), %r13        # 8-byte Reload
	subq	%rdx, %r13
	movl	$1, %eax
	subq	%rdx, %rax
	negq	%rcx
	subq	%rsi, %rcx
	movq	%rcx, 4600(%rsp)        # 8-byte Spill
	andl	$1, 912(%rsp)           # 4-byte Folded Spill
	shlq	$3, %rax
	movq	%rax, 4640(%rsp)        # 8-byte Spill
	addl	$-1, %edi
	movslq	%edi, %rax
	shlq	$3, %rax
	movq	%rax, 4632(%rsp)        # 8-byte Spill
	shlq	$3, %r13
	movq	%r13, 4648(%rsp)        # 8-byte Spill
	movq	%rsi, %rax
	negq	%rax
	movq	%rax, 4736(%rsp)        # 8-byte Spill
	cmpl	$7, %r8d
	movl	$8, %eax
	cmovgl	%r8d, %eax
	movq	440(%rsp), %rcx         # 8-byte Reload
	movq	728(%rsp), %r8          # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movl	%ecx, %ebx
	notl	%ebx
	cmpl	%ebx, %r11d
	movl	%ebx, %edi
	cmovgel	%r11d, %edi
	subl	%edi, %r10d
	imull	%eax, %r10d
	notl	%edi
	movslq	%edi, %rax
	addq	$1, %rax
	subq	%rsi, %rax
	movq	%rax, 4712(%rsp)        # 8-byte Spill
	movq	4448(%rsp), %rax        # 8-byte Reload
	testq	%rax, %rax
	movl	$0, %edx
	cmovsq	%rdx, %rax
	xorl	%edx, %edx
	movq	%rdx, 4624(%rsp)        # 8-byte Spill
	movl	$6, %edx
	subq	%rax, %rdx
	cmpq	$-2, %rdx
	movq	$-1, %rax
	cmovleq	%rax, %rdx
	movq	448(%rsp), %r15         # 8-byte Reload
	movl	%r15d, %edi
	negl	%edi
	subl	%r8d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	notl	%edi
	movslq	%edi, %r13
	addq	$1, %r13
	subq	%rsi, %r13
	shlq	$3, %r13
	movq	%r13, 4832(%rsp)        # 8-byte Spill
	movq	%r14, %rsi
	notq	%rsi
	cmpq	$-2, %rsi
	cmovleq	%rax, %rsi
	movq	%rsi, 5008(%rsp)        # 8-byte Spill
	leaq	1(%rdx), %rax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	addq	$2, %rdx
	movq	%rdx, 4704(%rsp)        # 8-byte Spill
	negl	%r9d
	movl	%r9d, 4720(%rsp)        # 4-byte Spill
	movq	4552(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 4688(%rsp)        # 8-byte Spill
	shlq	$5, %rax
	movq	%rax, 4680(%rsp)        # 8-byte Spill
	negq	%r14
	movq	%r14, 4880(%rsp)        # 8-byte Spill
	movq	1136(%rsp), %rdx        # 8-byte Reload
	cmpl	$7, %edx
	movl	$8, %eax
	cmovgl	%edx, %eax
	movq	%r8, %rdx
	movl	$31, %esi
	subl	%edx, %esi
	subl	%r15d, %esi
	cmpl	%r12d, %esi
	cmovll	%r12d, %esi
	movq	432(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rdx), %edi
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	notl	%ecx
	cmpl	%ecx, %r11d
	cmovgel	%r11d, %ecx
	negl	%ecx
	leal	1(%rsi,%rcx), %ecx
	imull	%eax, %ecx
	movq	344(%rsp), %rax         # 8-byte Reload
	cmpq	$7, %rax
	movl	$8, %edi
	cmovgq	%rax, %rdi
	movq	%rdi, 4824(%rsp)        # 8-byte Spill
	movq	416(%rsp), %rax         # 8-byte Reload
	cltq
	movq	%rax, 4584(%rsp)        # 8-byte Spill
	movslq	4560(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 4592(%rsp)        # 8-byte Spill
	movslq	%r10d, %rax
	movq	%rax, 4576(%rsp)        # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 4560(%rsp)        # 8-byte Spill
	notl	%esi
	movslq	%esi, %rax
	movl	$1, %ecx
	subq	%rax, %rcx
	movq	%rcx, 4800(%rsp)        # 8-byte Spill
	negq	%rax
	movq	%rax, 4816(%rsp)        # 8-byte Spill
	vroundss	$1, 4352(%rsp), %xmm0, %xmm2 # 4-byte Folded Reload
	vmovss	%xmm2, 4784(%rsp)       # 4-byte Spill
	vmovss	.LCPI147_40(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, %xmm1
	vfmadd213ss	4384(%rsp), %xmm2, %xmm1 # 4-byte Folded Reload
	vmovss	.LCPI147_41(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, %xmm3
	vfmadd213ss	%xmm1, %xmm2, %xmm3
	vmulss	%xmm3, %xmm3, %xmm0
	vmovss	.LCPI147_42(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovaps	%xmm1, %xmm4
	vfmadd213ss	.LCPI147_43(%rip), %xmm0, %xmm4
	vfmadd213ss	.LCPI147_44(%rip), %xmm0, %xmm4
	vmovss	.LCPI147_45(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovaps	%xmm1, %xmm2
	vfmadd213ss	.LCPI147_46(%rip), %xmm0, %xmm2
	vmovss	.LCPI147_17(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm0, %xmm4
	vfmadd213ss	.LCPI147_47(%rip), %xmm0, %xmm2
	vfmadd213ss	%xmm1, %xmm0, %xmm2
	vfmadd213ss	%xmm4, %xmm3, %xmm2
	vmovss	%xmm2, 4664(%rsp)       # 4-byte Spill
	movq	4856(%rsp), %rax        # 8-byte Reload
	leaq	4(%rax), %rax
	movq	%rax, 4768(%rsp)        # 8-byte Spill
	leaq	(,%rdi,8), %rax
	movq	%rax, 4984(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB147_1516:                           # %for transpose$1.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1518 Depth 2
                                        #       Child Loop BB147_1540 Depth 3
                                        #       Child Loop BB147_1545 Depth 3
                                        #       Child Loop BB147_1548 Depth 3
                                        #         Child Loop BB147_1549 Depth 4
	cmpl	$0, 5424(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1552
# BB#1517:                              # %for transpose$1.s0.v11.v167.preheader
                                        #   in Loop: Header=BB147_1516 Depth=1
	movq	4592(%rsp), %rax        # 8-byte Reload
	movq	4624(%rsp), %rcx        # 8-byte Reload
	imulq	%rcx, %rax
	movq	4600(%rsp), %rsi        # 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 4872(%rsp)        # 8-byte Spill
	imulq	4568(%rsp), %rcx        # 8-byte Folded Reload
	subq	4608(%rsp), %rcx        # 8-byte Folded Reload
	movq	%rcx, 4672(%rsp)        # 8-byte Spill
	xorl	%esi, %esi
	movl	4896(%rsp), %eax        # 4-byte Reload
	.align	16, 0x90
.LBB147_1518:                           # %for transpose$1.s0.v11.v167
                                        #   Parent Loop BB147_1516 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1540 Depth 3
                                        #       Child Loop BB147_1545 Depth 3
                                        #       Child Loop BB147_1548 Depth 3
                                        #         Child Loop BB147_1549 Depth 4
	movl	%eax, 4912(%rsp)        # 4-byte Spill
	movq	%rsi, 4928(%rsp)        # 8-byte Spill
	movl	4848(%rsp), %ecx        # 4-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %r15d
	cmovgel	%eax, %r15d
	cmpl	%ecx, %eax
	movl	%ecx, %r12d
	cmovgel	%eax, %r12d
	leal	(,%rsi,8), %eax
	movl	4896(%rsp), %ebx        # 4-byte Reload
	subl	%eax, %ebx
	cmpl	%ecx, %ebx
	cmovll	%ecx, %ebx
	leal	(%rdx,%rsi,8), %r14d
	movl	4752(%rsp), %eax        # 4-byte Reload
	cmpl	%r14d, %eax
	cmovlel	%eax, %r14d
	testq	$-2147483648, 4856(%rsp) # 8-byte Folded Reload
                                        # imm = 0xFFFFFFFF80000000
	jne	.LBB147_1519
# BB#1529:                              # %assert succeeded560
                                        #   in Loop: Header=BB147_1518 Depth=2
	xorl	%edi, %edi
	movq	4768(%rsp), %rsi        # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, %rdx
	testq	%rdx, %rdx
	je	.LBB147_3
# BB#1530:                              # %for blur$1.s1.v10.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	notl	%ebx
	movslq	%ebx, %rax
	movq	4872(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	5408(%rsp), %r8         # 8-byte Reload
	leaq	(%r8,%rax,4), %rax
	vmovups	(%rax), %ymm0
	movq	4840(%rsp), %rax        # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax)
	movq	720(%rsp), %r9          # 8-byte Reload
	cmpl	$1, %r9d
	jle	.LBB147_1546
# BB#1531:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	vcvttss2si	4784(%rsp), %eax # 4-byte Folded Reload
	leal	127(%rax), %ecx
	cmpl	$255, %ecx
	jl	.LBB147_1532
# BB#1533:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	vmovss	.LCPI147_48(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB147_1534
	.align	16, 0x90
.LBB147_1532:                           #   in Loop: Header=BB147_1518 Depth=2
	shll	$23, %ecx
	vmovd	%ecx, %xmm0
	vmulss	4664(%rsp), %xmm0, %xmm1 # 4-byte Folded Reload
.LBB147_1534:                           # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	cmpl	$-127, %eax
	jg	.LBB147_1536
# BB#1535:                              # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
.LBB147_1536:                           # %for blur$1.s2.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	vmovss	.LCPI147_17(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %esi
	cmpl	$0, 912(%rsp)           # 4-byte Folded Reload
	je	.LBB147_1538
# BB#1537:                              # %for blur$1.s2.r101$x.prol
                                        #   in Loop: Header=BB147_1518 Depth=2
	movslq	%r14d, %rax
	movq	4672(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	addq	4656(%rsp), %rax        # 8-byte Folded Reload
	vmulps	(%r8,%rax,4), %ymm0, %ymm2
	movq	4640(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%rdx,%rax,4), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%rdx,%rax,4)
	movl	$2, %esi
.LBB147_1538:                           # %for blur$1.s2.r101$x.preheader.split
                                        #   in Loop: Header=BB147_1518 Depth=2
	movq	%rdx, %r14
	cmpl	$0, 4944(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1541
# BB#1539:                              # %for blur$1.s2.r101$x.preheader.split.split
                                        #   in Loop: Header=BB147_1518 Depth=2
	notl	%r12d
	movslq	%r12d, %r10
	movl	%r9d, %eax
	subl	%esi, %eax
	movq	4696(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rsi), %rcx
	movq	4712(%rsp), %r11        # 8-byte Reload
	imulq	%r11, %rcx
	movq	4736(%rsp), %rbx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	addq	%r10, %rcx
	movq	5008(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	shlq	$5, %rdx
	leaq	(%rdx,%r14), %rdx
	movq	4704(%rsp), %rdi        # 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	imulq	%r11, %rsi
	leaq	(%rsi,%rbx), %rsi
	addq	%r10, %rsi
	movq	%r8, %rdi
	.align	16, 0x90
.LBB147_1540:                           # %for blur$1.s2.r101$x
                                        #   Parent Loop BB147_1516 Depth=1
                                        #     Parent Loop BB147_1518 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmulps	(%rdi,%rcx,4), %ymm0, %ymm2
	vmovaps	(%rdx), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rdx)
	vmulps	(%rdi,%rsi,4), %ymm0, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rdx)
	addq	$64, %rdx
	addq	%r13, %rdi
	addl	$-2, %eax
	jne	.LBB147_1540
.LBB147_1541:                           # %for blur$1.s3.r101$x.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	movl	$1, %r8d
	cmpl	$0, 912(%rsp)           # 4-byte Folded Reload
	movq	%r14, %rdx
	je	.LBB147_1543
# BB#1542:                              # %for blur$1.s3.r101$x.prol
                                        #   in Loop: Header=BB147_1518 Depth=2
	movq	4632(%rsp), %rax        # 8-byte Reload
	vmovaps	-32(%rdx,%rax,4), %ymm2
	vmulps	(%rdx,%rax,4), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	movq	4648(%rsp), %rax        # 8-byte Reload
	vmovaps	%ymm2, -64(%rdx,%rax,4)
	movl	$2, %r8d
.LBB147_1543:                           # %for blur$1.s3.r101$x.preheader.split
                                        #   in Loop: Header=BB147_1518 Depth=2
	cmpl	$0, 4944(%rsp)          # 4-byte Folded Reload
	je	.LBB147_1546
# BB#1544:                              # %for blur$1.s3.r101$x.preheader.split.split
                                        #   in Loop: Header=BB147_1518 Depth=2
	movl	4720(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %r10d
	subl	%r8d, %r10d
	movq	4680(%rsp), %rax        # 8-byte Reload
	leaq	(%rdx,%rax), %rsi
	movq	4688(%rsp), %rax        # 8-byte Reload
	subq	%r8, %rax
	movq	%r8, %rdi
	shlq	$5, %rdi
	subq	%rdi, %rsi
	shlq	$5, %rax
	leaq	-32(%rdx,%rax), %rax
	leal	1(%r8), %ebx
	movl	%ecx, %edi
	subl	%ebx, %edi
	movl	%r9d, %ebx
	.align	16, 0x90
.LBB147_1545:                           # %for blur$1.s3.r101$x
                                        #   Parent Loop BB147_1516 Depth=1
                                        #     Parent Loop BB147_1518 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leal	(%r10,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%rdx), %ymm2
	vmulps	(%rcx,%rdx), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	leal	(%rdi,%rbx), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%rdx), %ymm2
	vmulps	(%rcx,%rdx), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rax)
	addl	$-2, %ebx
	addq	$-64, %rsi
	addq	$-64, %rax
	cmpl	%ebx, %r8d
	jne	.LBB147_1545
.LBB147_1546:                           # %consume blur$1
                                        #   in Loop: Header=BB147_1518 Depth=2
	movq	%rdx, 4960(%rsp)        # 8-byte Spill
	cmpl	$0, 5392(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1551
# BB#1547:                              # %for transpose$1.s0.v10.v165.preheader
                                        #   in Loop: Header=BB147_1518 Depth=2
	notl	%r15d
	movslq	%r15d, %rax
	movq	4800(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	4816(%rsp), %rdx        # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	4824(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %rcx
	imulq	%rdx, %rax
	movq	4880(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	%rcx, 5376(%rsp)        # 8-byte Spill
	movq	4960(%rsp), %rcx        # 8-byte Reload
	movq	%rcx, %rsi
	subq	$-128, %rsi
	movq	%rsi, 5360(%rsp)        # 8-byte Spill
	leaq	260(%rcx), %rsi
	movq	%rsi, 5344(%rsp)        # 8-byte Spill
	leaq	228(%rcx), %rsi
	movq	%rsi, 5328(%rsp)        # 8-byte Spill
	leaq	196(%rcx), %rsi
	movq	%rsi, 5312(%rsp)        # 8-byte Spill
	leaq	164(%rcx), %rsi
	movq	%rsi, 5296(%rsp)        # 8-byte Spill
	leaq	132(%rcx), %rsi
	movq	%rsi, 5248(%rsp)        # 8-byte Spill
	leaq	100(%rcx), %rsi
	movq	%rsi, 5216(%rsp)        # 8-byte Spill
	leaq	68(%rcx), %rsi
	movq	%rsi, 5184(%rsp)        # 8-byte Spill
	leaq	36(%rcx), %rsi
	movq	%rsi, 5152(%rsp)        # 8-byte Spill
	leaq	256(%rcx), %rsi
	movq	%rsi, 5136(%rsp)        # 8-byte Spill
	leaq	224(%rcx), %rsi
	movq	%rsi, 5120(%rsp)        # 8-byte Spill
	leaq	192(%rcx), %rsi
	movq	%rsi, 5104(%rsp)        # 8-byte Spill
	leaq	(%rax,%rdx), %rax
	movq	%rax, 5088(%rsp)        # 8-byte Spill
	leaq	160(%rcx), %rax
	movq	%rax, 5072(%rsp)        # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, 5056(%rsp)        # 8-byte Spill
	leaq	64(%rcx), %rax
	movq	%rax, 5040(%rsp)        # 8-byte Spill
	leaq	32(%rcx), %rax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	4864(%rsp), %ecx        # 4-byte Reload
	.align	16, 0x90
.LBB147_1548:                           # %for transpose$1.s0.v10.v165
                                        #   Parent Loop BB147_1516 Depth=1
                                        #     Parent Loop BB147_1518 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1549 Depth 4
	movl	%ecx, 5440(%rsp)        # 4-byte Spill
	movl	%eax, 5464(%rsp)        # 4-byte Spill
	movl	4992(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	notl	%eax
	cltq
	movq	5008(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r9
	shlq	$5, %r9
	movq	5376(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	movq	%rcx, 5696(%rsp)        # 8-byte Spill
	movq	5088(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 5664(%rsp)        # 8-byte Spill
	movq	5344(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5632(%rsp)        # 8-byte Spill
	movq	5328(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5616(%rsp)        # 8-byte Spill
	movq	5312(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movq	5296(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5568(%rsp)        # 8-byte Spill
	movq	5248(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5536(%rsp)        # 8-byte Spill
	movq	5216(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	%rax, 5528(%rsp)        # 8-byte Spill
	movq	5184(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rcx
	movq	5152(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %r12
	movq	5136(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %rax
	movq	5120(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r9), %rdx
	movq	5104(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r15
	movq	5072(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %rbx
	movq	5360(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r13
	movq	5056(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r14
	movq	5040(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r9), %r8
	movq	5032(%rsp), %rsi        # 8-byte Reload
	leaq	(%r9,%rsi), %r9
	movq	5472(%rsp), %r10        # 8-byte Reload
	xorl	%r11d, %r11d
	movq	4984(%rsp), %rsi        # 8-byte Reload
	.align	16, 0x90
.LBB147_1549:                           # %for transpose$1.s0.v11.v11
                                        #   Parent Loop BB147_1516 Depth=1
                                        #     Parent Loop BB147_1518 Depth=2
                                        #       Parent Loop BB147_1548 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	(%rbx,%r11), %xmm0      # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r9,%r11), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r14,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r13,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5664(%rsp), %rdi        # 8-byte Reload
	vmovups	%ymm0, (%r10,%rdi,4)
	movq	5568(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%r11), %xmm0      # xmm0 = mem[0],zero,zero,zero
	movq	5608(%rsp), %rdi        # 8-byte Reload
	vinsertps	$16, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	5616(%rsp), %rdi        # 8-byte Reload
	vinsertps	$32, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	5632(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdi,%r11), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r12,%r11), %xmm1      # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	5528(%rsp), %rdi        # 8-byte Reload
	vinsertps	$32, (%rdi,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	5536(%rsp), %rdi        # 8-byte Reload
	vinsertps	$48, (%rdi,%r11), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	5696(%rsp), %rdi        # 8-byte Reload
	vmovups	%ymm0, (%r10,%rdi,4)
	addq	$8, %r11
	addq	%rsi, %r10
	cmpq	$32, %r11
	jne	.LBB147_1549
# BB#1550:                              # %end for transpose$1.s0.v11.v11
                                        #   in Loop: Header=BB147_1548 Depth=3
	movl	5464(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5440(%rsp), %ecx        # 4-byte Reload
	addl	$-8, %ecx
	cmpl	5392(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1548
.LBB147_1551:                           # %call_destructor.exit692
                                        #   in Loop: Header=BB147_1518 Depth=2
	xorl	%edi, %edi
	movq	4960(%rsp), %rsi        # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	4928(%rsp), %rsi        # 8-byte Reload
	addl	$1, %esi
	movl	4912(%rsp), %eax        # 4-byte Reload
	addl	$-8, %eax
	cmpl	5424(%rsp), %esi        # 4-byte Folded Reload
	movq	728(%rsp), %rdx         # 8-byte Reload
	movq	4832(%rsp), %r13        # 8-byte Reload
	jne	.LBB147_1518
.LBB147_1552:                           # %end for transpose$1.s0.v11.v167
                                        #   in Loop: Header=BB147_1516 Depth=1
	movq	4624(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	movq	%rax, 4624(%rsp)        # 8-byte Spill
	movq	4576(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4736(%rsp)        # 8-byte Folded Spill
	movq	4560(%rsp), %rcx        # 8-byte Reload
	addq	%rcx, 4880(%rsp)        # 8-byte Folded Spill
	cmpl	$3, %eax
	jne	.LBB147_1516
# BB#1520:                              # %end for transpose$1.s0.v12
	movq	5408(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB147_1522
# BB#1521:                              # %if.then.i.773
	xorl	%edi, %edi
	movq	%rdx, %rbx
	callq	halide_free@PLT
	movq	%rbx, %rdx
.LBB147_1522:                           # %for sharpi.s0.v12.preheader
	movq	448(%rsp), %r8          # 8-byte Reload
	leal	31(%r8), %eax
	sarl	$5, %eax
	movl	%eax, 5328(%rsp)        # 4-byte Spill
	movslq	4480(%rsp), %rcx        # 4-byte Folded Reload
	vmovss	80(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vbroadcastss	%xmm0, %ymm5
	movl	$7, %eax
	movq	464(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %eax
	movq	1136(%rsp), %rsi        # 8-byte Reload
	subl	%esi, %eax
	movl	%ebx, %edi
	notl	%edi
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	leal	1(%rbx,%rdi), %eax
	movq	%rax, 5312(%rsp)        # 8-byte Spill
	cmpl	$7, %esi
	movl	$8, %r9d
	cmovgl	%esi, %r9d
	movq	%r9, 5296(%rsp)         # 8-byte Spill
	movl	$31, %eax
	subl	%edx, %eax
	subl	%r8d, %eax
	movl	%eax, 5248(%rsp)        # 4-byte Spill
	movl	%edx, %edi
	notl	%edi
	movl	%edi, 5120(%rsp)        # 4-byte Spill
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	movl	%edi, 5216(%rsp)        # 4-byte Spill
	movq	5288(%rsp), %rax        # 8-byte Reload
	movq	824(%rsp), %rdi         # 8-byte Reload
	leal	31(%rax,%rdi), %eax
	movl	%ebx, %edi
	movl	364(%rsp), %r8d         # 4-byte Reload
	subl	%r8d, %edi
	movq	%rdi, 5288(%rsp)        # 8-byte Spill
	leal	-1(%rbx,%rsi), %esi
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	movq	720(%rsp), %rsi         # 8-byte Reload
	testl	%esi, %esi
	movl	$1, %edi
	cmovgl	%esi, %edi
	leal	(,%r9,8), %esi
	movl	%esi, 5408(%rsp)        # 4-byte Spill
	negl	%edi
	movl	376(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %edi
	cmovll	%esi, %edi
	notl	%edi
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	addl	$1, %edi
	subl	%r8d, %edi
	movq	%rdi, 5184(%rsp)        # 8-byte Spill
	notl	704(%rsp)               # 4-byte Folded Spill
	leal	(,%rdi,8), %eax
	movl	%eax, 5392(%rsp)        # 4-byte Spill
	movq	392(%rsp), %rdi         # 8-byte Reload
	imulq	%rdi, %rdx
	subq	%rdx, 848(%rsp)         # 8-byte Folded Spill
	leaq	(,%rdi,8), %rax
	movq	%rax, 5376(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	vpbroadcastd	.LCPI147_49(%rip), %ymm1
	vbroadcastss	.LCPI147_39(%rip), %ymm11
	vbroadcastss	.LCPI147_50(%rip), %ymm8
	vbroadcastss	.LCPI147_51(%rip), %ymm9
	vpbroadcastd	.LCPI147_52(%rip), %ymm12
	vbroadcastss	.LCPI147_42(%rip), %ymm7
	vbroadcastss	.LCPI147_43(%rip), %ymm13
	vbroadcastss	.LCPI147_44(%rip), %ymm6
	vbroadcastss	.LCPI147_17(%rip), %ymm10
	vbroadcastss	.LCPI147_45(%rip), %ymm2
	vmovaps	%ymm2, 5664(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI147_46(%rip), %ymm3
	vmovaps	%ymm3, 5696(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI147_47(%rip), %ymm4
	vmovaps	%ymm4, 5632(%rsp)       # 32-byte Spill
	vmovdqa	.LCPI147_7(%rip), %ymm14 # ymm14 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	movl	$0, 5136(%rsp)          # 4-byte Folded Spill
	movq	1176(%rsp), %r10        # 8-byte Reload
	movq	1008(%rsp), %r13        # 8-byte Reload
	movq	5472(%rsp), %r9         # 8-byte Reload
	.align	16, 0x90
.LBB147_1523:                           # %for sharpi.s0.v12
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB147_1560 Depth 2
                                        #       Child Loop BB147_1561 Depth 3
                                        #         Child Loop BB147_1562 Depth 4
	movq	%rax, 5152(%rsp)        # 8-byte Spill
	cmpl	$0, 5328(%rsp)          # 4-byte Folded Reload
	jle	.LBB147_1553
# BB#1524:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1523 Depth=1
	movl	$2, %eax
	subl	5136(%rsp), %eax        # 4-byte Folded Reload
	testb	$1, 96(%rbp)
	movq	5152(%rsp), %rsi        # 8-byte Reload
	cmovel	%esi, %eax
	vmovss	56(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
	cmpl	$1, %eax
	je	.LBB147_1526
# BB#1525:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1523 Depth=1
	vmovss	64(%rbp), %xmm0         # xmm0 = mem[0],zero,zero,zero
.LBB147_1526:                           # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1523 Depth=1
	movslq	%eax, %rbx
	vmovss	48(%rbp), %xmm15        # xmm15 = mem[0],zero,zero,zero
	testl	%eax, %eax
	je	.LBB147_1528
# BB#1527:                              # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1523 Depth=1
	vmovaps	%xmm0, %xmm15
.LBB147_1528:                           # %for sharpi.s0.v11.v14.preheader
                                        #   in Loop: Header=BB147_1523 Depth=1
	movq	%rsi, %r8
	movq	4584(%rsp), %rdx        # 8-byte Reload
	imulq	%rdx, %r8
	movq	%r8, 5616(%rsp)         # 8-byte Spill
	movq	%rsi, %r12
	movq	4616(%rsp), %rax        # 8-byte Reload
	imulq	%rax, %r12
	movq	%rbx, %rsi
	imulq	%rdx, %rsi
	movq	%rsi, 5536(%rsp)        # 8-byte Spill
	vaddss	.LCPI147_17(%rip), %xmm15, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 5568(%rsp)       # 32-byte Spill
	imulq	%rax, %rbx
	movq	%rbx, 5608(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movl	5120(%rsp), %edx        # 4-byte Reload
	vxorps	%ymm15, %ymm15, %ymm15
	.align	16, 0x90
.LBB147_1560:                           # %for sharpi.s0.v11.v14
                                        #   Parent Loop BB147_1523 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB147_1561 Depth 3
                                        #         Child Loop BB147_1562 Depth 4
	movl	%edx, 5344(%rsp)        # 4-byte Spill
	movl	%eax, 5360(%rsp)        # 4-byte Spill
	movl	5248(%rsp), %eax        # 4-byte Reload
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	5216(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	5296(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %edx
	movl	704(%rsp), %esi         # 4-byte Reload
	subl	%eax, %esi
	movq	5184(%rsp), %rbx        # 8-byte Reload
	imull	%ebx, %esi
	movq	5312(%rsp), %rbx        # 8-byte Reload
	leal	(%rdx,%rbx), %ebx
	movq	5288(%rsp), %rdx        # 8-byte Reload
	leal	(%rsi,%rdx), %esi
	notl	%eax
	cltq
	imulq	%rdi, %rax
	movq	848(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %r11
	xorl	%eax, %eax
	.align	16, 0x90
.LBB147_1561:                           # %for sharpi.s0.v11.v13.v13
                                        #   Parent Loop BB147_1523 Depth=1
                                        #     Parent Loop BB147_1560 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB147_1562 Depth 4
	movq	%rax, 5424(%rsp)        # 8-byte Spill
	movq	%r11, 5440(%rsp)        # 8-byte Spill
	movl	%esi, 5464(%rsp)        # 4-byte Spill
	movl	%ebx, 5528(%rsp)        # 4-byte Spill
	movq	1136(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r14d
	movl	%esi, %edx
	movl	%ebx, %esi
	testl	%eax, %eax
	jle	.LBB147_1563
	.align	16, 0x90
.LBB147_1562:                           # %for sharpi.s0.v10
                                        #   Parent Loop BB147_1523 Depth=1
                                        #     Parent Loop BB147_1560 Depth=2
                                        #       Parent Loop BB147_1561 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movslq	%esi, %rsi
	leaq	(%rsi,%r8), %rbx
	leaq	(%r9,%rbx,4), %r15
	movq	%r12, %r8
	leaq	(%r15,%rcx,4), %r12
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	leaq	(%r12,%rcx,4), %rbx
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rax
	vmovdqa	%ymm14, %ymm2
	vmovss	(%rax,%rcx,4), %xmm14   # xmm14 = mem[0],zero,zero,zero
	leaq	(%rax,%rcx,4), %rax
	vinsertps	$32, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r8, %r12
	movq	%rdi, %r15
	leaq	(%rax,%rcx,4), %rdi
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%edx, %rdx
	leaq	(%rdx,%r12), %rbx
	vinsertps	$16, (%rax,%rcx,4), %xmm14, %xmm3 # xmm3 = xmm14[0],mem[0],xmm14[2,3]
	leaq	(%r10,%rbx), %rax
	vpinsrb	$0, (%r10,%rbx), %xmm0, %xmm4
	vpinsrb	$2, (%r13,%rax), %xmm4, %xmm4
	leaq	(%rdi,%rcx,4), %rbx
	leaq	(%rax,%r13), %rax
	vpinsrb	$4, (%r13,%rax), %xmm4, %xmm4
	leaq	(%rax,%r13), %rax
	vinsertps	$32, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leaq	(%rax,%r13), %rdi
	vpinsrb	$6, (%r13,%rax), %xmm4, %xmm4
	leaq	(%rdi,%r13), %rax
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	leaq	(%rax,%r13), %rbx
	vpinsrb	$8, (%r13,%rdi), %xmm4, %xmm4
	vpinsrb	$10, (%r13,%rax), %xmm4, %xmm4
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vpinsrb	$12, (%r13,%rbx), %xmm4, %xmm3
	addq	%r13, %rbx
	vpinsrb	$14, (%r13,%rbx), %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpand	%ymm1, %ymm3, %ymm3
	vcvtdq2ps	%ymm3, %ymm3
	vsubps	%ymm0, %ymm3, %ymm0
	vmulps	%ymm0, %ymm0, %ymm0
	vfnmadd213ps	%ymm15, %ymm5, %ymm0
	vmulps	%ymm11, %ymm0, %ymm3
	vroundps	$1, %ymm3, %ymm3
	vcvttps2dq	%ymm3, %ymm4
	vmovaps	%ymm9, %ymm14
	vfnmadd213ps	%ymm0, %ymm3, %ymm14
	vfnmadd213ps	%ymm14, %ymm8, %ymm3
	vmulps	%ymm3, %ymm3, %ymm0
	vmovaps	%ymm7, %ymm14
	vfmadd213ps	%ymm13, %ymm0, %ymm14
	vfmadd213ps	%ymm6, %ymm0, %ymm14
	vmovaps	%ymm6, %ymm15
	vmovaps	%ymm13, %ymm6
	vmovaps	%ymm7, %ymm13
	vmovaps	%ymm9, %ymm7
	vmovaps	%ymm8, %ymm9
	vmovaps	%ymm11, %ymm8
	vmovaps	5664(%rsp), %ymm11      # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm0, %ymm14
	vfmadd213ps	5696(%rsp), %ymm0, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	5632(%rsp), %ymm0, %ymm11 # 32-byte Folded Reload
	vfmadd213ps	%ymm10, %ymm0, %ymm11
	movq	5536(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	leaq	(%r9,%rax,4), %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vpaddd	%ymm12, %ymm4, %ymm0
	vfmadd213ps	%ymm14, %ymm3, %ymm11
	vmovdqa	%ymm2, %ymm14
	vpslld	$23, %ymm0, %ymm3
	vfnmadd213ps	%ymm10, %ymm3, %ymm11
	vbroadcastss	.LCPI147_53(%rip), %ymm3
	vpcmpgtd	%ymm0, %ymm1, %ymm4
	vblendvps	%ymm4, %ymm11, %ymm3, %ymm3
	vmovss	(%r9,%rax,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rbx,%rcx,4), %rax
	leaq	(%rax,%rcx,4), %rdi
	vmovaps	%ymm5, %ymm11
	vmovss	(%rdi,%rcx,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	leaq	(%rdi,%rcx,4), %rdi
	vinsertps	$16, (%rdi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	leaq	(%rdi,%rcx,4), %rdi
	vinsertps	$32, (%rdi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	leaq	(%rdi,%rcx,4), %rdi
	vinsertps	$48, (%rdi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%r15, %rdi
	movq	5608(%rsp), %r15        # 8-byte Reload
	movq	5616(%rsp), %r8         # 8-byte Reload
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	vmovaps	%ymm11, %ymm5
	vmovaps	%ymm8, %ymm11
	vmovaps	%ymm9, %ymm8
	vmovaps	%ymm7, %ymm9
	vmovaps	%ymm13, %ymm7
	vmovaps	%ymm6, %ymm13
	vmovaps	%ymm15, %ymm6
	vxorps	%ymm15, %ymm15, %ymm15
	vpcmpgtd	%ymm15, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm3, %ymm10, %ymm0
	leaq	(%rdx,%r15), %rax
	vpinsrb	$0, (%r10,%rax), %xmm0, %xmm3
	leaq	(%r10,%rax), %rax
	vpinsrb	$2, (%r13,%rax), %xmm3, %xmm3
	leaq	(%rax,%r13), %rax
	vpinsrb	$4, (%r13,%rax), %xmm3, %xmm3
	leaq	(%rax,%r13), %rax
	vpinsrb	$6, (%r13,%rax), %xmm3, %xmm3
	leaq	(%rax,%r13), %rax
	vpinsrb	$8, (%r13,%rax), %xmm3, %xmm3
	leaq	(%rax,%r13), %rax
	vpinsrb	$10, (%r13,%rax), %xmm3, %xmm3
	leaq	(%rax,%r13), %rax
	vpinsrb	$12, (%r13,%rax), %xmm3, %xmm3
	addq	%r13, %rax
	vpinsrb	$14, (%r13,%rax), %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpand	%ymm1, %ymm3, %ymm3
	vcvtdq2ps	%ymm3, %ymm3
	vsubps	%ymm4, %ymm3, %ymm3
	vmulps	%ymm0, %ymm3, %ymm0
	vmovaps	5568(%rsp), %ymm2       # 32-byte Reload
	vfmadd213ps	%ymm4, %ymm2, %ymm0
	vbroadcastss	.LCPI147_54(%rip), %ymm3
	vminps	%ymm3, %ymm0, %ymm0
	vmaxps	%ymm15, %ymm0, %ymm0
	vcvttps2dq	%ymm0, %ymm0
	vpshufb	%ymm14, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	leaq	(%r11,%rdi), %rax
	vpextrb	$0, %xmm0, (%r11)
	vpextrb	$2, %xmm0, (%r11,%rdi)
	vpextrb	$4, %xmm0, (%rdi,%rax)
	addq	%rdi, %rax
	vpextrb	$6, %xmm0, (%rdi,%rax)
	addq	%rdi, %rax
	vpextrb	$8, %xmm0, (%rdi,%rax)
	addq	%rdi, %rax
	vpextrb	$10, %xmm0, (%rdi,%rax)
	addq	%rdi, %rax
	vpextrb	$12, %xmm0, (%rdi,%rax)
	addq	%rdi, %rax
	vpextrb	$14, %xmm0, (%rdi,%rax)
	addl	$1, %esi
	addl	$1, %edx
	addq	$3, %r11
	addl	$-1, %r14d
	jne	.LBB147_1562
.LBB147_1563:                           # %end for sharpi.s0.v10
                                        #   in Loop: Header=BB147_1561 Depth=3
	vmovaps	5632(%rsp), %ymm4       # 32-byte Reload
	vmovaps	5696(%rsp), %ymm3       # 32-byte Reload
	vmovaps	5664(%rsp), %ymm2       # 32-byte Reload
	movq	5424(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	movl	5528(%rsp), %ebx        # 4-byte Reload
	addl	5408(%rsp), %ebx        # 4-byte Folded Reload
	movl	5464(%rsp), %esi        # 4-byte Reload
	addl	5392(%rsp), %esi        # 4-byte Folded Reload
	movq	5440(%rsp), %r11        # 8-byte Reload
	addq	5376(%rsp), %r11        # 8-byte Folded Reload
	cmpq	$4, %rax
	jne	.LBB147_1561
# BB#1559:                              # %end for sharpi.s0.v11.v13.v13
                                        #   in Loop: Header=BB147_1560 Depth=2
	movl	5360(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	movl	5344(%rsp), %edx        # 4-byte Reload
	addl	$-32, %edx
	cmpl	5328(%rsp), %eax        # 4-byte Folded Reload
	jne	.LBB147_1560
.LBB147_1553:                           # %end for sharpi.s0.v11.v14
                                        #   in Loop: Header=BB147_1523 Depth=1
	movq	5152(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	addl	$1, 5136(%rsp)          # 4-byte Folded Spill
	addq	$1, 848(%rsp)           # 8-byte Folded Spill
	cmpq	$3, %rax
	jne	.LBB147_1523
# BB#1554:                              # %end for sharpi.s0.v12
	movq	1176(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB147_1556
# BB#1555:                              # %if.then.i.662
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_1556:                           # %call_destructor.exit663
	xorl	%esi, %esi
	movl	$0, %edx
	movl	$0, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	movl	$0, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	movl	$0, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	movl	$0, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	movl	$0, %r12d
	movl	$0, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movl	$0, %ecx
	movl	$0, %edi
	movq	5472(%rsp), %rax        # 8-byte Reload
	testq	%rax, %rax
	je	.LBB147_227
# BB#1557:                              # %if.then.i.633
	xorl	%edi, %edi
	movq	%rax, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_1558:                           # %call_destructor.exit.thread
	xorl	%esi, %esi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.LBB147_227:                            # %call_destructor.exit.thread
	movl	%edi, 5696(%rsp)        # 4-byte Spill
	movq	%r12, 4816(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	sete	%r14b
.LBB147_9:                              # %call_destructor.exit581
	movq	%rcx, %r15
	testq	%rsi, %rsi
	movq	4664(%rsp), %r12        # 8-byte Reload
	movq	4704(%rsp), %r13        # 8-byte Reload
	je	.LBB147_12
# BB#10:                                # %call_destructor.exit581
	testb	%r14b, %r14b
	jne	.LBB147_12
# BB#11:                                # %if.then.i.585
	xorl	%edi, %edi
	movq	%rdx, %rbx
	vzeroupper
	callq	halide_free@PLT
	movq	%rbx, %rdx
.LBB147_12:                             # %call_destructor.exit591
	testq	%rdx, %rdx
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_14
# BB#13:                                # %if.then.i.595
	xorl	%edi, %edi
	movq	%rdx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_14:                             # %call_destructor.exit596
	movq	5032(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_16
# BB#15:                                # %if.then.i.600
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_16:                             # %call_destructor.exit601
	testq	%r12, %r12
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_18
# BB#17:                                # %if.then.i.605
	xorl	%edi, %edi
	movq	%r12, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_18:                             # %call_destructor.exit606
	testq	%r13, %r13
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_20
# BB#19:                                # %if.then.i.610
	xorl	%edi, %edi
	movq	%r13, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_20:                             # %call_destructor.exit611
	movq	4696(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	movq	4816(%rsp), %rbx        # 8-byte Reload
	jne	.LBB147_22
# BB#21:                                # %if.then.i.615
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_22:                             # %call_destructor.exit616
	testq	%rbx, %rbx
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_24
# BB#23:                                # %if.then.i.620
	xorl	%edi, %edi
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_24:                             # %call_destructor.exit621
	movq	5608(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_26
# BB#25:                                # %if.then.i.625
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_26:                             # %call_destructor.exit626
	movq	%r15, %rsi
	testq	%rsi, %rsi
	sete	%al
	orb	%r14b, %al
	jne	.LBB147_28
# BB#27:                                # %if.then.i.630
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_28:                             # %call_destructor.exit631
	movl	5696(%rsp), %eax        # 4-byte Reload
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB147_1564:                           # %assert failed553
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	jmp	.LBB147_1512
.LBB147_1483:                           # %assert failed551
	leaq	.Lstr.174(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	4880(%rsp), %rdx        # 8-byte Reload
.LBB147_1511:                           # %call_destructor.exit.thread
	callq	halide_error_buffer_allocation_too_large@PLT
.LBB147_1512:                           # %call_destructor.exit.thread
	xorl	%edx, %edx
	movl	%eax, %edi
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	movq	5408(%rsp), %rsi        # 8-byte Reload
	jmp	.LBB147_227
.LBB147_1519:                           # %assert failed559
	leaq	.Lstr.176(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	4856(%rsp), %rdx        # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_4
.LBB147_3:                              # %assert failed561
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_4:                              # %destructor_block
	movl	%eax, 5696(%rsp)        # 4-byte Spill
	cmpl	$0, 5696(%rsp)          # 4-byte Folded Reload
	sete	%r14b
	cmpq	$0, 5472(%rsp)          # 8-byte Folded Reload
	je	.LBB147_5
# BB#6:                                 # %destructor_block
	cmpl	$0, 5696(%rsp)          # 4-byte Folded Reload
	je	.LBB147_5
# BB#7:                                 # %if.then.i.575
	xorl	%edi, %edi
	movq	5472(%rsp), %rsi        # 8-byte Reload
	callq	halide_free@PLT
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4816(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	xorl	%r14d, %r14d
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_8
.LBB147_5:
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4816(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	xorl	%edx, %edx
.LBB147_8:                              # %call_destructor.exit581
	movq	5408(%rsp), %rsi        # 8-byte Reload
	jmp	.LBB147_9
.LBB147_198:                            # %assert failed247
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	jmp	.LBB147_1473
.LBB147_196:                            # %assert failed245
	leaq	.Lstr.165(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	336(%rsp), %rdx         # 8-byte Reload
.LBB147_1472:                           # %call_destructor.exit.thread
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
.LBB147_1473:                           # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_227
.LBB147_200:                            # %assert failed249
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	leaq	.Lstr.166(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	608(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_203
.LBB147_202:                            # %assert failed251
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_203:                            # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_227
.LBB147_205:                            # %assert failed255
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_227
.LBB147_207:                            # %assert failed257
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	movq	%rax, %r13
	movq	%r13, 4696(%rsp)        # 8-byte Spill
	leaq	.Lstr.168(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	600(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_210
.LBB147_209:                            # %assert failed259
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_210:                            # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_226
.LBB147_212:                            # %assert failed263
	movq	%r14, 5608(%rsp)        # 8-byte Spill
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_226
.LBB147_214:                            # %assert failed265
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	movq	%r15, 4664(%rsp)        # 8-byte Spill
	leaq	.Lstr.170(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	584(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_217
.LBB147_216:                            # %assert failed267
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	movq	%r15, 4664(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_217:                            # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_226
.LBB147_219:                            # %assert failed269
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	movq	%r15, 4664(%rsp)        # 8-byte Spill
	movq	%r14, 5032(%rsp)        # 8-byte Spill
	leaq	.Lstr.171(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	592(%rsp), %rdx         # 8-byte Reload
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_222
.LBB147_221:                            # %assert failed271
	movq	%r12, 4704(%rsp)        # 8-byte Spill
	movq	%r15, 4664(%rsp)        # 8-byte Spill
	movq	%r14, 5032(%rsp)        # 8-byte Spill
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_222:                            # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	movq	1176(%rsp), %rcx        # 8-byte Reload
	jmp	.LBB147_226
.LBB147_225:                            # %assert failed275
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	xorl	%esi, %esi
	movl	%eax, %edi
	movq	1176(%rsp), %rcx        # 8-byte Reload
	movq	4656(%rsp), %rdx        # 8-byte Reload
.LBB147_226:                            # %call_destructor.exit.thread
	movq	4816(%rsp), %r12        # 8-byte Reload
	jmp	.LBB147_227
.LBB147_1:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_2
.LBB147_30:                             # %assert failed10
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_2
.LBB147_32:                             # %assert failed29
	leaq	.Lstr.139(%rip), %rsi
	jmp	.LBB147_2
.LBB147_34:                             # %assert failed48
	leaq	.Lstr.140(%rip), %rsi
	jmp	.LBB147_2
.LBB147_36:                             # %assert failed67
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_2
.LBB147_38:                             # %assert failed86
	leaq	.Lstr.142(%rip), %rsi
.LBB147_2:                              # %call_destructor.exit.thread
	xorl	%edi, %edi
	callq	halide_error_buffer_argument_is_null@PLT
	jmp	.LBB147_62
.LBB147_169:                            # %assert failed215
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_158
.LBB147_53:                             # %assert failed123
	leaq	.Lstr.143(%rip), %rsi
	jmp	.LBB147_54
.LBB147_56:                             # %assert failed125
	leaq	.Lstr.145(%rip), %rsi
	leaq	.Lstr.146(%rip), %rdx
	xorl	%edi, %edi
	movl	%ebx, %ecx
	movl	$2, %r8d
	jmp	.LBB147_61
.LBB147_58:                             # %assert failed127
	leaq	.Lstr.147(%rip), %rsi
	jmp	.LBB147_59
.LBB147_64:                             # %assert failed129
	leaq	.Lstr.149(%rip), %rsi
.LBB147_59:                             # %call_destructor.exit.thread
	leaq	.Lstr.148(%rip), %rdx
	xorl	%edi, %edi
	movl	$1, %r8d
	jmp	.LBB147_60
.LBB147_66:                             # %assert failed131
	leaq	.Lstr.150(%rip), %rsi
.LBB147_54:                             # %call_destructor.exit.thread
	leaq	.Lstr.144(%rip), %rdx
	xorl	%edi, %edi
	movl	$4, %r8d
	jmp	.LBB147_61
.LBB147_68:                             # %assert failed133
	leaq	.Lstr.151(%rip), %rsi
	leaq	.Lstr.144(%rip), %rdx
	xorl	%edi, %edi
	movl	$4, %r8d
.LBB147_60:                             # %call_destructor.exit.thread
	movl	%ebx, %ecx
.LBB147_61:                             # %call_destructor.exit.thread
	vzeroupper
	callq	halide_error_bad_elem_size@PLT
	jmp	.LBB147_62
.LBB147_71:                             # %assert failed135
	leal	-1(%r8,%rdx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	movq	%rdx, %r9
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$2, %r8d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_73:                             # %assert failed137
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	jmp	.LBB147_74
.LBB147_78:                             # %assert failed139
	leal	-1(%rsi,%r11), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.143(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	jmp	.LBB147_79
.LBB147_82:                             # %assert failed141
	movq	%rsi, %rcx
	leaq	.Lstr.143(%rip), %rsi
	jmp	.LBB147_93
.LBB147_85:                             # %assert failed143
	movl	4880(%rsp), %r8d        # 4-byte Reload
	addl	$-1, %r8d
	movl	4288(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4896(%rsp), %ecx        # 4-byte Reload
	movl	%r14d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_87:                             # %assert failed145
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	4840(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_90:                             # %assert failed147
	movl	4864(%rsp), %r8d        # 4-byte Reload
	addl	$-1, %r8d
	movl	1340(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.145(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	4872(%rsp), %ecx        # 4-byte Reload
	movl	%r15d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_92:                             # %assert failed149
	leaq	.Lstr.145(%rip), %rsi
	jmp	.LBB147_93
.LBB147_95:                             # %assert failed151
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1136(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_98:                             # %assert failed153
	movl	404(%rsp), %eax         # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	460(%rsp), %ecx         # 4-byte Reload
	movl	4912(%rsp), %r8d        # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_100:                            # %assert failed155
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%ebx, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_103:                            # %assert failed157
	leal	-1(%r8,%r12), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$2, %r8d
	movl	%r12d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_105:                            # %assert failed159
	leaq	.Lstr.147(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
.LBB147_74:                             # %call_destructor.exit.thread
	movl	%r8d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_108:                            # %assert failed161
	leal	-1(%rdx,%r11), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
.LBB147_79:                             # %call_destructor.exit.thread
	xorl	%ecx, %ecx
	movl	$2, %r8d
	movl	%r11d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_110:                            # %assert failed163
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	4640(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_113:                            # %assert failed165
	leal	-1(%rsi,%rdi), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	movq	%rdi, %r9
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$4095, %r8d             # imm = 0xFFF
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_115:                            # %assert failed167
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	4648(%rsp), %rcx        # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_117:                            # %assert failed169
	movl	5376(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4752(%rsp), %ecx        # 4-byte Reload
	movq	4680(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_119:                            # %assert failed171
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_122:                            # %assert failed173
	movl	4192(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	4960(%rsp), %ecx        # 4-byte Reload
	movl	5008(%rsp), %r8d        # 4-byte Reload
	movl	%r10d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_124:                            # %assert failed175
	leaq	.Lstr.150(%rip), %rsi
	movq	%rdi, %rcx
	jmp	.LBB147_93
.LBB147_126:                            # %assert failed177
	movl	4080(%rsp), %eax        # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.151(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	4736(%rsp), %ecx        # 4-byte Reload
	movq	5424(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_128:                            # %assert failed179
	leaq	.Lstr.151(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r10d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_130:                            # %assert failed181
	movl	1740(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, (%rsp)
	leaq	.Lstr.151(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r13d, %ecx
	movl	%eax, %r8d
	movq	1752(%rsp), %r9         # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_62
.LBB147_132:                            # %assert failed183
	leaq	.Lstr.151(%rip), %rsi
.LBB147_93:                             # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$1, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_62
.LBB147_134:                            # %assert failed185
	leaq	.Lstr.152(%rip), %rsi
	jmp	.LBB147_135
.LBB147_139:                            # %assert failed187
	leaq	.Lstr.154(%rip), %rsi
	jmp	.LBB147_140
.LBB147_142:                            # %assert failed189
	leaq	.Lstr.155(%rip), %rsi
	leaq	.Lstr.156(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	jmp	.LBB147_136
.LBB147_144:                            # %assert failed191
	leaq	.Lstr.157(%rip), %rsi
.LBB147_140:                            # %call_destructor.exit.thread
	leaq	.Lstr.153(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
	movl	%ebx, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_62
.LBB147_146:                            # %assert failed193
	leaq	.Lstr.158(%rip), %rsi
	leaq	.Lstr.159(%rip), %rcx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	movl	%r14d, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_62
.LBB147_148:                            # %assert failed195
	leaq	.Lstr.160(%rip), %rsi
	leaq	.Lstr.156(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	movl	%r11d, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_62
.LBB147_150:                            # %assert failed197
	leaq	.Lstr.161(%rip), %rsi
.LBB147_135:                            # %call_destructor.exit.thread
	leaq	.Lstr.153(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
.LBB147_136:                            # %call_destructor.exit.thread
	movl	%eax, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_62
.LBB147_152:                            # %assert failed199
	leaq	.Lstr.162(%rip), %rsi
	jmp	.LBB147_153
.LBB147_155:                            # %assert failed201
	leaq	.Lstr.163(%rip), %rsi
.LBB147_153:                            # %call_destructor.exit.thread
	leaq	.Lstr.153(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_62
.LBB147_157:                            # %assert failed205
	leaq	.Lstr.140(%rip), %rsi
	jmp	.LBB147_158
.LBB147_161:                            # %assert failed207
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_62
.LBB147_163:                            # %assert failed211
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_158
.LBB147_165:                            # %assert failed213
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_166
.LBB147_172:                            # %assert failed219
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_166
.LBB147_174:                            # %assert failed221
	leaq	(%rdx,%rdx,2), %rdx
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_166
.LBB147_176:                            # %assert failed225
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_158
.LBB147_178:                            # %assert failed227
	leaq	.Lstr.141(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbx, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_62
.LBB147_180:                            # %assert failed231
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_158
.LBB147_182:                            # %assert failed233
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_166
.LBB147_184:                            # %assert failed237
	leaq	.Lstr.139(%rip), %rsi
.LBB147_158:                            # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_62
.LBB147_186:                            # %assert failed239
	leaq	.Lstr.139(%rip), %rsi
.LBB147_166:                            # %call_destructor.exit.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_62
.LBB147_189:                            # %assert failed241
	leaq	.Lstr.164(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbx, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_62
.LBB147_1471:                           # %assert failed547
	leaq	.Lstr.173(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbx, %rdx
	jmp	.LBB147_1472
.LBB147_1510:                           # %assert failed555
	leaq	.Lstr.175(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r14, %rdx
	jmp	.LBB147_1511
.LBB147_192:                            # %assert failed243
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_62:                             # %call_destructor.exit.thread
	xorl	%esi, %esi
	movl	%eax, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 5032(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4664(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4704(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 4696(%rsp)        # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 5608(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
	jmp	.LBB147_227
.Lfunc_end147:
	.size	__sharpi, .Lfunc_end147-__sharpi

	.section	.text.sharpi,"ax",@progbits
	.globl	sharpi
	.align	16, 0x90
	.type	sharpi,@function
sharpi:                                 # @sharpi
# BB#0:                                 # %entry
	testq	%rdi, %rdi
	je	.LBB148_7
# BB#1:                                 # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB148_8
# BB#2:                                 # %assert succeeded11
	testq	%r8, %r8
	je	.LBB148_9
# BB#3:                                 # %assert succeeded30
	testq	%r9, %r9
	je	.LBB148_10
# BB#4:                                 # %assert succeeded49
	movq	80(%rsp), %rax
	testq	%rax, %rax
	je	.LBB148_11
# BB#5:                                 # %assert succeeded68
	movq	104(%rsp), %rax
	testq	%rax, %rax
	je	.LBB148_12
# BB#6:                                 # %assert succeeded87
	jmp	__sharpi@PLT            # TAILCALL
.LBB148_7:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_8:                              # %assert failed10
	leaq	.Lstr.138(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_9:                              # %assert failed29
	leaq	.Lstr.139(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_10:                             # %assert failed48
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_11:                             # %assert failed67
	leaq	.Lstr.141(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB148_12:                             # %assert failed86
	leaq	.Lstr.142(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.Lfunc_end148:
	.size	sharpi, .Lfunc_end148-sharpi

	.section	.text.sharpi_argv,"ax",@progbits
	.globl	sharpi_argv
	.align	16, 0x90
	.type	sharpi_argv,@function
sharpi_argv:                            # @sharpi_argv
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r14
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdi, %r10
	movq	(%r10), %rdi
	movq	8(%r10), %rcx
	movl	(%rcx), %esi
	movq	16(%r10), %rcx
	movl	(%rcx), %edx
	movq	24(%r10), %rcx
	movq	32(%r10), %r8
	movq	40(%r10), %rax
	vmovss	(%rax), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 108(%rsp)        # 4-byte Spill
	movq	48(%r10), %rax
	vmovss	(%rax), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	56(%r10), %rax
	vmovss	(%rax), %xmm2           # xmm2 = mem[0],zero,zero,zero
	movq	64(%r10), %rax
	vmovss	(%rax), %xmm3           # xmm3 = mem[0],zero,zero,zero
	movq	72(%r10), %rax
	vmovss	(%rax), %xmm4           # xmm4 = mem[0],zero,zero,zero
	movq	80(%r10), %rax
	vmovss	(%rax), %xmm5           # xmm5 = mem[0],zero,zero,zero
	movq	88(%r10), %rax
	vmovss	(%rax), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	96(%r10), %rax
	vmovss	(%rax), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	104(%r10), %rax
	vmovss	(%rax), %xmm8           # xmm8 = mem[0],zero,zero,zero
	movq	112(%r10), %rax
	vmovss	(%rax), %xmm9           # xmm9 = mem[0],zero,zero,zero
	movq	120(%r10), %rax
	vmovss	(%rax), %xmm10          # xmm10 = mem[0],zero,zero,zero
	movq	128(%r10), %rax
	vmovss	(%rax), %xmm11          # xmm11 = mem[0],zero,zero,zero
	movq	136(%r10), %rax
	vmovss	(%rax), %xmm12          # xmm12 = mem[0],zero,zero,zero
	movq	144(%r10), %rax
	vmovss	(%rax), %xmm13          # xmm13 = mem[0],zero,zero,zero
	movq	152(%r10), %rax
	vmovss	(%rax), %xmm14          # xmm14 = mem[0],zero,zero,zero
	movq	160(%r10), %rax
	vmovss	(%rax), %xmm15          # xmm15 = mem[0],zero,zero,zero
	movq	168(%r10), %r14
	movq	176(%r10), %r9
	movq	184(%r10), %r11
	movq	192(%r10), %rbx
	movq	200(%r10), %rax
	movl	(%rax), %ebp
	movq	208(%r10), %rax
	movzbl	(%rbx), %ebx
	vmovss	(%r14), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	%rax, 96(%rsp)
	movl	%ebp, 88(%rsp)
	movl	%ebx, 80(%rsp)
	movq	%r11, 72(%rsp)
	vmovss	%xmm0, 64(%rsp)
	vmovss	%xmm15, 56(%rsp)
	vmovss	%xmm14, 48(%rsp)
	vmovss	%xmm13, 40(%rsp)
	vmovss	%xmm12, 32(%rsp)
	vmovss	%xmm11, 24(%rsp)
	vmovss	%xmm10, 16(%rsp)
	vmovss	%xmm9, 8(%rsp)
	vmovss	%xmm8, (%rsp)
	vmovss	108(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	callq	sharpi@PLT
	addq	$112, %rsp
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end149:
	.size	sharpi_argv, .Lfunc_end149-sharpi_argv

	.section	.text.sharpi_metadata,"ax",@progbits
	.globl	sharpi_metadata
	.align	16, 0x90
	.type	sharpi_metadata,@function
sharpi_metadata:                        # @sharpi_metadata
# BB#0:                                 # %entry
	leaq	.Lsharpi_metadata_storage(%rip), %rax
	retq
.Lfunc_end150:
	.size	sharpi_metadata, .Lfunc_end150-sharpi_metadata

	.type	_ZN6Halide7Runtime8Internal13custom_mallocE,@object # @_ZN6Halide7Runtime8Internal13custom_mallocE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal13custom_mallocE
	.align	8
_ZN6Halide7Runtime8Internal13custom_mallocE:
	.quad	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.size	_ZN6Halide7Runtime8Internal13custom_mallocE, 8

	.type	_ZN6Halide7Runtime8Internal11custom_freeE,@object # @_ZN6Halide7Runtime8Internal11custom_freeE
	.weak	_ZN6Halide7Runtime8Internal11custom_freeE
	.align	8
_ZN6Halide7Runtime8Internal11custom_freeE:
	.quad	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.size	_ZN6Halide7Runtime8Internal11custom_freeE, 8

	.type	_ZN6Halide7Runtime8Internal13error_handlerE,@object # @_ZN6Halide7Runtime8Internal13error_handlerE
	.weak	_ZN6Halide7Runtime8Internal13error_handlerE
	.align	8
_ZN6Halide7Runtime8Internal13error_handlerE:
	.quad	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.size	_ZN6Halide7Runtime8Internal13error_handlerE, 8

	.type	.L.str,@object          # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"Error: "
	.size	.L.str, 8

	.type	_ZN6Halide7Runtime8Internal12custom_printE,@object # @_ZN6Halide7Runtime8Internal12custom_printE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal12custom_printE
	.align	8
_ZN6Halide7Runtime8Internal12custom_printE:
	.quad	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal12custom_printE, 8

	.type	halide_reference_clock_inited,@object # @halide_reference_clock_inited
	.bss
	.weak	halide_reference_clock_inited
halide_reference_clock_inited:
	.byte	0                       # 0x0
	.size	halide_reference_clock_inited, 1

	.type	halide_reference_clock,@object # @halide_reference_clock
	.weak	halide_reference_clock
	.align	8
halide_reference_clock:
	.zero	16
	.size	halide_reference_clock, 16

	.type	.L.str.7,@object        # @.str.7
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.7:
	.asciz	"/tmp/"
	.size	.L.str.7, 6

	.type	.L.str.1,@object        # @.str.1
.L.str.1:
	.asciz	"XXXXXX"
	.size	.L.str.1, 7

	.type	_ZN6Halide7Runtime8Internal10work_queueE,@object # @_ZN6Halide7Runtime8Internal10work_queueE
	.bss
	.weak	_ZN6Halide7Runtime8Internal10work_queueE
	.align	8
_ZN6Halide7Runtime8Internal10work_queueE:
	.zero	800
	.size	_ZN6Halide7Runtime8Internal10work_queueE, 800

	.type	custom_do_task,@object  # @custom_do_task
	.section	.data.rel,"aw",@progbits
	.weak	custom_do_task
	.align	8
custom_do_task:
	.quad	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.size	custom_do_task, 8

	.type	custom_do_par_for,@object # @custom_do_par_for
	.weak	custom_do_par_for
	.align	8
custom_do_par_for:
	.quad	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.size	custom_do_par_for, 8

	.section	.dtors,"aw",@progbits
	.align	8
	.quad	halide_thread_pool_cleanup
	.quad	halide_trace_cleanup
	.quad	halide_cache_cleanup
	.quad	halide_profiler_shutdown
	.type	.L.str.8,@object        # @.str.8
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.8:
	.asciz	"HL_NUM_THREADS"
	.size	.L.str.8, 15

	.type	.L.str.1.9,@object      # @.str.1.9
.L.str.1.9:
	.asciz	"HL_NUMTHREADS"
	.size	.L.str.1.9, 14

	.type	.L.str.2,@object        # @.str.2
.L.str.2:
	.asciz	"halide_set_num_threads: must be >= 0."
	.size	.L.str.2, 38

	.type	_ZN6Halide7Runtime8Internal17custom_get_symbolE,@object # @_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.align	8
_ZN6Halide7Runtime8Internal17custom_get_symbolE:
	.quad	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.size	_ZN6Halide7Runtime8Internal17custom_get_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal19custom_load_libraryE,@object # @_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.weak	_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.align	8
_ZN6Halide7Runtime8Internal19custom_load_libraryE:
	.quad	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.size	_ZN6Halide7Runtime8Internal19custom_load_libraryE, 8

	.type	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE,@object # @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.weak	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.align	8
_ZN6Halide7Runtime8Internal25custom_get_library_symbolE:
	.quad	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE,@object # @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.align	4
_ZN6Halide7Runtime8Internal17halide_gpu_deviceE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, 1

	.type	.L.str.10,@object       # @.str.10
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.10:
	.asciz	"HL_GPU_DEVICE"
	.size	.L.str.10, 14

	.type	_ZN6Halide7Runtime8Internal17halide_trace_fileE,@object # @_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.align	4
_ZN6Halide7Runtime8Internal17halide_trace_fileE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_trace_fileE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_trace_file_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE,@object # @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
	.weak	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, 1

	.type	_ZN6Halide7Runtime8Internal19halide_custom_traceE,@object # @_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.align	8
_ZN6Halide7Runtime8Internal19halide_custom_traceE:
	.quad	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.size	_ZN6Halide7Runtime8Internal19halide_custom_traceE, 8

	.type	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids
	.data
	.align	4
_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids:
	.long	1                       # 0x1
	.size	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids, 4

	.type	.L.str.14,@object       # @.str.14
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.14:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:59 Assert failed: written == total_size && \"Can't write to trace file\"\n"
	.size	.L.str.14, 112

	.type	.L.str.1.15,@object     # @.str.1.15
.L.str.1.15:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:68 Assert failed: print_bits <= 64 && \"Tracing bad type\"\n"
	.size	.L.str.1.15, 98

	.type	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types
	.section	.data.rel.ro.local,"aw",@progbits
	.align	8
.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types:
	.quad	.L.str.2.17
	.quad	.L.str.3
	.quad	.L.str.4
	.quad	.L.str.5
	.quad	.L.str.6
	.quad	.L.str.7.18
	.quad	.L.str.8.19
	.quad	.L.str.9
	.quad	.L.str.10.20
	.quad	.L.str.11
	.size	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types, 80

	.type	.L.str.15,@object       # @.str.15
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.15:
	.asciz	"<"
	.size	.L.str.15, 2

	.type	.L.str.16,@object       # @.str.16
.L.str.16:
	.asciz	">, <"
	.size	.L.str.16, 5

	.type	.L.str.17,@object       # @.str.17
.L.str.17:
	.asciz	", "
	.size	.L.str.17, 3

	.type	.L.str.18,@object       # @.str.18
.L.str.18:
	.asciz	">)"
	.size	.L.str.18, 3

	.type	.L.str.20,@object       # @.str.20
.L.str.20:
	.asciz	" = <"
	.size	.L.str.20, 5

	.type	.L.str.21,@object       # @.str.21
.L.str.21:
	.asciz	" = "
	.size	.L.str.21, 4

	.type	.L.str.22,@object       # @.str.22
.L.str.22:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:136 Assert failed: print_bits >= 16 && \"Tracing a bad type\"\n"
	.size	.L.str.22, 101

	.type	.L.str.23,@object       # @.str.23
.L.str.23:
	.asciz	">"
	.size	.L.str.23, 2

	.type	.L.str.25,@object       # @.str.25
.L.str.25:
	.asciz	"HL_TRACE_FILE"
	.size	.L.str.25, 14

	.type	.L.str.26,@object       # @.str.26
.L.str.26:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:194 Assert failed: (fd > 0) && \"Failed to open trace file\\n\"\n"
	.size	.L.str.26, 102

	.type	.L.str.2.17,@object     # @.str.2.17
.L.str.2.17:
	.asciz	"Load"
	.size	.L.str.2.17, 5

	.type	.L.str.3,@object        # @.str.3
.L.str.3:
	.asciz	"Store"
	.size	.L.str.3, 6

	.type	.L.str.4,@object        # @.str.4
.L.str.4:
	.asciz	"Begin realization"
	.size	.L.str.4, 18

	.type	.L.str.5,@object        # @.str.5
.L.str.5:
	.asciz	"End realization"
	.size	.L.str.5, 16

	.type	.L.str.6,@object        # @.str.6
.L.str.6:
	.asciz	"Produce"
	.size	.L.str.6, 8

	.type	.L.str.7.18,@object     # @.str.7.18
.L.str.7.18:
	.asciz	"End produce"
	.size	.L.str.7.18, 12

	.type	.L.str.8.19,@object     # @.str.8.19
.L.str.8.19:
	.asciz	"Consume"
	.size	.L.str.8.19, 8

	.type	.L.str.9,@object        # @.str.9
.L.str.9:
	.asciz	"End consume"
	.size	.L.str.9, 12

	.type	.L.str.10.20,@object    # @.str.10.20
.L.str.10.20:
	.asciz	"Begin pipeline"
	.size	.L.str.10.20, 15

	.type	.L.str.11,@object       # @.str.11
.L.str.11:
	.asciz	"End pipeline"
	.size	.L.str.11, 13

	.type	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE,@object # @_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.data
	.weak	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.align	2
_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE:
	.short	3                       # 0x3
	.short	3                       # 0x3
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.size	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE, 20

	.type	.L.str.27,@object       # @.str.27
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.27:
	.asciz	"wb"
	.size	.L.str.27, 3

	.type	_ZN6Halide7Runtime8Internal16memoization_lockE,@object # @_ZN6Halide7Runtime8Internal16memoization_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal16memoization_lockE
	.align	8
_ZN6Halide7Runtime8Internal16memoization_lockE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal16memoization_lockE, 64

	.type	_ZN6Halide7Runtime8Internal13cache_entriesE,@object # @_ZN6Halide7Runtime8Internal13cache_entriesE
	.weak	_ZN6Halide7Runtime8Internal13cache_entriesE
	.align	8
_ZN6Halide7Runtime8Internal13cache_entriesE:
	.zero	2048
	.size	_ZN6Halide7Runtime8Internal13cache_entriesE, 2048

	.type	_ZN6Halide7Runtime8Internal18most_recently_usedE,@object # @_ZN6Halide7Runtime8Internal18most_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal18most_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal18most_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal18most_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal19least_recently_usedE,@object # @_ZN6Halide7Runtime8Internal19least_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal19least_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal19least_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal19least_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal14max_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.data
	.weak	_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal14max_cache_sizeE:
	.quad	1048576                 # 0x100000
	.size	_ZN6Halide7Runtime8Internal14max_cache_sizeE, 8

	.type	_ZN6Halide7Runtime8Internal18current_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.bss
	.weak	_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal18current_cache_sizeE:
	.quad	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal18current_cache_sizeE, 8

	.type	.L.str.3.29,@object     # @.str.3.29
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.3.29:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:245 Assert failed: prev_hash_entry != NULL\n"
	.size	.L.str.3.29, 82

	.type	.L.str.4.30,@object     # @.str.4.30
.L.str.4.30:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:335 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.4.30, 85

	.type	.L.str.5.31,@object     # @.str.5.31
.L.str.5.31:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:339 Assert failed: least_recently_used == entry\n"
	.size	.L.str.5.31, 87

	.type	.L.str.6.32,@object     # @.str.6.32
.L.str.6.32:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:342 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.6.32, 85

	.type	.L.str.8.33,@object     # @.str.8.33
.L.str.8.33:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:433 Assert failed: no_host_pointers_equal\n"
	.size	.L.str.8.33, 81

	.type	.L.str.11.34,@object    # @.str.11.34
.L.str.11.34:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:518 Assert failed: entry->in_use_count > 0\n"
	.size	.L.str.11.34, 82

	.type	.L.str.45,@object       # @.str.45
.L.str.45:
	.asciz	"-nan"
	.size	.L.str.45, 5

	.type	.L.str.1.46,@object     # @.str.1.46
.L.str.1.46:
	.asciz	"nan"
	.size	.L.str.1.46, 4

	.type	.L.str.2.47,@object     # @.str.2.47
.L.str.2.47:
	.asciz	"-inf"
	.size	.L.str.2.47, 5

	.type	.L.str.3.48,@object     # @.str.3.48
.L.str.3.48:
	.asciz	"inf"
	.size	.L.str.3.48, 4

	.type	.L.str.4.49,@object     # @.str.4.49
.L.str.4.49:
	.asciz	"-0.000000e+00"
	.size	.L.str.4.49, 14

	.type	.L.str.5.50,@object     # @.str.5.50
.L.str.5.50:
	.asciz	"0.000000e+00"
	.size	.L.str.5.50, 13

	.type	.L.str.6.51,@object     # @.str.6.51
.L.str.6.51:
	.asciz	"-0.000000"
	.size	.L.str.6.51, 10

	.type	.L.str.7.52,@object     # @.str.7.52
.L.str.7.52:
	.asciz	"0.000000"
	.size	.L.str.7.52, 9

	.type	.L.str.8.53,@object     # @.str.8.53
.L.str.8.53:
	.asciz	"-"
	.size	.L.str.8.53, 2

	.type	.L.str.10.55,@object    # @.str.10.55
.L.str.10.55:
	.asciz	"e+"
	.size	.L.str.10.55, 3

	.type	.L.str.11.56,@object    # @.str.11.56
.L.str.11.56:
	.asciz	"e-"
	.size	.L.str.11.56, 3

	.type	.L.str.12.57,@object    # @.str.12.57
.L.str.12.57:
	.asciz	"0123456789abcdef"
	.size	.L.str.12.57, 17

	.type	_ZN6Halide7Runtime8Internal17device_copy_mutexE,@object # @_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.align	8
_ZN6Halide7Runtime8Internal17device_copy_mutexE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal17device_copy_mutexE, 64

	.type	.L.str.25.64,@object    # @.str.25.64
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.25.64:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:138 Assert failed: !buf->host_dirty\n"
	.size	.L.str.25.64, 86

	.type	.L.str.40,@object       # @.str.40
.L.str.40:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:248 Assert failed: buf->dev == 0\n"
	.size	.L.str.40, 83

	.type	.L.str.37,@object       # @.str.37
.L.str.37:
	.asciz	"halide_device_malloc doesn't support switching interfaces\n"
	.size	.L.str.37, 59

	.type	.L.str.42,@object       # @.str.42
.L.str.42:
	.asciz	"halide_device_and_host_malloc doesn't support switching interfaces\n"
	.size	.L.str.42, 68

	.type	.L.str.43,@object       # @.str.43
.L.str.43:
	.asciz	"allocating host and device memory failed\n"
	.size	.L.str.43, 42

	.type	.L.str.45.65,@object    # @.str.45.65
.L.str.45.65:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:322 Assert failed: buf->dev == 0\n"
	.size	.L.str.45.65, 83

	.type	.L.str.68,@object       # @.str.68
.L.str.68:
	.asciz	"Bounds inference call to external stage "
	.size	.L.str.68, 41

	.type	.L.str.1.69,@object     # @.str.1.69
.L.str.1.69:
	.asciz	" returned non-zero value: "
	.size	.L.str.1.69, 27

	.type	.L.str.53,@object       # @.str.53
.L.str.53:
	.asciz	"Printer buffer allocation failed.\n"
	.size	.L.str.53, 35

	.type	.L.str.2.70,@object     # @.str.2.70
.L.str.2.70:
	.asciz	"Call to external stage "
	.size	.L.str.2.70, 24

	.type	.L.str.3.71,@object     # @.str.3.71
.L.str.3.71:
	.asciz	"Bounds given for "
	.size	.L.str.3.71, 18

	.type	.L.str.4.72,@object     # @.str.4.72
.L.str.4.72:
	.asciz	" in "
	.size	.L.str.4.72, 5

	.type	.L.str.5.73,@object     # @.str.5.73
.L.str.5.73:
	.asciz	" (from "
	.size	.L.str.5.73, 8

	.type	.L.str.6.74,@object     # @.str.6.74
.L.str.6.74:
	.asciz	" to "
	.size	.L.str.6.74, 5

	.type	.L.str.7.75,@object     # @.str.7.75
.L.str.7.75:
	.asciz	") do not cover required region (from "
	.size	.L.str.7.75, 38

	.type	.L.str.8.76,@object     # @.str.8.76
.L.str.8.76:
	.asciz	")"
	.size	.L.str.8.76, 2

	.type	.L.str.9.77,@object     # @.str.9.77
.L.str.9.77:
	.asciz	" has type "
	.size	.L.str.9.77, 11

	.type	.L.str.10.78,@object    # @.str.10.78
.L.str.10.78:
	.asciz	" but elem_size of the buffer passed in is "
	.size	.L.str.10.78, 43

	.type	.L.str.11.79,@object    # @.str.11.79
.L.str.11.79:
	.asciz	" instead of "
	.size	.L.str.11.79, 13

	.type	.L.str.12.80,@object    # @.str.12.80
.L.str.12.80:
	.asciz	" is accessed at "
	.size	.L.str.12.80, 17

	.type	.L.str.13.81,@object    # @.str.13.81
.L.str.13.81:
	.asciz	", which is before the min ("
	.size	.L.str.13.81, 28

	.type	.L.str.14.82,@object    # @.str.14.82
.L.str.14.82:
	.asciz	") in dimension "
	.size	.L.str.14.82, 16

	.type	.L.str.15.83,@object    # @.str.15.83
.L.str.15.83:
	.asciz	", which is beyond the max ("
	.size	.L.str.15.83, 28

	.type	.L.str.16.84,@object    # @.str.16.84
.L.str.16.84:
	.asciz	"Total allocation for buffer "
	.size	.L.str.16.84, 29

	.type	.L.str.17.85,@object    # @.str.17.85
.L.str.17.85:
	.asciz	" is "
	.size	.L.str.17.85, 5

	.type	.L.str.18.86,@object    # @.str.18.86
.L.str.18.86:
	.asciz	", which exceeds the maximum size of "
	.size	.L.str.18.86, 37

	.type	.L.str.19.87,@object    # @.str.19.87
.L.str.19.87:
	.asciz	"The extents for buffer "
	.size	.L.str.19.87, 24

	.type	.L.str.20.88,@object    # @.str.20.88
.L.str.20.88:
	.asciz	" dimension "
	.size	.L.str.20.88, 12

	.type	.L.str.21.89,@object    # @.str.21.89
.L.str.21.89:
	.asciz	" is negative ("
	.size	.L.str.21.89, 15

	.type	.L.str.22.90,@object    # @.str.22.90
.L.str.22.90:
	.asciz	"Product of extents for buffer "
	.size	.L.str.22.90, 31

	.type	.L.str.23.91,@object    # @.str.23.91
.L.str.23.91:
	.asciz	"Applying the constraints on "
	.size	.L.str.23.91, 29

	.type	.L.str.24.92,@object    # @.str.24.92
.L.str.24.92:
	.asciz	" to the required region made it smaller. "
	.size	.L.str.24.92, 42

	.type	.L.str.25.93,@object    # @.str.25.93
.L.str.25.93:
	.asciz	"Required size: "
	.size	.L.str.25.93, 16

	.type	.L.str.26.94,@object    # @.str.26.94
.L.str.26.94:
	.asciz	". "
	.size	.L.str.26.94, 3

	.type	.L.str.27.95,@object    # @.str.27.95
.L.str.27.95:
	.asciz	"Constrained size: "
	.size	.L.str.27.95, 19

	.type	.L.str.28,@object       # @.str.28
.L.str.28:
	.asciz	"."
	.size	.L.str.28, 2

	.type	.L.str.29,@object       # @.str.29
.L.str.29:
	.asciz	"Constraint violated: "
	.size	.L.str.29, 22

	.type	.L.str.30,@object       # @.str.30
.L.str.30:
	.asciz	" ("
	.size	.L.str.30, 3

	.type	.L.str.31,@object       # @.str.31
.L.str.31:
	.asciz	") == "
	.size	.L.str.31, 6

	.type	.L.str.32,@object       # @.str.32
.L.str.32:
	.asciz	"Parameter "
	.size	.L.str.32, 11

	.type	.L.str.33,@object       # @.str.33
.L.str.33:
	.asciz	" but must be at least "
	.size	.L.str.33, 23

	.type	.L.str.34,@object       # @.str.34
.L.str.34:
	.asciz	" but must be at most "
	.size	.L.str.34, 22

	.type	.L.str.35,@object       # @.str.35
.L.str.35:
	.asciz	"Out of memory (halide_malloc returned NULL)"
	.size	.L.str.35, 44

	.type	.L.str.36,@object       # @.str.36
.L.str.36:
	.asciz	"Buffer argument "
	.size	.L.str.36, 17

	.type	.L.str.37.96,@object    # @.str.37.96
.L.str.37.96:
	.asciz	" is NULL"
	.size	.L.str.37.96, 9

	.type	.L.str.38,@object       # @.str.38
.L.str.38:
	.asciz	"Failed to dump function "
	.size	.L.str.38, 25

	.type	.L.str.39,@object       # @.str.39
.L.str.39:
	.asciz	" to file "
	.size	.L.str.39, 10

	.type	.L.str.40.97,@object    # @.str.40.97
.L.str.40.97:
	.asciz	" with error "
	.size	.L.str.40.97, 13

	.type	.L.str.41,@object       # @.str.41
.L.str.41:
	.asciz	"The host pointer of "
	.size	.L.str.41, 21

	.type	.L.str.42.98,@object    # @.str.42.98
.L.str.42.98:
	.asciz	" is not aligned to a "
	.size	.L.str.42.98, 22

	.type	.L.str.43.99,@object    # @.str.43.99
.L.str.43.99:
	.asciz	" bytes boundary."
	.size	.L.str.43.99, 17

	.type	.L.str.44,@object       # @.str.44
.L.str.44:
	.asciz	"The folded storage dimension "
	.size	.L.str.44, 30

	.type	.L.str.45.100,@object   # @.str.45.100
.L.str.45.100:
	.asciz	" of "
	.size	.L.str.45.100, 5

	.type	.L.str.46.101,@object   # @.str.46.101
.L.str.46.101:
	.asciz	" was accessed out of order by loop "
	.size	.L.str.46.101, 36

	.type	.L.str.47,@object       # @.str.47
.L.str.47:
	.asciz	"The fold factor ("
	.size	.L.str.47, 18

	.type	.L.str.48,@object       # @.str.48
.L.str.48:
	.asciz	") of dimension "
	.size	.L.str.48, 16

	.type	.L.str.49,@object       # @.str.49
.L.str.49:
	.asciz	" is too small to store the required region accessed by loop "
	.size	.L.str.49, 61

	.type	.L.str.50,@object       # @.str.50
.L.str.50:
	.asciz	")."
	.size	.L.str.50, 3

	.type	.L.str.51,@object       # @.str.51
.L.str.51:
	.asciz	"Requirement Failed: ("
	.size	.L.str.51, 22

	.type	.L.str.52,@object       # @.str.52
.L.str.52:
	.asciz	") "
	.size	.L.str.52, 3

	.type	_ZZ25halide_profiler_get_stateE1s,@object # @_ZZ25halide_profiler_get_stateE1s
	.data
	.align	8
_ZZ25halide_profiler_get_stateE1s:
	.zero	64
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	0                       # 0x0
	.long	0                       # 0x0
	.quad	0
	.quad	0
	.byte	0                       # 0x0
	.zero	7
	.size	_ZZ25halide_profiler_get_stateE1s, 104

	.type	.L.str.103,@object      # @.str.103
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.103:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:204 Assert failed: p_stats != NULL\n"
	.size	.L.str.103, 77

	.type	.L.str.1.104,@object    # @.str.1.104
.L.str.1.104:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:231 Assert failed: p_stats != NULL\n"
	.size	.L.str.1.104, 77

	.type	.L.str.2.105,@object    # @.str.2.105
.L.str.2.105:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:232 Assert failed: func_id >= 0\n"
	.size	.L.str.2.105, 74

	.type	.L.str.3.106,@object    # @.str.3.106
.L.str.3.106:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:233 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.3.106, 90

	.type	.L.str.4.107,@object    # @.str.4.107
.L.str.4.107:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:267 Assert failed: p_stats != NULL\n"
	.size	.L.str.4.107, 77

	.type	.L.str.5.108,@object    # @.str.5.108
.L.str.5.108:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:268 Assert failed: func_id >= 0\n"
	.size	.L.str.5.108, 74

	.type	.L.str.6.109,@object    # @.str.6.109
.L.str.6.109:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:269 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.6.109, 90

	.type	.L.str.7.110,@object    # @.str.7.110
.L.str.7.110:
	.asciz	"\n"
	.size	.L.str.7.110, 2

	.type	.L.str.8.111,@object    # @.str.8.111
.L.str.8.111:
	.asciz	" total time: "
	.size	.L.str.8.111, 14

	.type	.L.str.9.112,@object    # @.str.9.112
.L.str.9.112:
	.asciz	" ms"
	.size	.L.str.9.112, 4

	.type	.L.str.10.113,@object   # @.str.10.113
.L.str.10.113:
	.asciz	"  samples: "
	.size	.L.str.10.113, 12

	.type	.L.str.11.114,@object   # @.str.11.114
.L.str.11.114:
	.asciz	"  runs: "
	.size	.L.str.11.114, 9

	.type	.L.str.12.115,@object   # @.str.12.115
.L.str.12.115:
	.asciz	"  time/run: "
	.size	.L.str.12.115, 13

	.type	.L.str.13.116,@object   # @.str.13.116
.L.str.13.116:
	.asciz	" ms\n"
	.size	.L.str.13.116, 5

	.type	.L.str.14.117,@object   # @.str.14.117
.L.str.14.117:
	.asciz	" average threads used: "
	.size	.L.str.14.117, 24

	.type	.L.str.15.118,@object   # @.str.15.118
.L.str.15.118:
	.asciz	" heap allocations: "
	.size	.L.str.15.118, 20

	.type	.L.str.16.119,@object   # @.str.16.119
.L.str.16.119:
	.asciz	"  peak heap usage: "
	.size	.L.str.16.119, 20

	.type	.L.str.17.120,@object   # @.str.17.120
.L.str.17.120:
	.asciz	" bytes\n"
	.size	.L.str.17.120, 8

	.type	.L.str.18.121,@object   # @.str.18.121
.L.str.18.121:
	.asciz	"  "
	.size	.L.str.18.121, 3

	.type	.L.str.19.122,@object   # @.str.19.122
.L.str.19.122:
	.asciz	": "
	.size	.L.str.19.122, 3

	.type	.L.str.20.123,@object   # @.str.20.123
.L.str.20.123:
	.asciz	" "
	.size	.L.str.20.123, 2

	.type	.L.str.21.124,@object   # @.str.21.124
.L.str.21.124:
	.asciz	"ms"
	.size	.L.str.21.124, 3

	.type	.L.str.22.125,@object   # @.str.22.125
.L.str.22.125:
	.asciz	"("
	.size	.L.str.22.125, 2

	.type	.L.str.23.126,@object   # @.str.23.126
.L.str.23.126:
	.asciz	"%)"
	.size	.L.str.23.126, 3

	.type	.L.str.24.127,@object   # @.str.24.127
.L.str.24.127:
	.asciz	"threads: "
	.size	.L.str.24.127, 10

	.type	.L.str.25.128,@object   # @.str.25.128
.L.str.25.128:
	.asciz	" peak: "
	.size	.L.str.25.128, 8

	.type	.L.str.26.129,@object   # @.str.26.129
.L.str.26.129:
	.asciz	" num: "
	.size	.L.str.26.129, 7

	.type	.L.str.27.130,@object   # @.str.27.130
.L.str.27.130:
	.asciz	" avg: "
	.size	.L.str.27.130, 7

	.type	.L.str.28.131,@object   # @.str.28.131
.L.str.28.131:
	.asciz	" stack: "
	.size	.L.str.28.131, 9

	.type	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE,@object # @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.align	8
_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE:
	.quad	halide_default_can_use_target_features
	.size	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, 8

	.type	_ZZ38halide_default_can_use_target_featuresE11initialized,@object # @_ZZ38halide_default_can_use_target_featuresE11initialized
	.local	_ZZ38halide_default_can_use_target_featuresE11initialized
	.comm	_ZZ38halide_default_can_use_target_featuresE11initialized,1,1
	.type	_ZZ38halide_default_can_use_target_featuresE12cpu_features,@object # @_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.local	_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.comm	_ZZ38halide_default_can_use_target_featuresE12cpu_features,16,8
	.type	.Lstr,@object           # @str
	.section	.rodata,"a",@progbits
	.align	32
.Lstr:
	.asciz	"p0"
	.size	.Lstr, 3

	.type	.Lstr.138,@object       # @str.138
	.align	32
.Lstr.138:
	.asciz	"vignetteH"
	.size	.Lstr.138, 10

	.type	.Lstr.139,@object       # @str.139
	.align	32
.Lstr.139:
	.asciz	"vignetteV"
	.size	.Lstr.139, 10

	.type	.Lstr.140,@object       # @str.140
	.align	32
.Lstr.140:
	.asciz	"ccm"
	.size	.Lstr.140, 4

	.type	.Lstr.141,@object       # @str.141
	.align	32
.Lstr.141:
	.asciz	"toneTable"
	.size	.Lstr.141, 10

	.type	.Lstr.142,@object       # @str.142
	.align	32
.Lstr.142:
	.asciz	"sharpi"
	.size	.Lstr.142, 7

	.type	.Lstr.143,@object       # @str.143
	.align	32
.Lstr.143:
	.asciz	"Input buffer ccm"
	.size	.Lstr.143, 17

	.type	.Lstr.144,@object       # @str.144
	.align	32
.Lstr.144:
	.asciz	"float32"
	.size	.Lstr.144, 8

	.type	.Lstr.145,@object       # @str.145
	.align	32
.Lstr.145:
	.asciz	"Input buffer p0"
	.size	.Lstr.145, 16

	.type	.Lstr.146,@object       # @str.146
	.align	32
.Lstr.146:
	.asciz	"uint16"
	.size	.Lstr.146, 7

	.type	.Lstr.147,@object       # @str.147
	.align	32
.Lstr.147:
	.asciz	"Output buffer sharpi"
	.size	.Lstr.147, 21

	.type	.Lstr.148,@object       # @str.148
	.align	32
.Lstr.148:
	.asciz	"uint8"
	.size	.Lstr.148, 6

	.type	.Lstr.149,@object       # @str.149
	.align	32
.Lstr.149:
	.asciz	"Input buffer toneTable"
	.size	.Lstr.149, 23

	.type	.Lstr.150,@object       # @str.150
	.align	32
.Lstr.150:
	.asciz	"Input buffer vignetteH"
	.size	.Lstr.150, 23

	.type	.Lstr.151,@object       # @str.151
	.align	32
.Lstr.151:
	.asciz	"Input buffer vignetteV"
	.size	.Lstr.151, 23

	.type	.Lstr.152,@object       # @str.152
	.align	32
.Lstr.152:
	.asciz	"ccm.stride.0"
	.size	.Lstr.152, 13

	.type	.Lstr.153,@object       # @str.153
	.align	32
.Lstr.153:
	.asciz	"1"
	.size	.Lstr.153, 2

	.type	.Lstr.154,@object       # @str.154
	.align	32
.Lstr.154:
	.asciz	"p0.stride.0"
	.size	.Lstr.154, 12

	.type	.Lstr.155,@object       # @str.155
	.align	32
.Lstr.155:
	.asciz	"sharpi.stride.0"
	.size	.Lstr.155, 16

	.type	.Lstr.156,@object       # @str.156
	.align	32
.Lstr.156:
	.asciz	"3"
	.size	.Lstr.156, 2

	.type	.Lstr.157,@object       # @str.157
	.align	32
.Lstr.157:
	.asciz	"sharpi.stride.2"
	.size	.Lstr.157, 16

	.type	.Lstr.158,@object       # @str.158
	.align	32
.Lstr.158:
	.asciz	"sharpi.min.2"
	.size	.Lstr.158, 13

	.type	.Lstr.159,@object       # @str.159
	.align	32
.Lstr.159:
	.asciz	"0"
	.size	.Lstr.159, 2

	.type	.Lstr.160,@object       # @str.160
	.align	32
.Lstr.160:
	.asciz	"sharpi.extent.2"
	.size	.Lstr.160, 16

	.type	.Lstr.161,@object       # @str.161
	.align	32
.Lstr.161:
	.asciz	"toneTable.stride.0"
	.size	.Lstr.161, 19

	.type	.Lstr.162,@object       # @str.162
	.align	32
.Lstr.162:
	.asciz	"vignetteH.stride.0"
	.size	.Lstr.162, 19

	.type	.Lstr.163,@object       # @str.163
	.align	32
.Lstr.163:
	.asciz	"vignetteV.stride.0"
	.size	.Lstr.163, 19

	.type	.Lstr.164,@object       # @str.164
	.align	32
.Lstr.164:
	.asciz	"f0"
	.size	.Lstr.164, 3

	.type	.Lstr.165,@object       # @str.165
	.align	32
.Lstr.165:
	.asciz	"deinterleaved$1"
	.size	.Lstr.165, 16

	.type	.Lstr.166,@object       # @str.166
	.align	32
.Lstr.166:
	.asciz	"gH"
	.size	.Lstr.166, 3

	.type	.Lstr.168,@object       # @str.168
	.align	32
.Lstr.168:
	.asciz	"dV"
	.size	.Lstr.168, 3

	.type	.Lstr.170,@object       # @str.170
	.align	32
.Lstr.170:
	.asciz	"f4"
	.size	.Lstr.170, 3

	.type	.Lstr.171,@object       # @str.171
	.align	32
.Lstr.171:
	.asciz	"f7"
	.size	.Lstr.171, 3

	.type	.Lstr.173,@object       # @str.173
	.align	32
.Lstr.173:
	.asciz	"transpose"
	.size	.Lstr.173, 10

	.type	.Lstr.174,@object       # @str.174
	.align	32
.Lstr.174:
	.asciz	"blur"
	.size	.Lstr.174, 5

	.type	.Lstr.175,@object       # @str.175
	.align	32
.Lstr.175:
	.asciz	"transpose$1"
	.size	.Lstr.175, 12

	.type	.Lstr.176,@object       # @str.176
	.align	32
.Lstr.176:
	.asciz	"blur$1"
	.size	.Lstr.176, 7

	.type	.Lstr.177,@object       # @str.177
	.align	32
.Lstr.177:
	.asciz	"p1"
	.size	.Lstr.177, 3

	.type	.L__unnamed_1,@object   # @0
	.align	4
.L__unnamed_1:
	.long	0                       # 0x0
	.size	.L__unnamed_1, 4

	.type	.Lstr.178,@object       # @str.178
	.align	32
.Lstr.178:
	.asciz	"p2"
	.size	.Lstr.178, 3

	.type	.L__unnamed_2,@object   # @1
	.align	4
.L__unnamed_2:
	.long	0                       # 0x0
	.size	.L__unnamed_2, 4

	.type	.Lstr.179,@object       # @str.179
	.align	32
.Lstr.179:
	.asciz	"blackLevelR"
	.size	.Lstr.179, 12

	.type	.L__unnamed_3,@object   # @2
	.align	4
.L__unnamed_3:
	.long	0                       # float 0
	.size	.L__unnamed_3, 4

	.type	.Lstr.180,@object       # @str.180
	.align	32
.Lstr.180:
	.asciz	"blackLevelG"
	.size	.Lstr.180, 12

	.type	.L__unnamed_4,@object   # @3
	.align	4
.L__unnamed_4:
	.long	0                       # float 0
	.size	.L__unnamed_4, 4

	.type	.Lstr.181,@object       # @str.181
	.align	32
.Lstr.181:
	.asciz	"blackLevelB"
	.size	.Lstr.181, 12

	.type	.L__unnamed_5,@object   # @4
	.align	4
.L__unnamed_5:
	.long	0                       # float 0
	.size	.L__unnamed_5, 4

	.type	.Lstr.182,@object       # @str.182
	.align	32
.Lstr.182:
	.asciz	"whiteBalanceGainR"
	.size	.Lstr.182, 18

	.type	.L__unnamed_6,@object   # @5
	.align	4
.L__unnamed_6:
	.long	0                       # float 0
	.size	.L__unnamed_6, 4

	.type	.Lstr.183,@object       # @str.183
	.align	32
.Lstr.183:
	.asciz	"whiteBalanceGainG"
	.size	.Lstr.183, 18

	.type	.L__unnamed_7,@object   # @6
	.align	4
.L__unnamed_7:
	.long	0                       # float 0
	.size	.L__unnamed_7, 4

	.type	.Lstr.184,@object       # @str.184
	.align	32
.Lstr.184:
	.asciz	"whiteBalanceGainB"
	.size	.Lstr.184, 18

	.type	.L__unnamed_8,@object   # @7
	.align	4
.L__unnamed_8:
	.long	0                       # float 0
	.size	.L__unnamed_8, 4

	.type	.Lstr.185,@object       # @str.185
	.align	32
.Lstr.185:
	.asciz	"clampMinR"
	.size	.Lstr.185, 10

	.type	.L__unnamed_9,@object   # @8
	.align	4
.L__unnamed_9:
	.long	0                       # float 0
	.size	.L__unnamed_9, 4

	.type	.Lstr.186,@object       # @str.186
	.align	32
.Lstr.186:
	.asciz	"clampMinG"
	.size	.Lstr.186, 10

	.type	.L__unnamed_10,@object  # @9
	.align	4
.L__unnamed_10:
	.long	0                       # float 0
	.size	.L__unnamed_10, 4

	.type	.Lstr.187,@object       # @str.187
	.align	32
.Lstr.187:
	.asciz	"clampMinB"
	.size	.Lstr.187, 10

	.type	.L__unnamed_11,@object  # @10
	.align	4
.L__unnamed_11:
	.long	0                       # float 0
	.size	.L__unnamed_11, 4

	.type	.Lstr.188,@object       # @str.188
	.align	32
.Lstr.188:
	.asciz	"clampMaxR"
	.size	.Lstr.188, 10

	.type	.L__unnamed_12,@object  # @11
	.align	4
.L__unnamed_12:
	.long	0                       # float 0
	.size	.L__unnamed_12, 4

	.type	.Lstr.189,@object       # @str.189
	.align	32
.Lstr.189:
	.asciz	"clampMaxG"
	.size	.Lstr.189, 10

	.type	.L__unnamed_13,@object  # @12
	.align	4
.L__unnamed_13:
	.long	0                       # float 0
	.size	.L__unnamed_13, 4

	.type	.Lstr.190,@object       # @str.190
	.align	32
.Lstr.190:
	.asciz	"clampMaxB"
	.size	.Lstr.190, 10

	.type	.L__unnamed_14,@object  # @13
	.align	4
.L__unnamed_14:
	.long	0                       # float 0
	.size	.L__unnamed_14, 4

	.type	.Lstr.191,@object       # @str.191
	.align	32
.Lstr.191:
	.asciz	"sharpenningR"
	.size	.Lstr.191, 13

	.type	.L__unnamed_15,@object  # @14
	.align	4
.L__unnamed_15:
	.long	0                       # float 0
	.size	.L__unnamed_15, 4

	.type	.Lstr.192,@object       # @str.192
	.align	32
.Lstr.192:
	.asciz	"sharpenningG"
	.size	.Lstr.192, 13

	.type	.L__unnamed_16,@object  # @15
	.align	4
.L__unnamed_16:
	.long	0                       # float 0
	.size	.L__unnamed_16, 4

	.type	.Lstr.193,@object       # @str.193
	.align	32
.Lstr.193:
	.asciz	"sharpenninngB"
	.size	.Lstr.193, 14

	.type	.L__unnamed_17,@object  # @16
	.align	4
.L__unnamed_17:
	.long	0                       # float 0
	.size	.L__unnamed_17, 4

	.type	.Lstr.194,@object       # @str.194
	.align	32
.Lstr.194:
	.asciz	"sharpeningSupport"
	.size	.Lstr.194, 18

	.type	.L__unnamed_18,@object  # @17
	.align	4
.L__unnamed_18:
	.long	0                       # float 0
	.size	.L__unnamed_18, 4

	.type	.Lstr.195,@object       # @str.195
	.align	32
.Lstr.195:
	.asciz	"noiseCore"
	.size	.Lstr.195, 10

	.type	.L__unnamed_19,@object  # @18
	.align	4
.L__unnamed_19:
	.long	0                       # float 0
	.size	.L__unnamed_19, 4

	.type	.Lstr.196,@object       # @str.196
	.align	32
.Lstr.196:
	.asciz	"p3"
	.size	.Lstr.196, 3

	.type	.L__unnamed_20,@object  # @19
.L__unnamed_20:
	.byte	0                       # 0x0
	.size	.L__unnamed_20, 1

	.type	.Lstr.197,@object       # @str.197
	.align	32
.Lstr.197:
	.asciz	"bayerPattern"
	.size	.Lstr.197, 13

	.type	.L__unnamed_21,@object  # @20
	.align	4
.L__unnamed_21:
	.long	0                       # 0x0
	.size	.L__unnamed_21, 4

	.type	.L__unnamed_22,@object  # @21
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.L__unnamed_22:
	.quad	.Lstr
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	16                      # 0x10
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.177
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_1
	.quad	0
	.quad	0
	.quad	.Lstr.178
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_2
	.quad	0
	.quad	0
	.quad	.Lstr.138
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.139
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.179
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_3
	.quad	0
	.quad	0
	.quad	.Lstr.180
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_4
	.quad	0
	.quad	0
	.quad	.Lstr.181
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_5
	.quad	0
	.quad	0
	.quad	.Lstr.182
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_6
	.quad	0
	.quad	0
	.quad	.Lstr.183
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_7
	.quad	0
	.quad	0
	.quad	.Lstr.184
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_8
	.quad	0
	.quad	0
	.quad	.Lstr.185
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_9
	.quad	0
	.quad	0
	.quad	.Lstr.186
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_10
	.quad	0
	.quad	0
	.quad	.Lstr.187
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_11
	.quad	0
	.quad	0
	.quad	.Lstr.188
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_12
	.quad	0
	.quad	0
	.quad	.Lstr.189
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_13
	.quad	0
	.quad	0
	.quad	.Lstr.190
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_14
	.quad	0
	.quad	0
	.quad	.Lstr.191
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_15
	.quad	0
	.quad	0
	.quad	.Lstr.192
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_16
	.quad	0
	.quad	0
	.quad	.Lstr.193
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_17
	.quad	0
	.quad	0
	.quad	.Lstr.194
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_18
	.quad	0
	.quad	0
	.quad	.Lstr.195
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_19
	.quad	0
	.quad	0
	.quad	.Lstr.140
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.141
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	8                       # 0x8
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.196
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	1                       # 0x1
	.byte	1                       # 0x1
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_20
	.quad	0
	.quad	0
	.quad	.Lstr.197
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_21
	.quad	0
	.quad	0
	.quad	.Lstr.142
	.long	2                       # 0x2
	.long	3                       # 0x3
	.byte	1                       # 0x1
	.byte	8                       # 0x8
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.size	.L__unnamed_22, 1296

	.type	.Lstr.198,@object       # @str.198
	.section	.rodata,"a",@progbits
	.align	32
.Lstr.198:
	.asciz	"x86-64-linux-avx-avx2-f16c-fma-sse41"
	.size	.Lstr.198, 37

	.type	.Lsharpi_metadata_storage,@object # @sharpi_metadata_storage
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.Lsharpi_metadata_storage:
	.long	0                       # 0x0
	.long	27                      # 0x1b
	.quad	.L__unnamed_22
	.quad	.Lstr.198
	.quad	.Lstr.142
	.size	.Lsharpi_metadata_storage, 32


	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.ident	"clang version 3.7.1 (branches/release_37 297356)"
	.section	".note.GNU-stack","",@progbits
