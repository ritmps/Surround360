	.text
	.file	"sharpi"
	.section	.text._ZN6Halide7Runtime8Internal14default_mallocEPvm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal14default_mallocEPvm,@function
_ZN6Halide7Runtime8Internal14default_mallocEPvm: # @_ZN6Halide7Runtime8Internal14default_mallocEPvm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$-128, %rsi
	movq	%rsi, %rdi
	callq	malloc@PLT
	movq	%rax, %rcx
	xorl	%eax, %eax
	testq	%rcx, %rcx
	je	.LBB0_2
# BB#1:                                 # %if.end
	movq	%rcx, %rax
	addq	$135, %rax
	andq	$-128, %rax
	movq	%rcx, -8(%rax)
.LBB0_2:                                # %cleanup
	popq	%rbp
	retq
.Lfunc_end0:
	.size	_ZN6Halide7Runtime8Internal14default_mallocEPvm, .Lfunc_end0-_ZN6Halide7Runtime8Internal14default_mallocEPvm

	.section	.text._ZN6Halide7Runtime8Internal12default_freeEPvS2_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12default_freeEPvS2_,@function
_ZN6Halide7Runtime8Internal12default_freeEPvS2_: # @_ZN6Halide7Runtime8Internal12default_freeEPvS2_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	-8(%rsi), %rdi
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end1:
	.size	_ZN6Halide7Runtime8Internal12default_freeEPvS2_, .Lfunc_end1-_ZN6Halide7Runtime8Internal12default_freeEPvS2_

	.section	.text.halide_set_custom_malloc,"ax",@progbits
	.weak	halide_set_custom_malloc
	.align	16, 0x90
	.type	halide_set_custom_malloc,@function
halide_set_custom_malloc:               # @halide_set_custom_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end2:
	.size	halide_set_custom_malloc, .Lfunc_end2-halide_set_custom_malloc

	.section	.text.halide_set_custom_free,"ax",@progbits
	.weak	halide_set_custom_free
	.align	16, 0x90
	.type	halide_set_custom_free,@function
halide_set_custom_free:                 # @halide_set_custom_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end3:
	.size	halide_set_custom_free, .Lfunc_end3-halide_set_custom_free

	.section	.text.halide_malloc,"ax",@progbits
	.weak	halide_malloc
	.align	16, 0x90
	.type	halide_malloc,@function
halide_malloc:                          # @halide_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13custom_mallocE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end4:
	.size	halide_malloc, .Lfunc_end4-halide_malloc

	.section	.text.halide_free,"ax",@progbits
	.weak	halide_free
	.align	16, 0x90
	.type	halide_free,@function
halide_free:                            # @halide_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal11custom_freeE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end5:
	.size	halide_free, .Lfunc_end5-halide_free

	.section	.text._ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc,@function
_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc: # @_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$4096, %rsp             # imm = 0x1000
	movq	%rsi, %rbx
	movq	%rdi, %r15
	leaq	-34(%rbp), %r12
	leaq	.L.str(%rip), %rdx
	leaq	-4128(%rbp), %r14
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movzbl	-1(%rax), %ecx
	cmpl	$10, %ecx
	je	.LBB6_2
# BB#1:                                 # %if.then
	movw	$10, (%rax)
	addq	$1, %rax
.LBB6_2:                                # %if.end
	movl	$1, %edx
	subq	%r14, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	addq	$4096, %rsp             # imm = 0x1000
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end6:
	.size	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc, .Lfunc_end6-_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc

	.section	.text.halide_error,"ax",@progbits
	.weak	halide_error
	.align	16, 0x90
	.type	halide_error,@function
halide_error:                           # @halide_error
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end7:
	.size	halide_error, .Lfunc_end7-halide_error

	.section	.text.halide_set_error_handler,"ax",@progbits
	.weak	halide_set_error_handler
	.align	16, 0x90
	.type	halide_set_error_handler,@function
halide_set_error_handler:               # @halide_set_error_handler
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal13error_handlerE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end8:
	.size	halide_set_error_handler, .Lfunc_end8-halide_set_error_handler

	.section	.text.halide_print,"ax",@progbits
	.weak	halide_print
	.align	16, 0x90
	.type	halide_print,@function
halide_print:                           # @halide_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end9:
	.size	halide_print, .Lfunc_end9-halide_print

	.section	.text.halide_set_custom_print,"ax",@progbits
	.weak	halide_set_custom_print
	.align	16, 0x90
	.type	halide_set_custom_print,@function
halide_set_custom_print:                # @halide_set_custom_print
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal12custom_printE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end10:
	.size	halide_set_custom_print, .Lfunc_end10-halide_set_custom_print

	.section	.text.halide_start_clock,"ax",@progbits
	.weak	halide_start_clock
	.align	16, 0x90
	.type	halide_start_clock,@function
halide_start_clock:                     # @halide_start_clock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	halide_reference_clock_inited@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	jne	.LBB11_2
# BB#1:                                 # %if.then
	movq	halide_reference_clock@GOTPCREL(%rip), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movb	$1, (%rbx)
.LBB11_2:                               # %if.end
	xorl	%eax, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end11:
	.size	halide_start_clock, .Lfunc_end11-halide_start_clock

	.section	.text.halide_current_time_ns,"ax",@progbits
	.weak	halide_current_time_ns
	.align	16, 0x90
	.type	halide_current_time_ns,@function
halide_current_time_ns:                 # @halide_current_time_ns
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$16, %rsp
	leaq	-16(%rbp), %rdx
	movl	$228, %edi
	xorl	%esi, %esi
	xorl	%eax, %eax
	callq	syscall@PLT
	movq	halide_reference_clock@GOTPCREL(%rip), %rcx
	movq	-16(%rbp), %rdx
	movq	-8(%rbp), %rax
	subq	(%rcx), %rdx
	imulq	$1000000000, %rdx, %rdx # imm = 0x3B9ACA00
	subq	8(%rcx), %rax
	addq	%rdx, %rax
	addq	$16, %rsp
	popq	%rbp
	retq
.Lfunc_end12:
	.size	halide_current_time_ns, .Lfunc_end12-halide_current_time_ns

	.section	.text.halide_sleep_ms,"ax",@progbits
	.weak	halide_sleep_ms
	.align	16, 0x90
	.type	halide_sleep_ms,@function
halide_sleep_ms:                        # @halide_sleep_ms
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	imull	$1000, %esi, %edi       # imm = 0x3E8
	popq	%rbp
	jmp	usleep@PLT              # TAILCALL
.Lfunc_end13:
	.size	halide_sleep_ms, .Lfunc_end13-halide_sleep_ms

	.section	.text._ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc,@function
_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc: # @_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rbx, %rdi
	callq	strlen@PLT
	movl	$2, %edi
	movq	%rbx, %rsi
	movq	%rax, %rdx
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	write@PLT               # TAILCALL
.Lfunc_end14:
	.size	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc, .Lfunc_end14-_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc

	.section	.text.halide_create_temp_file,"ax",@progbits
	.weak	halide_create_temp_file
	.align	16, 0x90
	.type	halide_create_temp_file,@function
halide_create_temp_file:                # @halide_create_temp_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%r8, %r13
	movq	%rdx, %r12
	movq	%rsi, %rbx
	movl	$-22, %edx
	testq	%rbx, %rbx
	je	.LBB15_7
# BB#1:                                 # %entry
	testq	%r12, %r12
	je	.LBB15_7
# BB#2:                                 # %entry
	testq	%rcx, %rcx
	je	.LBB15_7
# BB#3:                                 # %if.end
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	leaq	.L.str.7(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	%rbx, %rdi
	callq	strlen@PLT
	movq	%rax, %r14
	leaq	.L.str.1(%rip), %rdi
	callq	strlen@PLT
	movq	%rax, %r15
	movq	%r12, %rdi
	callq	strlen@PLT
	addq	-48(%rbp), %r14         # 8-byte Folded Reload
	addq	%r15, %r14
	leaq	1(%rax,%r14), %rax
	cmpq	%r13, %rax
	jbe	.LBB15_5
# BB#4:
	movl	$-22, %edx
	jmp	.LBB15_7
.LBB15_5:                               # %if.end.11
	movq	-56(%rbp), %r15         # 8-byte Reload
	leaq	-1(%r15,%r13), %r14
	leaq	.L.str.7(%rip), %rdx
	movq	%r15, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1(%rip), %rdx
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movb	$0, (%rax)
	movq	%r12, %rdi
	callq	strlen@PLT
	movq	%r15, %rdi
	movl	%eax, %esi
	callq	mkstemps@PLT
	cmpl	$-1, %eax
	movl	$-22, %edx
	je	.LBB15_7
# BB#6:                                 # %if.end.21
	movl	%eax, %edi
	callq	close@PLT
	xorl	%edx, %edx
.LBB15_7:                               # %return
	movl	%edx, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end15:
	.size	halide_create_temp_file, .Lfunc_end15-halide_create_temp_file

	.section	.text.halide_host_cpu_count,"ax",@progbits
	.weak	halide_host_cpu_count
	.align	16, 0x90
	.type	halide_host_cpu_count,@function
halide_host_cpu_count:                  # @halide_host_cpu_count
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$84, %edi
	popq	%rbp
	jmp	sysconf@PLT             # TAILCALL
.Lfunc_end16:
	.size	halide_host_cpu_count, .Lfunc_end16-halide_host_cpu_count

	.section	.text._ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv,@function
_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv: # @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	movq	8(%rax), %rdi
	callq	*(%rax)
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end17:
	.size	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv, .Lfunc_end17-_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv

	.section	.text.halide_spawn_thread,"ax",@progbits
	.weak	halide_spawn_thread
	.align	16, 0x90
	.type	halide_spawn_thread,@function
halide_spawn_thread:                    # @halide_spawn_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %r15
	movl	$24, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	movq	%r15, (%rbx)
	movq	%r14, 8(%rbx)
	leaq	16(%rbx), %rdi
	movq	$0, 16(%rbx)
	movq	_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv@GOTPCREL(%rip), %rdx
	xorl	%esi, %esi
	movq	%rbx, %rcx
	callq	pthread_create@PLT
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end18:
	.size	halide_spawn_thread, .Lfunc_end18-halide_spawn_thread

	.section	.text.halide_join_thread,"ax",@progbits
	.weak	halide_join_thread
	.align	16, 0x90
	.type	halide_join_thread,@function
halide_join_thread:                     # @halide_join_thread
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	$0, -16(%rbp)
	movq	16(%rbx), %rdi
	leaq	-16(%rbp), %rsi
	callq	pthread_join@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end19:
	.size	halide_join_thread, .Lfunc_end19-halide_join_thread

	.section	.text.halide_mutex_lock,"ax",@progbits
	.weak	halide_mutex_lock
	.align	16, 0x90
	.type	halide_mutex_lock,@function
halide_mutex_lock:                      # @halide_mutex_lock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_lock@PLT  # TAILCALL
.Lfunc_end20:
	.size	halide_mutex_lock, .Lfunc_end20-halide_mutex_lock

	.section	.text.halide_mutex_unlock,"ax",@progbits
	.weak	halide_mutex_unlock
	.align	16, 0x90
	.type	halide_mutex_unlock,@function
halide_mutex_unlock:                    # @halide_mutex_unlock
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_mutex_unlock@PLT # TAILCALL
.Lfunc_end21:
	.size	halide_mutex_unlock, .Lfunc_end21-halide_mutex_unlock

	.section	.text.halide_mutex_destroy,"ax",@progbits
	.weak	halide_mutex_destroy
	.align	16, 0x90
	.type	halide_mutex_destroy,@function
halide_mutex_destroy:                   # @halide_mutex_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	pthread_mutex_destroy@PLT
	xorl	%esi, %esi
	movl	$64, %edx
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	memset@PLT              # TAILCALL
.Lfunc_end22:
	.size	halide_mutex_destroy, .Lfunc_end22-halide_mutex_destroy

	.section	.text.halide_cond_init,"ax",@progbits
	.weak	halide_cond_init
	.align	16, 0x90
	.type	halide_cond_init,@function
halide_cond_init:                       # @halide_cond_init
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%esi, %esi
	popq	%rbp
	jmp	pthread_cond_init@PLT   # TAILCALL
.Lfunc_end23:
	.size	halide_cond_init, .Lfunc_end23-halide_cond_init

	.section	.text.halide_cond_destroy,"ax",@progbits
	.weak	halide_cond_destroy
	.align	16, 0x90
	.type	halide_cond_destroy,@function
halide_cond_destroy:                    # @halide_cond_destroy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_destroy@PLT # TAILCALL
.Lfunc_end24:
	.size	halide_cond_destroy, .Lfunc_end24-halide_cond_destroy

	.section	.text.halide_cond_broadcast,"ax",@progbits
	.weak	halide_cond_broadcast
	.align	16, 0x90
	.type	halide_cond_broadcast,@function
halide_cond_broadcast:                  # @halide_cond_broadcast
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_broadcast@PLT # TAILCALL
.Lfunc_end25:
	.size	halide_cond_broadcast, .Lfunc_end25-halide_cond_broadcast

	.section	.text.halide_cond_wait,"ax",@progbits
	.weak	halide_cond_wait
	.align	16, 0x90
	.type	halide_cond_wait,@function
halide_cond_wait:                       # @halide_cond_wait
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	pthread_cond_wait@PLT   # TAILCALL
.Lfunc_end26:
	.size	halide_cond_wait, .Lfunc_end26-halide_cond_wait

	.section	.text._ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_,@function
_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_: # @_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rsi, %rax
	movl	%edx, %esi
	movq	%rcx, %rdx
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end27:
	.size	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_, .Lfunc_end27-_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_

	.section	.text._ZN6Halide7Runtime8Internal17clamp_num_threadsEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi,@function
_ZN6Halide7Runtime8Internal17clamp_num_threadsEi: # @_ZN6Halide7Runtime8Internal17clamp_num_threadsEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpl	$64, %edi
	jle	.LBB28_1
# BB#2:                                 # %if.end.3
	movl	$64, %eax
	popq	%rbp
	retq
.LBB28_1:                               # %if.else
	testl	%edi, %edi
	movl	$1, %eax
	cmovgl	%edi, %eax
	popq	%rbp
	retq
.Lfunc_end28:
	.size	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi, .Lfunc_end28-_ZN6Halide7Runtime8Internal17clamp_num_threadsEi

	.section	.text._ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv,@function
_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv: # @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.8(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	jne	.LBB29_2
# BB#1:                                 # %if.end
	leaq	.L.str.1.9(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB29_3
.LBB29_2:                               # %if.then.3
	movq	%rax, %rdi
	popq	%rbp
	jmp	atoi@PLT                # TAILCALL
.LBB29_3:                               # %if.else
	popq	%rbp
	jmp	halide_host_cpu_count@PLT # TAILCALL
.Lfunc_end29:
	.size	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv, .Lfunc_end29-_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv

	.section	.text._ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE,@function
_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE: # @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdi, %rbx
	movq	%rbx, -56(%rbp)         # 8-byte Spill
	testq	%rbx, %rbx
	je	.LBB30_13
# BB#1:
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	leaq	80(%r13), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	jmp	.LBB30_2
	.align	16, 0x90
.LBB30_28:                              # %if.then.3.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%r13, %rsi
	callq	halide_cond_wait@PLT
.LBB30_2:                               # %cond.true.us
                                        # =>This Inner Loop Header: Depth=1
	movl	24(%rbx), %eax
	cmpl	28(%rbx), %eax
	jl	.LBB30_4
# BB#3:                                 # %cond.end.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$0, 40(%rbx)
	jle	.LBB30_20
.LBB30_4:                               # %while.body.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	64(%r13), %r12
	testq	%r12, %r12
	je	.LBB30_28
# BB#5:                                 # %if.else.8.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	8(%r12), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r12), %r15
	movq	24(%r12), %rbx
	movq	32(%r12), %r14
	leal	1(%rbx), %eax
	movl	%eax, 24(%r12)
	movq	%rbx, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_7
# BB#6:                                 # %if.then.12.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	(%r12), %rax
	movq	%rax, 64(%r13)
.LBB30_7:                               # %if.end.13.us
                                        #   in Loop: Header=BB30_2 Depth=1
	incl	40(%r12)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%ebx, %edx
	movq	%r14, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %ebx
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	je	.LBB30_9
# BB#8:                                 # %if.then.18.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	%ebx, 44(%r12)
.LBB30_9:                               # %if.end.19.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	40(%r12), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r12)
	movl	24(%r12), %ecx
	cmpl	28(%r12), %ecx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	jl	.LBB30_2
# BB#10:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpq	%rbx, %r12
	je	.LBB30_2
# BB#11:                                # %_ZN6Halide7Runtime8Internal4work7runningEv.exit65.us
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$1, %eax
	jg	.LBB30_2
# BB#12:                                # %if.then.24.us
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-64(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_2
.LBB30_13:                              # %cond.false.preheader
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	cmpb	$0, 792(%rbx)
	jne	.LBB30_20
# BB#14:
	leaq	208(%rbx), %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	leaq	144(%rbx), %rax
	movq	%rax, -72(%rbp)         # 8-byte Spill
	leaq	80(%rbx), %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB30_15:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	64(%rbx), %r13
	testq	%r13, %r13
	je	.LBB30_16
# BB#21:                                # %if.else.8
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	8(%r13), %rax
	movq	%rax, -48(%rbp)         # 8-byte Spill
	movq	16(%r13), %r15
	movq	24(%r13), %r14
	movq	32(%r13), %r12
	leal	1(%r14), %eax
	movl	%eax, 24(%r13)
	movq	%r14, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jne	.LBB30_23
# BB#22:                                # %if.then.12
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	(%r13), %rax
	movq	%rax, 64(%rbx)
.LBB30_23:                              # %if.end.13
                                        #   in Loop: Header=BB30_15 Depth=1
	incl	40(%r13)
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rdi
	movq	-48(%rbp), %rsi         # 8-byte Reload
	movl	%r14d, %edx
	movq	%r12, %rcx
	callq	halide_do_task@PLT
	movl	%eax, %r14d
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	testl	%r14d, %r14d
	je	.LBB30_25
# BB#24:                                # %if.then.18
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	%r14d, 44(%r13)
.LBB30_25:                              # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	40(%r13), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, 40(%r13)
	cmpl	$1, %eax
	jg	.LBB30_19
# BB#26:                                # %if.end.19
                                        #   in Loop: Header=BB30_15 Depth=1
	movl	28(%r13), %eax
	cmpl	%eax, 24(%r13)
	jl	.LBB30_19
# BB#27:                                # %if.then.24
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-56(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_broadcast@PLT
	jmp	.LBB30_19
	.align	16, 0x90
.LBB30_16:                              # %if.else
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	72(%rbx), %rax
	movq	%rax, %rcx
	shrq	$32, %rcx
	cmpl	%ecx, %eax
	jle	.LBB30_17
# BB#18:                                # %if.else.6
                                        #   in Loop: Header=BB30_15 Depth=1
	leal	-1(%rax), %eax
	movl	%eax, 72(%rbx)
	movq	-64(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	incl	72(%rbx)
	jmp	.LBB30_19
.LBB30_17:                              # %if.then.5
                                        #   in Loop: Header=BB30_15 Depth=1
	movq	-72(%rbp), %rdi         # 8-byte Reload
	movq	%rbx, %rsi
	callq	halide_cond_wait@PLT
	.align	16, 0x90
.LBB30_19:                              # %cond.false.backedge
                                        #   in Loop: Header=BB30_15 Depth=1
	cmpb	$0, 792(%rbx)
	je	.LBB30_15
.LBB30_20:                              # %while.end
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end30:
	.size	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE, .Lfunc_end30-_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE

	.section	.text.halide_do_task,"ax",@progbits
	.weak	halide_do_task
	.align	16, 0x90
	.type	halide_do_task,@function
halide_do_task:                         # @halide_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end31:
	.size	halide_do_task, .Lfunc_end31-halide_do_task

	.section	.text._ZN6Halide7Runtime8Internal13worker_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13worker_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13worker_threadEPv,@function
_ZN6Halide7Runtime8Internal13worker_threadEPv: # @_ZN6Halide7Runtime8Internal13worker_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	xorl	%edi, %edi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end32:
	.size	_ZN6Halide7Runtime8Internal13worker_threadEPv, .Lfunc_end32-_ZN6Halide7Runtime8Internal13worker_threadEPv

	.section	.text._ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_,@function
_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_: # @_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r8, -96(%rbp)          # 8-byte Spill
	movl	%ecx, %r14d
	movl	%edx, %r12d
	movq	%rsi, -104(%rbp)        # 8-byte Spill
	movq	%rdi, %r13
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 793(%rbx)
	je	.LBB33_2
# BB#1:                                 # %entry.while.cond.preheader_crit_edge
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	jmp	.LBB33_5
.LBB33_2:                               # %if.then
	movb	$0, 792(%rbx)
	leaq	80(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	144(%rbx), %rdi
	callq	halide_cond_init@PLT
	leaq	208(%rbx), %rdi
	callq	halide_cond_init@PLT
	movq	$0, 64(%rbx)
	movl	788(%rbx), %edi
	testl	%edi, %edi
	jne	.LBB33_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %edi
	movl	%edi, 788(%rbx)
.LBB33_4:                               # %if.end
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%rbx)
	movl	$0, 784(%rbx)
	movl	%eax, 72(%rbx)
	movb	$1, 793(%rbx)
	xorl	%ecx, %ecx
.LBB33_5:                               # %while.cond.preheader
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jge	.LBB33_8
# BB#6:
	movq	_ZN6Halide7Runtime8Internal13worker_threadEPv@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB33_7:                               # %while.body
                                        # =>This Inner Loop Header: Depth=1
	xorl	%esi, %esi
	movq	%r15, %rdi
	callq	halide_spawn_thread@PLT
	movslq	784(%rbx), %rcx
	leal	1(%rcx), %edx
	movl	%edx, 784(%rbx)
	movq	%rax, 272(%rbx,%rcx,8)
	movq	784(%rbx), %rcx
	movq	%rcx, %rax
	shrq	$32, %rax
	leal	-1(%rax), %edx
	cmpl	%edx, %ecx
	jl	.LBB33_7
.LBB33_8:                               # %while.end
	movq	-104(%rbp), %rcx        # 8-byte Reload
	movq	%rcx, -80(%rbp)
	movq	%r13, -72(%rbp)
	movl	%r12d, -64(%rbp)
	leal	(%r12,%r14), %ecx
	movl	%ecx, -60(%rbp)
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	%rcx, -56(%rbp)
	movq	$0, -48(%rbp)
	movq	64(%rbx), %rcx
	testq	%rcx, %rcx
	movl	%eax, %edx
	cmovel	%r14d, %edx
	cmpl	%r14d, %eax
	cmovlel	%eax, %edx
	movl	%edx, 76(%rbx)
	movq	%rcx, -88(%rbp)
	leaq	-88(%rbp), %rax
	movq	%rax, 64(%rbx)
	leaq	144(%rbx), %rdi
	callq	halide_cond_broadcast@PLT
	movl	76(%rbx), %eax
	cmpl	72(%rbx), %eax
	jle	.LBB33_10
# BB#9:                                 # %if.then.14
	movl	$208, %edi
	addq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_cond_broadcast@PLT
.LBB33_10:                              # %if.end.15
	leaq	-88(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	-44(%rbp), %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end33:
	.size	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_, .Lfunc_end33-_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_

	.section	.text.halide_set_num_threads,"ax",@progbits
	.weak	halide_set_num_threads
	.align	16, 0x90
	.type	halide_set_num_threads,@function
halide_set_num_threads:                 # @halide_set_num_threads
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movl	%edi, %ebx
	testl	%ebx, %ebx
	js	.LBB34_1
# BB#2:                                 # %if.end
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	testl	%ebx, %ebx
	jne	.LBB34_4
# BB#3:                                 # %if.then.2
	callq	_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv@PLT
	movl	%eax, %ebx
	jmp	.LBB34_4
.LBB34_1:                               # %if.end.thread
	leaq	.L.str.2(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_error@PLT
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
.LBB34_4:                               # %if.end.3
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r14
	movl	788(%r14), %r15d
	movl	%ebx, %edi
	callq	_ZN6Halide7Runtime8Internal17clamp_num_threadsEi@PLT
	movl	%eax, 788(%r14)
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r15d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end34:
	.size	halide_set_num_threads, .Lfunc_end34-halide_set_num_threads

	.section	.text.halide_shutdown_thread_pool,"ax",@progbits
	.weak	halide_shutdown_thread_pool
	.align	16, 0x90
	.type	halide_shutdown_thread_pool,@function
halide_shutdown_thread_pool:            # @halide_shutdown_thread_pool
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal10work_queueE@GOTPCREL(%rip), %r13
	cmpb	$0, 793(%r13)
	je	.LBB35_5
# BB#1:                                 # %if.end
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movb	$1, 792(%r13)
	leaq	80(%r13), %rdi
	movq	%rdi, -48(%rbp)         # 8-byte Spill
	callq	halide_cond_broadcast@PLT
	leaq	144(%r13), %r15
	movq	%r15, %rdi
	callq	halide_cond_broadcast@PLT
	leaq	208(%r13), %r12
	movq	%r12, %rdi
	callq	halide_cond_broadcast@PLT
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%ebx, %ebx
	cmpl	$0, 784(%r13)
	jle	.LBB35_4
# BB#2:
	leaq	272(%r13), %r14
	.align	16, 0x90
.LBB35_3:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rdi
	callq	halide_join_thread@PLT
	addq	$1, %rbx
	movslq	784(%r13), %rax
	addq	$8, %r14
	cmpq	%rax, %rbx
	jl	.LBB35_3
.LBB35_4:                               # %for.cond.cleanup
	movq	%r13, %rdi
	callq	halide_mutex_destroy@PLT
	movq	-48(%rbp), %rdi         # 8-byte Reload
	callq	halide_cond_destroy@PLT
	movq	%r15, %rdi
	callq	halide_cond_destroy@PLT
	movq	%r12, %rdi
	callq	halide_cond_destroy@PLT
	movb	$0, 793(%r13)
.LBB35_5:                               # %return
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end35:
	.size	halide_shutdown_thread_pool, .Lfunc_end35-halide_shutdown_thread_pool

	.section	.text.halide_thread_pool_cleanup,"ax",@progbits
	.weak	halide_thread_pool_cleanup
	.align	16, 0x90
	.type	halide_thread_pool_cleanup,@function
halide_thread_pool_cleanup:             # @halide_thread_pool_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_thread_pool@PLT # TAILCALL
.Lfunc_end36:
	.size	halide_thread_pool_cleanup, .Lfunc_end36-halide_thread_pool_cleanup

	.section	.text.halide_set_custom_do_task,"ax",@progbits
	.weak	halide_set_custom_do_task
	.align	16, 0x90
	.type	halide_set_custom_do_task,@function
halide_set_custom_do_task:              # @halide_set_custom_do_task
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_task@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end37:
	.size	halide_set_custom_do_task, .Lfunc_end37-halide_set_custom_do_task

	.section	.text.halide_set_custom_do_par_for,"ax",@progbits
	.weak	halide_set_custom_do_par_for
	.align	16, 0x90
	.type	halide_set_custom_do_par_for,@function
halide_set_custom_do_par_for:           # @halide_set_custom_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end38:
	.size	halide_set_custom_do_par_for, .Lfunc_end38-halide_set_custom_do_par_for

	.section	.text.halide_do_par_for,"ax",@progbits
	.weak	halide_do_par_for
	.align	16, 0x90
	.type	halide_do_par_for,@function
halide_do_par_for:                      # @halide_do_par_for
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	custom_do_par_for@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end39:
	.size	halide_do_par_for, .Lfunc_end39-halide_do_par_for

	.section	.text._ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc,@function
_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc: # @_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end40:
	.size	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc, .Lfunc_end40-_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc,@function
_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc: # @_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movl	$1, %esi
	callq	dlopen@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	jne	.LBB41_2
# BB#1:                                 # %if.then
	callq	dlerror@PLT
.LBB41_2:                               # %if.end
	movq	%rbx, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end41:
	.size	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc, .Lfunc_end41-_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc

	.section	.text._ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc,@function
_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc: # @_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	dlsym@PLT               # TAILCALL
.Lfunc_end42:
	.size	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc, .Lfunc_end42-_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc

	.section	.text.halide_set_custom_get_symbol,"ax",@progbits
	.weak	halide_set_custom_get_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_symbol,@function
halide_set_custom_get_symbol:           # @halide_set_custom_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end43:
	.size	halide_set_custom_get_symbol, .Lfunc_end43-halide_set_custom_get_symbol

	.section	.text.halide_set_custom_load_library,"ax",@progbits
	.weak	halide_set_custom_load_library
	.align	16, 0x90
	.type	halide_set_custom_load_library,@function
halide_set_custom_load_library:         # @halide_set_custom_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end44:
	.size	halide_set_custom_load_library, .Lfunc_end44-halide_set_custom_load_library

	.section	.text.halide_set_custom_get_library_symbol,"ax",@progbits
	.weak	halide_set_custom_get_library_symbol
	.align	16, 0x90
	.type	halide_set_custom_get_library_symbol,@function
halide_set_custom_get_library_symbol:   # @halide_set_custom_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end45:
	.size	halide_set_custom_get_library_symbol, .Lfunc_end45-halide_set_custom_get_library_symbol

	.section	.text.halide_get_symbol,"ax",@progbits
	.weak	halide_get_symbol
	.align	16, 0x90
	.type	halide_get_symbol,@function
halide_get_symbol:                      # @halide_get_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17custom_get_symbolE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end46:
	.size	halide_get_symbol, .Lfunc_end46-halide_get_symbol

	.section	.text.halide_load_library,"ax",@progbits
	.weak	halide_load_library
	.align	16, 0x90
	.type	halide_load_library,@function
halide_load_library:                    # @halide_load_library
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19custom_load_libraryE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end47:
	.size	halide_load_library, .Lfunc_end47-halide_load_library

	.section	.text.halide_get_library_symbol,"ax",@progbits
	.weak	halide_get_library_symbol
	.align	16, 0x90
	.type	halide_get_library_symbol,@function
halide_get_library_symbol:              # @halide_get_library_symbol
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end48:
	.size	halide_get_library_symbol, .Lfunc_end48-halide_get_library_symbol

	.section	.text.halide_set_gpu_device,"ax",@progbits
	.weak	halide_set_gpu_device
	.align	16, 0x90
	.type	halide_set_gpu_device,@function
halide_set_gpu_device:                  # @halide_set_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end49:
	.size	halide_set_gpu_device, .Lfunc_end49-halide_set_gpu_device

	.section	.text.halide_get_gpu_device,"ax",@progbits
	.weak	halide_get_gpu_device
	.align	16, 0x90
	.type	halide_get_gpu_device,@function
halide_get_gpu_device:                  # @halide_get_gpu_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB50_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB50_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE@GOTPCREL(%rip), %r14
	cmpb	$0, (%r14)
	je	.LBB50_4
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit.if.end.4_crit_edge
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	jmp	.LBB50_7
.LBB50_4:                               # %if.then
	leaq	.L.str.10(%rip), %rdi
	callq	getenv@PLT
	movq	%rax, %rcx
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB50_6
# BB#5:                                 # %if.then.2
	movq	%rcx, %rdi
	callq	atoi@PLT
.LBB50_6:                               # %if.end
	movq	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE@GOTPCREL(%rip), %rcx
	movl	%eax, (%rcx)
	movb	$1, (%r14)
.LBB50_7:                               # %if.end.4
	movl	$0, (%rbx)
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end50:
	.size	halide_get_gpu_device, .Lfunc_end50-halide_get_gpu_device

	.section	.text._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,@function
_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t: # @_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4168, %rsp             # imm = 0x1048
	movq	%rsi, %r12
	movq	%rdi, -4160(%rbp)       # 8-byte Spill
	movl	$1, %r14d
	lock		xaddl	%r14d, _ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids(%rip)
	callq	halide_get_trace_file@PLT
	testl	%eax, %eax
	jle	.LBB51_10
# BB#1:                                 # %if.then
	movl	%eax, %r15d
	movzwl	26(%r12), %eax
	movzbl	25(%r12), %r13d
	addl	$7, %r13d
	shrl	$3, %r13d
	imulq	%rax, %r13
	movl	40(%r12), %ebx
	leal	(,%rbx,4), %eax
	movl	%eax, -4192(%rbp)       # 4-byte Spill
	movq	(%r12), %rdi
	callq	strlen@PLT
	addq	$1, %rax
	movq	%rax, -4168(%rbp)       # 8-byte Spill
	leal	(%r13,%rbx,4), %ecx
	leal	28(%rax,%rcx), %edx
	movl	%edx, -4196(%rbp)       # 4-byte Spill
	leal	31(%rax,%rcx), %eax
	andl	$-4, %eax
	movl	%eax, -4152(%rbp)       # 4-byte Spill
	movl	%eax, -4144(%rbp)
	movl	%r14d, -4140(%rbp)
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	vmovupd	24(%r12), %xmm0
	vmovupd	%xmm0, -4136(%rbp)
	movl	%ebx, -4120(%rbp)
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %r14
	.align	16, 0x90
.LBB51_2:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%r14)
	testl	%eax, %eax
	jne	.LBB51_2
# BB#3:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	leaq	-4144(%rbp), %rsi
	movl	$28, %edx
	movl	%r15d, %ebx
	movl	%ebx, %edi
	callq	write@PLT
	movq	%rax, %r15
	movq	%r12, %rcx
	movq	16(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_5
# BB#4:                                 # %if.then.19
	movl	-4192(%rbp), %edx       # 4-byte Reload
	movq	%rcx, %r12
	movl	%ebx, %edi
	callq	write@PLT
	movq	%r12, %rcx
	addq	%rax, %r15
.LBB51_5:                               # %if.end
	movq	%r15, -4192(%rbp)       # 8-byte Spill
	movl	%ebx, %r15d
	movl	-4152(%rbp), %r12d      # 4-byte Reload
	subl	-4196(%rbp), %r12d      # 4-byte Folded Reload
	movq	8(%rcx), %rsi
	testq	%rsi, %rsi
	je	.LBB51_6
# BB#7:                                 # %if.then.25
	movq	%rcx, %rbx
	movl	%r15d, %edi
	movq	%r13, %rdx
	callq	write@PLT
	movq	%rbx, %rcx
	movq	-4192(%rbp), %r13       # 8-byte Reload
	addq	%rax, %r13
	jmp	.LBB51_8
.LBB51_10:                              # %if.else
	leaq	-49(%rbp), %r15
	movb	$0, -49(%rbp)
	movzbl	25(%r12), %eax
	movl	$8, %ecx
	.align	16, 0x90
.LBB51_11:                              # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movl	%ecx, %edx
	leal	(%rdx,%rdx), %ecx
	cmpl	%eax, %edx
	jl	.LBB51_11
# BB#12:                                # %while.end
	cmpl	$65, %edx
	movq	%rdx, -4168(%rbp)       # 8-byte Spill
	jl	.LBB51_14
# BB#13:                                # %if.then.46
	leaq	.L.str.1.15(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB51_14:                              # %if.end.47
	movl	28(%r12), %r13d
	leaq	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types(%rip), %rax
	movq	(%rax,%r13,8), %rdx
	leaq	-4144(%rbp), %rdi
	movq	%r12, %rbx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.122(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movslq	36(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.22.124(%rip), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_15
# BB#16:                                # %if.then.63
	movq	%r13, -4152(%rbp)       # 8-byte Spill
	leaq	.L.str.15(%rip), %rdx
	movq	%rsi, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
	jmp	.LBB51_17
.LBB51_6:
	movq	-4192(%rbp), %r13       # 8-byte Reload
.LBB51_8:                               # %if.end.30
	movq	(%rcx), %rsi
	movq	-4168(%rbp), %rax       # 8-byte Reload
	movl	%eax, %edx
	movl	%r15d, %edi
	callq	write@PLT
	movq	%rax, %rbx
	addq	%r13, %rbx
	movl	$0, -44(%rbp)
	movl	%r12d, %edx
	leaq	-44(%rbp), %rsi
	movl	%r15d, %edi
	callq	write@PLT
	addq	%rbx, %rax
	movl	$0, (%r14)
	movl	-4152(%rbp), %ecx       # 4-byte Reload
	cmpq	%rcx, %rax
	je	.LBB51_45
# BB#9:                                 # %if.then.40
	leaq	.L.str.14(%rip), %rsi
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	jmp	.LBB51_45
.LBB51_15:
	movq	%r13, -4152(%rbp)       # 8-byte Spill
.LBB51_17:                              # %for.cond.preheader
	movq	%r15, %r13
	cmpl	$0, 40(%rbx)
	jle	.LBB51_18
# BB#21:                                # %for.body.lr.ph
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
	xorl	%r12d, %r12d
	xorl	%r15d, %r15d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_22:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_28
# BB#23:                                # %if.then.69
                                        #   in Loop: Header=BB51_22 Depth=1
	movzwl	26(%rbx), %ecx
	cmpl	$2, %ecx
	jb	.LBB51_26
# BB#24:                                # %land.lhs.true
                                        #   in Loop: Header=BB51_22 Depth=1
	movl	%r12d, %eax
	cltd
	idivl	%ecx
	testl	%edx, %edx
	je	.LBB51_25
.LBB51_26:                              # %if.else.80
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.17(%rip), %rdx
	jmp	.LBB51_27
.LBB51_25:                              # %if.then.78
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	%rsi, %rdi
	movq	%r13, %rsi
	leaq	.L.str.16(%rip), %rdx
	.align	16, 0x90
.LBB51_27:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	callq	halide_string_to_string@PLT
	movq	%rax, %rsi
.LBB51_28:                              # %if.end.83
                                        #   in Loop: Header=BB51_22 Depth=1
	movq	16(%rbx), %rax
	movslq	(%rax,%r15), %rdx
	movl	$1, %ecx
	movq	%rsi, %rdi
	movq	%r13, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rsi
	addq	$1, %r14
	movslq	40(%rbx), %rax
	addq	$4, %r15
	addl	$1, %r12d
	cmpq	%rax, %r14
	jl	.LBB51_22
	jmp	.LBB51_19
.LBB51_18:
	movl	%r14d, -4172(%rbp)      # 4-byte Spill
.LBB51_19:                              # %for.cond.cleanup
	movzwl	26(%rbx), %eax
	cmpl	$1, %eax
	jbe	.LBB51_29
# BB#20:                                # %if.then.92
	leaq	.L.str.18(%rip), %rdx
	jmp	.LBB51_30
.LBB51_29:                              # %if.else.94
	leaq	.L.str.8.76(%rip), %rdx
.LBB51_30:                              # %if.end.96
	movq	%rsi, %rdi
	movq	%r13, %r15
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	-4160(%rbp), %r12       # 8-byte Reload
	movq	-4152(%rbp), %rax       # 8-byte Reload
	cmpl	$1, %eax
	jg	.LBB51_42
# BB#31:                                # %if.then.98
	movzwl	26(%rbx), %eax
	cmpl	$2, %eax
	jb	.LBB51_33
# BB#32:                                # %if.then.103
	leaq	.L.str.20(%rip), %rdx
	jmp	.LBB51_34
.LBB51_33:                              # %if.else.105
	leaq	.L.str.21(%rip), %rdx
.LBB51_34:                              # %for.cond.109.preheader
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	cmpw	$0, 26(%rbx)
	je	.LBB51_42
# BB#35:                                # %for.body.115.lr.ph
	movq	%r12, -4160(%rbp)       # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -4152(%rbp)       # 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB51_36:                              # %for.body.115
                                        # =>This Inner Loop Header: Depth=1
	testq	%r14, %r14
	jle	.LBB51_38
# BB#37:                                # %if.then.117
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%r15, %rsi
	leaq	.L.str.17(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_38:                              # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	movzbl	24(%rbx), %eax
	cmpq	$3, %rax
	ja	.LBB51_74
# BB#39:                                # %if.end.119
                                        #   in Loop: Header=BB51_36 Depth=1
	leaq	.LJTI51_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	movq	-4168(%rbp), %rdx       # 8-byte Reload
	jmpq	*%rax
.LBB51_46:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_47
# BB#50:                                # %if.then.133
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movswq	(%rax,%r12), %rdx
	jmp	.LBB51_49
.LBB51_56:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$16, %edx
	jne	.LBB51_57
# BB#59:                                # %if.then.169
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzwl	(%rax,%r12), %edx
	jmp	.LBB51_49
.LBB51_64:                              # %if.then.195
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	movq	%rdi, %rbx
	cmpl	$15, %edx
	jle	.LBB51_65
# BB#67:                                # %if.end.198
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$32, %edx
	movq	-4184(%rbp), %rax       # 8-byte Reload
	jne	.LBB51_70
# BB#68:                                # %if.then.200
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	vcvtss2sd	(%rax,%rcx), %xmm0, %xmm0
	xorl	%edx, %edx
	jmp	.LBB51_69
.LBB51_72:                              # %if.then.224
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	(%rax,%r13), %rdx
	movq	%r15, %rsi
	callq	halide_pointer_to_string@PLT
	jmp	.LBB51_73
.LBB51_47:                              # %if.then.123
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_51
# BB#48:                                # %if.then.125
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movsbq	(%rax,%r14), %rdx
	jmp	.LBB51_49
.LBB51_57:                              # %if.then.159
                                        #   in Loop: Header=BB51_36 Depth=1
	cmpl	$8, %edx
	jne	.LBB51_60
# BB#58:                                # %if.then.161
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movzbl	(%rax,%r14), %edx
.LBB51_49:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_73
.LBB51_65:                              # %if.else.205.thread
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4160(%rbp), %rdi       # 8-byte Reload
	leaq	.L.str.22(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
	movq	-4184(%rbp), %rax       # 8-byte Reload
	movq	8(%rax), %rax
	jmp	.LBB51_66
.LBB51_70:                              # %if.else.205
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rax), %rax
	cmpl	$16, %edx
	jne	.LBB51_66
# BB#71:                                # %if.then.207
                                        #   in Loop: Header=BB51_36 Depth=1
	movzwl	(%rax,%r12), %edi
	callq	halide_float16_bits_to_double@PLT
	movl	$1, %edx
.LBB51_69:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_73:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	jmp	.LBB51_74
.LBB51_66:                              # %if.else.212
                                        #   in Loop: Header=BB51_36 Depth=1
	vmovsd	(%rax,%r13), %xmm0      # xmm0 = mem[0],zero
	movl	$1, %edx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB51_55
.LBB51_51:                              # %if.else.139
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_53
# BB#52:                                # %if.then.141
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movslq	(%rax,%rcx), %rdx
	jmp	.LBB51_54
.LBB51_60:                              # %if.else.175
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	8(%rbx), %rax
	movq	%rbx, -4184(%rbp)       # 8-byte Spill
	cmpl	$32, %edx
	jne	.LBB51_62
# BB#61:                                # %if.then.177
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	-4152(%rbp), %rcx       # 8-byte Reload
	movl	(%rax,%rcx), %edx
	jmp	.LBB51_63
.LBB51_53:                              # %if.else.146
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_54:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	jmp	.LBB51_55
.LBB51_62:                              # %if.else.182
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	(%rax,%r13), %rdx
.LBB51_63:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movl	$1, %ecx
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
.LBB51_55:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	movq	%rax, %rdi
	movq	-4184(%rbp), %rbx       # 8-byte Reload
	.align	16, 0x90
.LBB51_74:                              # %for.inc.233
                                        #   in Loop: Header=BB51_36 Depth=1
	addq	$1, %r14
	movzwl	26(%rbx), %eax
	addq	$8, %r13
	addq	$2, %r12
	addq	$4, -4152(%rbp)         # 8-byte Folded Spill
	cmpq	%rax, %r14
	jl	.LBB51_36
# BB#40:                                # %for.cond.cleanup.114
	movzwl	%ax, %eax
	cmpl	$1, %eax
	movq	-4160(%rbp), %r12       # 8-byte Reload
	jbe	.LBB51_42
# BB#41:                                # %if.then.240
	leaq	.L.str.23(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
.LBB51_42:                              # %if.end.243
	leaq	.L.str.7.109(%rip), %rdx
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %r15d
	leaq	-4144(%rbp), %rsi
	subq	%rsi, %r15
	addq	%rax, %r15
	movq	%r12, %rdi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB51_43:                              # %while.cond.i.353
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB51_43
# BB#44:                                # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy4096EED2Ev.exit
	leaq	-4144(%rbp), %r14
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	halide_print@PLT
	movl	$0, (%rbx)
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%r15, %rdx
	callq	halide_msan_annotate_memory_is_initialized@PLT
.LBB51_45:                              # %if.end.247
	movl	-4172(%rbp), %eax       # 4-byte Reload
	addq	$4168, %rsp             # imm = 0x1048
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end51:
	.size	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t, .Lfunc_end51-_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.section	.rodata._ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t,"a",@progbits
	.align	4
.LJTI51_0:
	.long	.LBB51_46-.LJTI51_0
	.long	.LBB51_56-.LJTI51_0
	.long	.LBB51_64-.LJTI51_0
	.long	.LBB51_72-.LJTI51_0

	.section	.text.halide_get_trace_file,"ax",@progbits
	.weak	halide_get_trace_file
	.align	16, 0x90
	.type	halide_get_trace_file,@function
halide_get_trace_file:                  # @halide_get_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE@GOTPCREL(%rip), %rbx
	.align	16, 0x90
.LBB52_1:                               # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	movl	$1, %eax
	xchgl	%eax, (%rbx)
	testl	%eax, %eax
	jne	.LBB52_1
# BB#2:                                 # %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVi.exit
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	cmpb	$0, (%rax)
	jne	.LBB52_8
# BB#3:                                 # %if.then
	leaq	.L.str.25(%rip), %rdi
	callq	getenv@PLT
	testq	%rax, %rax
	je	.LBB52_7
# BB#4:                                 # %if.then.2
	movl	$1089, %esi             # imm = 0x441
	movl	$420, %edx              # imm = 0x1A4
	movq	%rax, %rdi
	callq	open@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jg	.LBB52_6
# BB#5:                                 # %if.then.4
	leaq	.L.str.26(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB52_6:                               # %if.end
	movl	%r15d, %edi
	callq	halide_set_trace_file@PLT
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	jmp	.LBB52_8
.LBB52_7:                               # %if.else
	xorl	%edi, %edi
	callq	halide_set_trace_file@PLT
.LBB52_8:                               # %if.end.6
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	(%rax), %eax
	movl	$0, (%rbx)
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end52:
	.size	halide_get_trace_file, .Lfunc_end52-halide_get_trace_file

	.section	.text.halide_set_custom_trace,"ax",@progbits
	.weak	halide_set_custom_trace
	.align	16, 0x90
	.type	halide_set_custom_trace,@function
halide_set_custom_trace:                # @halide_set_custom_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end53:
	.size	halide_set_custom_trace, .Lfunc_end53-halide_set_custom_trace

	.section	.text.halide_set_trace_file,"ax",@progbits
	.weak	halide_set_trace_file
	.align	16, 0x90
	.type	halide_set_trace_file,@function
halide_set_trace_file:                  # @halide_set_trace_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %rax
	movl	%edi, (%rax)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rax
	movb	$1, (%rax)
	popq	%rbp
	retq
.Lfunc_end54:
	.size	halide_set_trace_file, .Lfunc_end54-halide_set_trace_file

	.section	.text.halide_trace,"ax",@progbits
	.weak	halide_trace
	.align	16, 0x90
	.type	halide_trace,@function
halide_trace:                           # @halide_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal19halide_custom_traceE@GOTPCREL(%rip), %rax
	movq	(%rax), %rax
	popq	%rbp
	jmpq	*%rax                   # TAILCALL
.Lfunc_end55:
	.size	halide_trace, .Lfunc_end55-halide_trace

	.section	.text.halide_shutdown_trace,"ax",@progbits
	.weak	halide_shutdown_trace
	.align	16, 0x90
	.type	halide_shutdown_trace,@function
halide_shutdown_trace:                  # @halide_shutdown_trace
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE@GOTPCREL(%rip), %rbx
	cmpb	$0, (%rbx)
	je	.LBB56_2
# BB#1:                                 # %if.then
	movq	_ZN6Halide7Runtime8Internal17halide_trace_fileE@GOTPCREL(%rip), %r14
	movl	(%r14), %edi
	callq	close@PLT
	movl	$0, (%r14)
	movq	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE@GOTPCREL(%rip), %rcx
	movb	$0, (%rcx)
	movb	$0, (%rbx)
	jmp	.LBB56_3
.LBB56_2:                               # %return
	xorl	%eax, %eax
.LBB56_3:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end56:
	.size	halide_shutdown_trace, .Lfunc_end56-halide_shutdown_trace

	.section	.text.halide_trace_cleanup,"ax",@progbits
	.weak	halide_trace_cleanup
	.align	16, 0x90
	.type	halide_trace_cleanup,@function
halide_trace_cleanup:                   # @halide_trace_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_shutdown_trace@PLT # TAILCALL
.Lfunc_end57:
	.size	halide_trace_cleanup, .Lfunc_end57-halide_trace_cleanup

	.section	.text.halide_trace_helper,"ax",@progbits
	.weak	halide_trace_helper
	.align	16, 0x90
	.type	halide_trace_helper,@function
halide_trace_helper:                    # @halide_trace_helper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$56, %rsp
	movl	40(%rbp), %r10d
	movl	32(%rbp), %r11d
	movl	24(%rbp), %eax
	movl	16(%rbp), %ebx
	movq	%rsi, -56(%rbp)
	movq	%rcx, -40(%rbp)
	movq	%rdx, -48(%rbp)
	movb	%r8b, -32(%rbp)
	movb	%r9b, -31(%rbp)
	movw	%bx, -30(%rbp)
	movl	%eax, -28(%rbp)
	movl	%r11d, -24(%rbp)
	movl	%r10d, -20(%rbp)
	movzwl	%bx, %eax
	cmpl	$1, %eax
	movl	$1, %ecx
	cmoval	%eax, %ecx
	imull	48(%rbp), %ecx
	movl	%ecx, -16(%rbp)
	leaq	-56(%rbp), %rsi
	callq	halide_trace@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end58:
	.size	halide_trace_helper, .Lfunc_end58-halide_trace_helper

	.section	.text._ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc,@function
_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc: # @_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	%rdi, %rax
	.align	16, 0x90
.LBB59_1:                               # %while.cond
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %rcx
	leaq	1(%rcx), %rax
	cmpb	$0, (%rcx)
	jne	.LBB59_1
# BB#2:                                 # %while.cond.1.preheader
	xorl	%eax, %eax
	cmpq	%rdi, %rcx
	je	.LBB59_19
	.align	16, 0x90
.LBB59_3:                               # %land.rhs
                                        # =>This Inner Loop Header: Depth=1
	testb	$1, %al
	jne	.LBB59_4
# BB#7:                                 # %while.body.5
                                        #   in Loop: Header=BB59_3 Depth=1
	movzbl	-1(%rcx), %edx
	addq	$-1, %rcx
	cmpl	$46, %edx
	sete	%al
	cmpq	%rcx, %rdi
	jne	.LBB59_3
# BB#8:                                 # %while.end.7
	movzbl	%dl, %eax
	cmpl	$46, %eax
	je	.LBB59_5
# BB#9:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_4:
	movq	%rcx, %rdi
.LBB59_5:                               # %if.end
	movb	1(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$116, %eax
	jne	.LBB59_6
# BB#10:                                # %if.end.16
	movb	2(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$105, %eax
	jne	.LBB59_11
# BB#12:                                # %if.end.24
	movb	3(%rdi), %al
	orb	$32, %al
	movzbl	%al, %eax
	cmpl	$102, %eax
	jne	.LBB59_13
# BB#14:                                # %if.end.32
	movb	4(%rdi), %cl
	movb	$1, %al
	testb	%cl, %cl
	je	.LBB59_19
# BB#15:                                # %if.end.32
	movzbl	%cl, %eax
	cmpl	$70, %eax
	je	.LBB59_18
# BB#16:                                # %if.end.32
	cmpl	$102, %eax
	jne	.LBB59_17
.LBB59_18:                              # %if.end.44
	cmpb	$0, 5(%rdi)
	sete	%al
.LBB59_19:                              # %cleanup
	popq	%rbp
	retq
.LBB59_6:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_11:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_13:
	xorl	%eax, %eax
	popq	%rbp
	retq
.LBB59_17:                              # %if.then.43
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end59:
	.size	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc, .Lfunc_end59-_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc

	.section	.text._ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t,@function
_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t: # @_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%r8), %r9
	imull	32(%r8), %edi
	imull	36(%r8), %esi
	addl	%edi, %esi
	imull	40(%r8), %edx
	addl	%esi, %edx
	imull	44(%r8), %ecx
	addl	%edx, %ecx
	movslq	%ecx, %rax
	imulq	%r9, %rax
	addq	8(%r8), %rax
	popq	%rbp
	retq
.Lfunc_end60:
	.size	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t, .Lfunc_end60-_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI61_0:
	.long	1                       # 0x1
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI61_1:
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	1                       # 0x1
	.long	1                       # 0x1
	.section	.text.halide_debug_to_file,"ax",@progbits
	.weak	halide_debug_to_file
	.align	16, 0x90
	.type	halide_debug_to_file,@function
halide_debug_to_file:                   # @halide_debug_to_file
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$4232, %rsp             # imm = 0x1088
	movq	%rcx, %r12
	movq	%r12, -4152(%rbp)       # 8-byte Spill
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%r12, %rsi
	callq	halide_copy_to_host@PLT
	leaq	.L.str.27(%rip), %rsi
	movq	%rbx, %rdi
	callq	fopen@PLT
	movq	%rax, %r14
	movl	$-1, %r13d
	testq	%r14, %r14
	je	.LBB61_44
# BB#1:                                 # %if.end
	vmovdqu	16(%r12), %xmm1
	vmovdqa	%xmm1, -4208(%rbp)      # 16-byte Spill
	vpbroadcastd	.LCPI61_0(%rip), %xmm0
	vpmaxsd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, -4176(%rbp)      # 16-byte Spill
	movslq	64(%r12), %r12
	movq	%r12, -4184(%rbp)       # 8-byte Spill
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal18has_tiff_extensionEPKc@PLT
	testb	%al, %al
	je	.LBB61_18
# BB#2:                                 # %if.then.19
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %ecx
	cmpl	$2, %ecx
	setb	%bl
	vpextrd	$2, %xmm0, %eax
	cmpl	$5, %eax
	setl	%dl
	andb	%bl, %dl
	movl	$1, %esi
	cmovel	%eax, %esi
	movl	%esi, -4212(%rbp)       # 4-byte Spill
	movl	%ecx, %r13d
	cmovnel	%eax, %r13d
	movw	$18761, -4144(%rbp)     # imm = 0x4949
	movw	$42, -4142(%rbp)
	movl	$8, -4140(%rbp)
	movw	$15, -4136(%rbp)
	movw	$256, -4134(%rbp)       # imm = 0x100
	movw	$4, -4132(%rbp)
	movl	$1, -4130(%rbp)
	vmovd	%xmm0, %r8d
	vmovd	%xmm0, -4126(%rbp)
	movw	$257, -4122(%rbp)       # imm = 0x101
	movw	$4, -4120(%rbp)
	movl	$1, -4118(%rbp)
	vpextrd	$1, %xmm0, -4114(%rbp)
	leal	(,%r12,8), %edi
	movw	$258, -4110(%rbp)       # imm = 0x102
	movw	$3, -4108(%rbp)
	movl	$1, -4106(%rbp)
	movw	%di, -4102(%rbp)
	movw	$259, -4098(%rbp)       # imm = 0x103
	movw	$3, -4096(%rbp)
	movl	$1, -4094(%rbp)
	movw	$1, -4090(%rbp)
	cmpl	$2, %r13d
	setg	%bl
	movzbl	%bl, %edi
	addl	$1, %edi
	movw	$262, -4086(%rbp)       # imm = 0x106
	movw	$3, -4084(%rbp)
	movl	$1, -4082(%rbp)
	movw	%di, -4078(%rbp)
	movw	$273, -4074(%rbp)       # imm = 0x111
	movw	$4, -4072(%rbp)
	movl	%r13d, -4070(%rbp)
	movl	$210, -4066(%rbp)
	testb	%dl, %dl
	movw	%cx, %dx
	cmovnew	%ax, %dx
	movw	$277, -4062(%rbp)       # imm = 0x115
	movw	$3, -4060(%rbp)
	movl	$1, -4058(%rbp)
	movw	%dx, -4054(%rbp)
	movw	$278, -4050(%rbp)       # imm = 0x116
	movw	$4, -4048(%rbp)
	movl	$1, -4046(%rbp)
	vpextrd	$1, %xmm0, -4042(%rbp)
	vpextrd	$1, %xmm0, %ebx
	imull	%r8d, %ebx
	imull	%ebx, %eax
	imull	%r12d, %eax
	imull	%ecx, %eax
	cmpl	$1, %r13d
	leal	210(,%r13,4), %ecx
	cmovel	%eax, %ecx
	movw	$279, -4038(%rbp)       # imm = 0x117
	movw	$4, -4036(%rbp)
	movl	%r13d, -4034(%rbp)
	movl	%ecx, -4030(%rbp)
	movw	$282, -4026(%rbp)       # imm = 0x11A
	movw	$5, -4024(%rbp)
	movl	$1, -4022(%rbp)
	movl	$194, -4018(%rbp)
	movw	$283, -4014(%rbp)       # imm = 0x11B
	movw	$5, -4012(%rbp)
	movl	$1, -4010(%rbp)
	movl	$202, -4006(%rbp)
	movw	$284, -4002(%rbp)       # imm = 0x11C
	movw	$3, -4000(%rbp)
	movl	$1, -3998(%rbp)
	movw	$2, -3994(%rbp)
	movw	$296, -3990(%rbp)       # imm = 0x128
	movw	$3, -3988(%rbp)
	movl	$1, -3986(%rbp)
	movw	$1, -3982(%rbp)
	movslq	%r15d, %rax
	movq	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE@GOTPCREL(%rip), %rcx
	movw	(%rcx,%rax,2), %ax
	movw	$339, -3978(%rbp)       # imm = 0x153
	movw	$3, -3976(%rbp)
	movl	$1, -3974(%rbp)
	movw	%ax, -3970(%rbp)
	movw	$-32539, -3966(%rbp)    # imm = 0xFFFFFFFFFFFF80E5
	movw	$4, -3964(%rbp)
	movl	$1, -3962(%rbp)
	movl	%esi, -3958(%rbp)
	vmovdqa	.LCPI61_1(%rip), %xmm0  # xmm0 = [0,1,1,1]
	vmovdqu	%xmm0, -3954(%rbp)
	movl	$1, -3938(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$210, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#4:                                 # %if.end.67
	cmpl	$2, %r13d
	jl	.LBB61_11
# BB#5:                                 # %for.body.lr.ph
	movq	%r14, %r12
	movl	%ebx, -4216(%rbp)       # 4-byte Spill
	movq	-4184(%rbp), %rax       # 8-byte Reload
	imull	%eax, %ebx
	imull	-4212(%rbp), %ebx       # 4-byte Folded Reload
	leal	210(,%r13,8), %eax
	movl	%eax, -44(%rbp)
	xorl	%r15d, %r15d
	leaq	-44(%rbp), %r14
	.align	16, 0x90
.LBB61_6:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r14, %rdi
	movq	%r12, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_16
# BB#7:                                 # %if.end.80
                                        #   in Loop: Header=BB61_6 Depth=1
	addl	%ebx, -44(%rbp)
	addl	$1, %r15d
	cmpl	%r13d, %r15d
	jl	.LBB61_6
# BB#8:                                 # %for.body.91.lr.ph
	movl	-4212(%rbp), %eax       # 4-byte Reload
	imull	-4216(%rbp), %eax       # 4-byte Folded Reload
	movl	%eax, -48(%rbp)
	xorl	%ebx, %ebx
	leaq	-48(%rbp), %r15
	movq	%r12, %r14
	.align	16, 0x90
.LBB61_10:                              # %for.body.91
                                        # =>This Inner Loop Header: Depth=1
	movl	$4, %esi
	movl	$1, %edx
	movq	%r15, %rdi
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_3
# BB#9:                                 # %for.cond.88
                                        #   in Loop: Header=BB61_10 Depth=1
	addl	$1, %ebx
	cmpl	%r13d, %ebx
	jl	.LBB61_10
.LBB61_11:                              # %cleanup.106.thread
	movq	-4184(%rbp), %r12       # 8-byte Reload
	jmp	.LBB61_12
.LBB61_18:                              # %if.else.116
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vmovdqa	%xmm0, -4144(%rbp)
	movl	%r15d, -4128(%rbp)
	leaq	-4144(%rbp), %rdi
	movl	$20, %esi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_19
.LBB61_12:                              # %if.end.130
	movl	$4096, %eax             # imm = 0x1000
	xorl	%edx, %edx
	idivl	%r12d
	xorl	%r13d, %r13d
	vmovdqa	-4176(%rbp), %xmm0      # 16-byte Reload
	vpextrd	$3, %xmm0, %r10d
	testl	%r10d, %r10d
	jle	.LBB61_43
# BB#13:                                # %for.cond.135.preheader.lr.ph
	vmovd	%xmm0, %r9d
	movl	%r9d, -4216(%rbp)       # 4-byte Spill
	movl	%eax, %ecx
	imull	%r12d, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, -4232(%rbp)       # 8-byte Spill
	vmovdqa	-4208(%rbp), %xmm1      # 16-byte Reload
	vpextrd	$2, %xmm1, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovgl	%ecx, %ebx
	movq	%rbx, -4248(%rbp)       # 8-byte Spill
	leal	-128(%rbx), %r8d
	movl	%r8d, -4252(%rbp)       # 4-byte Spill
	movl	%r8d, %edi
	shrl	$7, %edi
	addl	$1, %edi
	vpextrd	$1, %xmm0, -4236(%rbp)  # 4-byte Folded Spill
	vpextrd	$2, %xmm0, %esi
	movl	%esi, -4260(%rbp)       # 4-byte Spill
	movl	%ebx, %r11d
	andl	$-128, %r11d
	movl	%r11d, -4264(%rbp)      # 4-byte Spill
	andl	$7, %edi
	movl	%edi, -4256(%rbp)       # 4-byte Spill
	movl	%edi, %r12d
	negl	%r12d
	movl	%r12d, -4268(%rbp)      # 4-byte Spill
	movl	$0, -4208(%rbp)         # 4-byte Folded Spill
	xorl	%r15d, %r15d
.LBB61_14:                              # %for.cond.143.preheader.lr.ph.us.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB61_23 Depth 2
                                        #     Child Loop BB61_26 Depth 2
                                        #     Child Loop BB61_29 Depth 2
                                        #     Child Loop BB61_40 Depth 2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	testl	%r9d, %r9d
	jle	.LBB61_20
# BB#15:                                #   in Loop: Header=BB61_14 Depth=1
	movl	$0, -4212(%rbp)         # 4-byte Folded Spill
.LBB61_40:                              # %for.body.146.lr.ph.us.us.preheader.us
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB61_34 Depth 3
                                        #         Child Loop BB61_35 Depth 4
	movl	%r10d, -4240(%rbp)      # 4-byte Spill
	movl	%eax, %r12d
	movq	%r14, -4224(%rbp)       # 8-byte Spill
	movl	$0, -4176(%rbp)         # 4-byte Folded Spill
.LBB61_34:                              # %for.body.146.lr.ph.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB61_35 Depth 4
	xorl	%r13d, %r13d
	.align	16, 0x90
.LBB61_35:                              # %for.body.146.us.us.us
                                        #   Parent Loop BB61_14 Depth=1
                                        #     Parent Loop BB61_40 Depth=2
                                        #       Parent Loop BB61_34 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	leal	1(%r15), %r14d
	movl	%r13d, %edi
	movl	-4176(%rbp), %esi       # 4-byte Reload
	movl	-4212(%rbp), %edx       # 4-byte Reload
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	movq	-4152(%rbp), %r8        # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal19get_pointer_to_dataEiiiiPK8buffer_t@PLT
	movq	-4184(%rbp), %rdx       # 8-byte Reload
	movl	%edx, %ecx
	imull	%r15d, %ecx
	movslq	%ecx, %rcx
	leaq	-4144(%rbp,%rcx), %rdi
	movq	%rax, %rsi
	callq	memcpy@PLT
	movl	%r14d, %r15d
	cmpl	%r12d, %r14d
	jne	.LBB61_37
# BB#36:                                # %if.then.153.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	movl	$1, %edx
	leaq	-4144(%rbp), %rdi
	movq	-4232(%rbp), %rsi       # 8-byte Reload
	movq	-4224(%rbp), %rcx       # 8-byte Reload
	callq	fwrite@PLT
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB61_41
.LBB61_37:                              # %for.inc.167.us.us.us
                                        #   in Loop: Header=BB61_35 Depth=4
	addl	$1, %r13d
	cmpl	-4216(%rbp), %r13d      # 4-byte Folded Reload
	jl	.LBB61_35
# BB#38:                                # %for.inc.172.us.us.us
                                        #   in Loop: Header=BB61_34 Depth=3
	movl	-4176(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4176(%rbp)       # 4-byte Spill
	cmpl	-4236(%rbp), %eax       # 4-byte Folded Reload
	jl	.LBB61_34
# BB#39:                                # %for.inc.177.us.us
                                        #   in Loop: Header=BB61_40 Depth=2
	movl	-4212(%rbp), %eax       # 4-byte Reload
	addl	$1, %eax
	movl	%eax, -4212(%rbp)       # 4-byte Spill
	movl	-4260(%rbp), %esi       # 4-byte Reload
	cmpl	%esi, %eax
	movq	-4224(%rbp), %r14       # 8-byte Reload
	movl	$0, %r13d
	movl	%r12d, %eax
	movl	-4240(%rbp), %r10d      # 4-byte Reload
	movl	-4216(%rbp), %r9d       # 4-byte Reload
	movq	-4248(%rbp), %rbx       # 8-byte Reload
	movl	-4252(%rbp), %r8d       # 4-byte Reload
	movl	-4256(%rbp), %edi       # 4-byte Reload
	movl	-4264(%rbp), %r11d      # 4-byte Reload
	movl	-4268(%rbp), %r12d      # 4-byte Reload
	jl	.LBB61_40
	jmp	.LBB61_30
.LBB61_20:                              # %overflow.checked.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%ebx, %ebx
	je	.LBB61_29
# BB#21:                                # %overflow.checked61
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%edx, %edx
	testl	%r11d, %r11d
	je	.LBB61_28
# BB#22:                                # %vector.body.preheader
                                        #   in Loop: Header=BB61_14 Depth=1
	xorl	%ecx, %ecx
	movl	%r12d, %edx
	testl	%edi, %edi
	je	.LBB61_24
	.align	16, 0x90
.LBB61_23:                              # %vector.body.prol
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	subl	$-128, %ecx
	addl	$1, %edx
	jne	.LBB61_23
.LBB61_24:                              # %vector.body.preheader.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	cmpl	$896, %r8d              # imm = 0x380
	jb	.LBB61_28
# BB#25:                                # %vector.body.preheader.split.split
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
	subl	%ecx, %edx
	.align	16, 0x90
.LBB61_26:                              # %vector.body
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$-1024, %edx            # imm = 0xFFFFFFFFFFFFFC00
	jne	.LBB61_26
# BB#27:                                #   in Loop: Header=BB61_14 Depth=1
	movl	%r11d, %edx
.LBB61_28:                              # %middle.block
                                        #   in Loop: Header=BB61_14 Depth=1
	cmpl	%edx, %ebx
	je	.LBB61_30
	.align	16, 0x90
.LBB61_29:                              # %overflow.checked
                                        #   Parent Loop BB61_14 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addl	$1, %edx
	cmpl	%esi, %edx
	jl	.LBB61_29
.LBB61_30:                              # %for.inc.182
                                        #   in Loop: Header=BB61_14 Depth=1
	movl	-4208(%rbp), %ecx       # 4-byte Reload
	addl	$1, %ecx
	movl	%ecx, -4208(%rbp)       # 4-byte Spill
	cmpl	%r10d, %ecx
	movq	-4184(%rbp), %rcx       # 8-byte Reload
	jl	.LBB61_14
# BB#31:                                # %for.end.186
	testl	%r15d, %r15d
	jle	.LBB61_43
# BB#32:                                # %if.then.188
	imull	%ecx, %r15d
	movslq	%r15d, %rsi
	leaq	-4144(%rbp), %rdi
	movl	$1, %edx
	movq	%r14, %rcx
	callq	fwrite@PLT
	testq	%rax, %rax
	je	.LBB61_33
.LBB61_43:                              # %if.end.197
	movq	%r14, %rdi
	callq	fclose@PLT
	jmp	.LBB61_44
.LBB61_41:                              # %cleanup.184
	movq	-4224(%rbp), %rdi       # 8-byte Reload
.LBB61_42:                              # %cleanup.199
	callq	fclose@PLT
	movl	$-1, %r13d
	jmp	.LBB61_44
.LBB61_3:                               # %if.then.65
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_19:                              # %if.then.124
	movq	%r14, %rdi
	jmp	.LBB61_17
.LBB61_16:                              # %cleanup
	movq	%r12, %rdi
.LBB61_17:                              # %cleanup.106.thread415
	callq	fclose@PLT
	movl	$-2, %r13d
.LBB61_44:                              # %cleanup.209
	movl	%r13d, %eax
	addq	$4232, %rsp             # imm = 0x1088
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB61_33:                              # %if.then.194
	movq	%r14, %rdi
	jmp	.LBB61_42
.Lfunc_end61:
	.size	halide_debug_to_file, .Lfunc_end61-halide_debug_to_file

	.section	.text._ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t,@function
_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t: # @_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	64(%rdi), %r8
	movl	32(%rdi), %eax
	movl	36(%rdi), %edx
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	%esi, %rax
	movslq	16(%rdi), %rsi
	imulq	%r8, %rsi
	imulq	%rax, %rsi
	cmpq	%r8, %rsi
	cmovbeq	%r8, %rsi
	movl	%edx, %eax
	negl	%eax
	cmovll	%edx, %eax
	cltq
	movslq	20(%rdi), %rdx
	imulq	%r8, %rdx
	imulq	%rax, %rdx
	cmpq	%rsi, %rdx
	cmovbeq	%rsi, %rdx
	movl	40(%rdi), %eax
	movl	%eax, %esi
	negl	%esi
	cmovll	%eax, %esi
	movslq	24(%rdi), %rcx
	imulq	%r8, %rcx
	movslq	%esi, %rax
	imulq	%rax, %rcx
	cmpq	%rdx, %rcx
	cmovbeq	%rdx, %rcx
	movl	44(%rdi), %eax
	movl	%eax, %edx
	negl	%edx
	cmovll	%eax, %edx
	movslq	%edx, %rdx
	movslq	28(%rdi), %rax
	imulq	%r8, %rax
	imulq	%rdx, %rax
	cmpq	%rcx, %rax
	cmovbeq	%rcx, %rax
	popq	%rbp
	retq
.Lfunc_end62:
	.size	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t, .Lfunc_end62-_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t

	.section	.text._ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m,@function
_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m: # @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	memcmp@PLT
	testl	%eax, %eax
	sete	%al
	popq	%rbp
	retq
.Lfunc_end63:
	.size	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m, .Lfunc_end63-_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m

	.section	.text._ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_,@function
_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_: # @_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	64(%rdi), %eax
	cmpl	64(%rsi), %eax
	jne	.LBB64_13
# BB#1:                                 # %for.cond.preheader
	movl	48(%rdi), %eax
	cmpl	48(%rsi), %eax
	jne	.LBB64_13
# BB#2:                                 # %lor.lhs.false
	movl	16(%rdi), %eax
	cmpl	16(%rsi), %eax
	jne	.LBB64_13
# BB#3:                                 # %lor.lhs.false.10
	movl	32(%rdi), %eax
	cmpl	32(%rsi), %eax
	jne	.LBB64_13
# BB#4:                                 # %for.cond
	movl	52(%rdi), %eax
	cmpl	52(%rsi), %eax
	jne	.LBB64_13
# BB#5:                                 # %lor.lhs.false.1
	movl	20(%rdi), %eax
	cmpl	20(%rsi), %eax
	jne	.LBB64_13
# BB#6:                                 # %lor.lhs.false.10.1
	movl	36(%rdi), %eax
	cmpl	36(%rsi), %eax
	jne	.LBB64_13
# BB#7:                                 # %for.cond.1
	movl	56(%rdi), %eax
	cmpl	56(%rsi), %eax
	jne	.LBB64_13
# BB#8:                                 # %lor.lhs.false.2
	movl	24(%rdi), %eax
	cmpl	24(%rsi), %eax
	jne	.LBB64_13
# BB#9:                                 # %lor.lhs.false.10.2
	movl	40(%rdi), %eax
	cmpl	40(%rsi), %eax
	jne	.LBB64_13
# BB#10:                                # %for.cond.2
	movl	60(%rdi), %eax
	cmpl	60(%rsi), %eax
	jne	.LBB64_13
# BB#11:                                # %lor.lhs.false.3
	movl	28(%rdi), %eax
	cmpl	28(%rsi), %eax
	jne	.LBB64_13
# BB#12:                                # %lor.lhs.false.10.3
	movl	44(%rdi), %eax
	cmpl	44(%rsi), %eax
	sete	%al
	popq	%rbp
	retq
.LBB64_13:                              # %cleanup
	xorl	%eax, %eax
	popq	%rbp
	retq
.Lfunc_end64:
	.size	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_, .Lfunc_end64-_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_

	.section	.text._ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh,@function
_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh: # @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	-16(%rdi), %rax
	popq	%rbp
	retq
.Lfunc_end65:
	.size	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh, .Lfunc_end65-_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI66_0:
	.zero	16
	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_,@function
_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_: # @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%r8, %r12
	movq	%rsi, %r14
	movq	%rdi, %r13
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r13)
	movq	$0, 16(%r13)
	movq	%rdx, 24(%r13)
	movl	%ecx, 40(%r13)
	movl	$0, 44(%r13)
	movl	%r9d, 48(%r13)
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 32(%r13)
	testq	%rax, %rax
	je	.LBB66_7
# BB#1:                                 # %if.end
	movq	64(%r12), %rcx
	movq	%rcx, 120(%r13)
	vmovups	(%r12), %ymm0
	vmovups	32(%r12), %ymm1
	vmovups	%ymm1, 88(%r13)
	vmovups	%ymm0, 56(%r13)
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 56(%r13)
	movq	24(%r13), %rcx
	testq	%rcx, %rcx
	je	.LBB66_4
# BB#2:                                 # %for.body.preheader
	movb	(%r14), %dl
	movb	%dl, (%rax)
	cmpq	$2, %rcx
	jb	.LBB66_4
# BB#3:                                 # %for.body.for.body_crit_edge.preheader
	movb	1(%r14), %cl
	movb	%cl, 1(%rax)
	movl	$2, %eax
	cmpq	$2, 24(%r13)
	jbe	.LBB66_4
	.align	16, 0x90
.LBB66_8:                               # %for.body.for.body_crit_edge.for.body.for.body_crit_edge_crit_edge
                                        # =>This Inner Loop Header: Depth=1
	movq	32(%r13), %rcx
	movb	(%r14,%rax), %dl
	movb	%dl, (%rcx,%rax)
	addq	$1, %rax
	cmpq	24(%r13), %rax
	jb	.LBB66_8
.LBB66_4:                               # %for.cond.11.preheader
	movb	$1, %r15b
	cmpl	$0, 48(%r13)
	je	.LBB66_7
# BB#5:
	movq	16(%rbp), %r14
	xorl	%ebx, %ebx
	.align	16, 0x90
.LBB66_6:                               # %for.body.15
                                        # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	movl	%ebx, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movl	%ebx, %ecx
	movq	(%r14,%rcx,8), %rcx
	movq	64(%rcx), %rdx
	movq	%rdx, 64(%rax)
	vmovups	(%rcx), %ymm0
	vmovups	32(%rcx), %ymm1
	vmovups	%ymm1, 32(%rax)
	vmovups	%ymm0, (%rax)
	addl	$1, %ebx
	cmpl	48(%r13), %ebx
	jb	.LBB66_6
.LBB66_7:                               # %return
	movb	%r15b, %al
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end66:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_, .Lfunc_end66-_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi,@function
_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi: # @_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	leaq	128(%rdi,%rax,8), %rax
	popq	%rbp
	retq
.Lfunc_end67:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi, .Lfunc_end67-_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi

	.section	.text._ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv,@function
_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv: # @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	32(%r14), %rsi
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	callq	halide_free@PLT
	cmpl	$0, 48(%r14)
	je	.LBB68_2
	.align	16, 0x90
.LBB68_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_device_free@PLT
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	xorl	%edi, %edi
	movq	%rax, %rsi
	callq	halide_free@PLT
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB68_1
.LBB68_2:                               # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end68:
	.size	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv, .Lfunc_end68-_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv

	.section	.text._ZN6Halide7Runtime8Internal8djb_hashEPKhm,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal8djb_hashEPKhm
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal8djb_hashEPKhm,@function
_ZN6Halide7Runtime8Internal8djb_hashEPKhm: # @_ZN6Halide7Runtime8Internal8djb_hashEPKhm
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$5381, %eax             # imm = 0x1505
	testq	%rsi, %rsi
	je	.LBB69_8
# BB#1:                                 # %for.body.preheader
	leaq	-1(%rsi), %r8
	xorl	%ecx, %ecx
	testb	$7, %sil
	je	.LBB69_2
# BB#3:                                 # %for.body.prol.preheader
	movl	%esi, %r9d
	andl	$7, %r9d
	movl	$5381, %eax             # imm = 0x1505
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB69_4:                               # %for.body.prol
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %edx
	movzbl	(%rdi,%rcx), %eax
	addl	%edx, %eax
	addq	$1, %rcx
	cmpq	%rcx, %r9
	jne	.LBB69_4
	jmp	.LBB69_5
.LBB69_2:
	movl	$5381, %eax             # imm = 0x1505
.LBB69_5:                               # %for.body.preheader.split
	cmpq	$7, %r8
	jb	.LBB69_8
# BB#6:                                 # %for.body.preheader.split.split
	movq	%rcx, %r9
	subq	%rsi, %r9
	leaq	(%rdi,%rcx), %r8
	xorl	%esi, %esi
	.align	16, 0x90
.LBB69_7:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	imull	$33, %eax, %eax
	movzbl	(%r8,%rsi), %edx
	addl	%eax, %edx
	imull	$33, %edx, %r10d
	leaq	(%rcx,%rsi), %rdx
	movzbl	1(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	2(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	3(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	4(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	5(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	6(%rdi,%rdx), %eax
	addl	%r10d, %eax
	imull	$33, %eax, %r10d
	movzbl	7(%rdi,%rdx), %eax
	addl	%r10d, %eax
	addq	$8, %rsi
	movq	%r9, %rdx
	addq	%rsi, %rdx
	jne	.LBB69_7
.LBB69_8:                               # %for.cond.cleanup
	popq	%rbp
	retq
.Lfunc_end69:
	.size	_ZN6Halide7Runtime8Internal8djb_hashEPKhm, .Lfunc_end69-_ZN6Halide7Runtime8Internal8djb_hashEPKhm

	.section	.text._ZN6Halide7Runtime8Internal11prune_cacheEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal11prune_cacheEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal11prune_cacheEv,@function
_ZN6Halide7Runtime8Internal11prune_cacheEv: # @_ZN6Halide7Runtime8Internal11prune_cacheEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	movq	(%r15), %r14
	testq	%r14, %r14
	je	.LBB70_21
# BB#1:                                 # %entry
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r12
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
	jmp	.LBB70_2
	.align	16, 0x90
.LBB70_20:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, %r14
.LBB70_2:                               # %entry
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB70_6 Depth 2
                                        #     Child Loop BB70_23 Depth 2
	cmpq	%rcx, %rax
	jle	.LBB70_21
# BB#3:                                 # %while.body
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	8(%r14), %r13
	cmpl	$0, 44(%r14)
	jne	.LBB70_19
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB70_2 Depth=1
	movzbl	40(%r14), %eax
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rcx
	cmpq	%r14, %rcx
	je	.LBB70_5
	.align	16, 0x90
.LBB70_6:                               # %while.cond.9
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rcx, %rax
	testq	%rax, %rax
	je	.LBB70_22
# BB#7:                                 # %land.rhs.11
                                        #   in Loop: Header=BB70_6 Depth=2
	movq	(%rax), %rcx
	cmpq	%r14, %rcx
	jne	.LBB70_6
# BB#8:                                 # %if.end
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	%rcx, (%rax)
	jmp	.LBB70_9
.LBB70_5:                               # %if.then.6
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	(%r14), %rcx
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rdx
	movq	%rcx, (%rdx,%rax,8)
.LBB70_9:                               # %if.end.21
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	%r14, (%r15)
	jne	.LBB70_11
# BB#10:                                # %if.then.23
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, (%r15)
.LBB70_11:                              # %if.end.24
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	je	.LBB70_13
# BB#12:                                # %if.then.26
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	%rax, 16(%r13)
.LBB70_13:                              # %if.end.28
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	%r14, (%rax)
	jne	.LBB70_15
# BB#14:                                # %if.then.30
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	16(%r14), %rax
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rcx
	movq	%rax, (%rcx)
.LBB70_15:                              # %if.end.32
                                        #   in Loop: Header=BB70_2 Depth=1
	cmpq	$0, 16(%r14)
	je	.LBB70_17
# BB#16:                                # %if.then.35
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r13, 16(%r14)
.LBB70_17:                              # %for.cond.preheader
                                        #   in Loop: Header=BB70_2 Depth=1
	xorl	%ebx, %ebx
	cmpl	$0, 48(%r14)
	je	.LBB70_18
	.align	16, 0x90
.LBB70_23:                              # %for.body
                                        #   Parent Loop BB70_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	subq	%rax, (%r12)
	addl	$1, %ebx
	cmpl	48(%r14), %ebx
	jb	.LBB70_23
.LBB70_18:                              # %for.cond.cleanup
                                        #   in Loop: Header=BB70_2 Depth=1
	movq	%r14, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	movq	(%r12), %rax
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rcx
.LBB70_19:                              # %while.cond.backedge
                                        #   in Loop: Header=BB70_2 Depth=1
	testq	%r13, %r13
	jne	.LBB70_20
.LBB70_21:                              # %while.end.41
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB70_22:                              # %if.then.18
	leaq	.L.str.3.29(%rip), %rsi
	xorl	%edi, %edi
	callq	halide_print@PLT
	callq	abort@PLT
.Lfunc_end70:
	.size	_ZN6Halide7Runtime8Internal11prune_cacheEv, .Lfunc_end70-_ZN6Halide7Runtime8Internal11prune_cacheEv

	.section	.text.halide_memoization_cache_set_size,"ax",@progbits
	.weak	halide_memoization_cache_set_size
	.align	16, 0x90
	.type	halide_memoization_cache_set_size,@function
halide_memoization_cache_set_size:      # @halide_memoization_cache_set_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	testq	%rdi, %rdi
	movl	$1048576, %ebx          # imm = 0x100000
	cmovneq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal14max_cache_sizeE@GOTPCREL(%rip), %rax
	movq	%rbx, (%rax)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end71:
	.size	halide_memoization_cache_set_size, .Lfunc_end71-halide_memoization_cache_set_size

	.section	.text.halide_memoization_cache_lookup,"ax",@progbits
	.weak	halide_memoization_cache_lookup
	.align	16, 0x90
	.type	halide_memoization_cache_lookup,@function
halide_memoization_cache_lookup:        # @halide_memoization_cache_lookup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%r9, -104(%rbp)         # 8-byte Spill
	movl	%r8d, -84(%rbp)         # 4-byte Spill
	movq	%rcx, -96(%rbp)         # 8-byte Spill
	movq	%rsi, -80(%rbp)         # 8-byte Spill
	movq	%rdi, -72(%rbp)         # 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %rdi
	movq	%rax, %rsi
	callq	_ZN6Halide7Runtime8Internal8djb_hashEPKhm@PLT
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movzbl	%al, %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rax
	movq	(%rax,%rbx,8), %rbx
	testq	%rbx, %rbx
	je	.LBB72_11
# BB#1:                                 # %while.body.lr.ph
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_18
# BB#2:                                 # %while.body.lr.ph.split.us
	cltq
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB72_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB72_8 Depth 2
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_10
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_10
# BB#5:                                 # %land.lhs.true.7.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#6:                                 # %land.lhs.true.10.us
                                        #   in Loop: Header=BB72_3 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_10
# BB#7:                                 # %land.lhs.true.13.us
                                        #   in Loop: Header=BB72_3 Depth=1
	xorl	%r15d, %r15d
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	movq	-104(%rbp), %r14        # 8-byte Reload
	movl	$1, %r13d
	jne	.LBB72_10
	.align	16, 0x90
.LBB72_8:                               # %for.body.us
                                        #   Parent Loop BB72_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%r14), %r12
	movq	%rbx, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	jge	.LBB72_9
# BB#43:                                # %for.body.us
                                        #   in Loop: Header=BB72_8 Depth=2
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %r14
	testb	%al, %al
	jne	.LBB72_8
.LBB72_9:                               # %for.cond.cleanup.us
                                        #   in Loop: Header=BB72_3 Depth=1
	testb	%al, %al
	jne	.LBB72_23
	.align	16, 0x90
.LBB72_10:                              # %if.end.64.us
                                        #   in Loop: Header=BB72_3 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_3
	jmp	.LBB72_11
	.align	16, 0x90
.LBB72_18:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movl	-44(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 40(%rbx)
	jne	.LBB72_17
# BB#19:                                # %land.lhs.true
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%rbx)
	jne	.LBB72_17
# BB#20:                                # %land.lhs.true.7
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	32(%rbx), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#21:                                # %land.lhs.true.10
                                        #   in Loop: Header=BB72_18 Depth=1
	leaq	56(%rbx), %rdi
	movq	-96(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB72_17
# BB#22:                                # %land.lhs.true.13
                                        #   in Loop: Header=BB72_18 Depth=1
	movl	-84(%rbp), %eax         # 4-byte Reload
	cmpl	%eax, 48(%rbx)
	je	.LBB72_23
	.align	16, 0x90
.LBB72_17:                              # %if.end.64
                                        #   in Loop: Header=BB72_18 Depth=1
	movq	(%rbx), %rbx
	testq	%rbx, %rbx
	jne	.LBB72_18
.LBB72_11:                              # %for.cond.66.preheader
	movl	$1, %r12d
	movl	-84(%rbp), %eax         # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB72_42
# BB#12:                                # %for.body.69.lr.ph
	movslq	%eax, %r12
	xorl	%r15d, %r15d
	movq	-104(%rbp), %r13        # 8-byte Reload
	movq	%r13, %r14
	.align	16, 0x90
.LBB72_13:                              # %for.body.69
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r14), %rbx
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	leaq	16(%rax), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_malloc@PLT
	movq	%rax, 8(%rbx)
	testq	%rax, %rax
	je	.LBB72_14
# BB#40:                                # %for.inc.103
                                        #   in Loop: Header=BB72_13 Depth=1
	addq	$16, %rax
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	-44(%rbp), %ecx         # 4-byte Reload
	movl	%ecx, 8(%rax)
	movq	$0, (%rax)
	addq	$1, %r15
	addq	$8, %r14
	cmpq	%r12, %r15
	jl	.LBB72_13
# BB#41:
	movl	$1, %r12d
	jmp	.LBB72_42
.LBB72_14:                              # %for.cond.79.preheader
	movl	$-1, %r12d
	testl	%r15d, %r15d
	jle	.LBB72_42
# BB#15:                                # %for.body.82.lr.ph
	movslq	%r15d, %r14
	leaq	-8(%r13,%r14,8), %rbx
	addq	$1, %r14
	movq	-72(%rbp), %r15         # 8-byte Reload
	.align	16, 0x90
.LBB72_16:                              # %for.body.82
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r15, %rdi
	movq	%rax, %rsi
	callq	halide_free@PLT
	movq	(%rbx), %rax
	movq	$0, 8(%rax)
	addq	$-1, %r14
	addq	$-8, %rbx
	cmpq	$1, %r14
	jg	.LBB72_16
	jmp	.LBB72_42
.LBB72_23:                              # %if.then.22
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %r14
	cmpq	(%r14), %rbx
	movq	-104(%rbp), %r12        # 8-byte Reload
	movl	-84(%rbp), %r13d        # 4-byte Reload
	je	.LBB72_36
# BB#24:                                # %if.then.24
	cmpq	$0, 8(%rbx)
	jne	.LBB72_26
# BB#25:                                # %if.then.26
	leaq	.L.str.4.30(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_26:                              # %if.end
	movq	16(%rbx), %rax
	testq	%rax, %rax
	je	.LBB72_28
# BB#27:                                # %if.then.28
	movq	8(%rbx), %rcx
	movq	%rcx, 8(%rax)
	jmp	.LBB72_31
.LBB72_28:                              # %if.else
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %r15
	cmpq	%rbx, (%r15)
	je	.LBB72_30
# BB#29:                                # %if.then.33
	leaq	.L.str.5.31(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB72_30:                              # %if.end.34
	movq	8(%rbx), %rax
	movq	%rax, (%r15)
.LBB72_31:                              # %if.end.36
	movq	8(%rbx), %rax
	testq	%rax, %rax
	jne	.LBB72_33
# BB#32:                                # %if.then.39
	leaq	.L.str.6.32(%rip), %rsi
	movq	-72(%rbp), %rdi         # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
	movq	8(%rbx), %rax
.LBB72_33:                              # %if.end.40
	movq	16(%rbx), %rcx
	movq	%rcx, 16(%rax)
	movq	$0, 8(%rbx)
	movq	(%r14), %rax
	movq	%rax, 16(%rbx)
	movq	(%r14), %rax
	testq	%rax, %rax
	je	.LBB72_35
# BB#34:                                # %if.then.47
	movq	%rbx, 8(%rax)
.LBB72_35:                              # %if.end.49
	movq	%rbx, (%r14)
.LBB72_36:                              # %for.cond.52.preheader
	testl	%r13d, %r13d
	jle	.LBB72_39
# BB#37:
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB72_38:                              # %for.body.55
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12), %r15
	movq	%rbx, %rdi
	movl	%r14d, %esi
	vzeroupper
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	64(%rax), %rcx
	movq	%rcx, 64(%r15)
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	%ymm1, 32(%r15)
	vmovups	%ymm0, (%r15)
	addl	$1, %r14d
	addq	$8, %r12
	cmpl	%r14d, %r13d
	jne	.LBB72_38
.LBB72_39:                              # %for.cond.cleanup.54
	addl	%r13d, 44(%rbx)
	xorl	%r12d, %r12d
.LBB72_42:                              # %cleanup.108
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	vzeroupper
	callq	halide_mutex_unlock@PLT
	movl	%r12d, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end72:
	.size	halide_memoization_cache_lookup, .Lfunc_end72-halide_memoization_cache_lookup

	.section	.text.halide_memoization_cache_store,"ax",@progbits
	.weak	halide_memoization_cache_store
	.align	16, 0x90
	.type	halide_memoization_cache_store,@function
halide_memoization_cache_store:         # @halide_memoization_cache_store
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movq	%r9, %r13
	movl	%r8d, %r14d
	movq	%rcx, -80(%rbp)         # 8-byte Spill
	movl	%edx, -100(%rbp)        # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, -120(%rbp)        # 8-byte Spill
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movl	8(%rax), %ebx
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movzbl	%bl, %eax
	movq	%rax, -112(%rbp)        # 8-byte Spill
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %r12
	testq	%r12, %r12
	je	.LBB73_14
# BB#1:                                 # %while.body.lr.ph
	movslq	-100(%rbp), %rax        # 4-byte Folded Reload
	movq	%rax, -56(%rbp)         # 8-byte Spill
	testl	%r14d, %r14d
	jle	.LBB73_16
# BB#2:                                 # %while.body.lr.ph.split.us
	movslq	%r14d, %rax
	movq	%rax, -64(%rbp)         # 8-byte Spill
	.align	16, 0x90
.LBB73_3:                               # %while.body.us
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB73_9 Depth 2
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_13
# BB#4:                                 # %land.lhs.true.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_13
# BB#5:                                 # %land.lhs.true.12.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#6:                                 # %land.lhs.true.15.us
                                        #   in Loop: Header=BB73_3 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_13
# BB#7:                                 # %land.lhs.true.18.us
                                        #   in Loop: Header=BB73_3 Depth=1
	cmpl	%r14d, 48(%r12)
	jne	.LBB73_13
# BB#8:                                 #   in Loop: Header=BB73_3 Depth=1
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	movq	%r14, -136(%rbp)        # 8-byte Spill
	movb	$1, %al
	movl	%eax, -68(%rbp)         # 4-byte Spill
	xorl	%r15d, %r15d
	movq	%r13, %rbx
	movq	%r13, -128(%rbp)        # 8-byte Spill
	movl	$1, %r13d
	.align	16, 0x90
.LBB73_9:                               # %for.body.us
                                        #   Parent Loop BB73_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r14
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	%rax, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	movb	%al, -41(%rbp)          # 1-byte Spill
	movq	%r12, %rdi
	movl	%r15d, %esi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry6bufferEi@PLT
	movq	8(%rax), %rax
	cmpq	8(%r14), %rax
	jne	.LBB73_11
# BB#10:                                #   in Loop: Header=BB73_9 Depth=2
	movl	$0, -68(%rbp)           # 4-byte Folded Spill
.LBB73_11:                              # %select.mid
                                        #   in Loop: Header=BB73_9 Depth=2
	movb	-41(%rbp), %cl          # 1-byte Reload
	cmpq	-64(%rbp), %r13         # 8-byte Folded Reload
	setl	%al
	addq	$1, %r13
	addl	$1, %r15d
	addq	$8, %rbx
	testb	%cl, %al
	jne	.LBB73_9
# BB#12:                                # %for.cond.cleanup.us
                                        #   in Loop: Header=BB73_3 Depth=1
	testb	%cl, %cl
	movq	-128(%rbp), %r13        # 8-byte Reload
	movq	-136(%rbp), %r14        # 8-byte Reload
	movq	-88(%rbp), %r15         # 8-byte Reload
	movq	-96(%rbp), %rbx         # 8-byte Reload
	jne	.LBB73_22
	.align	16, 0x90
.LBB73_13:                              # %if.end.56.us
                                        #   in Loop: Header=BB73_3 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_3
	jmp	.LBB73_14
	.align	16, 0x90
.LBB73_16:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%ebx, 40(%r12)
	jne	.LBB73_21
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	-56(%rbp), %rax         # 8-byte Reload
	cmpq	%rax, 24(%r12)
	jne	.LBB73_21
# BB#18:                                # %land.lhs.true.12
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	32(%r12), %rdi
	movq	%r15, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#19:                                # %land.lhs.true.15
                                        #   in Loop: Header=BB73_16 Depth=1
	leaq	56(%r12), %rdi
	movq	-80(%rbp), %rsi         # 8-byte Reload
	callq	_ZN6Halide7Runtime8Internal12bounds_equalERK8buffer_tS4_@PLT
	testb	%al, %al
	je	.LBB73_21
# BB#20:                                # %land.lhs.true.18
                                        #   in Loop: Header=BB73_16 Depth=1
	cmpl	%r14d, 48(%r12)
	je	.LBB73_24
	.align	16, 0x90
.LBB73_21:                              # %if.end.56
                                        #   in Loop: Header=BB73_16 Depth=1
	movq	(%r12), %r12
	testq	%r12, %r12
	jne	.LBB73_16
.LBB73_14:                              # %for.cond.60.preheader
	movq	%rbx, -96(%rbp)         # 8-byte Spill
	movq	%r15, -88(%rbp)         # 8-byte Spill
	xorl	%r15d, %r15d
	testl	%r14d, %r14d
	movq	%r14, %rax
	jle	.LBB73_15
# BB#26:
	movq	%r13, %rbx
	movl	%eax, %r14d
	movq	%rax, %r12
	.align	16, 0x90
.LBB73_27:                              # %for.body.63
                                        # =>This Inner Loop Header: Depth=1
	movq	(%rbx), %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	addq	%rax, %r15
	addq	$8, %rbx
	addl	$-1, %r14d
	jne	.LBB73_27
	jmp	.LBB73_28
.LBB73_15:
	movq	%rax, %r12
.LBB73_28:                              # %for.cond.cleanup.62
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %r14
	addq	%r15, (%r14)
	callq	_ZN6Halide7Runtime8Internal11prune_cacheEv@PLT
	movq	%r12, %rbx
	leal	-1(%rbx), %eax
	cltq
	leaq	(%rax,%rax,8), %rax
	leaq	200(,%rax,8), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB73_29
# BB#31:                                # %if.end.96
	movslq	-100(%rbp), %rdx        # 4-byte Folded Reload
	movq	%r13, (%rsp)
	movq	%r12, %rdi
	movq	-88(%rbp), %rsi         # 8-byte Reload
	movq	-96(%rbp), %rcx         # 8-byte Reload
	movq	-80(%rbp), %r8          # 8-byte Reload
	movl	%ebx, %r9d
	callq	_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjRK8buffer_tiPPS5_@PLT
	testb	%al, %al
	je	.LBB73_32
# BB#35:                                # %if.end.120
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	(%rcx,%rax,8), %rax
	movq	%rax, (%r12)
	movq	_ZN6Halide7Runtime8Internal18most_recently_usedE@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	movq	%rcx, 16(%r12)
	testq	%rcx, %rcx
	je	.LBB73_37
# BB#36:                                # %if.then.125
	movq	%r12, 8(%rcx)
.LBB73_37:                              # %if.end.126
	movq	%r12, (%rax)
	movq	_ZN6Halide7Runtime8Internal19least_recently_usedE@GOTPCREL(%rip), %rax
	cmpq	$0, (%rax)
	jne	.LBB73_39
# BB#38:                                # %if.then.128
	movq	%r12, (%rax)
.LBB73_39:                              # %if.end.129
	movq	-112(%rbp), %rax        # 8-byte Reload
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %rcx
	movq	%r12, (%rcx,%rax,8)
	movl	%ebx, 44(%r12)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_40:                              # %for.body.137
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	%r12, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_40
	jmp	.LBB73_41
.LBB73_29:                              # %if.then.79
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_30:                              # %for.body.86
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_30
	jmp	.LBB73_41
.LBB73_32:                              # %if.then.103
	subq	%r15, (%r14)
	testl	%ebx, %ebx
	jle	.LBB73_33
	.align	16, 0x90
.LBB73_34:                              # %for.body.110
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %ebx
	jne	.LBB73_34
.LBB73_33:                              # %for.cond.cleanup.109
	movq	-120(%rbp), %rdi        # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB73_41:                              # %cleanup.153
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%eax, %eax
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB73_22:                              # %if.then.36
	movl	-68(%rbp), %eax         # 4-byte Reload
	testb	$1, %al
	jne	.LBB73_24
# BB#23:                                # %if.then.38
	leaq	.L.str.8.33(%rip), %rsi
	movq	-120(%rbp), %rdi        # 8-byte Reload
	callq	halide_print@PLT
	callq	abort@PLT
.LBB73_24:                              # %for.cond.42.preheader
	testl	%r14d, %r14d
	jle	.LBB73_41
	.align	16, 0x90
.LBB73_25:                              # %for.body.45
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r13), %rax
	movq	8(%rax), %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	$0, (%rax)
	addq	$8, %r13
	addl	$-1, %r14d
	jne	.LBB73_25
	jmp	.LBB73_41
.Lfunc_end73:
	.size	halide_memoization_cache_store, .Lfunc_end73-halide_memoization_cache_store

	.section	.text.halide_memoization_cache_release,"ax",@progbits
	.weak	halide_memoization_cache_release
	.align	16, 0x90
	.type	halide_memoization_cache_release,@function
halide_memoization_cache_release:       # @halide_memoization_cache_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	movq	%rsi, %rdi
	callq	_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh@PLT
	movq	(%rax), %rbx
	testq	%rbx, %rbx
	je	.LBB74_4
# BB#1:                                 # %if.else
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movl	44(%rbx), %eax
	testl	%eax, %eax
	jne	.LBB74_3
# BB#2:                                 # %if.then.6
	leaq	.L.str.11.34(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
	movl	44(%rbx), %eax
.LBB74_3:                               # %if.end
	addl	$-1, %eax
	movl	%eax, 44(%rbx)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.LBB74_4:                               # %if.then
	movq	%r14, %rdi
	movq	%rax, %rsi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_free@PLT         # TAILCALL
.Lfunc_end74:
	.size	halide_memoization_cache_release, .Lfunc_end74-halide_memoization_cache_release

	.section	.text.halide_memoization_cache_cleanup,"ax",@progbits
	.weak	halide_memoization_cache_cleanup
	.align	16, 0x90
	.type	halide_memoization_cache_cleanup,@function
halide_memoization_cache_cleanup:       # @halide_memoization_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	xorl	%r14d, %r14d
	movq	_ZN6Halide7Runtime8Internal13cache_entriesE@GOTPCREL(%rip), %r15
	.align	16, 0x90
.LBB75_1:                               # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB75_2 Depth 2
	movq	(%r15,%r14,8), %rbx
	movq	$0, (%r15,%r14,8)
	testq	%rbx, %rbx
	je	.LBB75_3
	.align	16, 0x90
.LBB75_2:                               # %while.body
                                        #   Parent Loop BB75_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	(%rbx), %r12
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv@PLT
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	%r12, %rbx
	testq	%r12, %r12
	jne	.LBB75_2
.LBB75_3:                               # %while.end
                                        #   in Loop: Header=BB75_1 Depth=1
	addq	$1, %r14
	cmpq	$256, %r14              # imm = 0x100
	jne	.LBB75_1
# BB#4:                                 # %for.cond.cleanup
	movq	_ZN6Halide7Runtime8Internal18current_cache_sizeE@GOTPCREL(%rip), %rax
	movq	$0, (%rax)
	movq	_ZN6Halide7Runtime8Internal16memoization_lockE@GOTPCREL(%rip), %rdi
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_mutex_destroy@PLT # TAILCALL
.Lfunc_end75:
	.size	halide_memoization_cache_cleanup, .Lfunc_end75-halide_memoization_cache_cleanup

	.section	.text.halide_cache_cleanup,"ax",@progbits
	.weak	halide_cache_cleanup
	.align	16, 0x90
	.type	halide_cache_cleanup,@function
halide_cache_cleanup:                   # @halide_cache_cleanup
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_memoization_cache_cleanup@PLT # TAILCALL
.Lfunc_end76:
	.size	halide_cache_cleanup, .Lfunc_end76-halide_cache_cleanup

	.section	.text.halide_string_to_string,"ax",@progbits
	.weak	halide_string_to_string
	.align	16, 0x90
	.type	halide_string_to_string,@function
halide_string_to_string:                # @halide_string_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB77_3
# BB#1:                                 # %while.body.preheader
	je	.LBB77_2
	.align	16, 0x90
.LBB77_4:                               # %if.end.3
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rdx), %al
	movb	%al, (%rdi)
	testb	%al, %al
	je	.LBB77_3
# BB#5:                                 # %if.end.6
                                        #   in Loop: Header=BB77_4 Depth=1
	addq	$1, %rdi
	addq	$1, %rdx
	cmpq	%rdi, %rsi
	jne	.LBB77_4
# BB#6:
	movq	%rsi, %rdi
.LBB77_2:                               # %if.then.2
	movb	$0, -1(%rdi)
.LBB77_3:                               # %return
	movq	%rdi, %rax
	popq	%rbp
	retq
.Lfunc_end77:
	.size	halide_string_to_string, .Lfunc_end77-halide_string_to_string

	.section	.text.halide_uint64_to_string,"ax",@progbits
	.weak	halide_uint64_to_string
	.align	16, 0x90
	.type	halide_uint64_to_string,@function
halide_uint64_to_string:                # @halide_uint64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	movb	$0, -1(%rbp)
	leaq	-2(%rbp), %r8
	movl	$1, %r9d
	testq	%rdx, %rdx
	jne	.LBB78_2
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jle	.LBB78_4
	.align	16, 0x90
.LBB78_2:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdx, %r11
	movl	%r9d, %r10d
	movabsq	$-3689348814741910323, %r9 # imm = 0xCCCCCCCCCCCCCCCD
	mulxq	%r9, %rax, %rdx
	shrq	$3, %rdx
	imull	$-10, %edx, %eax
	leal	48(%r11,%rax), %eax
	movb	%al, (%r8)
	addq	$-1, %r8
	leal	1(%r10), %r9d
	cmpq	$9, %r11
	ja	.LBB78_2
# BB#3:                                 # %for.body
                                        #   in Loop: Header=BB78_2 Depth=1
	cmpl	%ecx, %r10d
	jl	.LBB78_2
.LBB78_4:                               # %for.cond.cleanup
	addq	$1, %r8
	movq	%r8, %rdx
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end78:
	.size	halide_uint64_to_string, .Lfunc_end78-halide_uint64_to_string

	.section	.text.halide_int64_to_string,"ax",@progbits
	.weak	halide_int64_to_string
	.align	16, 0x90
	.type	halide_int64_to_string,@function
halide_int64_to_string:                 # @halide_int64_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	cmpq	%rsi, %rdi
	jae	.LBB79_3
# BB#1:                                 # %entry
	testq	%rdx, %rdx
	jns	.LBB79_3
# BB#2:                                 # %if.then
	movb	$45, (%rdi)
	addq	$1, %rdi
	negq	%rdx
.LBB79_3:                               # %if.end
	popq	%rbp
	jmp	halide_uint64_to_string@PLT # TAILCALL
.Lfunc_end79:
	.size	halide_int64_to_string, .Lfunc_end79-halide_int64_to_string

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI80_0:
	.quad	-9223372036854775808    # 0x8000000000000000
	.quad	-9223372036854775808    # 0x8000000000000000
.LCPI80_6:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI80_7:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI80_1:
	.quad	4607182418800017408     # double 1
.LCPI80_2:
	.quad	4621819117588971520     # double 10
.LCPI80_3:
	.quad	4696837146684686336     # double 1.0E+6
.LCPI80_4:
	.quad	4602678819172646912     # double 0.5
.LCPI80_5:
	.quad	4890909195324358656     # double 9.2233720368547758E+18
	.section	.text.halide_double_to_string,"ax",@progbits
	.weak	halide_double_to_string
	.align	16, 0x90
	.type	halide_double_to_string,@function
halide_double_to_string:                # @halide_double_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$552, %rsp              # imm = 0x228
	movl	%edx, %ebx
	vmovapd	%xmm0, -592(%rbp)       # 16-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	vmovsd	%xmm0, -48(%rbp)
	movq	$0, -56(%rbp)
	leaq	-56(%rbp), %rdi
	leaq	-48(%rbp), %rsi
	movl	$8, %edx
	callq	memcpy@PLT
	movq	-56(%rbp), %rax
	movb	$52, %cl
	bzhiq	%rcx, %rax, %r13
	movq	%rax, %r15
	shrq	$52, %r15
	andl	$2047, %r15d            # imm = 0x7FF
	shrq	$63, %rax
	cmpl	$2047, %r15d            # imm = 0x7FF
	jne	.LBB80_9
# BB#1:                                 # %if.then
	testq	%r13, %r13
	je	.LBB80_6
# BB#2:                                 # %if.then.4
	testl	%eax, %eax
	je	.LBB80_5
# BB#3:                                 # %if.then.6
	leaq	.L.str.45(%rip), %rdx
	jmp	.LBB80_4
.LBB80_9:                               # %if.else.15
	testq	%r13, %r13
	jne	.LBB80_18
# BB#10:                                # %if.else.15
	testl	%r15d, %r15d
	jne	.LBB80_18
# BB#11:                                # %if.then.18
	testl	%ebx, %ebx
	je	.LBB80_15
# BB#12:                                # %if.then.20
	testl	%eax, %eax
	je	.LBB80_14
# BB#13:                                # %if.then.22
	leaq	.L.str.4.49(%rip), %rdx
	jmp	.LBB80_4
.LBB80_18:                              # %if.end.32
	testl	%eax, %eax
	je	.LBB80_19
# BB#20:                                # %if.then.34
	leaq	.L.str.8.53(%rip), %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %r14
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
	vxorpd	.LCPI80_0(%rip), %xmm2, %xmm2
	vmovlpd	%xmm2, -48(%rbp)
	jmp	.LBB80_21
.LBB80_6:                               # %if.else.9
	testl	%eax, %eax
	je	.LBB80_8
# BB#7:                                 # %if.then.11
	leaq	.L.str.2.47(%rip), %rdx
	jmp	.LBB80_4
.LBB80_5:                               # %if.else
	leaq	.L.str.1.46(%rip), %rdx
	jmp	.LBB80_4
.LBB80_15:                              # %if.else.26
	testl	%eax, %eax
	je	.LBB80_17
# BB#16:                                # %if.then.28
	leaq	.L.str.6.51(%rip), %rdx
	jmp	.LBB80_4
.LBB80_19:
	vmovapd	-592(%rbp), %xmm2       # 16-byte Reload
.LBB80_21:                              # %if.end.37
	testl	%ebx, %ebx
	je	.LBB80_36
# BB#22:                                # %while.condthread-pre-split
	xorl	%ebx, %ebx
	vmovsd	.LCPI80_1(%rip), %xmm0  # xmm0 = mem[0],zero
	vucomisd	%xmm2, %xmm0
	jbe	.LBB80_26
# BB#23:
	vmovsd	.LCPI80_2(%rip), %xmm1  # xmm1 = mem[0],zero
	.align	16, 0x90
.LBB80_24:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	vmulsd	%xmm1, %xmm2, %xmm2
	addl	$-1, %ebx
	vucomisd	%xmm2, %xmm0
	ja	.LBB80_24
# BB#25:                                # %while.cond.while.cond.41thread-pre-split_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_26:                              # %while.cond.41thread-pre-split
	vucomisd	.LCPI80_2(%rip), %xmm2
	jb	.LBB80_30
# BB#27:
	vmovsd	.LCPI80_2(%rip), %xmm0  # xmm0 = mem[0],zero
	.align	16, 0x90
.LBB80_28:                              # %while.body.43
                                        # =>This Inner Loop Header: Depth=1
	vdivsd	%xmm0, %xmm2, %xmm2
	addl	$1, %ebx
	vucomisd	%xmm0, %xmm2
	jae	.LBB80_28
# BB#29:                                # %while.cond.41.while.end.44_crit_edge
	vmovsd	%xmm2, -48(%rbp)
.LBB80_30:                              # %while.end.44
	vmovsd	.LCPI80_3(%rip), %xmm0  # xmm0 = mem[0],zero
	vfmadd213sd	.LCPI80_4(%rip), %xmm2, %xmm0
	vmovsd	.LCPI80_5(%rip), %xmm1  # xmm1 = mem[0],zero
	vsubsd	%xmm1, %xmm0, %xmm2
	vcvttsd2si	%xmm2, %rax
	movabsq	$-9223372036854775808, %rcx # imm = 0x8000000000000000
	xorq	%rax, %rcx
	vcvttsd2si	%xmm0, %rdx
	vucomisd	%xmm1, %xmm0
	cmovaeq	%rcx, %rdx
	movabsq	$4835703278458516699, %rax # imm = 0x431BDE82D7B634DB
	mulxq	%rax, %rcx, %rax
	shrq	$18, %rax
	imulq	$-1000000, %rax, %r15   # imm = 0xFFFFFFFFFFF0BDC0
	addq	%rdx, %r15
	movl	$1, %ecx
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	js	.LBB80_32
# BB#31:                                # %if.then.54
	leaq	.L.str.10.55(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	jmp	.LBB80_33
.LBB80_36:                              # %if.else.62
	testl	%r15d, %r15d
	je	.LBB80_37
# BB#38:                                # %if.end.66
	movabsq	$4503599627370496, %rax # imm = 0x10000000000000
	orq	%rax, %r13
	leal	-1075(%r15), %ecx
	cmpl	$1074, %r15d            # imm = 0x432
	ja	.LBB80_39
# BB#40:                                # %if.then.72
	cmpl	$-52, %ecx
	jge	.LBB80_42
# BB#41:
	xorl	%eax, %eax
	jmp	.LBB80_43
.LBB80_8:                               # %if.else.13
	leaq	.L.str.3.48(%rip), %rdx
	jmp	.LBB80_4
.LBB80_14:                              # %if.else.24
	leaq	.L.str.5.50(%rip), %rdx
	jmp	.LBB80_4
.LBB80_32:                              # %if.else.56
	leaq	.L.str.11.56(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	negl	%ebx
.LBB80_33:                              # %if.end.59
	movslq	%ebx, %rdx
	movl	$2, %ecx
	movq	%r12, %rsi
	jmp	.LBB80_34
.LBB80_17:                              # %if.else.30
	leaq	.L.str.7.52(%rip), %rdx
.LBB80_4:                               # %cleanup.148
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	jmp	.LBB80_35
.LBB80_37:                              # %if.then.64
	vxorpd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_double_to_string@PLT
	jmp	.LBB80_35
.LBB80_39:
	xorl	%eax, %eax
	movq	%rax, -592(%rbp)        # 8-byte Spill
	movl	%ecx, %ebx
	jmp	.LBB80_44
.LBB80_42:                              # %if.else.76
	movl	$1075, %edx             # imm = 0x433
	subl	%r15d, %edx
	shrxq	%rdx, %r13, %rax
	shlxq	%rdx, %rax, %rdx
	subq	%rdx, %r13
.LBB80_43:                              # %if.end.84
	vmovq	%r13, %xmm0
	vmovdqa	.LCPI80_6(%rip), %xmm1  # xmm1 = [1127219200,1160773632,0,0]
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vmovapd	.LCPI80_7(%rip), %xmm2  # xmm2 = [4.503600e+15,1.934281e+25]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	shlq	$52, %rcx
	movabsq	$4696837146684686336, %rdx # imm = 0x412E848000000000
	addq	%rcx, %rdx
	vmovq	%rdx, %xmm3
	vfmadd213sd	.LCPI80_4(%rip), %xmm0, %xmm3
	vmovsd	.LCPI80_5(%rip), %xmm0  # xmm0 = mem[0],zero
	vsubsd	%xmm0, %xmm3, %xmm4
	vcvttsd2si	%xmm4, %rcx
	movabsq	$-9223372036854775808, %rdx # imm = 0x8000000000000000
	xorq	%rcx, %rdx
	vcvttsd2si	%xmm3, %rcx
	vucomisd	%xmm0, %xmm3
	cmovaeq	%rdx, %rcx
	vmovq	%rcx, %xmm0
	vpunpckldq	%xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
	vsubpd	%xmm2, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vucomisd	%xmm3, %xmm0
	setnp	%dl
	sete	%bl
	andb	%dl, %bl
	andb	%cl, %bl
	movzbl	%bl, %edx
	subq	%rdx, %rcx
	cmpq	$1000000, %rcx          # imm = 0xF4240
	sete	%dl
	movzbl	%dl, %r13d
	movl	$0, %edx
	cmovneq	%rcx, %rdx
	movq	%rdx, -592(%rbp)        # 8-byte Spill
	addq	%rax, %r13
	xorl	%ebx, %ebx
.LBB80_44:                              # %if.end.105
	leaq	-56(%rbp), %rsi
	leaq	-88(%rbp), %r15
	movl	$1, %ecx
	movq	%r15, %rdi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	testl	%ebx, %ebx
	movq	%rbx, %r13
	jle	.LBB80_65
# BB#45:                                # %for.cond.112.preheader.preheader
	testb	$1, %r13b
	jne	.LBB80_47
# BB#46:
	xorl	%r8d, %r8d
	jmp	.LBB80_53
.LBB80_47:                              # %for.cond.112.preheader.prol
	movl	$1, %r8d
	cmpq	%r15, %rax
	je	.LBB80_48
# BB#49:                                # %for.body.116.preheader.prol
	movl	$480, %ecx              # imm = 0x1E0
	subq	%rax, %rcx
	leaq	-568(%rbp,%rcx), %r11
	leaq	-1(%rax), %rsi
	xorl	%r9d, %r9d
	.align	16, 0x90
.LBB80_50:                              # %for.body.116.prol
                                        # =>This Inner Loop Header: Depth=1
	movb	(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edi
	addl	%edi, %edi
	orl	%r9d, %edi
	movsbl	%dil, %r10d
	leal	246(%rdi), %ecx
	cmpl	$9, %r10d
	setg	%dl
	movzbl	%dl, %r9d
	cmovlel	%edi, %ecx
	addl	$48, %ecx
	movb	%cl, (%rsi)
	addq	$-1, %rsi
	addq	$1, %r11
	jne	.LBB80_50
# BB#51:                                # %for.cond.cleanup.115.prol
	cmpl	$10, %r10d
	jl	.LBB80_53
# BB#52:                                # %if.then.136.prol
	leaq	-89(%rbp), %r15
	movb	$49, -89(%rbp)
	jmp	.LBB80_53
.LBB80_48:
	movq	%rax, %r15
.LBB80_53:                              # %for.cond.112.preheader.preheader.split
	cmpl	$1, %r13d
	je	.LBB80_65
	.align	16, 0x90
.LBB80_54:                              # %for.cond.112.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB80_55 Depth 2
                                        #     Child Loop BB80_60 Depth 2
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r10
	cmpq	%r15, %rax
	je	.LBB80_59
	.align	16, 0x90
.LBB80_55:                              # %for.body.116
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %edx
	addl	%edx, %edx
	orl	%r9d, %edx
	movsbl	%dl, %edi
	leal	246(%rdx), %ebx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%edx, %ebx
	addl	$48, %ebx
	movb	%bl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r15
	jne	.LBB80_55
# BB#56:                                # %for.cond.cleanup.115
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$9, %edi
	jle	.LBB80_58
# BB#57:                                # %if.then.136
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r15)
	addq	$-1, %r15
.LBB80_58:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r15, %r10
.LBB80_59:                              # %if.end.138
                                        #   in Loop: Header=BB80_54 Depth=1
	xorl	%r9d, %r9d
	movq	%rax, %rsi
	movq	%rax, %r15
	cmpq	%r10, %rax
	je	.LBB80_64
	.align	16, 0x90
.LBB80_60:                              # %for.body.116.1
                                        #   Parent Loop BB80_54 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movb	-1(%rsi), %cl
	addb	$-48, %cl
	movsbl	%cl, %ebx
	addl	%ebx, %ebx
	orl	%r9d, %ebx
	movsbl	%bl, %edi
	leal	246(%rbx), %edx
	cmpl	$9, %edi
	setg	%cl
	movzbl	%cl, %r9d
	cmovlel	%ebx, %edx
	addl	$48, %edx
	movb	%dl, -1(%rsi)
	leaq	-1(%rsi), %rsi
	cmpq	%rsi, %r10
	jne	.LBB80_60
# BB#61:                                # %for.cond.cleanup.115.1
                                        #   in Loop: Header=BB80_54 Depth=1
	cmpl	$10, %edi
	jl	.LBB80_63
# BB#62:                                # %if.then.136.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movb	$49, -1(%r10)
	addq	$-1, %r10
.LBB80_63:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	movq	%r10, %r15
.LBB80_64:                              # %if.end.138.1
                                        #   in Loop: Header=BB80_54 Depth=1
	addl	$2, %r8d
	cmpl	%r13d, %r8d
	jne	.LBB80_54
.LBB80_65:                              # %for.cond.cleanup
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movl	$6, %ecx
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	-592(%rbp), %rdx        # 8-byte Reload
.LBB80_34:                              # %cleanup.148
	callq	halide_int64_to_string@PLT
.LBB80_35:                              # %cleanup.148
	addq	$552, %rsp              # imm = 0x228
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end80:
	.size	halide_double_to_string, .Lfunc_end80-halide_double_to_string

	.section	.text.halide_pointer_to_string,"ax",@progbits
	.weak	halide_pointer_to_string
	.align	16, 0x90
	.type	halide_pointer_to_string,@function
halide_pointer_to_string:               # @halide_pointer_to_string
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	subq	$32, %rsp
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, -32(%rbp)
	movl	$0, -16(%rbp)
	leaq	-14(%rbp), %r10
	leaq	-13(%rbp), %r8
	movl	$1, %ecx
	leaq	.L.str.12.57(%rip), %r9
	.align	16, 0x90
.LBB81_1:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movl	%edx, %eax
	andl	$15, %eax
	movb	(%rax,%r9), %al
	movb	%al, (%r10)
	addq	$-1, %r10
	addq	$-1, %r8
	cmpl	$15, %ecx
	jg	.LBB81_3
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB81_1 Depth=1
	shrq	$4, %rdx
	addl	$1, %ecx
	testq	%rdx, %rdx
	jne	.LBB81_1
.LBB81_3:                               # %cleanup
	movq	%r8, %rdx
	addq	$-2, %rdx
	movb	$120, (%r10)
	movb	$48, -2(%r8)
	callq	halide_string_to_string@PLT
	addq	$32, %rsp
	popq	%rbp
	retq
.Lfunc_end81:
	.size	halide_pointer_to_string, .Lfunc_end81-halide_pointer_to_string

	.section	.text.halide_get_device_handle,"ax",@progbits
	.weak	halide_get_device_handle
	.align	16, 0x90
	.type	halide_get_device_handle,@function
halide_get_device_handle:               # @halide_get_device_handle
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB82_2
# BB#1:                                 # %if.end
	movq	(%rdi), %rax
.LBB82_2:                               # %cleanup
	popq	%rbp
	retq
.Lfunc_end82:
	.size	halide_get_device_handle, .Lfunc_end82-halide_get_device_handle

	.section	.text._ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t,@function
_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t: # @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rsi, %rbx
	movq	%rdi, %r14
	xorl	%eax, %eax
	cmpb	$0, 69(%rbx)
	je	.LBB83_5
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %rcx
	movl	$-14, %eax
	cmpb	$0, 68(%rbx)
	jne	.LBB83_5
# BB#2:                                 # %if.end.10
	movl	$-19, %eax
	testq	%rcx, %rcx
	je	.LBB83_5
# BB#3:                                 # %if.end.16
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	*48(%rcx)
	movl	%eax, %ecx
	movl	$-14, %eax
	testl	%ecx, %ecx
	jne	.LBB83_5
# BB#4:                                 # %if.end.25
	movb	$0, 69(%rbx)
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_buffer_is_initialized@PLT
	xorl	%eax, %eax
.LBB83_5:                               # %return
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end83:
	.size	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t, .Lfunc_end83-_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t

	.section	.text.halide_get_device_interface,"ax",@progbits
	.weak	halide_get_device_interface
	.align	16, 0x90
	.type	halide_get_device_interface,@function
halide_get_device_interface:            # @halide_get_device_interface
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	xorl	%eax, %eax
	testq	%rdi, %rdi
	je	.LBB84_2
# BB#1:                                 # %if.end
	movq	8(%rdi), %rax
.LBB84_2:                               # %return
	popq	%rbp
	retq
.Lfunc_end84:
	.size	halide_get_device_interface, .Lfunc_end84-halide_get_device_interface

	.section	.text.halide_new_device_wrapper,"ax",@progbits
	.weak	halide_new_device_wrapper
	.align	16, 0x90
	.type	halide_new_device_wrapper,@function
halide_new_device_wrapper:              # @halide_new_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$16, %edi
	callq	malloc@PLT
	movq	%rax, %rbx
	xorl	%eax, %eax
	testq	%rbx, %rbx
	je	.LBB85_2
# BB#1:                                 # %if.end
	movq	%r14, (%rbx)
	movq	%r15, 8(%rbx)
	callq	*(%r15)
	movq	%rbx, %rax
.LBB85_2:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end85:
	.size	halide_new_device_wrapper, .Lfunc_end85-halide_new_device_wrapper

	.section	.text.halide_delete_device_wrapper,"ax",@progbits
	.weak	halide_delete_device_wrapper
	.align	16, 0x90
	.type	halide_delete_device_wrapper,@function
halide_delete_device_wrapper:           # @halide_delete_device_wrapper
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	movq	8(%rbx), %rax
	callq	*8(%rax)
	movq	%rbx, %rdi
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	jmp	free@PLT                # TAILCALL
.Lfunc_end86:
	.size	halide_delete_device_wrapper, .Lfunc_end86-halide_delete_device_wrapper

	.section	.text.halide_device_release,"ax",@progbits
	.weak	halide_device_release
	.align	16, 0x90
	.type	halide_device_release,@function
halide_device_release:                  # @halide_device_release
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmpq	*40(%rsi)               # TAILCALL
.Lfunc_end87:
	.size	halide_device_release, .Lfunc_end87-halide_device_release

	.section	.text.halide_copy_to_host,"ax",@progbits
	.weak	halide_copy_to_host
	.align	16, 0x90
	.type	halide_copy_to_host,@function
halide_copy_to_host:                    # @halide_copy_to_host
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r14
	movq	%rdi, %rbx
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %r15
	movq	%r15, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %rdi
	movq	%r14, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	movq	%r15, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end88:
	.size	halide_copy_to_host, .Lfunc_end88-halide_copy_to_host

	.section	.text.halide_copy_to_device,"ax",@progbits
	.weak	halide_copy_to_device
	.align	16, 0x90
	.type	halide_copy_to_device,@function
halide_copy_to_device:                  # @halide_copy_to_device
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r15
	movq	%rsi, %r12
	movq	%rdi, %r14
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_lock@PLT
	movq	(%r12), %rdi
	callq	halide_get_device_interface@PLT
	testq	%r15, %r15
	jne	.LBB89_2
# BB#1:                                 # %if.then
	movl	$-19, %ebx
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB89_17
.LBB89_2:                               # %if.end.24
	movq	(%r12), %rcx
	cmpq	%r15, %rax
	je	.LBB89_11
# BB#3:                                 # %if.end.24
	testq	%rcx, %rcx
	je	.LBB89_11
# BB#4:                                 # %if.then.28
	testq	%rax, %rax
	je	.LBB89_9
# BB#5:                                 # %land.lhs.true.34
	cmpb	$0, 69(%r12)
	je	.LBB89_9
# BB#6:                                 # %if.then.37
	cmpb	$0, 68(%r12)
	je	.LBB89_8
# BB#7:                                 # %if.then.40
	leaq	.L.str.25.64(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB89_8:                               # %if.end.41
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP8buffer_t@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_9:                               # %if.end.50
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_device_free@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
# BB#10:                                # %if.end.58
	movb	$1, 68(%r12)
	movq	(%r12), %rcx
.LBB89_11:                              # %if.end.60
	testq	%rcx, %rcx
	jne	.LBB89_13
# BB#12:                                # %if.then.63
	movq	%r14, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %ebx
	testl	%ebx, %ebx
	jne	.LBB89_17
.LBB89_13:                              # %if.end.72
	xorl	%ebx, %ebx
	cmpb	$0, 68(%r12)
	je	.LBB89_17
# BB#14:                                # %if.then.75
	cmpb	$0, 69(%r12)
	jne	.LBB89_17
# BB#15:                                # %if.else
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	*56(%r15)
	movl	$-15, %ebx
	testl	%eax, %eax
	jne	.LBB89_17
# BB#16:                                # %if.then.89
	movb	$0, 68(%r12)
	xorl	%ebx, %ebx
.LBB89_17:                              # %cleanup
	movq	_ZN6Halide7Runtime8Internal17device_copy_mutexE@GOTPCREL(%rip), %rdi
	callq	halide_mutex_unlock@PLT
	movl	%ebx, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end89:
	.size	halide_copy_to_device, .Lfunc_end89-halide_copy_to_device

	.section	.text.halide_device_free,"ax",@progbits
	.weak	halide_device_free
	.align	16, 0x90
	.type	halide_device_free,@function
halide_device_free:                     # @halide_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r13
	movq	%rdi, %r14
	testq	%r13, %r13
	je	.LBB90_1
# BB#3:                                 # %if.then.8
	movq	(%r13), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB90_2
# BB#4:                                 # %if.then.12
	callq	*(%r12)
	movq	%r14, %rdi
	movq	%r13, %rsi
	callq	*24(%r12)
	movl	%eax, %r15d
	callq	*8(%r12)
	cmpq	$0, (%r13)
	je	.LBB90_6
# BB#5:                                 # %if.then.17
	leaq	.L.str.40(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB90_6:                               # %cleanup.22
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB90_7
.LBB90_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
.LBB90_2:                               # %if.end.23
	movb	$0, 69(%r13)
	xorl	%eax, %eax
.LBB90_7:                               # %cleanup.24
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end90:
	.size	halide_device_free, .Lfunc_end90-halide_device_free

	.section	.text.halide_device_malloc,"ax",@progbits
	.weak	halide_device_malloc
	.align	16, 0x90
	.type	halide_device_malloc,@function
halide_device_malloc:                   # @halide_device_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB91_6
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB91_6
# BB#2:                                 # %if.then
	movl	$1024, %esi             # imm = 0x400
	movq	%r14, %rdi
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB91_3
# BB#4:                                 # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.37(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB91_5
.LBB91_6:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*16(%rbx)
	movl	%eax, %r14d
	callq	*8(%rbx)
	testl	%r14d, %r14d
	movl	$-16, %eax
	cmovel	%r14d, %eax
	jmp	.LBB91_7
.LBB91_3:                               # %if.then.i
	leaq	.L.str.37(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB91_5:                               # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-16, %eax
.LBB91_7:                               # %cleanup.23
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end91:
	.size	halide_device_malloc, .Lfunc_end91-halide_device_malloc

	.section	.text.halide_device_sync,"ax",@progbits
	.weak	halide_device_sync
	.align	16, 0x90
	.type	halide_device_sync,@function
halide_device_sync:                     # @halide_device_sync
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r15
	movl	$-19, %r14d
	testq	%rbx, %rbx
	je	.LBB92_3
# BB#1:                                 # %if.end
	movq	(%rbx), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB92_3
# BB#2:                                 # %if.end.2
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	*32(%rax)
	testl	%eax, %eax
	movl	$-17, %r14d
	cmovel	%eax, %r14d
.LBB92_3:                               # %cleanup.7
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end92:
	.size	halide_device_sync, .Lfunc_end92-halide_device_sync

	.section	.text.halide_weak_device_free,"ax",@progbits
	.weak	halide_weak_device_free
	.align	16, 0x90
	.type	halide_weak_device_free,@function
halide_weak_device_free:                # @halide_weak_device_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end93:
	.size	halide_weak_device_free, .Lfunc_end93-halide_weak_device_free

	.section	.text.halide_device_free_as_destructor,"ax",@progbits
	.weak	halide_device_free_as_destructor
	.align	16, 0x90
	.type	halide_device_free_as_destructor,@function
halide_device_free_as_destructor:       # @halide_device_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_free@PLT  # TAILCALL
.Lfunc_end94:
	.size	halide_device_free_as_destructor, .Lfunc_end94-halide_device_free_as_destructor

	.section	.text.halide_device_and_host_malloc,"ax",@progbits
	.weak	halide_device_and_host_malloc
	.align	16, 0x90
	.type	halide_device_and_host_malloc,@function
halide_device_and_host_malloc:          # @halide_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movq	(%r15), %rdi
	callq	halide_get_device_interface@PLT
	testq	%rax, %rax
	je	.LBB95_6
# BB#1:                                 # %entry
	cmpq	%rbx, %rax
	je	.LBB95_6
# BB#2:                                 # %if.then
	movl	$1024, %esi             # imm = 0x400
	movq	%r14, %rdi
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	testq	%rbx, %rbx
	je	.LBB95_3
# BB#4:                                 # %if.else.i
	leaq	1023(%rbx), %rsi
	movb	$0, 1023(%rbx)
	leaq	.L.str.42(%rip), %rdx
	movq	%rbx, %rdi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB95_5
.LBB95_6:                               # %if.end
	callq	*(%rbx)
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*64(%rbx)
	movl	%eax, %r14d
	callq	*8(%rbx)
	testl	%r14d, %r14d
	movl	$-16, %eax
	cmovel	%r14d, %eax
	jmp	.LBB95_7
.LBB95_3:                               # %if.then.i
	leaq	.L.str.42(%rip), %rdx
	xorl	%edi, %edi
	xorl	%esi, %esi
	callq	halide_string_to_string@PLT
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB95_5:                               # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-16, %eax
.LBB95_7:                               # %cleanup.23
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end95:
	.size	halide_device_and_host_malloc, .Lfunc_end95-halide_device_and_host_malloc

	.section	.text.halide_device_and_host_free,"ax",@progbits
	.weak	halide_device_and_host_free
	.align	16, 0x90
	.type	halide_device_and_host_free,@function
halide_device_and_host_free:            # @halide_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %r13
	movq	%rdi, %r14
	testq	%r13, %r13
	je	.LBB96_1
# BB#3:                                 # %if.then.8
	movq	(%r13), %rbx
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rbx, %rdi
	callq	halide_get_device_interface@PLT
	movq	%rax, %r12
	testq	%r12, %r12
	je	.LBB96_2
# BB#4:                                 # %if.then.12
	callq	*(%r12)
	movq	%r14, %rdi
	movq	%r13, %rsi
	callq	*72(%r12)
	movl	%eax, %r15d
	callq	*8(%r12)
	cmpq	$0, (%r13)
	je	.LBB96_6
# BB#5:                                 # %if.then.17
	leaq	.L.str.44(%rip), %rsi
	movq	%r14, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB96_6:                               # %cleanup.22
	testl	%r15d, %r15d
	movl	$-18, %eax
	cmovel	%r15d, %eax
	jmp	.LBB96_7
.LBB96_1:                               # %if.end
	xorl	%edi, %edi
	callq	halide_get_device_interface@PLT
.LBB96_2:                               # %if.end.23
	movb	$0, 69(%r13)
	xorl	%eax, %eax
.LBB96_7:                               # %cleanup.24
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end96:
	.size	halide_device_and_host_free, .Lfunc_end96-halide_device_and_host_free

	.section	.text.halide_default_device_and_host_malloc,"ax",@progbits
	.weak	halide_default_device_and_host_malloc
	.align	16, 0x90
	.type	halide_default_device_and_host_malloc,@function
halide_default_device_and_host_malloc:  # @halide_default_device_and_host_malloc
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %rbx
	movq	%rdi, %r14
	movq	%rbx, %rdi
	callq	_ZN6Halide7Runtime8Internal8buf_sizeEPK8buffer_t@PLT
	movq	%r14, %rdi
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %rcx
	movq	%rcx, 8(%rbx)
	movl	$-1, %eax
	testq	%rcx, %rcx
	je	.LBB97_3
# BB#1:                                 # %if.end
	movq	%r14, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_device_malloc@PLT
	movl	%eax, %r15d
	xorl	%eax, %eax
	testl	%r15d, %r15d
	je	.LBB97_3
# BB#2:                                 # %if.then.5
	movq	8(%rbx), %rsi
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
	movl	%r15d, %eax
.LBB97_3:                               # %cleanup
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end97:
	.size	halide_default_device_and_host_malloc, .Lfunc_end97-halide_default_device_and_host_malloc

	.section	.text.halide_default_device_and_host_free,"ax",@progbits
	.weak	halide_default_device_and_host_free
	.align	16, 0x90
	.type	halide_default_device_and_host_free,@function
halide_default_device_and_host_free:    # @halide_default_device_and_host_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rsi, %rbx
	movq	%rdi, %r14
	callq	halide_device_free@PLT
	movl	%eax, %r15d
	movq	8(%rbx), %rsi
	movq	%r14, %rdi
	callq	halide_free@PLT
	movq	$0, 8(%rbx)
	movw	$0, 68(%rbx)
	movl	%r15d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end98:
	.size	halide_default_device_and_host_free, .Lfunc_end98-halide_default_device_and_host_free

	.section	.text.halide_device_and_host_free_as_destructor,"ax",@progbits
	.weak	halide_device_and_host_free_as_destructor
	.align	16, 0x90
	.type	halide_device_and_host_free_as_destructor,@function
halide_device_and_host_free_as_destructor: # @halide_device_and_host_free_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	jmp	halide_device_and_host_free@PLT # TAILCALL
.Lfunc_end99:
	.size	halide_device_and_host_free_as_destructor, .Lfunc_end99-halide_device_and_host_free_as_destructor

	.section	.text.halide_device_host_nop_free,"ax",@progbits
	.weak	halide_device_host_nop_free
	.align	16, 0x90
	.type	halide_device_host_nop_free,@function
halide_device_host_nop_free:            # @halide_device_host_nop_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end100:
	.size	halide_device_host_nop_free, .Lfunc_end100-halide_device_host_nop_free

	.section	.text.halide_float16_bits_to_float,"ax",@progbits
	.weak	halide_float16_bits_to_float
	.align	16, 0x90
	.type	halide_float16_bits_to_float,@function
halide_float16_bits_to_float:           # @halide_float16_bits_to_float
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	%edi, %eax
	shll	$16, %eax
	andl	$-2147483648, %eax      # imm = 0xFFFFFFFF80000000
	movl	$1290, %ecx             # imm = 0x50A
	bextrl	%ecx, %edi, %ecx
	andl	$1023, %edi             # imm = 0x3FF
	je	.LBB101_3
# BB#1:                                 # %entry
	testl	%ecx, %ecx
	jne	.LBB101_3
# BB#2:                                 # %if.then
	lzcntl	%edi, %ecx
	xorl	$31, %ecx
	movl	$-2, %edx
	roll	%cl, %edx
	andl	%edi, %edx
	movl	$23, %esi
	subl	%ecx, %esi
	shlxl	%esi, %edx, %edx
	shll	$23, %ecx
	addl	$864026624, %ecx        # imm = 0x33800000
	orl	%eax, %ecx
	orl	%edx, %ecx
	movl	%ecx, %edi
	jmp	.LBB101_7
.LBB101_3:                              # %if.else
	shll	$13, %edi
	xorl	%edx, %edx
	testl	%ecx, %ecx
	je	.LBB101_6
# BB#4:                                 # %if.else.18
	movl	$2139095040, %edx       # imm = 0x7F800000
	cmpl	$31, %ecx
	je	.LBB101_6
# BB#5:                                 # %if.else.21
	shll	$23, %ecx
	addl	$939524096, %ecx        # imm = 0x38000000
	movl	%ecx, %edx
.LBB101_6:                              # %if.end.23
	orl	%eax, %edi
	orl	%edx, %edi
.LBB101_7:                              # %if.end.28
	vmovd	%edi, %xmm0
	popq	%rbp
	retq
.Lfunc_end101:
	.size	halide_float16_bits_to_float, .Lfunc_end101-halide_float16_bits_to_float

	.section	.text.halide_float16_bits_to_double,"ax",@progbits
	.weak	halide_float16_bits_to_double
	.align	16, 0x90
	.type	halide_float16_bits_to_double,@function
halide_float16_bits_to_double:          # @halide_float16_bits_to_double
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_float16_bits_to_float@PLT
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	popq	%rbp
	retq
.Lfunc_end102:
	.size	halide_float16_bits_to_double, .Lfunc_end102-halide_float16_bits_to_double

	.section	.text.halide_error_bounds_inference_call_failed,"ax",@progbits
	.weak	halide_error_bounds_inference_call_failed
	.align	16, 0x90
	.type	halide_error_bounds_inference_call_failed,@function
halide_error_bounds_inference_call_failed: # @halide_error_bounds_inference_call_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB103_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB103_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.68(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB103_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB103_5
.LBB103_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB103_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end103:
	.size	halide_error_bounds_inference_call_failed, .Lfunc_end103-halide_error_bounds_inference_call_failed

	.section	.text.halide_error_extern_stage_failed,"ax",@progbits
	.weak	halide_error_extern_stage_failed
	.align	16, 0x90
	.type	halide_error_extern_stage_failed,@function
halide_error_extern_stage_failed:       # @halide_error_extern_stage_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r14d
	movq	%rsi, %r13
	movq	%rdi, %r15
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB104_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB104_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.2.70(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.1.69(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB104_3
# BB#4:                                 # %if.else.i.20
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	jmp	.LBB104_5
.LBB104_3:                              # %if.then.i.19
	leaq	.L.str.53(%rip), %rsi
	movq	%r15, %rdi
.LBB104_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end104:
	.size	halide_error_extern_stage_failed, .Lfunc_end104-halide_error_extern_stage_failed

	.section	.text.halide_error_explicit_bounds_too_small,"ax",@progbits
	.weak	halide_error_explicit_bounds_too_small
	.align	16, 0x90
	.type	halide_error_explicit_bounds_too_small,@function
halide_error_explicit_bounds_too_small: # @halide_error_explicit_bounds_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movl	%r8d, -48(%rbp)         # 4-byte Spill
	movl	%ecx, -52(%rbp)         # 4-byte Spill
	movq	%rdx, %r12
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB105_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB105_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.3.71(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.4.72(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.5.73(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r12
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.7.75(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	movslq	16(%rbp), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB105_3
# BB#4:                                 # %if.else.i.58
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB105_5
.LBB105_3:                              # %if.then.i.57
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB105_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-2, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end105:
	.size	halide_error_explicit_bounds_too_small, .Lfunc_end105-halide_error_explicit_bounds_too_small

	.section	.text.halide_error_bad_elem_size,"ax",@progbits
	.weak	halide_error_bad_elem_size
	.align	16, 0x90
	.type	halide_error_bad_elem_size,@function
halide_error_bad_elem_size:             # @halide_error_bad_elem_size
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%r8d, -44(%rbp)         # 4-byte Spill
	movl	%ecx, -48(%rbp)         # 4-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB106_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB106_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.9.77(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.10.78(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-48(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.11.79(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB106_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB106_5
.LBB106_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB106_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-3, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end106:
	.size	halide_error_bad_elem_size, .Lfunc_end106-halide_error_bad_elem_size

	.section	.text.halide_error_access_out_of_bounds,"ax",@progbits
	.weak	halide_error_access_out_of_bounds
	.align	16, 0x90
	.type	halide_error_access_out_of_bounds,@function
halide_error_access_out_of_bounds:      # @halide_error_access_out_of_bounds
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movl	%r8d, %r15d
	movl	%ecx, %r14d
	cmpl	%r13d, %r14d
	jge	.LBB107_6
# BB#1:                                 # %if.then
	movq	%rsi, %r15
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_3
# BB#2:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_3:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.13.81(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	jmp	.LBB107_4
.LBB107_6:                              # %if.else
	movl	16(%rbp), %r14d
	cmpl	%r14d, %r15d
	jle	.LBB107_12
# BB#7:                                 # %if.then.8
	movq	%rsi, %r13
	movl	%edx, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	movq	%rdi, -56(%rbp)         # 8-byte Spill
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB107_9
# BB#8:                                 # %if.then.i.61
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB107_9:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit65
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.12.80(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.15.83(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r14d, %rdx
.LBB107_4:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.14.82(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB107_5
# BB#10:                                # %if.else.i.98
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	jmp	.LBB107_11
.LBB107_5:                              # %if.then.i.50
	leaq	.L.str.53(%rip), %rsi
	movq	-56(%rbp), %rbx         # 8-byte Reload
	movq	%rbx, %rdi
.LBB107_11:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit103
	callq	halide_error@PLT
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB107_12:                             # %if.end.17
	movl	$-4, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end107:
	.size	halide_error_access_out_of_bounds, .Lfunc_end107-halide_error_access_out_of_bounds

	.section	.text.halide_error_buffer_allocation_too_large,"ax",@progbits
	.weak	halide_error_buffer_allocation_too_large
	.align	16, 0x90
	.type	halide_error_buffer_allocation_too_large,@function
halide_error_buffer_allocation_too_large: # @halide_error_buffer_allocation_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB108_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB108_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.16.84(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB108_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB108_5
.LBB108_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB108_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-5, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end108:
	.size	halide_error_buffer_allocation_too_large, .Lfunc_end108-halide_error_buffer_allocation_too_large

	.section	.text.halide_error_buffer_extents_negative,"ax",@progbits
	.weak	halide_error_buffer_extents_negative
	.align	16, 0x90
	.type	halide_error_buffer_extents_negative,@function
halide_error_buffer_extents_negative:   # @halide_error_buffer_extents_negative
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movl	%edx, %r13d
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB109_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB109_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.19.87(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.20.88(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.21.89(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB109_3
# BB#4:                                 # %if.else.i.32
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB109_5
.LBB109_3:                              # %if.then.i.31
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB109_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-28, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end109:
	.size	halide_error_buffer_extents_negative, .Lfunc_end109-halide_error_buffer_extents_negative

	.section	.text.halide_error_buffer_extents_too_large,"ax",@progbits
	.weak	halide_error_buffer_extents_too_large
	.align	16, 0x90
	.type	halide_error_buffer_extents_too_large,@function
halide_error_buffer_extents_too_large:  # @halide_error_buffer_extents_too_large
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB110_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB110_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.22.90(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.18.86(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB110_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB110_5
.LBB110_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB110_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-6, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end110:
	.size	halide_error_buffer_extents_too_large, .Lfunc_end110-halide_error_buffer_extents_too_large

	.section	.text.halide_error_constraints_make_required_region_smaller,"ax",@progbits
	.weak	halide_error_constraints_make_required_region_smaller
	.align	16, 0x90
	.type	halide_error_constraints_make_required_region_smaller,@function
halide_error_constraints_make_required_region_smaller: # @halide_error_constraints_make_required_region_smaller
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, %r13d
	movq	%rcx, -56(%rbp)         # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	16(%rbp), %eax
	leal	-1(%r13,%rax), %edx
	movl	%edx, -60(%rbp)         # 4-byte Spill
	leal	-1(%rcx,%rax), %eax
	movl	%eax, -44(%rbp)         # 4-byte Spill
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB111_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB111_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.23.91(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.24.92(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.25.93(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r13d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.6.74(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-60(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.26.94(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	leaq	.L.str.27.95(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	-56(%rbp), %rcx         # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r15, %r15
	je	.LBB111_3
# BB#4:                                 # %if.else.i.65
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB111_5
.LBB111_3:                              # %if.then.i.64
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB111_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-7, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end111:
	.size	halide_error_constraints_make_required_region_smaller, .Lfunc_end111-halide_error_constraints_make_required_region_smaller

	.section	.text.halide_error_constraint_violated,"ax",@progbits
	.weak	halide_error_constraint_violated
	.align	16, 0x90
	.type	halide_error_constraint_violated,@function
halide_error_constraint_violated:       # @halide_error_constraint_violated
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movl	%edx, -52(%rbp)         # 4-byte Spill
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB112_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB112_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.29(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %r13
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movslq	-52(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.31(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %r15         # 8-byte Reload
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.8.76(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB112_3
# BB#4:                                 # %if.else.i.40
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB112_5
.LBB112_3:                              # %if.then.i.39
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB112_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-8, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end112:
	.size	halide_error_constraint_violated, .Lfunc_end112-halide_error_constraint_violated

	.section	.text.halide_error_param_too_small_i64,"ax",@progbits
	.weak	halide_error_param_too_small_i64
	.align	16, 0x90
	.type	halide_error_param_too_small_i64,@function
halide_error_param_too_small_i64:       # @halide_error_param_too_small_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB113_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB113_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB113_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB113_5
.LBB113_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB113_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end113:
	.size	halide_error_param_too_small_i64, .Lfunc_end113-halide_error_param_too_small_i64

	.section	.text.halide_error_param_too_small_u64,"ax",@progbits
	.weak	halide_error_param_too_small_u64
	.align	16, 0x90
	.type	halide_error_param_too_small_u64,@function
halide_error_param_too_small_u64:       # @halide_error_param_too_small_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB114_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB114_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB114_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB114_5
.LBB114_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB114_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end114:
	.size	halide_error_param_too_small_u64, .Lfunc_end114-halide_error_param_too_small_u64

	.section	.text.halide_error_param_too_small_f64,"ax",@progbits
	.weak	halide_error_param_too_small_f64
	.align	16, 0x90
	.type	halide_error_param_too_small_f64,@function
halide_error_param_too_small_f64:       # @halide_error_param_too_small_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB115_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB115_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.33(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB115_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB115_5
.LBB115_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB115_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-9, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end115:
	.size	halide_error_param_too_small_f64, .Lfunc_end115-halide_error_param_too_small_f64

	.section	.text.halide_error_param_too_large_i64,"ax",@progbits
	.weak	halide_error_param_too_large_i64
	.align	16, 0x90
	.type	halide_error_param_too_large_i64,@function
halide_error_param_too_large_i64:       # @halide_error_param_too_large_i64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB116_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB116_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_int64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB116_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB116_5
.LBB116_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB116_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end116:
	.size	halide_error_param_too_large_i64, .Lfunc_end116-halide_error_param_too_large_i64

	.section	.text.halide_error_param_too_large_u64,"ax",@progbits
	.weak	halide_error_param_too_large_u64
	.align	16, 0x90
	.type	halide_error_param_too_large_u64,@function
halide_error_param_too_large_u64:       # @halide_error_param_too_large_u64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB117_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB117_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_uint64_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_uint64_to_string@PLT
	testq	%r12, %r12
	je	.LBB117_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB117_5
.LBB117_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB117_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end117:
	.size	halide_error_param_too_large_u64, .Lfunc_end117-halide_error_param_too_large_u64

	.section	.text.halide_error_param_too_large_f64,"ax",@progbits
	.weak	halide_error_param_too_large_f64
	.align	16, 0x90
	.type	halide_error_param_too_large_f64,@function
halide_error_param_too_large_f64:       # @halide_error_param_too_large_f64
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$16, %rsp
	vmovsd	%xmm1, -40(%rbp)        # 8-byte Spill
	vmovsd	%xmm0, -48(%rbp)        # 8-byte Spill
	movq	%rsi, %r12
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r15
	xorl	%ebx, %ebx
	testq	%r15, %r15
	je	.LBB118_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r15), %rbx
	movb	$0, 1023(%r15)
.LBB118_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.32(%rip), %rdx
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.17.85(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-48(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	leaq	.L.str.34(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	vmovsd	-40(%rbp), %xmm0        # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	halide_double_to_string@PLT
	testq	%r15, %r15
	je	.LBB118_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r15, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	jmp	.LBB118_5
.LBB118_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB118_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	$-10, %eax
	addq	$16, %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end118:
	.size	halide_error_param_too_large_f64, .Lfunc_end118-halide_error_param_too_large_f64

	.section	.text.halide_error_out_of_memory,"ax",@progbits
	.weak	halide_error_out_of_memory
	.align	16, 0x90
	.type	halide_error_out_of_memory,@function
halide_error_out_of_memory:             # @halide_error_out_of_memory
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	.L.str.35(%rip), %rsi
	callq	halide_error@PLT
	movl	$-11, %eax
	popq	%rbp
	retq
.Lfunc_end119:
	.size	halide_error_out_of_memory, .Lfunc_end119-halide_error_out_of_memory

	.section	.text.halide_error_buffer_argument_is_null,"ax",@progbits
	.weak	halide_error_buffer_argument_is_null
	.align	16, 0x90
	.type	halide_error_buffer_argument_is_null,@function
halide_error_buffer_argument_is_null:   # @halide_error_buffer_argument_is_null
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	xorl	%r12d, %r12d
	testq	%rbx, %rbx
	je	.LBB120_2
# BB#1:                                 # %if.then.i
	leaq	1023(%rbx), %r12
	movb	$0, 1023(%rbx)
.LBB120_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.36(%rip), %rdx
	movq	%rbx, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r12, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.37.96(%rip), %rdx
	movq	%rax, %rdi
	movq	%r12, %rsi
	callq	halide_string_to_string@PLT
	testq	%rbx, %rbx
	je	.LBB120_3
# BB#4:                                 # %if.else.i.15
	movl	$1, %edx
	subq	%rbx, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	jmp	.LBB120_5
.LBB120_3:                              # %if.then.i.14
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB120_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movl	$-12, %eax
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end120:
	.size	halide_error_buffer_argument_is_null, .Lfunc_end120-halide_error_buffer_argument_is_null

	.section	.text.halide_error_debug_to_file_failed,"ax",@progbits
	.weak	halide_error_debug_to_file_failed
	.align	16, 0x90
	.type	halide_error_debug_to_file_failed,@function
halide_error_debug_to_file_failed:      # @halide_error_debug_to_file_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%ecx, -44(%rbp)         # 4-byte Spill
	movq	%rdx, %r13
	movq	%rsi, %r15
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB121_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB121_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.38(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.39(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.40.97(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	testq	%r12, %r12
	je	.LBB121_3
# BB#4:                                 # %if.else.i.27
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB121_5
.LBB121_3:                              # %if.then.i.26
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB121_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-13, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end121:
	.size	halide_error_debug_to_file_failed, .Lfunc_end121-halide_error_debug_to_file_failed

	.section	.text.halide_error_unaligned_host_ptr,"ax",@progbits
	.weak	halide_error_unaligned_host_ptr
	.align	16, 0x90
	.type	halide_error_unaligned_host_ptr,@function
halide_error_unaligned_host_ptr:        # @halide_error_unaligned_host_ptr
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movl	%edx, %r15d
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB122_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB122_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.41(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.42.98(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.43(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB122_3
# BB#4:                                 # %if.else.i.23
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB122_5
.LBB122_3:                              # %if.then.i.22
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB122_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-24, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end122:
	.size	halide_error_unaligned_host_ptr, .Lfunc_end122-halide_error_unaligned_host_ptr

	.section	.text.halide_error_bad_fold,"ax",@progbits
	.weak	halide_error_bad_fold
	.align	16, 0x90
	.type	halide_error_bad_fold,@function
halide_error_bad_fold:                  # @halide_error_bad_fold
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, -48(%rbp)         # 8-byte Spill
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB123_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB123_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.44.99(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.46(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.28(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB123_3
# BB#4:                                 # %if.else.i.31
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB123_5
.LBB123_3:                              # %if.then.i.30
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB123_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-25, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end123:
	.size	halide_error_bad_fold, .Lfunc_end123-halide_error_bad_fold

	.section	.text.halide_error_fold_factor_too_small,"ax",@progbits
	.weak	halide_error_fold_factor_too_small
	.align	16, 0x90
	.type	halide_error_fold_factor_too_small,@function
halide_error_fold_factor_too_small:     # @halide_error_fold_factor_too_small
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%r9d, -44(%rbp)         # 4-byte Spill
	movq	%r8, -56(%rbp)          # 8-byte Spill
	movl	%ecx, %r15d
	movq	%rdx, %r13
	movq	%rsi, -64(%rbp)         # 8-byte Spill
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB124_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB124_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.47(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	%r15d, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.48(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.45.100(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-64(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.49(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	-56(%rbp), %rdx         # 8-byte Reload
	callq	halide_string_to_string@PLT
	leaq	.L.str.30(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movslq	-44(%rbp), %rdx         # 4-byte Folded Reload
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_int64_to_string@PLT
	leaq	.L.str.50(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB124_3
# BB#4:                                 # %if.else.i.48
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB124_5
.LBB124_3:                              # %if.then.i.47
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB124_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-26, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end124:
	.size	halide_error_fold_factor_too_small, .Lfunc_end124-halide_error_fold_factor_too_small

	.section	.text.halide_error_requirement_failed,"ax",@progbits
	.weak	halide_error_requirement_failed
	.align	16, 0x90
	.type	halide_error_requirement_failed,@function
halide_error_requirement_failed:        # @halide_error_requirement_failed
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r15
	movq	%rsi, %r13
	movq	%rdi, %r14
	movl	$1024, %esi             # imm = 0x400
	callq	halide_malloc@PLT
	movq	%rax, %r12
	xorl	%ebx, %ebx
	testq	%r12, %r12
	je	.LBB125_2
# BB#1:                                 # %if.then.i
	leaq	1023(%r12), %rbx
	movb	$0, 1023(%r12)
.LBB125_2:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EEC2EPvPc.exit
	leaq	.L.str.51(%rip), %rdx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r13, %rdx
	callq	halide_string_to_string@PLT
	leaq	.L.str.52(%rip), %rdx
	movq	%rax, %rdi
	movq	%rbx, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	halide_string_to_string@PLT
	testq	%r12, %r12
	je	.LBB125_3
# BB#4:                                 # %if.else.i.19
	movl	$1, %edx
	subq	%r12, %rdx
	addq	%rax, %rdx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_msan_annotate_memory_is_initialized@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	jmp	.LBB125_5
.LBB125_3:                              # %if.then.i.18
	leaq	.L.str.53(%rip), %rsi
	movq	%r14, %rdi
.LBB125_5:                              # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi1ELy1024EED2Ev.exit
	callq	halide_error@PLT
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	$-27, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end125:
	.size	halide_error_requirement_failed, .Lfunc_end125-halide_error_requirement_failed

	.section	.text.halide_profiler_get_state,"ax",@progbits
	.weak	halide_profiler_get_state
	.align	16, 0x90
	.type	halide_profiler_get_state,@function
halide_profiler_get_state:              # @halide_profiler_get_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	leaq	_ZZ25halide_profiler_get_stateE1s(%rip), %rax
	popq	%rbp
	retq
.Lfunc_end126:
	.size	halide_profiler_get_state, .Lfunc_end126-halide_profiler_get_state

	.section	.text._ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy,@function
_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy: # @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rdx, %r12
	movl	%esi, %r13d
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	80(%r14), %rax
	jmp	.LBB127_1
	.align	16, 0x90
.LBB127_4:                              # %for.inc
                                        #   in Loop: Header=BB127_1 Depth=1
	movq	64(%rax), %rax
.LBB127_1:                              # %entry
                                        # =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	je	.LBB127_5
# BB#2:                                 # %for.body
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpq	%rbx, 48(%rax)
	jne	.LBB127_4
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB127_1 Depth=1
	cmpl	%r13d, 72(%rax)
	jne	.LBB127_4
	jmp	.LBB127_16
.LBB127_5:                              # %for.end.critedge
	movl	$96, %edi
	callq	malloc@PLT
	movq	%rax, %r15
	xorl	%eax, %eax
	testq	%r15, %r15
	je	.LBB127_16
# BB#6:                                 # %if.end.7
	movq	80(%r14), %rax
	movq	%rax, 64(%r15)
	movq	%rbx, 48(%r15)
	movl	68(%r14), %eax
	movl	%eax, 76(%r15)
	movl	%r13d, 72(%r15)
	movl	$0, 80(%r15)
	movl	$0, 84(%r15)
	movl	$0, 88(%r15)
	movslq	%r13d, %rax
	shlq	$3, %rax
	leaq	(%rax,%rax,8), %rdi
	vxorps	%ymm0, %ymm0, %ymm0
	vmovups	%ymm0, 16(%r15)
	vmovups	%ymm0, (%r15)
	vzeroupper
	callq	malloc@PLT
	movq	%rax, 56(%r15)
	testq	%rax, %rax
	je	.LBB127_15
# BB#7:                                 # %for.cond.17.preheader
	testl	%r13d, %r13d
	jle	.LBB127_14
# BB#8:                                 # %for.body.20.preheader
	leal	-1(%r13), %r8d
	xorl	%ecx, %ecx
	testb	$3, %r13b
	je	.LBB127_11
# BB#9:                                 # %for.body.20.prol.preheader
	movl	%r13d, %esi
	andl	$3, %esi
	negl	%esi
	xorl	%ecx, %ecx
	vxorps	%ymm0, %ymm0, %ymm0
	movq	%r12, %rdi
	movq	%rax, %rbx
	.align	16, 0x90
.LBB127_10:                             # %for.body.20.prol
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rbx)
	movq	(%rdi), %rdx
	movq	%rdx, 56(%rbx)
	movl	$0, 64(%rbx)
	addq	$1, %rcx
	vmovups	%ymm0, 24(%rbx)
	vmovups	%ymm0, 8(%rbx)
	addq	$72, %rbx
	addq	$8, %rdi
	addl	$1, %esi
	jne	.LBB127_10
.LBB127_11:                             # %for.body.20.preheader.split
	cmpl	$3, %r8d
	jb	.LBB127_14
# BB#12:                                # %for.body.20.preheader.split.split
	movl	%r13d, %edx
	subl	%ecx, %edx
	leaq	(%rcx,%rcx,8), %rsi
	leaq	(%rax,%rsi,8), %rax
	leaq	(%r12,%rcx,8), %rcx
	vxorps	%ymm0, %ymm0, %ymm0
	.align	16, 0x90
.LBB127_13:                             # %for.body.20
                                        # =>This Inner Loop Header: Depth=1
	movq	$0, (%rax)
	movq	(%rcx), %rsi
	movq	%rsi, 56(%rax)
	movl	$0, 64(%rax)
	vmovups	%ymm0, 24(%rax)
	vmovups	%ymm0, 8(%rax)
	movq	$0, 72(%rax)
	movq	8(%rcx), %rsi
	movq	%rsi, 128(%rax)
	movl	$0, 136(%rax)
	vmovups	%ymm0, 96(%rax)
	vmovups	%ymm0, 80(%rax)
	movq	$0, 144(%rax)
	movq	16(%rcx), %rsi
	movq	%rsi, 200(%rax)
	movl	$0, 208(%rax)
	vmovups	%ymm0, 168(%rax)
	vmovups	%ymm0, 152(%rax)
	movq	$0, 216(%rax)
	movq	24(%rcx), %rsi
	movq	%rsi, 272(%rax)
	movl	$0, 280(%rax)
	vmovups	%ymm0, 240(%rax)
	vmovups	%ymm0, 224(%rax)
	addq	$288, %rax              # imm = 0x120
	addq	$32, %rcx
	addl	$-4, %edx
	jne	.LBB127_13
.LBB127_14:                             # %for.cond.cleanup.19
	addl	%r13d, 68(%r14)
	movq	%r15, 80(%r14)
	movq	%r15, %rax
	jmp	.LBB127_16
.LBB127_15:                             # %if.then.15
	movq	%r15, %rdi
	callq	free@PLT
	xorl	%eax, %eax
.LBB127_16:                             # %cleanup.62
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end127:
	.size	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy, .Lfunc_end127-_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy

	.section	.text._ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi,@function
_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi: # @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	80(%rdi), %r8
	xorl	%r9d, %r9d
	testq	%r8, %r8
	je	.LBB128_8
# BB#1:
	movq	%r8, %rax
	.align	16, 0x90
.LBB128_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	movq	%rax, %r11
	movslq	76(%r11), %r10
	cmpl	%esi, %r10d
	jg	.LBB128_7
# BB#3:                                 # %land.lhs.true
                                        #   in Loop: Header=BB128_2 Depth=1
	movl	72(%r11), %eax
	addl	%r10d, %eax
	cmpl	%esi, %eax
	jg	.LBB128_4
.LBB128_7:                              # %if.end.23
                                        #   in Loop: Header=BB128_2 Depth=1
	movq	64(%r11), %rax
	movq	%r11, %r9
	testq	%rax, %rax
	jne	.LBB128_2
.LBB128_8:                              # %cleanup.25
	popq	%rbp
	retq
.LBB128_4:                              # %if.then
	testq	%r9, %r9
	je	.LBB128_6
# BB#5:                                 # %if.then.4
	movq	64(%r11), %rax
	movq	%rax, 64(%r9)
	movq	%r8, 64(%r11)
	movq	%r11, 80(%rdi)
.LBB128_6:                              # %if.end
	movslq	%esi, %rax
	leaq	(%rax,%rax,8), %rax
	shlq	$3, %rax
	addq	56(%r11), %rax
	negq	%r10
	leaq	(%r10,%r10,8), %rsi
	addq	%rdx, (%rax,%rsi,8)
	movslq	%ecx, %rcx
	movl	$1, %edi
	vmovq	%rdi, %xmm0
	vmovq	%rcx, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0]
	vpaddq	40(%rax,%rsi,8), %xmm0, %xmm1
	vmovdqu	%xmm1, 40(%rax,%rsi,8)
	addq	%rdx, (%r11)
	incl	84(%r11)
	vpaddq	32(%r11), %xmm0, %xmm0
	vmovdqu	%xmm0, 32(%r11)
	popq	%rbp
	retq
.Lfunc_end128:
	.size	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi, .Lfunc_end128-_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi

	.section	.text._ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv,@function
_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv: # @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r13
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	cmpl	$-2, 72(%r13)
	je	.LBB129_11
# BB#1:                                 # %while.body.lr.ph
	leaq	-44(%rbp), %r14
	leaq	-48(%rbp), %r15
	.align	16, 0x90
.LBB129_2:                              # %while.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB129_3 Depth 2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %r12
	jmp	.LBB129_3
	.align	16, 0x90
.LBB129_9:                              # %cleanup.thread
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	64(%r13), %r12d
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	xorl	%edi, %edi
	movl	%r12d, %esi
	callq	halide_sleep_ms@PLT
	movq	%r13, %rdi
	callq	halide_mutex_lock@PLT
	movq	%rbx, %r12
.LBB129_3:                              # %while.body.3
                                        #   Parent Loop BB129_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	88(%r13), %rax
	testq	%rax, %rax
	je	.LBB129_5
# BB#4:                                 # %if.then
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	*%rax
	jmp	.LBB129_6
	.align	16, 0x90
.LBB129_5:                              # %if.else
                                        #   in Loop: Header=BB129_3 Depth=2
	movl	72(%r13), %eax
	movl	%eax, -44(%rbp)
	movl	76(%r13), %eax
	movl	%eax, -48(%rbp)
.LBB129_6:                              # %if.end
                                        #   in Loop: Header=BB129_3 Depth=2
	xorl	%edi, %edi
	callq	halide_current_time_ns@PLT
	movq	%rax, %rbx
	movl	-44(%rbp), %esi
	cmpl	$-2, %esi
	je	.LBB129_10
# BB#7:                                 # %if.else.10
                                        #   in Loop: Header=BB129_3 Depth=2
	testl	%esi, %esi
	js	.LBB129_9
# BB#8:                                 # %if.then.12
                                        #   in Loop: Header=BB129_3 Depth=2
	movq	%rbx, %rdx
	subq	%r12, %rdx
	movl	-48(%rbp), %ecx
	movq	%r13, %rdi
	callq	_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi@PLT
	jmp	.LBB129_9
	.align	16, 0x90
.LBB129_10:                             # %cleanup
                                        #   in Loop: Header=BB129_2 Depth=1
	cmpl	$-2, 72(%r13)
	jne	.LBB129_2
.LBB129_11:                             # %while.end.19
	movb	$0, 96(%r13)
	movq	%r13, %rdi
	callq	halide_mutex_unlock@PLT
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end129:
	.size	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv, .Lfunc_end129-_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv

	.section	.text.halide_profiler_get_pipeline_state,"ax",@progbits
	.weak	halide_profiler_get_pipeline_state
	.align	16, 0x90
	.type	halide_profiler_get_pipeline_state,@function
halide_profiler_get_pipeline_state:     # @halide_profiler_get_pipeline_state
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	pushq	%rax
	movq	%rdi, %rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	movq	80(%r14), %rax
	xorl	%r15d, %r15d
	testq	%rax, %rax
	je	.LBB130_5
# BB#1:
	xorl	%r15d, %r15d
	.align	16, 0x90
.LBB130_2:                              # %for.body
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rbx, 48(%rax)
	je	.LBB130_3
# BB#4:                                 # %for.inc
                                        #   in Loop: Header=BB130_2 Depth=1
	movq	64(%rax), %rax
	testq	%rax, %rax
	jne	.LBB130_2
	jmp	.LBB130_5
.LBB130_3:
	movq	%rax, %r15
.LBB130_5:                              # %cleanup
	movq	%r14, %rdi
	callq	halide_mutex_unlock@PLT
	movq	%r15, %rax
	addq	$8, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end130:
	.size	halide_profiler_get_pipeline_state, .Lfunc_end130-halide_profiler_get_pipeline_state

	.section	.text.halide_profiler_pipeline_start,"ax",@progbits
	.weak	halide_profiler_pipeline_start
	.align	16, 0x90
	.type	halide_profiler_pipeline_start,@function
halide_profiler_pipeline_start:         # @halide_profiler_pipeline_start
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	pushq	%rax
	movq	%rcx, %r15
	movl	%edx, %r12d
	movq	%rsi, %r13
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	cmpb	$0, 96(%rbx)
	jne	.LBB131_2
# BB#1:                                 # %if.then
	movq	%r14, %rdi
	callq	halide_start_clock@PLT
	movq	_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv@GOTPCREL(%rip), %rdi
	xorl	%esi, %esi
	callq	halide_spawn_thread@PLT
	movb	$1, 96(%rbx)
.LBB131_2:                              # %if.end
	movq	%r13, %rdi
	movl	%r12d, %esi
	movq	%r15, %rdx
	callq	_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy@PLT
	testq	%rax, %rax
	je	.LBB131_3
# BB#4:                                 # %if.end.9
	incl	80(%rax)
	movl	76(%rax), %r14d
	jmp	.LBB131_5
.LBB131_3:                              # %if.then.7
	movq	%r14, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r14d
.LBB131_5:                              # %cleanup
	movq	%rbx, %rdi
	callq	halide_mutex_unlock@PLT
	movl	%r14d, %eax
	addq	$8, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end131:
	.size	halide_profiler_pipeline_start, .Lfunc_end131-halide_profiler_pipeline_start

	.section	.text.halide_profiler_stack_peak_update,"ax",@progbits
	.weak	halide_profiler_stack_peak_update
	.align	16, 0x90
	.type	halide_profiler_stack_peak_update,@function
halide_profiler_stack_peak_update:      # @halide_profiler_stack_peak_update
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdx, %r14
	movq	%rsi, %rbx
	testq	%rbx, %rbx
	jne	.LBB132_2
# BB#1:                                 # %if.then
	leaq	.L.str.102(%rip), %rsi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB132_2:                              # %for.cond.preheader
	movl	72(%rbx), %eax
	testl	%eax, %eax
	jle	.LBB132_10
# BB#3:                                 # %for.body.lr.ph
	xorl	%edx, %edx
	.align	16, 0x90
.LBB132_4:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB132_6 Depth 2
	movq	(%r14,%rdx,8), %rsi
	testq	%rsi, %rsi
	je	.LBB132_9
# BB#5:                                 # %if.then.3
                                        #   in Loop: Header=BB132_4 Depth=1
	movq	56(%rbx), %rax
	leaq	(%rdx,%rdx,8), %rcx
	leaq	32(%rax,%rcx,8), %rdi
	movq	32(%rax,%rcx,8), %rcx
	.align	16, 0x90
.LBB132_6:                              # %while.cond.i
                                        #   Parent Loop BB132_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rsi, %rcx
	jae	.LBB132_8
# BB#7:                                 # %while.body.i
                                        #   in Loop: Header=BB132_6 Depth=2
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB132_6
.LBB132_8:                              # %for.inc.loopexit
                                        #   in Loop: Header=BB132_4 Depth=1
	movl	72(%rbx), %eax
.LBB132_9:                              # %for.inc
                                        #   in Loop: Header=BB132_4 Depth=1
	addq	$1, %rdx
	movslq	%eax, %rcx
	cmpq	%rcx, %rdx
	jl	.LBB132_4
.LBB132_10:                             # %for.cond.cleanup
	popq	%rbx
	popq	%r14
	popq	%rbp
	retq
.Lfunc_end132:
	.size	halide_profiler_stack_peak_update, .Lfunc_end132-halide_profiler_stack_peak_update

	.section	.text.halide_profiler_memory_allocate,"ax",@progbits
	.weak	halide_profiler_memory_allocate
	.align	16, 0x90
	.type	halide_profiler_memory_allocate,@function
halide_profiler_memory_allocate:        # @halide_profiler_memory_allocate
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %r14
	movl	%edx, %r15d
	movq	%rsi, %rbx
	movq	%rdi, %r12
	testq	%r14, %r14
	je	.LBB133_13
# BB#1:                                 # %if.end
	testq	%rbx, %rbx
	jne	.LBB133_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.1.103(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_3:                              # %if.end.3
	testl	%r15d, %r15d
	jns	.LBB133_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.2.104(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_5:                              # %if.end.6
	cmpl	%r15d, 72(%rbx)
	jg	.LBB133_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.3.105(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB133_7:                              # %if.end.9
	movq	56(%rbx), %rdx
	lock		addl	$1, 88(%rbx)
	lock		addq	%r14, 24(%rbx)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rbx)
	movslq	%r15d, %rdi
	addq	%r14, %rsi
	movq	16(%rbx), %rcx
	addq	$16, %rbx
	.align	16, 0x90
.LBB133_8:                              # %while.cond.i
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_10
# BB#9:                                 # %while.body.i
                                        #   in Loop: Header=BB133_8 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rbx)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_8
.LBB133_10:                             # %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit
	leaq	(%rdi,%rdi,8), %rax
	lock		addl	$1, 64(%rdx,%rax,8)
	lock		addq	%r14, 24(%rdx,%rax,8)
	movq	%r14, %rsi
	lock		xaddq	%rsi, 8(%rdx,%rax,8)
	addq	%r14, %rsi
	leaq	16(%rdx,%rax,8), %rdi
	movq	16(%rdx,%rax,8), %rcx
	.align	16, 0x90
.LBB133_11:                             # %while.cond.i.37
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %rcx
	jae	.LBB133_13
# BB#12:                                # %while.body.i.39
                                        #   in Loop: Header=BB133_11 Depth=1
	movq	%rcx, %rax
	lock		cmpxchgq	%rsi, (%rdi)
	cmpq	%rax, %rcx
	movq	%rax, %rcx
	jne	.LBB133_11
.LBB133_13:                             # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end133:
	.size	halide_profiler_memory_allocate, .Lfunc_end133-halide_profiler_memory_allocate

	.section	.text.halide_profiler_memory_free,"ax",@progbits
	.weak	halide_profiler_memory_free
	.align	16, 0x90
	.type	halide_profiler_memory_free,@function
halide_profiler_memory_free:            # @halide_profiler_memory_free
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	movq	%rcx, %rbx
	movl	%edx, %r14d
	movq	%rsi, %r15
	movq	%rdi, %r12
	testq	%rbx, %rbx
	je	.LBB134_8
# BB#1:                                 # %if.end
	testq	%r15, %r15
	jne	.LBB134_3
# BB#2:                                 # %if.then.2
	leaq	.L.str.4.106(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_3:                              # %if.end.3
	testl	%r14d, %r14d
	jns	.LBB134_5
# BB#4:                                 # %if.then.5
	leaq	.L.str.5.107(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_5:                              # %if.end.6
	cmpl	%r14d, 72(%r15)
	jg	.LBB134_7
# BB#6:                                 # %if.then.8
	leaq	.L.str.6.108(%rip), %rsi
	movq	%r12, %rdi
	callq	halide_print@PLT
	callq	abort@PLT
.LBB134_7:                              # %if.end.9
	movq	56(%r15), %rax
	negq	%rbx
	lock		addq	%rbx, 8(%r15)
	movslq	%r14d, %rcx
	leaq	(%rcx,%rcx,8), %rcx
	lock		addq	%rbx, 8(%rax,%rcx,8)
.LBB134_8:                              # %return
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end134:
	.size	halide_profiler_memory_free, .Lfunc_end134-halide_profiler_memory_free

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI135_0:
	.long	1232348160              # float 1.0E+6
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI135_1:
	.long	1127219200              # 0x43300000
	.long	1160773632              # 0x45300000
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI135_2:
	.quad	4841369599423283200     # double 4.503600e+15
	.quad	4985484787499139072     # double 1.934281e+25
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.LCPI135_3:
	.quad	4457293557087583675     # double 1.0E-10
	.section	.text.halide_profiler_report_unlocked,"ax",@progbits
	.weak	halide_profiler_report_unlocked
	.align	16, 0x90
	.type	halide_profiler_report_unlocked,@function
halide_profiler_report_unlocked:        # @halide_profiler_report_unlocked
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1144, %rsp             # imm = 0x478
	movq	%rdi, -1136(%rbp)       # 8-byte Spill
	leaq	-1064(%rbp), %rbx
	movb	$0, -41(%rbp)
	movq	80(%rsi), %r13
	testq	%r13, %r13
	je	.LBB135_50
# BB#1:
	leaq	-41(%rbp), %r15
	leaq	-1064(%rbp), %r12
	leaq	.L.str.20.122(%rip), %r14
	leaq	-1064(%rbp), %rbx
	.align	16, 0x90
.LBB135_2:                              # %for.body
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB135_13 Depth 2
                                        #     Child Loop BB135_16 Depth 2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movq	%r13, -1072(%rbp)       # 8-byte Spill
	movq	(%r13), %rax
	movl	%eax, %ecx
	andl	$1, %ecx
	testq	%rax, %rax
	js	.LBB135_3
# BB#4:                                 # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	vcvtsi2ssq	%rax, %xmm0, %xmm0
	jmp	.LBB135_5
	.align	16, 0x90
.LBB135_3:                              #   in Loop: Header=BB135_2 Depth=1
	shrq	%rax
	orq	%rax, %rcx
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_5:                              # %for.body
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 80(%r13)
	je	.LBB135_49
# BB#6:                                 # %if.end
                                        #   in Loop: Header=BB135_2 Depth=1
	vdivss	.LCPI135_0(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movb	$0, -1064(%rbp)
	movq	32(%r13), %rax
	movq	%rax, -1120(%rbp)       # 8-byte Spill
	movq	40(%r13), %rax
	movq	%rax, -1128(%rbp)       # 8-byte Spill
	movq	48(%r13), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.7.109(%rip), %rbx
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.8.110(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.9.111(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.10.112(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	84(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.11.113(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	80(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.12.114(%rip), %rdx
	callq	halide_string_to_string@PLT
	vcvtsi2ssl	80(%r13), %xmm0, %xmm0
	vmovss	-1080(%rbp), %xmm1      # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm1, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.13.115(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1128(%rbp), %rdx       # 8-byte Reload
	movq	-1120(%rbp), %rcx       # 8-byte Reload
	cmpq	%rdx, %rcx
	je	.LBB135_8
# BB#7:                                 # %if.then.31
                                        #   in Loop: Header=BB135_2 Depth=1
	vmovq	%rcx, %xmm0
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	%rdx, %xmm1
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1080(%rbp)      # 4-byte Spill
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.14.116(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1080(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%rbx, %rdx
	callq	halide_string_to_string@PLT
.LBB135_8:                              # %if.end.35
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.15.117(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	88(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.16.118(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	16(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.17.119(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	cmpq	$0, (%r13)
	jne	.LBB135_14
# BB#9:                                 # %lor.end
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpq	$0, 24(%r13)
	jne	.LBB135_14
# BB#10:                                # %for.cond.50.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	movslq	72(%r13), %rax
	testq	%rax, %rax
	jle	.LBB135_49
# BB#11:                                # %for.body.53.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	56(%r13), %rcx
	addq	$32, %rcx
	xorl	%edx, %edx
	.align	16, 0x90
.LBB135_13:                             # %for.body.53
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	$0, (%rcx)
	jne	.LBB135_14
# BB#12:                                # %for.cond.50
                                        #   in Loop: Header=BB135_13 Depth=2
	addq	$1, %rdx
	addq	$72, %rcx
	cmpq	%rax, %rdx
	jl	.LBB135_13
	jmp	.LBB135_49
	.align	16, 0x90
.LBB135_14:                             # %for.cond.62.preheader
                                        #   in Loop: Header=BB135_2 Depth=1
	cmpl	$0, 72(%r13)
	jle	.LBB135_49
# BB#15:                                # %if.then.i.352.lr.ph
                                        #   in Loop: Header=BB135_2 Depth=1
	xorl	%ecx, %ecx
	.align	16, 0x90
.LBB135_16:                             # %if.then.i.352
                                        #   Parent Loop BB135_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB135_19 Depth 3
                                        #       Child Loop BB135_26 Depth 3
                                        #       Child Loop BB135_30 Depth 3
                                        #       Child Loop BB135_35 Depth 3
                                        #       Child Loop BB135_41 Depth 3
                                        #       Child Loop BB135_43 Depth 3
	movb	$0, -1064(%rbp)
	movq	56(%r13), %rax
	leaq	(%rcx,%rcx,8), %r8
	leaq	(%rax,%r8,8), %rdx
	testl	%ecx, %ecx
	jne	.LBB135_18
# BB#17:                                # %land.lhs.true
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, (%rdx)
	movq	%r12, %rbx
	je	.LBB135_48
.LBB135_18:                             # %if.end.75
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rdx, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, -1096(%rbp)       # 8-byte Spill
	movq	%rax, %r13
	movq	%r12, %rdi
	movq	%r15, %rsi
	leaq	.L.str.18.120(%rip), %rdx
	movq	%r8, %rbx
	movq	%rbx, -1080(%rbp)       # 8-byte Spill
	callq	halide_string_to_string@PLT
	movq	56(%r13,%rbx,8), %rdx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.19.121(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$24, %rcx
	ja	.LBB135_20
	.align	16, 0x90
.LBB135_19:                             # %while.body
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	$25, %rcx
	jb	.LBB135_19
.LBB135_20:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rcx       # 8-byte Reload
	movq	(%rcx), %rcx
	movl	%ecx, %edx
	andl	$1, %edx
	testq	%rcx, %rcx
	js	.LBB135_21
# BB#22:                                # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	vcvtsi2ssq	%rcx, %xmm0, %xmm0
	jmp	.LBB135_23
	.align	16, 0x90
.LBB135_21:                             #   in Loop: Header=BB135_16 Depth=2
	shrq	%rcx
	orq	%rcx, %rdx
	vcvtsi2ssq	%rdx, %xmm0, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB135_23:                             # %while.end
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rcx       # 8-byte Reload
	vcvtsi2ssl	80(%rcx), %xmm0, %xmm1
	vmulss	.LCPI135_0(%rip), %xmm1, %xmm1
	vdivss	%xmm1, %xmm0, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %edi
	testq	%rax, %rax
	je	.LBB135_25
# BB#24:                                # %if.then.i.382
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rdi
.LBB135_25:                             # %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILi2ELy1024EE5eraseEi.exit
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r15, %rsi
	leaq	.L.str.21.123(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$34, %rax
	ja	.LBB135_27
	.align	16, 0x90
.LBB135_26:                             # %while.body.95
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$35, %rax
	jb	.LBB135_26
.LBB135_27:                             # %while.end.97
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1072(%rbp), %rax       # 8-byte Reload
	movq	(%rax), %rsi
	movl	$0, %ebx
	testq	%rsi, %rsi
	je	.LBB135_29
# BB#28:                                # %if.then.100
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1088(%rbp), %rax       # 8-byte Reload
	imulq	$100, (%rax), %rax
	xorl	%edx, %edx
	divq	%rsi
	movq	%rax, %rbx
.LBB135_29:                             # %if.end.106
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.22.124(%rip), %rdx
	callq	halide_string_to_string@PLT
	movslq	%ebx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.23.125(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$42, %rax
	ja	.LBB135_31
	.align	16, 0x90
.LBB135_30:                             # %while.body.114
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	cmpq	$43, %rax
	jb	.LBB135_30
.LBB135_31:                             # %while.end.116
                                        #   in Loop: Header=BB135_16 Depth=2
	movl	$58, %ebx
	movq	-1128(%rbp), %rax       # 8-byte Reload
	cmpq	%rax, -1120(%rbp)       # 8-byte Folded Reload
	je	.LBB135_36
# BB#32:                                # %if.then.118
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	-1080(%rbp), %rax       # 8-byte Reload
	vmovq	40(%r13,%rax,8), %xmm0  # xmm0 = mem[0],zero
	vmovdqa	.LCPI135_1(%rip), %xmm1 # xmm1 = [1127219200,1160773632,0,0]
	vmovdqa	%xmm1, %xmm2
	vpunpckldq	%xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
	vmovapd	.LCPI135_2(%rip), %xmm1 # xmm1 = [4.503600e+15,1.934281e+25]
	vmovapd	%xmm1, %xmm3
	vsubpd	%xmm3, %xmm0, %xmm0
	vhaddpd	%xmm0, %xmm0, %xmm0
	vmovq	48(%r13,%rax,8), %xmm1  # xmm1 = mem[0],zero
	vpunpckldq	%xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1]
	vsubpd	%xmm3, %xmm1, %xmm1
	vhaddpd	%xmm1, %xmm1, %xmm1
	vaddsd	.LCPI135_3(%rip), %xmm1, %xmm1
	vdivsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -1088(%rbp)      # 4-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.24.126(%rip), %rdx
	callq	halide_string_to_string@PLT
	vmovss	-1088(%rbp), %xmm0      # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_double_to_string@PLT
	movl	$0, %ecx
	testq	%rax, %rax
	je	.LBB135_34
# BB#33:                                # %if.then.i.429
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$-3, %rax
	cmpq	%r12, %rax
	cmovbq	%r12, %rax
	movb	$0, (%rax)
	movq	%rax, %rcx
.LBB135_34:                             # %while.cond.130.preheader
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rcx, %rax
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$57, %rax
	ja	.LBB135_36
	.align	16, 0x90
.LBB135_35:                             # %while.body.133
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rax
	movl	$73, %ebx
	cmpq	$58, %rax
	jb	.LBB135_35
.LBB135_36:                             # %if.end.136
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%r13, %rdi
	movq	-1080(%rbp), %rsi       # 8-byte Reload
	movslq	64(%rdi,%rsi,8), %r8
	movl	$0, %eax
	testq	%r8, %r8
	je	.LBB135_38
# BB#37:                                # %if.then.140
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	24(%rdi,%rsi,8), %rax
	xorl	%edx, %edx
	divq	%r8
.LBB135_38:                             # %if.end.146
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 16(%rdi,%rsi,8)
	movq	-1072(%rbp), %r13       # 8-byte Reload
	je	.LBB135_45
# BB#39:                                # %if.then.149
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, -1104(%rbp)       # 8-byte Spill
	leaq	64(%rdi,%rsi,8), %rax
	movq	%rax, -1112(%rbp)       # 8-byte Spill
	leaq	16(%rdi,%rsi,8), %r13
	movq	%rdi, -1088(%rbp)       # 8-byte Spill
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.25.127(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%r13), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	jmp	.LBB135_41
	.align	16, 0x90
.LBB135_40:                             # %while.body.157
                                        #   in Loop: Header=BB135_41 Depth=3
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
.LBB135_41:                             # %while.body.157
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rcx
	subq	%r12, %rcx
	movq	%rax, %rdi
	movq	%r15, %rsi
	cmpq	%rbx, %rcx
	jb	.LBB135_40
# BB#42:                                # %while.end.159
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.26.128(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1112(%rbp), %rcx       # 8-byte Reload
	movslq	(%rcx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	addq	$15, %rbx
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	movq	-1072(%rbp), %r13       # 8-byte Reload
	jae	.LBB135_44
	.align	16, 0x90
.LBB135_43:                             # %while.body.167
                                        #   Parent Loop BB135_2 Depth=1
                                        #     Parent Loop BB135_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rax, %rdi
	movq	%r15, %rsi
	movq	%r14, %rdx
	callq	halide_string_to_string@PLT
	movq	%rax, %rcx
	subq	%r12, %rcx
	cmpq	%rbx, %rcx
	jb	.LBB135_43
.LBB135_44:                             # %while.end.169
                                        #   in Loop: Header=BB135_16 Depth=2
	movq	%rax, %rdi
	movq	%r15, %rsi
	leaq	.L.str.27.129(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	-1104(%rbp), %rcx       # 8-byte Reload
	movslq	%ecx, %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_int64_to_string@PLT
	movq	%rax, %rcx
	movq	-1088(%rbp), %rdi       # 8-byte Reload
	movq	-1080(%rbp), %rsi       # 8-byte Reload
.LBB135_45:                             # %if.end.172
                                        #   in Loop: Header=BB135_16 Depth=2
	cmpq	$0, 32(%rdi,%rsi,8)
	je	.LBB135_47
# BB#46:                                # %if.then.175
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	32(%rdi,%rsi,8), %rbx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	leaq	.L.str.28.130(%rip), %rdx
	callq	halide_string_to_string@PLT
	movq	(%rbx), %rdx
	movl	$1, %ecx
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	halide_uint64_to_string@PLT
	movq	%rax, %rcx
.LBB135_47:                             # %if.end.179
                                        #   in Loop: Header=BB135_16 Depth=2
	leaq	.L.str.7.109(%rip), %rdx
	movq	%rcx, %rdi
	movq	%r15, %rsi
	callq	halide_string_to_string@PLT
	movq	%rax, %rbx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	movq	%r12, %rsi
	callq	halide_print@PLT
	movq	-1096(%rbp), %rcx       # 8-byte Reload
.LBB135_48:                             # %cleanup.182
                                        #   in Loop: Header=BB135_16 Depth=2
	addq	$1, %rcx
	movslq	72(%r13), %rax
	cmpq	%rax, %rcx
	jl	.LBB135_16
.LBB135_49:                             # %cleanup.191
                                        #   in Loop: Header=BB135_2 Depth=1
	movq	64(%r13), %r13
	testq	%r13, %r13
	jne	.LBB135_2
.LBB135_50:                             # %if.else.i
	movl	$1, %edx
	leaq	-1064(%rbp), %rsi
	subq	%rsi, %rdx
	addq	%rbx, %rdx
	movq	-1136(%rbp), %rdi       # 8-byte Reload
	callq	halide_msan_annotate_memory_is_initialized@PLT
	addq	$1144, %rsp             # imm = 0x478
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end135:
	.size	halide_profiler_report_unlocked, .Lfunc_end135-halide_profiler_report_unlocked

	.section	.text.halide_profiler_report,"ax",@progbits
	.weak	halide_profiler_report
	.align	16, 0x90
	.type	halide_profiler_report,@function
halide_profiler_report:                 # @halide_profiler_report
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	movq	%rdi, %r14
	callq	halide_profiler_get_state@PLT
	movq	%rax, %rbx
	movq	%rbx, %rdi
	callq	halide_mutex_lock@PLT
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	halide_profiler_report_unlocked@PLT
	movq	%rbx, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end136:
	.size	halide_profiler_report, .Lfunc_end136-halide_profiler_report

	.section	.text.halide_profiler_reset,"ax",@progbits
	.weak	halide_profiler_reset
	.align	16, 0x90
	.type	halide_profiler_reset,@function
halide_profiler_reset:                  # @halide_profiler_reset
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	pushq	%rbx
	callq	halide_profiler_get_state@PLT
	movq	%rax, %r14
	movq	%r14, %rdi
	callq	halide_mutex_lock@PLT
	jmp	.LBB137_2
	.align	16, 0x90
.LBB137_1:                              # %while.body
                                        #   in Loop: Header=BB137_2 Depth=1
	movq	64(%rbx), %rax
	movq	%rax, 80(%r14)
	movq	56(%rbx), %rdi
	callq	free@PLT
	movq	%rbx, %rdi
	callq	free@PLT
.LBB137_2:                              # %while.body
                                        # =>This Inner Loop Header: Depth=1
	movq	80(%r14), %rbx
	testq	%rbx, %rbx
	jne	.LBB137_1
# BB#3:                                 # %while.end
	movl	$0, 68(%r14)
	movq	%r14, %rdi
	popq	%rbx
	popq	%r14
	popq	%rbp
	jmp	halide_mutex_unlock@PLT # TAILCALL
.Lfunc_end137:
	.size	halide_profiler_reset, .Lfunc_end137-halide_profiler_reset

	.section	.text.halide_profiler_shutdown,"ax",@progbits
	.weak	halide_profiler_shutdown
	.align	16, 0x90
	.type	halide_profiler_shutdown,@function
halide_profiler_shutdown:               # @halide_profiler_shutdown
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	callq	halide_profiler_get_state@PLT
	cmpb	$0, 96(%rax)
	je	.LBB138_4
# BB#1:                                 # %if.end
	movl	$-2, 72(%rax)
	.align	16, 0x90
.LBB138_2:                              # %do.body
                                        # =>This Inner Loop Header: Depth=1
	mfence
	cmpb	$0, 96(%rax)
	jne	.LBB138_2
# BB#3:                                 # %do.end
	movl	$-1, 72(%rax)
	xorl	%edi, %edi
	movq	%rax, %rsi
	popq	%rbp
	jmp	halide_profiler_report_unlocked@PLT # TAILCALL
.LBB138_4:                              # %cleanup
	popq	%rbp
	retq
.Lfunc_end138:
	.size	halide_profiler_shutdown, .Lfunc_end138-halide_profiler_shutdown

	.section	.text.halide_profiler_pipeline_end,"ax",@progbits
	.weak	halide_profiler_pipeline_end
	.align	16, 0x90
	.type	halide_profiler_pipeline_end,@function
halide_profiler_pipeline_end:           # @halide_profiler_pipeline_end
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movl	$-1, 72(%rsi)
	popq	%rbp
	retq
.Lfunc_end139:
	.size	halide_profiler_pipeline_end, .Lfunc_end139-halide_profiler_pipeline_end

	.section	.text.halide_msan_annotate_memory_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_memory_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_memory_is_initialized,@function
halide_msan_annotate_memory_is_initialized: # @halide_msan_annotate_memory_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end140:
	.size	halide_msan_annotate_memory_is_initialized, .Lfunc_end140-halide_msan_annotate_memory_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized,@function
halide_msan_annotate_buffer_is_initialized: # @halide_msan_annotate_buffer_is_initialized
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end141:
	.size	halide_msan_annotate_buffer_is_initialized, .Lfunc_end141-halide_msan_annotate_buffer_is_initialized

	.section	.text.halide_msan_annotate_buffer_is_initialized_as_destructor,"ax",@progbits
	.weak	halide_msan_annotate_buffer_is_initialized_as_destructor
	.align	16, 0x90
	.type	halide_msan_annotate_buffer_is_initialized_as_destructor,@function
halide_msan_annotate_buffer_is_initialized_as_destructor: # @halide_msan_annotate_buffer_is_initialized_as_destructor
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	popq	%rbp
	retq
.Lfunc_end142:
	.size	halide_msan_annotate_buffer_is_initialized_as_destructor, .Lfunc_end142-halide_msan_annotate_buffer_is_initialized_as_destructor

	.section	.text.halide_default_can_use_target_features,"ax",@progbits
	.weak	halide_default_can_use_target_features
	.align	16, 0x90
	.type	halide_default_can_use_target_features,@function
halide_default_can_use_target_features: # @halide_default_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	subq	$24, %rsp
	movq	%rdi, %rbx
	movb	_ZZ38halide_default_can_use_target_featuresE11initialized(%rip), %al
	andb	$1, %al
	jne	.LBB143_2
# BB#1:                                 # %if.then
	leaq	-24(%rbp), %rdi
	callq	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv@PLT
	vmovups	-24(%rbp), %xmm0
	vmovups	%xmm0, _ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip)
	movb	$1, _ZZ38halide_default_can_use_target_featuresE11initialized(%rip)
.LBB143_2:                              # %if.end
	andq	_ZZ38halide_default_can_use_target_featuresE12cpu_features(%rip), %rbx
	je	.LBB143_4
# BB#3:                                 # %if.then.1
	movq	_ZZ38halide_default_can_use_target_featuresE12cpu_features+8(%rip), %rcx
	andq	%rbx, %rcx
	xorl	%eax, %eax
	cmpq	%rbx, %rcx
	jne	.LBB143_5
.LBB143_4:                              # %if.end.6
	movl	$1, %eax
.LBB143_5:                              # %cleanup
	addq	$24, %rsp
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end143:
	.size	halide_default_can_use_target_features, .Lfunc_end143-halide_default_can_use_target_features

	.section	.text.halide_set_custom_can_use_target_features,"ax",@progbits
	.weak	halide_set_custom_can_use_target_features
	.align	16, 0x90
	.type	halide_set_custom_can_use_target_features,@function
halide_set_custom_can_use_target_features: # @halide_set_custom_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rcx
	movq	(%rcx), %rax
	movq	%rdi, (%rcx)
	popq	%rbp
	retq
.Lfunc_end144:
	.size	halide_set_custom_can_use_target_features, .Lfunc_end144-halide_set_custom_can_use_target_features

	.section	.text.halide_can_use_target_features,"ax",@progbits
	.weak	halide_can_use_target_features
	.align	16, 0x90
	.type	halide_can_use_target_features,@function
halide_can_use_target_features:         # @halide_can_use_target_features
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	movq	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE@GOTPCREL(%rip), %rax
	popq	%rbp
	jmpq	*(%rax)                 # TAILCALL
.Lfunc_end145:
	.size	halide_can_use_target_features, .Lfunc_end145-halide_can_use_target_features

	.section	.text._ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,"ax",@progbits
	.weak	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
	.align	16, 0x90
	.type	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv,@function
_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv: # @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%rbx
	movl	$1, -24(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-24(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -24(%rbp)
	movl	%ebx, -20(%rbp)
	movl	%ecx, -16(%rbp)
	movl	%edx, -12(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-16(%rbp), %eax
	movl	%eax, %ecx
	andl	$524288, %ecx           # imm = 0x80000
	shrq	$15, %rcx
	movl	%eax, %edx
	shrl	$23, %edx
	andl	$32, %edx
	orq	%rcx, %rdx
	movl	%eax, %ecx
	shrl	$20, %ecx
	andl	$512, %ecx              # imm = 0x200
	orq	%rdx, %rcx
	movl	%eax, %r8d
	shrl	$5, %r8d
	andl	$128, %r8d
	orq	%rcx, %r8
	andl	$1879048192, %eax       # imm = 0x70000000
	cmpl	$1879048192, %eax       # imm = 0x70000000
	jne	.LBB146_4
# BB#1:                                 # %if.then.33
	movl	$7, -40(%rbp)
	#APP

	xchgl	%esi, %ebx
	movl	-40(%rbp), %eax
	movl	$0, %ecx
	cpuid
	movl	%eax, -40(%rbp)
	movl	%ebx, -36(%rbp)
	movl	%ecx, -32(%rbp)
	movl	%edx, -28(%rbp)
	xchgl	%esi, %ebx

	#NO_APP
	movl	-36(%rbp), %ecx
	movl	%ecx, %eax
	andl	$32, %eax
	addq	%rax, %rax
	orq	%r8, %rax
	movl	%ecx, %edx
	andl	$268500992, %edx        # imm = 0x10010000
	cmpl	$268500992, %edx        # imm = 0x10010000
	jne	.LBB146_3
# BB#2:                                 # %if.then.44
	movl	%ecx, %edx
	andl	$469827584, %edx        # imm = 0x1C010000
	cmpl	$469827584, %edx        # imm = 0x1C010000
	movabsq	$824633720832, %rdx     # imm = 0xC000000000
	movabsq	$274877906944, %rsi     # imm = 0x4000000000
	cmoveq	%rdx, %rsi
	orq	%rsi, %rax
	movl	%ecx, %edx
	andl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	movabsq	$1099511627776, %rsi    # imm = 0x10000000000
	orq	%rax, %rsi
	cmpl	$-805109760, %edx       # imm = 0xFFFFFFFFD0030000
	cmovneq	%rax, %rsi
	andl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	movabsq	$2199023255552, %rax    # imm = 0x20000000000
	orq	%rsi, %rax
	cmpl	$-803012608, %ecx       # imm = 0xFFFFFFFFD0230000
	cmovneq	%rsi, %rax
.LBB146_3:                              # %if.end.64
	movq	%rax, %r8
.LBB146_4:                              # %if.end.65
	movabsq	$4123168604912, %rax    # imm = 0x3C0000002F0
	movq	%rax, (%rdi)
	movq	%r8, 8(%rdi)
	movq	%rdi, %rax
	popq	%rbx
	popq	%rbp
	retq
.Lfunc_end146:
	.size	_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv, .Lfunc_end146-_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv

	.section	.rodata,"a",@progbits
	.align	32
.LCPI147_0:
	.long	3                       # 0x3
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
.LCPI147_1:
	.long	3                       # 0x3
	.long	4096                    # 0x1000
	.long	0                       # 0x0
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	3                       # 0x3
	.long	0                       # 0x0
	.long	0                       # 0x0
	.section	.text.__sharpi,"ax",@progbits
	.globl	__sharpi
	.align	16, 0x90
	.type	__sharpi,@function
__sharpi:                               # @__sharpi
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1224, %rsp             # imm = 0x4C8
	movq	%r8, %r14
	testq	%rdi, %rdi
	je	.LBB147_1
# BB#13:                                # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB147_14
# BB#15:                                # %assert succeeded11
	testq	%r14, %r14
	je	.LBB147_16
# BB#17:                                # %assert succeeded30
	testq	%r9, %r9
	je	.LBB147_18
# BB#19:                                # %assert succeeded49
	movq	%r9, 600(%rsp)          # 8-byte Spill
	cmpq	$0, 1352(%rsp)
	je	.LBB147_20
# BB#21:                                # %assert succeeded68
	movq	1368(%rsp), %r15
	testq	%r15, %r15
	je	.LBB147_22
# BB#23:                                # %assert succeeded87
	movl	16(%rdi), %r12d
	movq	%r12, 112(%rsp)         # 8-byte Spill
	movl	48(%rdi), %r8d
	movq	%r8, 224(%rsp)          # 8-byte Spill
	movl	16(%r15), %r10d
	movq	%r10, 592(%rsp)         # 8-byte Spill
	movl	20(%r15), %ebp
	movq	%rbp, 616(%rsp)         # 8-byte Spill
	movq	%rbp, %rax
	movl	48(%r15), %ebp
	movq	%rbp, 648(%rsp)         # 8-byte Spill
	movl	52(%r15), %r11d
	movq	%r11, 656(%rsp)         # 8-byte Spill
	leal	(%r11,%rax), %ebx
	movl	%ebx, 328(%rsp)         # 4-byte Spill
	cmpl	%edx, %ebx
	movl	%ebx, %r13d
	cmovll	%edx, %r13d
	movq	%r13, 536(%rsp)         # 8-byte Spill
	movq	%rdx, 144(%rsp)         # 8-byte Spill
	leal	(%rbp,%r10), %eax
	movq	%rax, 488(%rsp)         # 8-byte Spill
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movq	%rsi, 256(%rsp)         # 8-byte Spill
	movl	%ebp, %r10d
	sarl	$31, %r10d
	andl	%ebp, %r10d
	movq	%r10, 640(%rsp)         # 8-byte Spill
	leal	-8(%r10), %esi
	movl	%esi, 672(%rsp)         # 4-byte Spill
	leal	-1(%r8,%r12), %ebp
	movl	%ebp, 36(%rsp)          # 4-byte Spill
	cmpl	%esi, %ebp
	movl	%ebp, %ebx
	cmovgl	%esi, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	leal	(%r12,%r12), %edx
	movl	$2, %esi
	subl	%edx, %esi
	cmpl	$1, %edx
	leal	-2(%r12,%r12), %edx
	cmovgl	%edx, %esi
	subl	%r12d, %esi
	leal	-1(%r12), %edx
	cmpl	%esi, %edx
	cmoval	%edx, %esi
	leal	-1(%r12,%r8), %edx
	subl	%esi, %edx
	cmpl	%ebx, %edx
	cmovgl	%ebx, %edx
	movl	%edx, 320(%rsp)         # 4-byte Spill
	xorl	%edx, %edx
	cmpl	$1, %eax
	leal	-1(%rax), %eax
	cmovlel	%edx, %eax
	subl	%r10d, %eax
	movq	%rax, 552(%rsp)         # 8-byte Spill
	leal	16(%rax), %eax
	andl	$-32, %eax
	leal	23(%r10,%rax), %eax
	cmpl	%eax, %ebp
	cmovlel	%ebp, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	leal	(%r8,%r12), %edx
	cmpl	%edx, %eax
	leal	1(%rax), %eax
	cmovll	%edx, %eax
	movl	%eax, 316(%rsp)         # 4-byte Spill
	movl	%r11d, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	movq	%rax, 624(%rsp)         # 8-byte Spill
	movl	20(%rdi), %ebp
	movq	%rbp, 272(%rsp)         # 8-byte Spill
	movl	52(%rdi), %ebx
	movq	%rbx, 216(%rsp)         # 8-byte Spill
	leal	-8(%rax), %edx
	movl	%edx, 676(%rsp)         # 4-byte Spill
	leal	-1(%rbx,%rbp), %r8d
	movl	%r8d, 32(%rsp)          # 4-byte Spill
	cmpl	%edx, %r8d
	movl	%r8d, %eax
	cmovgl	%edx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	(%rbp,%rbp), %esi
	movl	$2, %edx
	subl	%esi, %edx
	cmpl	$1, %esi
	leal	-2(%rbp,%rbp), %esi
	cmovgl	%esi, %edx
	subl	%ebp, %edx
	leal	-1(%rbp), %esi
	cmpl	%edx, %esi
	cmoval	%esi, %edx
	leal	-1(%rbp,%rbx), %esi
	subl	%edx, %esi
	cmpl	%eax, %esi
	cmovgl	%eax, %esi
	movl	%esi, 312(%rsp)         # 4-byte Spill
	cmpl	$1, %r13d
	leal	-1(%r13), %edx
	movl	$0, %eax
	cmovlel	%eax, %edx
	movq	%rdx, 608(%rsp)         # 8-byte Spill
	movl	$0, %eax
	cmovgl	%r13d, %eax
	movl	%eax, 428(%rsp)         # 4-byte Spill
	leal	8(%rdx), %edx
	movl	%edx, 408(%rsp)         # 4-byte Spill
	cmpl	%edx, %r8d
	movl	%r8d, %eax
	cmovgl	%edx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	leal	(%rbx,%rbp), %edx
	cmpl	%edx, %eax
	leal	1(%rax), %eax
	cmovll	%edx, %eax
	movl	%eax, 308(%rsp)         # 4-byte Spill
	movq	616(%rsp), %rdx         # 8-byte Reload
	leal	-32(%r11,%rdx), %eax
	movl	%eax, 324(%rsp)         # 4-byte Spill
	cmpl	%r11d, %eax
	cmovgl	%r11d, %eax
	movl	%eax, 332(%rsp)         # 4-byte Spill
	leal	-1(%rdx), %eax
	movq	%rdx, %rsi
	movq	%rax, 480(%rsp)         # 8-byte Spill
	movl	%eax, %edx
	orl	$31, %edx
	addl	%r11d, %edx
	movl	%edx, 40(%rsp)          # 4-byte Spill
	leal	-1(%r11,%rsi), %eax
	movl	%eax, 136(%rsp)         # 4-byte Spill
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	movl	%eax, 336(%rsp)         # 4-byte Spill
	movl	48(%rcx), %r12d
	movq	%r12, 176(%rsp)         # 8-byte Spill
	movl	$1, %eax
	subl	%r12d, %eax
	movq	(%rdi), %rdx
	movq	%rdx, 432(%rsp)         # 8-byte Spill
	movq	8(%rdi), %rdx
	movq	%rdx, 520(%rsp)         # 8-byte Spill
	movl	32(%rdi), %edx
	movl	%edx, 56(%rsp)          # 4-byte Spill
	movl	36(%rdi), %edx
	movl	%edx, 124(%rsp)         # 4-byte Spill
	movl	64(%rdi), %edx
	movl	%edx, 84(%rsp)          # 4-byte Spill
	movq	%rdi, 280(%rsp)         # 8-byte Spill
	movq	(%rcx), %rdx
	movq	%rdx, 456(%rsp)         # 8-byte Spill
	movq	8(%rcx), %rdx
	movq	%rdx, 528(%rsp)         # 8-byte Spill
	movl	16(%rcx), %edi
	movq	%rdi, 264(%rsp)         # 8-byte Spill
	movl	20(%rcx), %edx
	movq	%rdx, 568(%rsp)         # 8-byte Spill
	movl	32(%rcx), %edx
	movl	%edx, 68(%rsp)          # 4-byte Spill
	movl	36(%rcx), %edx
	movl	%edx, 128(%rsp)         # 4-byte Spill
	movl	52(%rcx), %r9d
	movq	%r9, 208(%rsp)          # 8-byte Spill
	movl	64(%rcx), %edx
	movl	%edx, 92(%rsp)          # 4-byte Spill
	movq	%rcx, 288(%rsp)         # 8-byte Spill
	movq	(%r14), %rcx
	movq	%rcx, 464(%rsp)         # 8-byte Spill
	movq	8(%r14), %rcx
	movq	%rcx, 512(%rsp)         # 8-byte Spill
	movl	16(%r14), %ecx
	movq	%rcx, 560(%rsp)         # 8-byte Spill
	movl	20(%r14), %ecx
	movq	%rcx, 576(%rsp)         # 8-byte Spill
	movl	32(%r14), %ecx
	movl	%ecx, 64(%rsp)          # 4-byte Spill
	movl	36(%r14), %ecx
	movl	%ecx, 132(%rsp)         # 4-byte Spill
	movl	48(%r14), %ecx
	movq	%rcx, 584(%rsp)         # 8-byte Spill
	movl	52(%r14), %ecx
	movq	%rcx, 632(%rsp)         # 8-byte Spill
	movl	64(%r14), %ecx
	movl	%ecx, 88(%rsp)          # 4-byte Spill
	movq	%r14, 296(%rsp)         # 8-byte Spill
	movq	600(%rsp), %rdx         # 8-byte Reload
	movq	(%rdx), %rcx
	movq	%rcx, 416(%rsp)         # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 240(%rsp)         # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 104(%rsp)         # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 48(%rsp)          # 4-byte Spill
	movl	36(%rdx), %ecx
	movl	%ecx, 156(%rsp)         # 4-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 192(%rsp)         # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 200(%rsp)         # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 76(%rsp)          # 4-byte Spill
	movq	1352(%rsp), %rcx
	movq	%rcx, %rdx
	movq	(%rdx), %rcx
	movq	%rcx, 448(%rsp)         # 8-byte Spill
	movq	8(%rdx), %rcx
	movq	%rcx, 504(%rsp)         # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 184(%rsp)         # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 472(%rsp)         # 8-byte Spill
	movl	32(%rdx), %ecx
	movl	%ecx, 60(%rsp)          # 4-byte Spill
	movl	36(%rdx), %ecx
	movl	%ecx, 120(%rsp)         # 4-byte Spill
	movl	48(%rdx), %ecx
	movq	%rcx, 160(%rsp)         # 8-byte Spill
	movl	52(%rdx), %ecx
	movq	%rcx, 168(%rsp)         # 8-byte Spill
	movl	64(%rdx), %ecx
	movl	%ecx, 80(%rsp)          # 4-byte Spill
	movq	(%r15), %rcx
	movq	%rcx, 440(%rsp)         # 8-byte Spill
	movq	8(%r15), %rcx
	movq	%rcx, 544(%rsp)         # 8-byte Spill
	movl	24(%r15), %ecx
	movq	%rcx, 232(%rsp)         # 8-byte Spill
	movl	32(%r15), %ecx
	movl	%ecx, 44(%rsp)          # 4-byte Spill
	movl	36(%r15), %ecx
	movl	%ecx, 140(%rsp)         # 4-byte Spill
	movl	40(%r15), %ecx
	movl	%ecx, 52(%rsp)          # 4-byte Spill
	movl	56(%r15), %ecx
	movq	%rcx, 96(%rsp)          # 8-byte Spill
	movl	64(%r15), %ecx
	movl	%ecx, 72(%rsp)          # 4-byte Spill
	leal	(%rdi,%rdi), %ebp
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%edi, %ebx
	sarl	$31, %ebx
	andnl	%ebp, %ebx, %ecx
	andl	%eax, %ebx
	orl	%ecx, %ebx
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%ebx, %esi
	addl	%edx, %esi
	movl	%ebp, %r11d
	subl	%esi, %r11d
	addl	$-1, %r11d
	cmpl	%esi, %r11d
	movl	%esi, %eax
	cmovgel	%r11d, %eax
	movl	%eax, 664(%rsp)         # 4-byte Spill
	movl	$2, %eax
	subl	%r12d, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	movl	%r12d, %eax
	negl	%eax
	cltd
	idivl	%ebp
	movl	%ecx, %r10d
	sarl	$31, %r10d
	andl	%ebx, %r10d
	addl	%ecx, %r10d
	movl	%edx, %r8d
	sarl	$31, %r8d
	andl	%ebx, %r8d
	addl	%edx, %r8d
	movl	%ebp, %eax
	subl	%r10d, %eax
	addl	$-1, %eax
	cmpl	%eax, %r10d
	movl	%r10d, %edx
	cmovgl	%eax, %edx
	addl	%r12d, %edx
	leal	-1(%rdi,%r12), %r15d
	movl	%r15d, 28(%rsp)         # 4-byte Spill
	cmpl	%edx, %r15d
	cmovlel	%r15d, %edx
	subl	%r8d, %ebp
	addl	$-1, %ebp
	cmpl	%ebp, %r8d
	movl	%r8d, %ecx
	cmovgl	%ebp, %ecx
	addl	%r12d, %ecx
	cmpl	%ecx, %r15d
	cmovlel	%r15d, %ecx
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	addl	%r12d, %r11d
	cmpl	%r11d, %r15d
	cmovlel	%r15d, %r11d
	movl	%r11d, %r13d
	sarl	$31, %r13d
	cmpl	$3, %edx
	movl	$2, %edi
	cmovll	%edx, %edi
	cmpl	%r11d, %edi
	cmovgl	%r11d, %edi
	movl	%ecx, %r14d
	sarl	$31, %r14d
	andl	%ecx, %r14d
	cmpl	%edi, %r14d
	cmovlel	%r14d, %edi
	movl	%edi, %ebx
	sarl	$31, %ebx
	testl	%edi, %edi
	setg	%sil
	andl	%edi, %ebx
	movzbl	%sil, %edi
	orl	%edi, %ebx
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %ebx
	movl	%ebx, %esi
	sarl	$31, %esi
	andl	%ebx, %esi
	cmpl	%r11d, %esi
	cmovgl	%r11d, %esi
	movl	%esi, %edi
	sarl	$31, %edi
	testl	%esi, %esi
	setg	%bl
	andl	%esi, %edi
	movzbl	%bl, %esi
	orl	%esi, %edi
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	cmpl	$3, %edi
	movl	$2, %esi
	cmovgel	%esi, %edi
	cmpl	%r11d, %edi
	cmovgl	%r11d, %edi
	movl	%edi, %esi
	sarl	$31, %esi
	testl	%edi, %edi
	setg	%bl
	andl	%edi, %esi
	movzbl	%bl, %edi
	orl	%edi, %esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	movl	%esi, %ecx
	sarl	$31, %ecx
	andl	%esi, %ecx
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	testl	%r11d, %r11d
	setg	%bl
	andl	%r11d, %r13d
	movzbl	%bl, %esi
	orl	%esi, %r13d
	cmpl	%ecx, %r13d
	cmovgl	%ecx, %r13d
	cmpl	%r13d, %edx
	cmovlel	%edx, %r13d
	cmpl	%r13d, %r14d
	cmovlel	%r14d, %r13d
	cmpl	$3, %r13d
	movl	$2, %ecx
	cmovgel	%ecx, %r13d
	cmpl	%r12d, %r13d
	cmovll	%r12d, %r13d
	movl	%r13d, 252(%rsp)        # 4-byte Spill
	movl	664(%rsp), %edx         # 4-byte Reload
	addl	%r12d, %edx
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	addl	%r12d, %eax
	cmpl	$1, %eax
	cmovlel	%ecx, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	addl	%r12d, %ebp
	movl	$0, %ecx
	cmovsl	%ecx, %ebp
	cmpl	%ebp, %eax
	cmovgel	%eax, %ebp
	cmpl	%edx, %ebp
	cmovll	%edx, %ebp
	cmpl	$2, %ebp
	setl	%al
	cmpl	$1, %ebp
	cmovlel	%ecx, %ebp
	movzbl	%al, %eax
	orl	%eax, %ebp
	cmpl	%ebp, %r15d
	cmovlel	%r15d, %ebp
	cmpl	%r12d, %ebp
	cmovll	%r12d, %ebp
	movl	%ebp, 304(%rsp)         # 4-byte Spill
	movq	568(%rsp), %rbx         # 8-byte Reload
	leal	(%rbx,%rbx), %edx
	movl	%edx, %eax
	negl	%eax
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %esi
	andl	%eax, %ecx
	orl	%esi, %ecx
	movq	552(%rsp), %rbp         # 8-byte Reload
	movl	%ebp, %eax
	andl	$-8, %eax
	movq	%rax, 392(%rsp)         # 8-byte Spill
	movq	%rax, %rsi
	movl	%edx, %r8d
	subl	%ecx, %r8d
	cmovgel	%edx, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	movq	640(%rsp), %rdx         # 8-byte Reload
	leal	7(%rsi,%rdx), %eax
	movq	%rsi, %r10
	leal	-1(%r9,%rbx), %esi
	movq	%rbx, %r11
	movl	%esi, 24(%rsp)          # 4-byte Spill
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	leal	-1(%rcx,%r9), %edi
	movl	%edi, 340(%rsp)         # 4-byte Spill
	addl	%r9d, %ecx
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movl	%eax, 400(%rsp)         # 4-byte Spill
	leal	12(%rbp), %ebx
	movq	%rbp, %rax
	andl	$-8, %ebx
	movq	%rbx, 376(%rsp)         # 8-byte Spill
	movl	%ebx, %r12d
	addl	%edx, %r12d
	leal	1(%rbx,%rdx), %ebp
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	cmpl	%edi, %ebp
	cmovll	%edi, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	movl	%ebp, 388(%rsp)         # 4-byte Spill
	leal	3(%rbx,%rdx), %ebp
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	cmpl	%edi, %ebp
	cmovll	%edi, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	movl	%ebp, 368(%rsp)         # 4-byte Spill
	leal	(%r9,%r11), %ebx
	cmpl	%r12d, %ebx
	movl	%ebx, %ebp
	movl	%ebx, %r15d
	cmovgl	%r12d, %ebp
	cmpl	%ecx, %ebp
	cmovll	%ecx, %ebp
	addl	$-1, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	movl	%ebp, 364(%rsp)         # 4-byte Spill
	leal	4(%rax), %eax
	andl	$-8, %eax
	movq	%rax, 352(%rsp)         # 8-byte Spill
	movq	%rdx, %rcx
	leal	5(%rax,%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movl	%eax, 348(%rsp)         # 4-byte Spill
	leal	9(%r10,%rcx), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movl	%eax, 372(%rsp)         # 4-byte Spill
	movl	%r8d, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%r9d, %eax
	leal	2(%rcx), %edx
	cmpl	%eax, %edx
	cmovgl	%eax, %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 344(%rsp)         # 4-byte Spill
	cmpl	%eax, %ecx
	movl	%ecx, %r13d
	cmovgl	%eax, %r13d
	leal	1(%rcx), %edx
	cmovgel	%eax, %edx
	movl	%edx, 664(%rsp)         # 4-byte Spill
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%r9d, %r13d
	cmovll	%r9d, %r13d
	leal	-2(%rcx), %r11d
	movq	%rcx, %rbx
	cmpl	%eax, %r11d
	movl	%r11d, %r8d
	cmovgl	%eax, %r8d
	cmpl	%r8d, %esi
	cmovlel	%esi, %r8d
	cmpl	%r9d, %r8d
	cmovll	%r9d, %r8d
	cmpl	%eax, %esi
	movl	%esi, %edx
	cmovgl	%eax, %edx
	movq	648(%rsp), %rcx         # 8-byte Reload
	cmpl	%ecx, %r15d
	movl	%r15d, %ebp
	movl	%r15d, %r14d
	cmovgl	%ecx, %ebp
	movl	%ebp, %edi
	sarl	$31, %edi
	andl	%ebp, %edi
	addl	$-1, %edi
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	cmpl	%r9d, %edi
	cmovll	%r9d, %edi
	leal	-6(%rbx), %r15d
	cmpl	%eax, %r15d
	movl	%r15d, %r10d
	cmovgl	%eax, %r10d
	cmpl	%r10d, %esi
	cmovlel	%esi, %r10d
	cmpl	%r9d, %r10d
	cmovll	%r9d, %r10d
	leal	-4(%rbx), %ebx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebx
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	movl	672(%rsp), %ebp         # 4-byte Reload
	cmpl	%eax, %ebp
	cmovgl	%eax, %ebp
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	movl	%ebp, 672(%rsp)         # 4-byte Spill
	movl	664(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	movl	%ebp, 664(%rsp)         # 4-byte Spill
	movl	%r14d, %ecx
	cmpl	%r15d, %ecx
	cmovlel	%ecx, %r15d
	addl	$-1, %r15d
	cmpl	%edx, %r15d
	cmovgl	%edx, %r15d
	cmpl	%r9d, %r15d
	cmovll	%r9d, %r15d
	movq	640(%rsp), %r14         # 8-byte Reload
	leal	-5(%r14), %ebp
	cmpl	%eax, %ebp
	cmovgl	%eax, %ebp
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	cmpl	%r9d, %ebp
	cmovll	%r9d, %ebp
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	addl	$-1, %r11d
	cmpl	%edx, %r11d
	cmovgl	%edx, %r11d
	cmpl	%r9d, %r11d
	cmovll	%r9d, %r11d
	leal	-1(%r14), %edx
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%r11d, %eax
	cmovgl	%r11d, %eax
	cmpl	%eax, %r8d
	cmovlel	%r8d, %eax
	cmpl	%eax, %r13d
	cmovlel	%r13d, %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%eax, %r10d
	cmovlel	%r10d, %eax
	movl	672(%rsp), %edx         # 4-byte Reload
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cmpl	%ebp, %eax
	cmovgl	%ebp, %eax
	cmpl	%r15d, %eax
	cmovgl	%r15d, %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cmpl	%eax, %r10d
	cmovlel	%r10d, %eax
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	movl	344(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	cmpl	%r8d, %eax
	cmovgl	%r8d, %eax
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	movl	664(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %eax
	cmovgl	%ebp, %eax
	cmpl	%edi, %eax
	cmovgl	%edi, %eax
	cmpl	%r8d, %eax
	cmovgl	%r8d, %eax
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	cmpl	%edx, %eax
	cmovgl	%edx, %eax
	movl	%eax, 344(%rsp)         # 4-byte Spill
	movq	352(%rsp), %rdx         # 8-byte Reload
	leal	6(%rdx,%r14), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	340(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	movl	348(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	leal	4(%rdx,%r14), %ecx
	movq	%rdx, %rdi
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	7(%rdi,%r14), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	leal	3(%rdi,%r14), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	movl	%ebp, %ebx
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movq	376(%rsp), %rcx         # 8-byte Reload
	leal	2(%rcx,%r14), %ecx
	movl	388(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movl	368(%rsp), %edi         # 4-byte Reload
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	movl	364(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	cmpl	%r12d, %esi
	cmovlel	%esi, %r12d
	cmpl	%ebx, %r12d
	cmovll	%ebx, %r12d
	cmpl	%r9d, %r12d
	cmovll	%r9d, %r12d
	cmpl	%r12d, %ecx
	cmovgel	%ecx, %r12d
	cmpl	%edx, %r12d
	cmovll	%edx, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	%ebp, %r12d
	cmovll	%ebp, %r12d
	movl	400(%rsp), %eax         # 4-byte Reload
	cmpl	%r12d, %eax
	cmovgel	%eax, %r12d
	movl	%eax, %edi
	movl	372(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %r12d
	cmovll	%eax, %r12d
	movl	%eax, %ebp
	movq	392(%rsp), %rdx         # 8-byte Reload
	leal	5(%rdx,%r14), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%eax, %r12d
	cmovgel	%r12d, %eax
	leal	6(%rdx,%r14), %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	8(%rdx,%r14), %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	movl	%eax, 364(%rsp)         # 4-byte Spill
	movq	584(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	560(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rcx), %r11d
	cltd
	idivl	%r11d
	movl	%r11d, %eax
	negl	%eax
	movl	%ecx, %edi
	movq	%rcx, %r14
	sarl	$31, %edi
	andnl	%r11d, %edi, %ecx
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	movl	%r11d, %ebp
	subl	%ebx, %ebp
	addl	$-1, %ebp
	cmpl	%ebx, %ebp
	movl	%ebx, %r8d
	cmovgel	%ebp, %r8d
	movl	$1, %eax
	movq	%rsi, %rcx
	subl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r9d
	sarl	$31, %r9d
	andl	%edi, %r9d
	addl	%edx, %r9d
	movl	$2, %eax
	subl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r10d
	sarl	$31, %r10d
	andl	%edi, %r10d
	addl	%edx, %r10d
	movl	%r11d, %esi
	subl	%r10d, %esi
	addl	$-1, %esi
	cmpl	%esi, %r10d
	movl	%r10d, %edx
	cmovgl	%esi, %edx
	movq	%rcx, %rax
	addl	%eax, %edx
	leal	-1(%r14,%rax), %r14d
	movl	%r14d, 340(%rsp)        # 4-byte Spill
	cmpl	%edx, %r14d
	cmovlel	%r14d, %edx
	cmpl	%ebp, %ebx
	cmovlel	%ebx, %ebp
	addl	%eax, %ebp
	movq	%rax, %r15
	cmpl	%ebp, %r14d
	cmovlel	%r14d, %ebp
	subl	%r9d, %r11d
	addl	$-1, %r11d
	cmpl	%r11d, %r9d
	movl	%r9d, %edi
	cmovgl	%r11d, %edi
	addl	%r15d, %edi
	cmpl	%edi, %r14d
	cmovlel	%r14d, %edi
	movl	%edi, %ecx
	sarl	$31, %ecx
	testl	%edi, %edi
	setg	%bl
	andl	%edi, %ecx
	movzbl	%bl, %ebx
	orl	%ebx, %ecx
	cmpl	%ebp, %ecx
	movl	%ecx, %eax
	cmovgl	%ebp, %eax
	movl	%eax, %ebx
	sarl	$31, %ebx
	andl	%eax, %ebx
	cmpl	%edx, %ebx
	cmovgl	%edx, %ebx
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	cmpl	%ebx, %edx
	cmovlel	%edx, %ebx
	movl	%ebp, %ecx
	sarl	$31, %ecx
	andl	%ebp, %ecx
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	cmpl	$3, %ebx
	movl	$2, %r12d
	cmovgel	%r12d, %ebx
	cmpl	%edi, %ebx
	cmovgl	%edi, %ebx
	movl	%ebx, %eax
	sarl	$31, %eax
	testl	%ebx, %ebx
	setg	%dil
	andl	%ebx, %eax
	movzbl	%dil, %edi
	orl	%edi, %eax
	cmpl	%ebp, %eax
	cmovgl	%ebp, %eax
	movl	%eax, %edi
	sarl	$31, %edi
	andl	%eax, %edi
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	cmpl	%edi, %edx
	cmovlel	%edx, %edi
	cmpl	$3, %edi
	cmovgel	%r12d, %edi
	movl	$2, %edx
	movq	%r15, %rax
	cmpl	%eax, %edi
	cmovll	%eax, %edi
	movl	%edi, 348(%rsp)         # 4-byte Spill
	addl	%eax, %r8d
	cmpl	%r9d, %r11d
	cmovll	%r9d, %r11d
	addl	%eax, %r11d
	movq	%rax, %rcx
	cmpl	$2, %r11d
	setl	%al
	cmpl	$1, %r11d
	movl	$0, %edi
	cmovlel	%edi, %r11d
	movzbl	%al, %eax
	orl	%eax, %r11d
	cmpl	%r11d, %r8d
	cmovgel	%r8d, %r11d
	cmpl	%r10d, %esi
	cmovll	%r10d, %esi
	addl	%ecx, %esi
	cmpl	$1, %esi
	cmovlel	%edx, %esi
	cmpl	%esi, %r11d
	cmovgel	%r11d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	testl	%esi, %esi
	cmovsl	%edi, %esi
	cmpl	%esi, %r14d
	cmovlel	%r14d, %esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	movl	%esi, 352(%rsp)         # 4-byte Spill
	movq	576(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %eax
	movl	%eax, %ecx
	negl	%ecx
	movl	%edx, %edi
	movq	%rdx, %rsi
	sarl	$31, %edi
	andnl	%eax, %edi, %edx
	andl	%ecx, %edi
	orl	%edx, %edi
	movl	%eax, %ecx
	subl	%edi, %ecx
	cmovgel	%eax, %edi
	movq	%rsi, %rax
	cmpl	%edi, %eax
	movq	%rdi, %rbx
	cmovlel	%eax, %ebx
	movq	632(%rsp), %rsi         # 8-byte Reload
	leal	-1(%rsi,%rax), %r9d
	movl	%r9d, 20(%rsp)          # 4-byte Spill
	movq	%rax, %r10
	movq	608(%rsp), %rax         # 8-byte Reload
	leal	2(%rax), %edx
	cmpl	%edx, %r9d
	movl	%r9d, %edi
	cmovgl	%edx, %edi
	leal	-1(%rbx,%rsi), %ebp
	movl	%ebp, 376(%rsp)         # 4-byte Spill
	addl	%esi, %ebx
	movq	%rbx, 400(%rsp)         # 8-byte Spill
	movq	%rsi, %rbx
	cmpl	%ebp, %edi
	cmovll	%ebp, %edi
	movl	%edi, 672(%rsp)         # 4-byte Spill
	leal	4(%rax), %esi
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	movl	%esi, 664(%rsp)         # 4-byte Spill
	cmpl	%eax, %r9d
	movl	%r9d, %esi
	cmovgl	%eax, %esi
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	movl	%esi, 392(%rsp)         # 4-byte Spill
	leal	6(%rax), %edi
	cmpl	%edi, %r9d
	movl	%r9d, %eax
	cmovgl	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	movl	%eax, 388(%rsp)         # 4-byte Spill
	movl	%ecx, %r11d
	sarl	$31, %r11d
	andl	%ecx, %r11d
	addl	%ebx, %r11d
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	2(%rax), %ecx
	cmpl	%r11d, %ecx
	cmovgl	%r11d, %ecx
	cmpl	%ecx, %r9d
	cmovlel	%r9d, %ecx
	movl	%ecx, 372(%rsp)         # 4-byte Spill
	cmpl	%r11d, %eax
	movl	%eax, %ebp
	cmovgl	%r11d, %ebp
	leal	1(%rax), %r14d
	cmovgel	%r11d, %r14d
	cmpl	%ebp, %r9d
	cmovlel	%r9d, %ebp
	leal	-2(%rax), %ecx
	movq	%rax, %r15
	cmpl	%r11d, %ecx
	movl	%ecx, %r13d
	cmovgl	%r11d, %r13d
	cmpl	%r13d, %r9d
	cmovlel	%r9d, %r13d
	cmpl	%r11d, %r9d
	movl	%r9d, %r8d
	cmovgl	%r11d, %r8d
	leal	(%rbx,%r10), %ebx
	movq	656(%rsp), %rsi         # 8-byte Reload
	cmpl	%esi, %ebx
	movl	%ebx, %eax
	cmovgl	%esi, %eax
	movl	%eax, %r12d
	sarl	$31, %r12d
	andl	%eax, %r12d
	addl	$-1, %r12d
	cmpl	%r8d, %r12d
	cmovgl	%r8d, %r12d
	movq	%r15, %rax
	leal	-4(%rax), %r15d
	cmpl	%r11d, %r15d
	cmovgl	%r11d, %r15d
	cmpl	%r15d, %r9d
	cmovlel	%r9d, %r15d
	leal	-6(%rax), %r10d
	cmpl	%r11d, %r10d
	movl	%r10d, %esi
	cmovgl	%r11d, %esi
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	movl	676(%rsp), %eax         # 4-byte Reload
	cmpl	%r11d, %eax
	cmovgl	%r11d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	%eax, 676(%rsp)         # 4-byte Spill
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	movl	%r14d, 368(%rsp)        # 4-byte Spill
	cmpl	%r10d, %ebx
	cmovlel	%ebx, %r10d
	addl	$-1, %r10d
	cmpl	%r8d, %r10d
	cmovgl	%r8d, %r10d
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	-5(%rax), %eax
	cmpl	%r11d, %eax
	cmovgl	%r11d, %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ecx, %ebx
	movl	%ebx, %r14d
	cmovgl	%ecx, %r14d
	addl	$-1, %r14d
	cmpl	%r8d, %r14d
	cmovgl	%r8d, %r14d
	cmpl	%ecx, %r11d
	cmovlel	%r11d, %ecx
	cmpl	%ecx, %r9d
	cmovlel	%r9d, %ecx
	cmpl	%r14d, %ecx
	cmovgl	%r14d, %ecx
	movq	632(%rsp), %r14         # 8-byte Reload
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%ecx, %ebp
	cmovlel	%ebp, %ecx
	cmpl	%ecx, %r15d
	cmovlel	%r15d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	676(%rsp), %r8d         # 4-byte Reload
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	cmpl	%r10d, %ecx
	cmovgl	%r10d, %ecx
	cmpl	%ecx, %r15d
	cmovlel	%r15d, %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%ebp, %ecx
	cmovgl	%ebp, %ecx
	movl	372(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	cmpl	%r13d, %ecx
	cmovgl	%r13d, %ecx
	cmpl	%r12d, %ecx
	cmovgl	%r12d, %ecx
	movl	368(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	cmpl	%r12d, %ecx
	cmovgl	%r12d, %ecx
	movq	608(%rsp), %r12         # 8-byte Reload
	cmpl	%r13d, %ecx
	cmovgl	%r13d, %ecx
	cmpl	%ebp, %ecx
	cmovgl	%ebp, %ecx
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	cmpl	%r14d, %ecx
	cmovll	%r14d, %ecx
	leal	3(%r12), %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	movl	376(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	movl	%esi, %ebp
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	movq	400(%rsp), %rsi         # 8-byte Reload
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	addl	$-1, %edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	672(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movl	%eax, %r8d
	movl	664(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movl	%eax, %r11d
	movl	392(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movl	%eax, %r10d
	movl	388(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movl	%eax, %r15d
	movl	408(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	%eax, %edx
	leal	7(%r12), %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	cmpl	%edi, %ebx
	cmovlel	%ebx, %edi
	cmpl	%esi, %edi
	cmovll	%esi, %edi
	addl	$-1, %edi
	cmpl	%edi, %eax
	cmovgel	%eax, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	cmpl	%r15d, %edi
	cmovll	%r15d, %edi
	cmpl	%r10d, %edi
	cmovll	%r10d, %edi
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	leal	-2(%r12), %eax
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%eax, %edi
	cmovgel	%edi, %eax
	cmpl	%r12d, %ebx
	cmovgl	%r12d, %ebx
	cmpl	%esi, %ebx
	cmovll	%esi, %ebx
	addl	$-1, %ebx
	cmpl	%ebx, %eax
	cmovgel	%eax, %ebx
	movq	536(%rsp), %rax         # 8-byte Reload
	cmpl	$2, %eax
	setl	%al
	movzbl	%al, %eax
	orl	428(%rsp), %eax         # 4-byte Folded Reload
	cmpl	%eax, %r9d
	cmovlel	%r9d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%eax, %ebx
	cmovgel	%ebx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	movq	496(%rsp), %rdx         # 8-byte Reload
	orq	%rdx, 416(%rsp)         # 8-byte Folded Spill
	sete	%r11b
	jne	.LBB147_25
# BB#24:                                # %true_bb
	vxorps	%xmm8, %xmm8, %xmm8
	movq	600(%rsp), %rdx         # 8-byte Reload
	vmovups	%xmm8, (%rdx)
	movl	$4, 64(%rdx)
	movb	$0, 68(%rdx)
	movb	$0, 69(%rdx)
	movl	$0, 48(%rdx)
	movl	$0, 52(%rdx)
	movl	$0, 56(%rdx)
	movl	$0, 60(%rdx)
	vmovaps	.LCPI147_0(%rip), %ymm8 # ymm8 = [3,3,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rdx)
.LBB147_25:                             # %after_bb
	movq	432(%rsp), %rdx         # 8-byte Reload
	orq	520(%rsp), %rdx         # 8-byte Folded Reload
	sete	%r8b
	jne	.LBB147_27
# BB#26:                                # %true_bb105
	movl	316(%rsp), %edx         # 4-byte Reload
	movl	320(%rsp), %ebp         # 4-byte Reload
	subl	%ebp, %edx
	movl	308(%rsp), %edi         # 4-byte Reload
	movl	312(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %edi
	vxorps	%xmm8, %xmm8, %xmm8
	movq	280(%rsp), %rsi         # 8-byte Reload
	vmovups	%xmm8, (%rsi)
	movl	$2, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movl	%ebp, 48(%rsi)
	movl	%edx, 16(%rsi)
	movl	$1, 32(%rsi)
	movl	%ebx, 52(%rsi)
	movl	%edi, 20(%rsi)
	movl	%edx, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$0, 24(%rsi)
	movl	$0, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_27:                             # %after_bb107
	movq	440(%rsp), %rdx         # 8-byte Reload
	orq	544(%rsp), %rdx         # 8-byte Folded Reload
	sete	%r15b
	movq	472(%rsp), %r9          # 8-byte Reload
	jne	.LBB147_29
# BB#28:                                # %true_bb108
	movl	$1, %edx
	movl	332(%rsp), %esi         # 4-byte Reload
	subl	%esi, %edx
	addl	336(%rsp), %edx         # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	1368(%rsp), %rbp
	vmovups	%xmm8, (%rbp)
	movl	$1, 64(%rbp)
	movb	$0, 68(%rbp)
	movb	$0, 69(%rbp)
	movq	648(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, 48(%rbp)
	movq	592(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, 16(%rbp)
	movl	$3, 32(%rbp)
	movl	%esi, 52(%rbp)
	movl	%edx, 20(%rbp)
	movl	%ebx, 36(%rbp)
	movl	$0, 56(%rbp)
	movl	$3, 24(%rbp)
	movl	$1, 40(%rbp)
	movl	$0, 60(%rbp)
	movl	$0, 28(%rbp)
	movl	$0, 44(%rbp)
.LBB147_29:                             # %after_bb110
	movq	448(%rsp), %rdx         # 8-byte Reload
	orq	504(%rsp), %rdx         # 8-byte Folded Reload
	sete	%r10b
	jne	.LBB147_31
# BB#30:                                # %true_bb111
	vxorps	%xmm8, %xmm8, %xmm8
	movq	1352(%rsp), %rdx
	vmovups	%xmm8, (%rdx)
	movl	$1, 64(%rdx)
	movb	$0, 68(%rdx)
	movb	$0, 69(%rdx)
	movl	$0, 48(%rdx)
	movl	$0, 52(%rdx)
	movl	$0, 56(%rdx)
	movl	$0, 60(%rdx)
	vmovaps	.LCPI147_1(%rip), %ymm8 # ymm8 = [3,4096,0,0,1,3,0,0]
	vmovups	%ymm8, 16(%rdx)
.LBB147_31:                             # %after_bb113
	movq	456(%rsp), %rdx         # 8-byte Reload
	orq	528(%rsp), %rdx         # 8-byte Folded Reload
	sete	%r13b
	jne	.LBB147_33
# BB#32:                                # %true_bb114
	movl	304(%rsp), %ebp         # 4-byte Reload
	movl	252(%rsp), %edi         # 4-byte Reload
	subl	%edi, %ebp
	addl	$1, %ebp
	movl	$1, %ebx
	movl	344(%rsp), %edx         # 4-byte Reload
	subl	%edx, %ebx
	addl	364(%rsp), %ebx         # 4-byte Folded Reload
	vxorps	%xmm8, %xmm8, %xmm8
	movq	288(%rsp), %rsi         # 8-byte Reload
	vmovups	%xmm8, (%rsi)
	movl	$4, 64(%rsi)
	movb	$0, 68(%rsi)
	movb	$0, 69(%rsi)
	movl	%edi, 48(%rsi)
	movl	%ebp, 16(%rsi)
	movl	$1, 32(%rsi)
	movl	%edx, 52(%rsi)
	movl	%ebx, 20(%rsi)
	movl	%ebp, 36(%rsi)
	movl	$0, 56(%rsi)
	movl	$0, 24(%rsi)
	movl	$0, 40(%rsi)
	movl	$0, 60(%rsi)
	movl	$0, 28(%rsi)
	movl	$0, 44(%rsi)
.LBB147_33:                             # %after_bb116
	movq	464(%rsp), %rsi         # 8-byte Reload
	orq	512(%rsp), %rsi         # 8-byte Folded Reload
	sete	%bl
	movq	656(%rsp), %rdi         # 8-byte Reload
	jne	.LBB147_35
# BB#34:                                # %after_bb119.thread
	movl	352(%rsp), %esi         # 4-byte Reload
	movl	348(%rsp), %edi         # 4-byte Reload
	subl	%edi, %esi
	addl	$1, %esi
	addl	$1, %eax
	subl	%ecx, %eax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	296(%rsp), %rdx         # 8-byte Reload
	vmovups	%xmm0, (%rdx)
	movl	$4, 64(%rdx)
	movb	$0, 68(%rdx)
	movb	$0, 69(%rdx)
	movl	%edi, 48(%rdx)
	movl	%esi, 16(%rdx)
	movl	$1, 32(%rdx)
	movl	%ecx, 52(%rdx)
	movl	%eax, 20(%rdx)
	movl	%esi, 36(%rdx)
	movl	$0, 56(%rdx)
	movl	$0, 24(%rdx)
	movl	$0, 40(%rdx)
	movl	$0, 60(%rdx)
	movl	$0, 28(%rdx)
	movl	$0, 44(%rdx)
	jmp	.LBB147_208
.LBB147_35:                             # %after_bb119
	orb	%r11b, %r8b
	orb	%r15b, %r8b
	orb	%r8b, %r10b
	orb	%r10b, %r13b
	xorl	%esi, %esi
	orb	%r13b, %bl
	movl	$0, %ebx
	movl	$0, %r15d
	jne	.LBB147_191
# BB#36:                                # %true_bb120
	movl	76(%rsp), %ebp          # 4-byte Reload
	cmpl	$4, %ebp
	jne	.LBB147_37
# BB#38:                                # %assert succeeded124
	movl	84(%rsp), %ebp          # 4-byte Reload
	cmpl	$2, %ebp
	movq	616(%rsp), %r15         # 8-byte Reload
	movq	224(%rsp), %r10         # 8-byte Reload
	movq	112(%rsp), %r8          # 8-byte Reload
	movq	272(%rsp), %r13         # 8-byte Reload
	movq	264(%rsp), %r11         # 8-byte Reload
	movq	192(%rsp), %rdx         # 8-byte Reload
	movl	92(%rsp), %ebx          # 4-byte Reload
	jne	.LBB147_39
# BB#40:                                # %assert succeeded126
	movl	72(%rsp), %ebp          # 4-byte Reload
	cmpl	$1, %ebp
	jne	.LBB147_41
# BB#46:                                # %assert succeeded128
	movl	80(%rsp), %ebp          # 4-byte Reload
	cmpl	$1, %ebp
	jne	.LBB147_47
# BB#48:                                # %assert succeeded130
	cmpl	$4, %ebx
	jne	.LBB147_49
# BB#51:                                # %assert succeeded132
	movl	88(%rsp), %ebx          # 4-byte Reload
	cmpl	$4, %ebx
	jne	.LBB147_52
# BB#53:                                # %assert succeeded134
	testl	%edx, %edx
	movq	240(%rsp), %rbp         # 8-byte Reload
	jg	.LBB147_55
# BB#54:                                # %assert succeeded134
	movq	%rdx, %rsi
	movl	$3, %edx
	subl	%ebp, %edx
	cmpl	%esi, %edx
	movq	%rsi, %rdx
	jg	.LBB147_55
# BB#58:                                # %assert succeeded136
	testl	%ebp, %ebp
	js	.LBB147_59
# BB#60:                                # %assert succeeded138
	movq	200(%rsp), %rdx         # 8-byte Reload
	testl	%edx, %edx
	movq	104(%rsp), %rbp         # 8-byte Reload
	jg	.LBB147_62
# BB#61:                                # %assert succeeded138
	movq	%rdx, %rsi
	movl	$3, %edx
	subl	%ebp, %edx
	cmpl	%esi, %edx
	movq	%rsi, %rdx
	jg	.LBB147_62
# BB#63:                                # %assert succeeded140
	testl	%ebp, %ebp
	movq	%rbp, %rsi
	js	.LBB147_64
# BB#67:                                # %assert succeeded142
	cmpl	320(%rsp), %r10d        # 4-byte Folded Reload
	movq	232(%rsp), %rbp         # 8-byte Reload
	jg	.LBB147_69
# BB#68:                                # %assert succeeded142
	movl	316(%rsp), %edx         # 4-byte Reload
	subl	%r8d, %edx
	cmpl	%r10d, %edx
	jg	.LBB147_69
# BB#72:                                # %assert succeeded144
	testl	%r8d, %r8d
	js	.LBB147_73
# BB#74:                                # %assert succeeded146
	movq	216(%rsp), %r10         # 8-byte Reload
	cmpl	312(%rsp), %r10d        # 4-byte Folded Reload
	jg	.LBB147_76
# BB#75:                                # %assert succeeded146
	movl	308(%rsp), %edx         # 4-byte Reload
	subl	%r13d, %edx
	cmpl	%r10d, %edx
	jg	.LBB147_76
# BB#77:                                # %assert succeeded148
	testl	%r13d, %r13d
	js	.LBB147_78
# BB#79:                                # %assert succeeded150
	movq	592(%rsp), %rdx         # 8-byte Reload
	testl	%edx, %edx
	movq	208(%rsp), %r13         # 8-byte Reload
	movq	96(%rsp), %rbx          # 8-byte Reload
	js	.LBB147_80
# BB#81:                                # %assert succeeded152
	movq	%rdi, %rdx
	cmpl	332(%rsp), %edx         # 4-byte Folded Reload
	jg	.LBB147_83
# BB#82:                                # %assert succeeded152
	movq	%rdx, %rdi
	movl	336(%rsp), %edx         # 4-byte Reload
	subl	%r15d, %edx
	cmpl	%edi, %edx
	movq	%rdi, %rdx
	jge	.LBB147_83
# BB#84:                                # %assert succeeded154
	testl	%r15d, %r15d
	js	.LBB147_85
# BB#86:                                # %assert succeeded156
	movq	%r8, %rdi
	testl	%ebx, %ebx
	jg	.LBB147_88
# BB#87:                                # %assert succeeded156
	movl	$3, %edx
	subl	%ebp, %edx
	cmpl	%ebx, %edx
	jg	.LBB147_88
# BB#89:                                # %assert succeeded158
	movq	%rbx, %r15
	movq	%rsi, %r10
	testl	%ebp, %ebp
	js	.LBB147_90
# BB#91:                                # %assert succeeded160
	movq	160(%rsp), %rdx         # 8-byte Reload
	testl	%edx, %edx
	movq	184(%rsp), %rbp         # 8-byte Reload
	jg	.LBB147_93
# BB#92:                                # %assert succeeded160
	movq	%rdx, %rsi
	movl	$3, %edx
	subl	%ebp, %edx
	cmpl	%esi, %edx
	movq	%rsi, %rdx
	jg	.LBB147_93
# BB#94:                                # %assert succeeded162
	testl	%ebp, %ebp
	js	.LBB147_95
# BB#96:                                # %assert succeeded164
	movq	168(%rsp), %rdx         # 8-byte Reload
	testl	%edx, %edx
	movl	304(%rsp), %r8d         # 4-byte Reload
	jg	.LBB147_98
# BB#97:                                # %assert succeeded164
	movq	%rdx, %rsi
	movl	$4096, %edx             # imm = 0x1000
	subl	%r9d, %edx
	cmpl	%esi, %edx
	movq	%rsi, %rdx
	jg	.LBB147_98
# BB#99:                                # %assert succeeded166
	testl	%r9d, %r9d
	js	.LBB147_100
# BB#101:                               # %assert succeeded168
	movl	%r8d, %edx
	subl	%r11d, %edx
	movq	176(%rsp), %rsi         # 8-byte Reload
	cmpl	%esi, %edx
	jge	.LBB147_102
# BB#103:                               # %assert succeeded170
	testl	%r11d, %r11d
	js	.LBB147_104
# BB#105:                               # %assert succeeded172
	cmpl	344(%rsp), %r13d        # 4-byte Folded Reload
	movq	568(%rsp), %r11         # 8-byte Reload
	jg	.LBB147_107
# BB#106:                               # %assert succeeded172
	movl	364(%rsp), %edx         # 4-byte Reload
	subl	%r11d, %edx
	cmpl	%r13d, %edx
	jge	.LBB147_107
# BB#108:                               # %assert succeeded174
	testl	%r11d, %r11d
	movl	352(%rsp), %r8d         # 4-byte Reload
	js	.LBB147_109
# BB#110:                               # %assert succeeded176
	movl	%r8d, %edx
	movq	560(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %edx
	movq	584(%rsp), %rsi         # 8-byte Reload
	cmpl	%esi, %edx
	movq	576(%rsp), %rbp         # 8-byte Reload
	jge	.LBB147_111
# BB#112:                               # %assert succeeded178
	testl	%ebx, %ebx
	movl	48(%rsp), %esi          # 4-byte Reload
	js	.LBB147_113
# BB#114:                               # %assert succeeded180
	movl	%eax, %edx
	subl	%ebp, %edx
	cmpl	%r14d, %edx
	jge	.LBB147_115
# BB#116:                               # %assert succeeded182
	testl	%ebp, %ebp
	js	.LBB147_117
# BB#118:                               # %assert succeeded184
	cmpl	$1, %esi
	movq	240(%rsp), %rcx         # 8-byte Reload
	movl	68(%rsp), %edx          # 4-byte Reload
	movl	56(%rsp), %eax          # 4-byte Reload
	jne	.LBB147_119
# BB#120:                               # %assert succeeded186
	cmpl	$1, %eax
	jne	.LBB147_121
# BB#125:                               # %assert succeeded188
	movl	44(%rsp), %eax          # 4-byte Reload
	cmpl	$3, %eax
	jne	.LBB147_126
# BB#127:                               # %assert succeeded190
	movl	52(%rsp), %eax          # 4-byte Reload
	cmpl	$1, %eax
	jne	.LBB147_128
# BB#129:                               # %assert succeeded192
	testl	%r15d, %r15d
	jne	.LBB147_130
# BB#131:                               # %assert succeeded194
	movq	232(%rsp), %rax         # 8-byte Reload
	cmpl	$3, %eax
	movq	616(%rsp), %r13         # 8-byte Reload
	jne	.LBB147_132
# BB#133:                               # %assert succeeded196
	movl	60(%rsp), %eax          # 4-byte Reload
	cmpl	$1, %eax
	jne	.LBB147_134
# BB#135:                               # %assert succeeded198
	cmpl	$1, %edx
	jne	.LBB147_136
# BB#138:                               # %assert succeeded200
	movl	64(%rsp), %edx          # 4-byte Reload
	cmpl	$1, %edx
	jne	.LBB147_139
# BB#140:                               # %assert succeeded204
	movslq	156(%rsp), %rax         # 4-byte Folded Reload
	imulq	%r10, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_141
# BB#144:                               # %assert succeeded206
	imulq	%rcx, %r10
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r10
	jge	.LBB147_145
# BB#146:                               # %assert succeeded210
	movq	272(%rsp), %rax         # 8-byte Reload
	movslq	%eax, %rdx
	movslq	124(%rsp), %rcx         # 4-byte Folded Reload
	imulq	%rdx, %rcx
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	testq	$-2147483648, %rax      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_147
# BB#149:                               # %assert succeeded212
	movslq	%edi, %rax
	imulq	%rax, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_150
# BB#153:                               # %assert succeeded214
	movq	592(%rsp), %rax         # 8-byte Reload
	movslq	%eax, %rcx
	leaq	(%rcx,%rcx,2), %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_154
# BB#155:                               # %assert succeeded216
	movslq	%r13d, %rdx
	movslq	140(%rsp), %rsi         # 4-byte Folded Reload
	imulq	%rdx, %rsi
	movq	%rsi, %rax
	negq	%rax
	cmovlq	%rsi, %rax
	testq	$-2147483648, %rax      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_156
# BB#157:                               # %assert succeeded218
	imulq	%rcx, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_158
# BB#159:                               # %assert succeeded220
	cmpq	$715827883, %rdx        # imm = 0x2AAAAAAB
	movq	184(%rsp), %rcx         # 8-byte Reload
	jge	.LBB147_160
# BB#161:                               # %assert succeeded224
	movslq	120(%rsp), %rax         # 4-byte Folded Reload
	imulq	%r9, %rax
	movq	%rax, %rdx
	negq	%rdx
	cmovlq	%rax, %rdx
	testq	$-2147483648, %rdx      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_162
# BB#163:                               # %assert succeeded226
	imulq	%rcx, %r9
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %r9
	jge	.LBB147_164
# BB#165:                               # %assert succeeded230
	movslq	%r11d, %rdx
	movslq	128(%rsp), %rcx         # 4-byte Folded Reload
	imulq	%rdx, %rcx
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	testq	$-2147483648, %rax      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_166
# BB#167:                               # %assert succeeded232
	movq	264(%rsp), %rax         # 8-byte Reload
	cltq
	imulq	%rax, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_168
# BB#169:                               # %assert succeeded236
	movslq	%ebp, %rdx
	movslq	132(%rsp), %rcx         # 4-byte Folded Reload
	imulq	%rdx, %rcx
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	testq	$-2147483648, %rax      # imm = 0xFFFFFFFF80000000
	jne	.LBB147_170
# BB#171:                               # %assert succeeded238
	movslq	%ebx, %rax
	imulq	%rax, %rdx
	movl	$2147483648, %eax       # imm = 0x80000000
	cmpq	%rax, %rdx
	jge	.LBB147_172
# BB#173:                               # %assert succeeded240
	movq	%rdi, %r13
	vmovss	%xmm2, 428(%rsp)        # 4-byte Spill
	vmovss	%xmm1, 432(%rsp)        # 4-byte Spill
	vmovss	%xmm0, 440(%rsp)        # 4-byte Spill
	vmovss	%xmm7, 448(%rsp)        # 4-byte Spill
	vmovss	%xmm6, 456(%rsp)        # 4-byte Spill
	vmovss	%xmm5, 464(%rsp)        # 4-byte Spill
	vmovss	%xmm4, 472(%rsp)        # 4-byte Spill
	vmovss	%xmm3, 664(%rsp)        # 4-byte Spill
	leal	-31(%r12), %eax
	movq	656(%rsp), %rcx         # 8-byte Reload
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	movl	332(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 676(%rsp)         # 4-byte Spill
	movl	%ecx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%r12d, %r15d
	movq	624(%rsp), %rdx         # 8-byte Reload
	subl	%edx, %r15d
	movl	%r15d, %ecx
	andl	$-32, %ecx
	leal	31(%rdx,%rcx), %ecx
	cmpl	%r12d, %ecx
	cmovgl	%r12d, %ecx
	movq	144(%rsp), %rsi         # 8-byte Reload
	leal	-1(%rsi), %r12d
	xorl	%edx, %edx
	cmpl	$1, %esi
	cmovlel	%edx, %r12d
	movl	336(%rsp), %esi         # 4-byte Reload
	cmpl	%r12d, %esi
	cmovgel	%esi, %r12d
	cmpl	%r12d, %ecx
	cmovgel	%ecx, %r12d
	subl	%eax, %r12d
	movq	256(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rcx), %eax
	cmpl	$1, %ecx
	cmovlel	%edx, %eax
	leal	-7(%rax), %edx
	movq	648(%rsp), %rcx         # 8-byte Reload
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	movl	%edx, 672(%rsp)         # 4-byte Spill
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movl	%eax, %r14d
	orl	$7, %r14d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r14d
	movq	488(%rsp), %rax         # 8-byte Reload
	leal	-1(%rax), %eax
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	movq	552(%rsp), %rax         # 8-byte Reload
	andl	$-32, %eax
	movq	640(%rsp), %rdx         # 8-byte Reload
	leal	31(%rdx,%rax), %eax
	cmpl	%eax, %r14d
	cmovll	%eax, %r14d
	subl	%ecx, %r14d
	addl	$1, %r12d
	leal	1(%r14), %ebx
	movq	%rbx, %rax
	imulq	%r12, %rax
	leaq	(%rax,%rax,2), %rbp
	movl	%eax, %ecx
	cmpq	$2147483647, %rbp       # imm = 0x7FFFFFFF
	ja	.LBB147_175
# BB#174:                               # %assert succeeded240
	shrq	$32, %rax
	leaq	(%rcx,%rcx,2), %rcx
	shrq	$32, %rcx
	leaq	(%rax,%rax,2), %rax
	addq	%rcx, %rax
	movabsq	$30064771072, %rcx      # imm = 0x700000000
	andq	%rax, %rcx
	jne	.LBB147_175
# BB#176:                               # %assert succeeded242
	leaq	1(%rbp), %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, %rcx
	addq	$1, %rbp
	je	.LBB147_179
# BB#177:                               # %assert succeeded242
	testq	%rcx, %rcx
	je	.LBB147_178
.LBB147_179:                            # %assert succeeded244
	imull	%ebx, %r12d
	vmovss	428(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 680(%rsp)
	vmovss	432(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 684(%rsp)
	vmovss	440(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 688(%rsp)
	movq	192(%rsp), %rax         # 8-byte Reload
	movl	%eax, 692(%rsp)
	movq	200(%rsp), %rax         # 8-byte Reload
	movl	%eax, 696(%rsp)
	movl	156(%rsp), %eax         # 4-byte Reload
	movl	%eax, 700(%rsp)
	vmovss	1304(%rsp), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vmovss	1296(%rsp), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vmovss	1288(%rsp), %xmm2       # xmm2 = mem[0],zero,zero,zero
	vmovss	1280(%rsp), %xmm3       # xmm3 = mem[0],zero,zero,zero
	vmovss	%xmm0, 704(%rsp)
	vmovss	%xmm1, 708(%rsp)
	vmovss	%xmm2, 712(%rsp)
	vmovss	%xmm3, 716(%rsp)
	vmovss	448(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 720(%rsp)
	vmovss	456(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 724(%rsp)
	movq	552(%rsp), %rax         # 8-byte Reload
	movl	%eax, 728(%rsp)
	movq	536(%rsp), %rax         # 8-byte Reload
	movl	%eax, 732(%rsp)
	movl	%r12d, 736(%rsp)
	movl	%r14d, 740(%rsp)
	movl	672(%rsp), %eax         # 4-byte Reload
	movl	%eax, 744(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 748(%rsp)
	movl	%r13d, 752(%rsp)
	movq	272(%rsp), %rax         # 8-byte Reload
	movl	%eax, 756(%rsp)
	movq	224(%rsp), %rax         # 8-byte Reload
	movl	%eax, 760(%rsp)
	movq	216(%rsp), %rax         # 8-byte Reload
	movl	%eax, 764(%rsp)
	movl	124(%rsp), %eax         # 4-byte Reload
	movl	%eax, 768(%rsp)
	movq	648(%rsp), %rax         # 8-byte Reload
	movl	%eax, 772(%rsp)
	movq	656(%rsp), %r13         # 8-byte Reload
	movl	%r13d, 776(%rsp)
	movq	160(%rsp), %rax         # 8-byte Reload
	movl	%eax, 780(%rsp)
	movq	168(%rsp), %rax         # 8-byte Reload
	movl	%eax, 784(%rsp)
	movl	120(%rsp), %eax         # 4-byte Reload
	movl	%eax, 788(%rsp)
	movq	264(%rsp), %rax         # 8-byte Reload
	movl	%eax, 792(%rsp)
	movq	568(%rsp), %rax         # 8-byte Reload
	movl	%eax, 796(%rsp)
	movq	176(%rsp), %rax         # 8-byte Reload
	movl	%eax, 800(%rsp)
	movq	208(%rsp), %rax         # 8-byte Reload
	movl	%eax, 804(%rsp)
	movl	128(%rsp), %eax         # 4-byte Reload
	movl	%eax, 808(%rsp)
	movq	560(%rsp), %rax         # 8-byte Reload
	movl	%eax, 812(%rsp)
	movq	576(%rsp), %rax         # 8-byte Reload
	movl	%eax, 816(%rsp)
	movq	584(%rsp), %rax         # 8-byte Reload
	movl	%eax, 820(%rsp)
	movq	632(%rsp), %rax         # 8-byte Reload
	movl	%eax, 824(%rsp)
	movl	132(%rsp), %eax         # 4-byte Reload
	movl	%eax, 828(%rsp)
	vmovss	464(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 832(%rsp)
	vmovss	472(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 836(%rsp)
	vmovss	664(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 840(%rsp)
	movq	496(%rsp), %rax         # 8-byte Reload
	movq	%rax, 848(%rsp)
	movq	600(%rsp), %rax         # 8-byte Reload
	movq	%rax, 856(%rsp)
	movq	%rcx, 864(%rsp)
	movq	%rcx, 664(%rsp)         # 8-byte Spill
	movq	$0, 872(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movq	%rax, 880(%rsp)
	movq	280(%rsp), %rax         # 8-byte Reload
	movq	%rax, 888(%rsp)
	movq	504(%rsp), %rax         # 8-byte Reload
	movq	%rax, 896(%rsp)
	movq	1352(%rsp), %rax
	movq	%rax, 904(%rsp)
	movq	528(%rsp), %rax         # 8-byte Reload
	movq	%rax, 912(%rsp)
	movq	288(%rsp), %rax         # 8-byte Reload
	movq	%rax, 920(%rsp)
	movq	512(%rsp), %rax         # 8-byte Reload
	movq	%rax, 928(%rsp)
	movq	296(%rsp), %rax         # 8-byte Reload
	movq	%rax, 936(%rsp)
	sarl	$5, %r15d
	addl	$1, %r15d
	leaq	par_for___sharpi_f0.s0.v11.v14(%rip), %rsi
	leaq	680(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r15d, %ecx
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB147_180
# BB#181:                               # %consume f0
	movl	328(%rsp), %eax         # 4-byte Reload
	addl	$-8, %eax
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	movl	%eax, 328(%rsp)         # 4-byte Spill
	movq	480(%rsp), %rcx         # 8-byte Reload
	orl	$7, %ecx
	addl	%r13d, %ecx
	movq	%rcx, 480(%rsp)         # 8-byte Spill
	movl	136(%rsp), %edx         # 4-byte Reload
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	movl	%edx, 640(%rsp)         # 4-byte Spill
	movq	256(%rsp), %r15         # 8-byte Reload
	cmpl	$7, %r15d
	movl	$8, %ebp
	cmovgl	%r15d, %ebp
	movl	%edx, %r13d
	subl	%eax, %r13d
	addl	$1, %r13d
	movl	%ebp, %eax
	leaq	(,%r13,4), %rdx
	movl	%edx, %ecx
	imulq	%rax, %rdx
	leaq	(%rdx,%rdx,2), %rbx
	movl	%edx, %edx
	cmpq	$2147483647, %rbx       # imm = 0x7FFFFFFF
	ja	.LBB147_183
# BB#182:                               # %consume f0
	imulq	%rax, %rcx
	movq	%r13, %rsi
	shrq	$30, %rsi
	imulq	%rax, %rsi
	shrq	$32, %rcx
	addq	%rcx, %rsi
	leaq	(%rdx,%rdx,2), %rax
	shrq	$32, %rax
	leaq	(%rsi,%rsi,2), %rcx
	addq	%rax, %rcx
	orq	%rsi, %rcx
	shrq	$32, %rcx
	jne	.LBB147_183
# BB#184:                               # %assert succeeded248
	leaq	4(%rbx), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %rcx
	addq	$4, %rbx
	je	.LBB147_188
# BB#185:                               # %assert succeeded248
	testq	%rcx, %rcx
	je	.LBB147_186
.LBB147_188:                            # %assert succeeded250
	vmovss	1336(%rsp), %xmm0       # xmm0 = mem[0],zero,zero,zero
	imull	%ebp, %r13d
	movl	%r12d, 944(%rsp)
	movl	%r14d, 948(%rsp)
	movl	672(%rsp), %eax         # 4-byte Reload
	movl	%eax, 952(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 956(%rsp)
	movl	%r15d, 960(%rsp)
	movq	144(%rsp), %rax         # 8-byte Reload
	movl	%eax, 964(%rsp)
	vmovss	%xmm0, 968(%rsp)
	movq	616(%rsp), %rax         # 8-byte Reload
	movl	%eax, 972(%rsp)
	movq	656(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, 976(%rsp)
	movl	%r13d, 980(%rsp)
	movl	640(%rsp), %eax         # 4-byte Reload
	movl	%eax, 984(%rsp)
	movl	328(%rsp), %eax         # 4-byte Reload
	movl	%eax, 988(%rsp)
	movq	664(%rsp), %rax         # 8-byte Reload
	movq	%rax, 992(%rsp)
	movq	$0, 1000(%rsp)
	movq	%rcx, 1008(%rsp)
	movq	%rcx, 632(%rsp)         # 8-byte Spill
	movq	$0, 1016(%rsp)
	leaq	par_for___sharpi_transpose.s0.v12(%rip), %rsi
	leaq	944(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB147_189
# BB#192:                               # %consume transpose
	movq	%r12, 624(%rsp)         # 8-byte Spill
	movl	324(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	movl	%edx, 324(%rsp)         # 4-byte Spill
	movq	480(%rsp), %rbx         # 8-byte Reload
	movl	40(%rsp), %eax          # 4-byte Reload
	cmpl	%ebx, %eax
	cmovgel	%eax, %ebx
	movl	136(%rsp), %eax         # 4-byte Reload
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movq	488(%rsp), %rcx         # 8-byte Reload
	addl	$-8, %ecx
	movq	648(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	movq	%rcx, 488(%rsp)         # 8-byte Spill
	subl	%edx, %ebx
	addl	$1, %ebx
	movq	592(%rsp), %rax         # 8-byte Reload
	cmpl	$7, %eax
	movl	$8, %r15d
	cmovgl	%eax, %r15d
	movl	%r15d, %eax
	leaq	(,%rax,4), %rdx
	movl	%edx, %ecx
	imulq	%rbx, %rdx
	leaq	(%rdx,%rdx,2), %rbp
	movl	%edx, %edx
	cmpq	$2147483647, %rbp       # imm = 0x7FFFFFFF
	ja	.LBB147_194
# BB#193:                               # %consume transpose
	imulq	%rbx, %rcx
	shrq	$30, %rax
	imulq	%rbx, %rax
	shrq	$32, %rcx
	addq	%rcx, %rax
	leaq	(%rdx,%rdx,2), %rcx
	shrq	$32, %rcx
	leaq	(%rax,%rax,2), %rdx
	addq	%rcx, %rdx
	orq	%rax, %rdx
	shrq	$32, %rdx
	jne	.LBB147_194
# BB#195:                               # %assert succeeded254
	leaq	4(%rbp), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, %r12
	addq	$4, %rbp
	je	.LBB147_199
# BB#196:                               # %assert succeeded254
	testq	%r12, %r12
	je	.LBB147_197
.LBB147_199:                            # %assert succeeded256
	imull	%r15d, %ebx
	movq	%rbx, 480(%rsp)         # 8-byte Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1024(%rsp)
	vmovss	1336(%rsp), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 1028(%rsp)
	movq	592(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1032(%rsp)
	movq	616(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1036(%rsp)
	movq	648(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1040(%rsp)
	movq	656(%rsp), %r15         # 8-byte Reload
	movl	%r15d, 1044(%rsp)
	movl	%ebx, 1048(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1052(%rsp)
	movl	324(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1056(%rsp)
	movl	%r13d, 1060(%rsp)
	movl	640(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1064(%rsp)
	movl	328(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1068(%rsp)
	movq	632(%rsp), %rbp         # 8-byte Reload
	movq	%rbp, 1072(%rsp)
	movq	$0, 1080(%rsp)
	movq	%r12, 1088(%rsp)
	movq	$0, 1096(%rsp)
	leaq	par_for___sharpi_transpose$1.s0.v12(%rip), %rsi
	leaq	1024(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB147_200
# BB#201:                               # %assert succeeded258
	movb	1360(%rsp), %bl
	vmovss	1344(%rsp), %xmm3       # xmm3 = mem[0],zero,zero,zero
	vmovss	1328(%rsp), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vmovss	1320(%rsp), %xmm2       # xmm2 = mem[0],zero,zero,zero
	vmovss	1312(%rsp), %xmm1       # xmm1 = mem[0],zero,zero,zero
	testq	%rbp, %rbp
	movq	%rbp, %rsi
	movq	624(%rsp), %rbp         # 8-byte Reload
	je	.LBB147_203
# BB#202:                               # %if.then.i.358
	xorl	%edi, %edi
	callq	halide_free@PLT
	vmovss	1344(%rsp), %xmm3       # xmm3 = mem[0],zero,zero,zero
	vmovss	1320(%rsp), %xmm2       # xmm2 = mem[0],zero,zero,zero
	vmovss	1312(%rsp), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vmovss	1328(%rsp), %xmm0       # xmm0 = mem[0],zero,zero,zero
.LBB147_203:                            # %call_destructor.exit359
	movl	%ebp, 1104(%rsp)
	movl	%r14d, 1108(%rsp)
	movl	672(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1112(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1116(%rsp)
	vmovss	%xmm3, 1120(%rsp)
	andb	$1, %bl
	movb	%bl, 1124(%rsp)
	vmovss	%xmm2, 1128(%rsp)
	vmovss	%xmm1, 1132(%rsp)
	vmovss	%xmm0, 1136(%rsp)
	movq	592(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1140(%rsp)
	movq	616(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1144(%rsp)
	movq	648(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1148(%rsp)
	movl	%r15d, 1152(%rsp)
	movl	140(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1156(%rsp)
	movq	480(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1160(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movl	%eax, 1164(%rsp)
	movl	324(%rsp), %eax         # 4-byte Reload
	movl	%eax, 1168(%rsp)
	movq	664(%rsp), %rbx         # 8-byte Reload
	movq	%rbx, 1176(%rsp)
	movq	$0, 1184(%rsp)
	movq	544(%rsp), %rax         # 8-byte Reload
	movq	%rax, 1192(%rsp)
	movq	1368(%rsp), %rax
	movq	%rax, 1200(%rsp)
	movq	%r12, 1208(%rsp)
	movq	$0, 1216(%rsp)
	leaq	par_for___sharpi_sharpi.s0.v12(%rip), %rsi
	xorl	%ebp, %ebp
	leaq	1104(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB147_3
# BB#204:                               # %assert succeeded260
	testq	%rbx, %rbx
	je	.LBB147_206
# BB#205:                               # %if.then.i.303
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
.LBB147_206:                            # %call_destructor.exit304
	xorl	%esi, %esi
	movl	$0, %ebx
	movl	$0, %r15d
	testq	%r12, %r12
	je	.LBB147_191
# BB#207:                               # %if.then.i.274
	xorl	%edi, %edi
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB147_208:                            # %destructor_block.thread
	xorl	%esi, %esi
	xorl	%ebx, %ebx
	xorl	%r15d, %r15d
.LBB147_191:                            # %destructor_block.thread
	testl	%r15d, %r15d
	sete	%bpl
.LBB147_7:                              # %call_destructor.exit
	testb	%bpl, %bpl
	jne	.LBB147_10
# BB#8:                                 # %call_destructor.exit
	testq	%rsi, %rsi
	je	.LBB147_10
# BB#9:                                 # %if.then.i.266
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB147_10:                             # %call_destructor.exit267
	testq	%rbx, %rbx
	sete	%al
	orb	%al, %bpl
	jne	.LBB147_12
# BB#11:                                # %if.then.i.271
	xorl	%edi, %edi
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB147_12:                             # %call_destructor.exit272
	movl	%r15d, %eax
	addq	$1224, %rsp             # imm = 0x4C8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB147_1:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_2
.LBB147_14:                             # %assert failed10
	leaq	.Lstr.137(%rip), %rsi
	jmp	.LBB147_2
.LBB147_16:                             # %assert failed29
	leaq	.Lstr.138(%rip), %rsi
	jmp	.LBB147_2
.LBB147_18:                             # %assert failed48
	leaq	.Lstr.139(%rip), %rsi
	jmp	.LBB147_2
.LBB147_20:                             # %assert failed67
	leaq	.Lstr.140(%rip), %rsi
	jmp	.LBB147_2
.LBB147_22:                             # %assert failed86
	leaq	.Lstr.141(%rip), %rsi
.LBB147_2:                              # %destructor_block.thread
	xorl	%edi, %edi
	callq	halide_error_buffer_argument_is_null@PLT
	jmp	.LBB147_45
.LBB147_37:                             # %assert failed123
	leaq	.Lstr.142(%rip), %rsi
	leaq	.Lstr.143(%rip), %rdx
	xorl	%edi, %edi
	movl	$4, %r8d
	jmp	.LBB147_43
.LBB147_39:                             # %assert failed125
	leaq	.Lstr.144(%rip), %rsi
	leaq	.Lstr.145(%rip), %rdx
	xorl	%edi, %edi
	movl	$2, %r8d
	jmp	.LBB147_43
.LBB147_41:                             # %assert failed127
	leaq	.Lstr.146(%rip), %rsi
	jmp	.LBB147_42
.LBB147_47:                             # %assert failed129
	leaq	.Lstr.148(%rip), %rsi
.LBB147_42:                             # %destructor_block.thread
	leaq	.Lstr.147(%rip), %rdx
	xorl	%edi, %edi
	movl	$1, %r8d
.LBB147_43:                             # %destructor_block.thread
	movl	%ebp, %ecx
	jmp	.LBB147_44
.LBB147_49:                             # %assert failed131
	leaq	.Lstr.149(%rip), %rsi
	jmp	.LBB147_50
.LBB147_52:                             # %assert failed133
	leaq	.Lstr.150(%rip), %rsi
.LBB147_50:                             # %destructor_block.thread
	leaq	.Lstr.143(%rip), %rdx
	xorl	%edi, %edi
	movl	$4, %r8d
	movl	%ebx, %ecx
.LBB147_44:                             # %destructor_block.thread
	vzeroupper
	callq	halide_error_bad_elem_size@PLT
	jmp	.LBB147_45
.LBB147_55:                             # %assert failed135
	leal	-1(%rbp,%rdx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_56
.LBB147_59:                             # %assert failed137
	leaq	.Lstr.142(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	240(%rsp), %rcx         # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_62:                             # %assert failed139
	leal	-1(%rbp,%rdx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.142(%rip), %rsi
	xorl	%edi, %edi
	movq	%rdx, %r9
	movl	$1, %edx
	jmp	.LBB147_57
.LBB147_64:                             # %assert failed141
	movq	%rsi, %rcx
	leaq	.Lstr.142(%rip), %rsi
	jmp	.LBB147_65
.LBB147_69:                             # %assert failed143
	movl	316(%rsp), %r8d         # 4-byte Reload
	addl	$-1, %r8d
	movl	36(%rsp), %eax          # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.144(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	320(%rsp), %ecx         # 4-byte Reload
	jmp	.LBB147_70
.LBB147_73:                             # %assert failed145
	leaq	.Lstr.144(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r8d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_76:                             # %assert failed147
	movl	308(%rsp), %r8d         # 4-byte Reload
	addl	$-1, %r8d
	movl	32(%rsp), %eax          # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.144(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	312(%rsp), %ecx         # 4-byte Reload
.LBB147_70:                             # %destructor_block.thread
	movl	%r10d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_78:                             # %assert failed149
	leaq	.Lstr.144(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r13d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_80:                             # %assert failed151
	leaq	.Lstr.146(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	592(%rsp), %rcx         # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_83:                             # %assert failed153
	movl	136(%rsp), %eax         # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.146(%rip), %rsi
	xorl	%edi, %edi
	movq	%rdx, %r9
	movl	$1, %edx
	movl	332(%rsp), %ecx         # 4-byte Reload
	movl	336(%rsp), %r8d         # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_85:                             # %assert failed155
	leaq	.Lstr.146(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r15d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_88:                             # %assert failed157
	leal	-1(%rbp,%rbx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.146(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$2, %r8d
	movl	%ebx, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_90:                             # %assert failed159
	leaq	.Lstr.146(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	232(%rsp), %rcx         # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_93:                             # %assert failed161
	leal	-1(%rbp,%rdx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.148(%rip), %rsi
.LBB147_56:                             # %destructor_block.thread
	xorl	%edi, %edi
	movq	%rdx, %r9
	xorl	%edx, %edx
.LBB147_57:                             # %destructor_block.thread
	xorl	%ecx, %ecx
	movl	$2, %r8d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_95:                             # %assert failed163
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	184(%rsp), %rcx         # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_98:                             # %assert failed165
	leal	-1(%r9,%rdx), %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	movq	%rdx, %r9
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$4095, %r8d             # imm = 0xFFF
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_100:                            # %assert failed167
	leaq	.Lstr.148(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%r9d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_102:                            # %assert failed169
	movl	28(%rsp), %eax          # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	252(%rsp), %ecx         # 4-byte Reload
	movq	176(%rsp), %r9          # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_104:                            # %assert failed171
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r11d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_107:                            # %assert failed173
	movl	24(%rsp), %eax          # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.149(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	344(%rsp), %ecx         # 4-byte Reload
	movl	364(%rsp), %r8d         # 4-byte Reload
	movl	%r13d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_109:                            # %assert failed175
	leaq	.Lstr.149(%rip), %rsi
	movq	%r11, %rcx
.LBB147_65:                             # %destructor_block.thread
	xorl	%edi, %edi
	movl	$1, %edx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_111:                            # %assert failed177
	movl	340(%rsp), %eax         # 4-byte Reload
	movl	%eax, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	348(%rsp), %ecx         # 4-byte Reload
	movq	584(%rsp), %r9          # 8-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_113:                            # %assert failed179
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_115:                            # %assert failed181
	movl	20(%rsp), %edx          # 4-byte Reload
	movl	%edx, (%rsp)
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%eax, %r8d
	movl	%r14d, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB147_45
.LBB147_117:                            # %assert failed183
	leaq	.Lstr.150(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	%ebp, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB147_45
.LBB147_119:                            # %assert failed185
	movl	%esi, %edx
	leaq	.Lstr.151(%rip), %rsi
	jmp	.LBB147_137
.LBB147_121:                            # %assert failed187
	leaq	.Lstr.153(%rip), %rsi
	jmp	.LBB147_122
.LBB147_126:                            # %assert failed189
	leaq	.Lstr.154(%rip), %rsi
	leaq	.Lstr.155(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	jmp	.LBB147_123
.LBB147_128:                            # %assert failed191
	leaq	.Lstr.156(%rip), %rsi
	jmp	.LBB147_122
.LBB147_130:                            # %assert failed193
	leaq	.Lstr.157(%rip), %rsi
	leaq	.Lstr.158(%rip), %rcx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	movl	%r15d, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_45
.LBB147_132:                            # %assert failed195
	leaq	.Lstr.159(%rip), %rsi
	leaq	.Lstr.155(%rip), %rcx
	xorl	%edi, %edi
	movl	$3, %r8d
	movl	%eax, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_45
.LBB147_134:                            # %assert failed197
	leaq	.Lstr.160(%rip), %rsi
.LBB147_122:                            # %destructor_block.thread
	leaq	.Lstr.152(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
.LBB147_123:                            # %destructor_block.thread
	movl	%eax, %edx
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_45
.LBB147_136:                            # %assert failed199
	leaq	.Lstr.161(%rip), %rsi
	jmp	.LBB147_137
.LBB147_139:                            # %assert failed201
	leaq	.Lstr.162(%rip), %rsi
.LBB147_137:                            # %destructor_block.thread
	leaq	.Lstr.152(%rip), %rcx
	xorl	%edi, %edi
	movl	$1, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB147_45
.LBB147_141:                            # %assert failed205
	leaq	.Lstr.139(%rip), %rsi
	jmp	.LBB147_142
.LBB147_145:                            # %assert failed207
	leaq	.Lstr.139(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r10, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_45
.LBB147_147:                            # %assert failed211
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_148
.LBB147_150:                            # %assert failed213
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB147_151
.LBB147_154:                            # %assert failed215
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_142
.LBB147_156:                            # %assert failed217
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_148
.LBB147_158:                            # %assert failed219
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_151
.LBB147_160:                            # %assert failed221
	leaq	(%rdx,%rdx,2), %rdx
	leaq	.Lstr.141(%rip), %rsi
	jmp	.LBB147_151
.LBB147_162:                            # %assert failed225
	leaq	.Lstr.140(%rip), %rsi
.LBB147_142:                            # %destructor_block.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_45
.LBB147_164:                            # %assert failed227
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r9, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_45
.LBB147_166:                            # %assert failed231
	leaq	.Lstr.137(%rip), %rsi
	jmp	.LBB147_148
.LBB147_168:                            # %assert failed233
	leaq	.Lstr.137(%rip), %rsi
	jmp	.LBB147_151
.LBB147_170:                            # %assert failed237
	leaq	.Lstr.138(%rip), %rsi
.LBB147_148:                            # %destructor_block.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rax, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_45
.LBB147_172:                            # %assert failed239
	leaq	.Lstr.138(%rip), %rsi
.LBB147_151:                            # %destructor_block.thread
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB147_45
.LBB147_175:                            # %assert failed241
	leaq	.Lstr.163(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbp, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_45
.LBB147_180:
	xorl	%esi, %esi
	jmp	.LBB147_190
.LBB147_183:                            # %assert failed247
	leaq	.Lstr.179(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB147_187
.LBB147_194:                            # %assert failed253
	leaq	.Lstr.181(%rip), %rsi
	xorl	%edi, %edi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rbp, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	movl	%eax, %r15d
	jmp	.LBB147_189
.LBB147_200:
	movq	664(%rsp), %rbx         # 8-byte Reload
.LBB147_3:                              # %destructor_block
	movq	%rbx, %r14
	movq	%rbp, %rbx
	testl	%eax, %eax
	movl	%eax, %r15d
	sete	%bpl
	testq	%r12, %r12
	je	.LBB147_6
# BB#4:                                 # %destructor_block
	testl	%r15d, %r15d
	je	.LBB147_6
# BB#5:                                 # %if.then.i
	xorl	%ebp, %ebp
	xorl	%edi, %edi
	movq	%r12, %rsi
	callq	halide_free@PLT
.LBB147_6:                              # %call_destructor.exit
	movq	%rbx, %rsi
	movq	%r14, %rbx
	jmp	.LBB147_7
.LBB147_178:                            # %assert failed243
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_45:                             # %destructor_block.thread
	xorl	%esi, %esi
	movl	%eax, %r15d
	xorl	%ebx, %ebx
	jmp	.LBB147_191
.LBB147_186:                            # %assert failed249
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB147_187:                            # %destructor_block.thread
	xorl	%esi, %esi
	movl	%eax, %r15d
	jmp	.LBB147_190
.LBB147_197:                            # %assert failed255
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r15d
.LBB147_189:
	movq	632(%rsp), %rsi         # 8-byte Reload
.LBB147_190:                            # %destructor_block.thread
	movq	664(%rsp), %rbx         # 8-byte Reload
	jmp	.LBB147_191
.Lfunc_end147:
	.size	__sharpi, .Lfunc_end147-__sharpi

	.section	.rodata,"a",@progbits
	.align	32
.LCPI148_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
	.long	4294967288              # 0xfffffff8
	.long	4294967286              # 0xfffffff6
	.long	4294967284              # 0xfffffff4
	.long	4294967282              # 0xfffffff2
.LCPI148_1:
	.long	4294967280              # 0xfffffff0
	.long	4294967278              # 0xffffffee
	.long	4294967276              # 0xffffffec
	.long	4294967274              # 0xffffffea
	.long	4294967272              # 0xffffffe8
	.long	4294967270              # 0xffffffe6
	.long	4294967268              # 0xffffffe4
	.long	4294967266              # 0xffffffe2
.LCPI148_3:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
.LCPI148_6:
	.long	16                      # 0x10
	.long	18                      # 0x12
	.long	20                      # 0x14
	.long	22                      # 0x16
	.long	24                      # 0x18
	.long	26                      # 0x1a
	.long	28                      # 0x1c
	.long	30                      # 0x1e
.LCPI148_7:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	8                       # 0x8
	.long	10                      # 0xa
	.long	12                      # 0xc
	.long	14                      # 0xe
.LCPI148_8:
	.zero	4
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
.LCPI148_9:
	.long	4                       # 0x4
	.zero	4
	.long	5                       # 0x5
	.zero	4
	.long	6                       # 0x6
	.zero	4
	.long	7                       # 0x7
	.zero	4
.LCPI148_10:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI148_11:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
.LCPI148_13:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
.LCPI148_14:
	.byte	2                       # 0x2
	.byte	3                       # 0x3
	.byte	6                       # 0x6
	.byte	7                       # 0x7
	.byte	10                      # 0xa
	.byte	11                      # 0xb
	.byte	14                      # 0xe
	.byte	15                      # 0xf
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	18                      # 0x12
	.byte	19                      # 0x13
	.byte	22                      # 0x16
	.byte	23                      # 0x17
	.byte	26                      # 0x1a
	.byte	27                      # 0x1b
	.byte	30                      # 0x1e
	.byte	31                      # 0x1f
	.byte	16                      # 0x10
	.byte	17                      # 0x11
	.byte	20                      # 0x14
	.byte	21                      # 0x15
	.byte	24                      # 0x18
	.byte	25                      # 0x19
	.byte	28                      # 0x1c
	.byte	29                      # 0x1d
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI148_2:
	.long	1                       # 0x1
.LCPI148_12:
	.long	1166012416              # float 4095
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI148_4:
	.byte	0                       # 0x0
	.byte	2                       # 0x2
	.byte	4                       # 0x4
	.byte	6                       # 0x6
	.byte	8                       # 0x8
	.byte	10                      # 0xa
	.byte	12                      # 0xc
	.byte	14                      # 0xe
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
	.zero	1
.LCPI148_5:
	.zero	16,1
	.section	.text.par_for___sharpi_f0.s0.v11.v14,"ax",@progbits
	.align	16, 0x90
	.type	par_for___sharpi_f0.s0.v11.v14,@function
par_for___sharpi_f0.s0.v11.v14:         # @par_for___sharpi_f0.s0.v11.v14
# BB#0:                                 # %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$5824, %rsp             # imm = 0x16C0
	movq	%rdi, %r12
	movl	48(%rdx), %ebx
	movl	%ebx, 676(%rsp)         # 4-byte Spill
	movl	52(%rdx), %edi
	movq	%rdi, 992(%rsp)         # 8-byte Spill
	movl	96(%rdx), %eax
	shll	$5, %esi
	movq	%rsi, 2720(%rsp)        # 8-byte Spill
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	movq	%rcx, 1080(%rsp)        # 8-byte Spill
	leal	(%rcx,%rsi), %eax
	leal	-32(%rdi), %ecx
	cmpl	$1, %edi
	movl	$-31, %edi
	cmovgl	%ecx, %edi
	cmpl	%eax, %edi
	cmovgl	%eax, %edi
	movq	%rdi, 680(%rsp)         # 8-byte Spill
	movl	%ebx, %r15d
	andl	$-32, %r15d
	leal	64(%r15), %ecx
	movq	%rcx, %rdi
	shlq	$6, %rdi
	leaq	(%rdi,%rdi,8), %rax
	movl	%ecx, %esi
	shll	$6, %esi
	leal	(%rsi,%rsi,2), %esi
	cmpq	$2147483647, %rax       # imm = 0x7FFFFFFF
	ja	.LBB148_2
# BB#1:                                 # %entry
	movabsq	$68719474688, %rbx      # imm = 0xFFFFFF800
	andq	%rbx, %rdi
	leaq	(%rdi,%rdi,2), %rdi
	shrq	$32, %rdi
	shrq	$30, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	shlq	$4, %rcx
	addq	%rdi, %rcx
	leaq	(%rsi,%rsi,2), %rsi
	shrq	$32, %rsi
	leaq	(%rcx,%rcx,2), %rcx
	addq	%rsi, %rcx
	movabsq	$64424509440, %rsi      # imm = 0xF00000000
	andq	%rcx, %rsi
	jne	.LBB148_2
# BB#20:                                # %assert succeeded
	movl	(%rdx), %ecx
	movl	%ecx, 604(%rsp)         # 4-byte Spill
	movl	4(%rdx), %ecx
	movl	%ecx, 556(%rsp)         # 4-byte Spill
	movl	8(%rdx), %ecx
	movl	%ecx, 600(%rsp)         # 4-byte Spill
	movslq	12(%rdx), %rcx
	movq	%rcx, 768(%rsp)         # 8-byte Spill
	movslq	16(%rdx), %rcx
	movq	%rcx, 776(%rsp)         # 8-byte Spill
	movslq	20(%rdx), %rcx
	movq	%rcx, 1064(%rsp)        # 8-byte Spill
	vmovups	24(%rdx), %xmm0
	vmovaps	%xmm0, 576(%rsp)        # 16-byte Spill
	movl	40(%rdx), %ecx
	movl	%ecx, 552(%rsp)         # 4-byte Spill
	movl	44(%rdx), %ecx
	movl	%ecx, 596(%rsp)         # 4-byte Spill
	movslq	56(%rdx), %rcx
	movq	%rcx, 2776(%rsp)        # 8-byte Spill
	movslq	60(%rdx), %rcx
	movq	%rcx, 816(%rsp)         # 8-byte Spill
	movslq	64(%rdx), %rcx
	movq	%rcx, 976(%rsp)         # 8-byte Spill
	movslq	68(%rdx), %rcx
	movq	%rcx, 888(%rsp)         # 8-byte Spill
	movl	72(%rdx), %ecx
	movq	%rcx, 1048(%rsp)        # 8-byte Spill
	vmovups	112(%rdx), %xmm0
	vmovaps	%xmm0, 448(%rsp)        # 16-byte Spill
	vmovups	132(%rdx), %xmm0
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vmovups	148(%rdx), %xmm0
	vmovaps	%xmm0, 560(%rsp)        # 16-byte Spill
	vmovdqu	232(%rdx), %ymm0
	vmovdqa	%ymm0, 608(%rsp)        # 32-byte Spill
	movl	76(%rdx), %ecx
	movq	%rcx, 808(%rsp)         # 8-byte Spill
	movl	80(%rdx), %ecx
	movq	%rcx, 1056(%rsp)        # 8-byte Spill
	movslq	84(%rdx), %rcx
	movq	%rcx, 824(%rsp)         # 8-byte Spill
	movslq	88(%rdx), %rcx
	movq	%rcx, 960(%rsp)         # 8-byte Spill
	movl	92(%rdx), %ecx
	movl	%ecx, 2172(%rsp)        # 4-byte Spill
	movl	100(%rdx), %ecx
	movl	%ecx, 728(%rsp)         # 4-byte Spill
	movl	104(%rdx), %ecx
	movl	%ecx, 732(%rsp)         # 4-byte Spill
	movl	108(%rdx), %ecx
	movl	%ecx, 764(%rsp)         # 4-byte Spill
	movl	128(%rdx), %ecx
	movl	%ecx, 1660(%rsp)        # 4-byte Spill
	movq	168(%rdx), %rcx
	movq	%rcx, 376(%rsp)         # 8-byte Spill
	movq	184(%rdx), %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	movq	200(%rdx), %rcx
	movq	%rcx, 2552(%rsp)        # 8-byte Spill
	movq	216(%rdx), %r14
	orq	$4, %rax
	movq	%r12, %rdi
	movq	%rax, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB148_21
# BB#23:                                # %assert succeeded2
	movq	%rax, 2504(%rsp)        # 8-byte Spill
	leal	40(%r15), %ebx
	shlq	$8, %rbx
	movabsq	$1097364144128, %r13    # imm = 0xFF80000000
	testq	%r13, %rbx
	jne	.LBB148_24
# BB#25:                                # %assert succeeded4
	orq	$4, %rbx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 440(%rsp)         # 8-byte Spill
	movq	%r12, %rdi
	testq	%rax, %rax
	je	.LBB148_26
# BB#28:                                # %assert succeeded6
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB148_29
# BB#30:                                # %assert succeeded10
	movq	%rax, 488(%rsp)         # 8-byte Spill
	leal	48(%r15), %ebx
	shlq	$8, %rbx
	testq	%r13, %rbx
	jne	.LBB148_31
# BB#32:                                # %assert succeeded12
	orq	$4, %rbx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 480(%rsp)         # 8-byte Spill
	movq	%r12, %rdi
	testq	%rax, %rax
	je	.LBB148_33
# BB#35:                                # %assert succeeded14
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB148_36
# BB#37:                                # %assert succeeded18
	movq	%rax, 464(%rsp)         # 8-byte Spill
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	movq	%r15, 392(%rsp)         # 8-byte Spill
	leal	(%rcx,%r15), %ebx
	leal	33(%rbx), %eax
	movl	%r15d, %edx
	orl	$31, %edx
	addl	%ebx, %edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	leal	37(%rbx), %eax
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	subl	%ecx, %edx
	movq	%rdx, 520(%rsp)         # 8-byte Spill
	movq	%rcx, %r15
	leal	3(%rdx), %eax
	shlq	$8, %rax
	testq	%r13, %rax
	jne	.LBB148_38
# BB#39:                                # %assert succeeded20
	orq	$4, %rax
	movq	%r12, %rdi
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB148_40
# BB#43:                                # %assert succeeded22
	movq	%rax, 752(%rsp)         # 8-byte Spill
	movl	676(%rsp), %eax         # 4-byte Reload
	sarl	$31, %eax
	movq	%r15, %rdx
	movq	%rdx, 968(%rsp)         # 8-byte Spill
	movl	%edx, %ecx
	andl	%eax, %ecx
	andnl	%ebx, %eax, %r15d
	orl	%ecx, %r15d
	movq	392(%rsp), %rax         # 8-byte Reload
	subl	%edx, %eax
	movq	%rax, 472(%rsp)         # 8-byte Spill
	leal	32(%rax,%r15), %ebx
	shlq	$8, %rbx
	testq	%r13, %rbx
	jne	.LBB148_44
# BB#45:                                # %assert succeeded24
	orq	$4, %rbx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 744(%rsp)         # 8-byte Spill
	movq	%r12, %rdi
	testq	%rax, %rax
	je	.LBB148_46
# BB#49:                                # %assert succeeded26
	movq	%r12, 664(%rsp)         # 8-byte Spill
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 736(%rsp)         # 8-byte Spill
	testq	%rax, %rax
	je	.LBB148_50
# BB#51:                                # %assert succeeded30
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, %ecx
	sarl	$5, %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	movq	472(%rsp), %rax         # 8-byte Reload
	addl	%r15d, %eax
	movq	%rax, 472(%rsp)         # 8-byte Spill
	movq	392(%rsp), %rax         # 8-byte Reload
	leal	79(%rax), %eax
	sarl	$5, %eax
	movl	%eax, 956(%rsp)         # 4-byte Spill
	movq	808(%rsp), %r15         # 8-byte Reload
	leal	(%r15,%r15), %eax
	leal	-2(%r15,%r15), %edx
	movl	%edx, 804(%rsp)         # 4-byte Spill
	movl	$2, %esi
	subl	%eax, %esi
	cmpl	$1, %eax
	cmovgl	%edx, %esi
	movl	%esi, 800(%rsp)         # 4-byte Spill
	movq	960(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edi
	movq	824(%rsp), %r9          # 8-byte Reload
	imull	%r9d, %edi
	movl	2172(%rsp), %r11d       # 4-byte Reload
	movslq	%r11d, %rdx
	movq	1056(%rsp), %r8         # 8-byte Reload
	addl	%r8d, %edi
	leal	8(%r8), %eax
	movl	%eax, 2688(%rsp)        # 4-byte Spill
	leal	7(%r8), %r13d
	movslq	%ecx, %rax
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	$64, %rcx
	movq	%rcx, 944(%rsp)         # 8-byte Spill
	movq	680(%rsp), %rcx         # 8-byte Reload
	movslq	%ecx, %rbx
	movq	%rbx, 720(%rsp)         # 8-byte Spill
	movl	$8, %esi
	subq	%rbx, %rsi
	movq	%rsi, 936(%rsp)         # 8-byte Spill
	movq	%rdx, %rsi
	sarq	$63, %rsi
	andq	%rdx, %rsi
	movq	%rsi, 2456(%rsp)        # 8-byte Spill
	movq	%rax, %rdx
	shlq	$9, %rdx
	shlq	$10, %rax
	movq	1080(%rsp), %rsi        # 8-byte Reload
	notl	%esi
	movq	2720(%rsp), %rbx        # 8-byte Reload
	subl	%ebx, %esi
	movq	%rsi, 1080(%rsp)        # 8-byte Spill
	movl	$31, %r10d
	movq	992(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %r10d
	cmpl	$1, %ebx
	vmovd	%edi, %xmm0
	vmovaps	%ymm0, 896(%rsp)        # 32-byte Spill
	movq	1048(%rsp), %r12        # 8-byte Reload
	leal	8(%r8,%r12), %edi
	vmovd	%edi, %xmm0
	leal	-1(%r9,%r15), %edi
	vpbroadcastd	%xmm0, %ymm2
	vmovdqa	.LCPI148_0(%rip), %ymm0 # ymm0 = [0,4294967294,4294967292,4294967290,4294967288,4294967286,4294967284,4294967282]
	vpaddd	%ymm0, %ymm2, %ymm1
	vmovdqa	%ymm1, 2304(%rsp)       # 32-byte Spill
	vmovdqa	.LCPI148_1(%rip), %ymm1 # ymm1 = [4294967280,4294967278,4294967276,4294967274,4294967272,4294967270,4294967268,4294967266]
	vpaddd	%ymm1, %ymm2, %ymm2
	vmovdqa	%ymm2, 2272(%rsp)       # 32-byte Spill
	movl	2688(%rsp), %ebx        # 4-byte Reload
	vmovd	%ebx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vmovdqa	%ymm3, 2112(%rsp)       # 32-byte Spill
	vpaddd	%ymm1, %ymm2, %ymm2
	vmovdqa	%ymm2, 2080(%rsp)       # 32-byte Spill
	vmovd	%r8d, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vmovd	%r12d, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	leal	-2(%r12,%r12), %ebx
	vmovd	%ebx, %xmm4
	vmovd	%ebx, %xmm5
	vbroadcastss	%xmm5, %ymm5
	vmovaps	%ymm5, 2336(%rsp)       # 32-byte Spill
	vbroadcastss	%xmm4, %xmm4
	vmovaps	%xmm4, 1120(%rsp)       # 16-byte Spill
	vpbroadcastd	.LCPI148_2(%rip), %ymm4
	vpsubd	%ymm3, %ymm4, %ymm4
	vmovdqa	%ymm4, 2464(%rsp)       # 32-byte Spill
	vpaddd	%ymm2, %ymm3, %ymm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpaddd	%ymm3, %ymm2, %ymm2
	vmovdqa	%ymm2, 2400(%rsp)       # 32-byte Spill
	leal	-1(%r8,%r12), %ebx
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 2528(%rsp)       # 16-byte Spill
	vmovd	%r8d, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 2512(%rsp)       # 16-byte Spill
	leal	7(%r8,%r12), %ebx
	vmovd	%ebx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vmovdqa	%ymm3, 2240(%rsp)       # 32-byte Spill
	vpaddd	%ymm1, %ymm2, %ymm2
	vmovdqa	%ymm2, 2208(%rsp)       # 32-byte Spill
	vmovd	%r13d, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm0
	vmovdqa	%ymm0, 2048(%rsp)       # 32-byte Spill
	vpaddd	%ymm1, %ymm2, %ymm0
	vmovdqa	%ymm0, 2016(%rsp)       # 32-byte Spill
	leaq	(%rdx,%rdx,2), %rdx
	movq	%rdx, 2392(%rsp)        # 8-byte Spill
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 2384(%rsp)        # 8-byte Spill
	movl	$30, %edx
	cmovgl	%r10d, %edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movq	%rdx, 984(%rsp)         # 8-byte Spill
	movl	$-9, %eax
	subl	%edx, %eax
	movslq	%eax, %rbx
	movslq	%edi, %rax
	movq	%rax, 792(%rsp)         # 8-byte Spill
	movl	$-7, %eax
	subl	%r8d, %eax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	movl	$-8, %eax
	subl	%r8d, %eax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	leal	9(%rcx), %eax
	movl	%eax, 1088(%rsp)        # 4-byte Spill
	leal	(%r9,%r15), %eax
	movq	%rbx, %r9
	movl	%eax, 372(%rsp)         # 4-byte Spill
	movl	956(%rsp), %r10d        # 4-byte Reload
	leal	(%r8,%r12), %ecx
	movq	%rcx, 1072(%rsp)        # 8-byte Spill
	cltq
	movq	%rax, 784(%rsp)         # 8-byte Spill
	.align	16, 0x90
.LBB148_52:                             # %for deinterleaved$1.s0.v11
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB148_54 Depth 2
	movq	%r9, 1264(%rsp)         # 8-byte Spill
	testl	%r10d, %r10d
	jle	.LBB148_85
# BB#53:                                # %for deinterleaved$1.s0.v10.v10.preheader
                                        #   in Loop: Header=BB148_52 Depth=1
	movl	%r9d, %eax
	movq	824(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	804(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	800(%rsp), %eax         # 4-byte Folded Reload
	movq	808(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movq	792(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %eax
	subl	%ecx, %eax
	cmpq	%r9, %rdx
	movl	%edx, %ecx
	cmovgl	%r9d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%r9, 784(%rsp)          # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %r9
	cmovll	%eax, %ecx
	movl	%r9d, %r8d
	andl	$1, %r8d
	movl	%r8d, 1952(%rsp)        # 4-byte Spill
	movq	960(%rsp), %rax         # 8-byte Reload
	imull	%eax, %ecx
	vmovd	%ecx, %xmm1
	vpabsd	1120(%rsp), %xmm0       # 16-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm0, %ymm0
	vmovdqa	%ymm0, 1216(%rsp)       # 32-byte Spill
	vpsubd	896(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpbroadcastd	%xmm1, %ymm0
	vmovdqa	%ymm0, 1920(%rsp)       # 32-byte Spill
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r9), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1208(%rsp)        # 8-byte Spill
	movl	%r10d, %ecx
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r10d
	.align	16, 0x90
.LBB148_54:                             # %for deinterleaved$1.s0.v10.v10
                                        #   Parent Loop BB148_52 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r10, 2720(%rsp)        # 8-byte Spill
	movl	%ecx, 1888(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	1856(%rsp)              # 1-byte Folded Spill
	setne	1824(%rsp)              # 1-byte Folded Spill
	movq	1152(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vmovdqa	.LCPI148_7(%rip), %ymm11 # ymm11 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm11, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vpextrd	$1, %xmm4, %eax
	vmovdqa	2336(%rsp), %ymm0       # 32-byte Reload
	vextracti128	$1, %ymm0, %xmm5
	vpextrd	$1, %xmm5, %r8d
	movl	%r8d, 1488(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	vmovd	%xmm4, %eax
	vmovd	%xmm5, %ebx
	movl	%ebx, 1512(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm5, %ecx
	movl	%ecx, 2176(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm5, %edi
	movl	%edi, 1536(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edx, %r13d
	vpextrd	$1, %xmm3, %eax
	vpextrd	$1, %xmm0, %esi
	movl	%esi, 1448(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vmovd	%r15d, %xmm4
	vmovd	%xmm3, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1408(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpinsrd	$1, %r9d, %xmm4, %xmm4
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1400(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r12d
	vpinsrd	$2, %r10d, %xmm4, %xmm5
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm0, %esi
	movl	%esi, 1392(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovdqa	.LCPI148_6(%rip), %ymm15 # ymm15 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm15, %ymm2, %ymm4
	vextracti128	$1, %ymm4, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$3, %r13d, %xmm5, %xmm9
	vmovd	%r15d, %xmm3
	vmovd	%xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpinsrd	$1, %r11d, %xmm3, %xmm3
	vpinsrd	$2, %r12d, %xmm3, %xmm14
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%ebx, %xmm5
	vpinsrd	$1, %esi, %xmm5, %xmm5
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpinsrd	$2, %ecx, %xmm5, %xmm5
	vpextrd	$1, %xmm4, %eax
	vpextrd	$1, %xmm0, %ecx
	movl	%ecx, 2560(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, %esi, %xmm5, %xmm6
	vmovd	%xmm4, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1984(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	vmovd	%edx, %xmm5
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1528(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm0, %ecx
	movl	%ecx, 1480(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vpinsrd	$2, %esi, %xmm5, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm7
	movq	2720(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm0
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %ymm5
	vmovdqa	2304(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm1, %ymm4
	vmovdqa	.LCPI148_3(%rip), %ymm1 # ymm1 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vmovdqa	%ymm1, %ymm10
	vpshufb	%ymm10, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2272(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm1, %ymm8
	vpshufb	%ymm10, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm1 # xmm1 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vmovdqa	%xmm1, %xmm3
	vpshufb	%xmm3, %xmm8, %xmm1
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm1, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm1[0]
	vmovdqa	2112(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm2, %ymm4
	vpshufb	%ymm10, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2080(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm2, %ymm8
	vpshufb	%ymm10, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm3, %xmm8, %xmm2
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm2, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm13
	vinserti128	$1, %xmm6, %ymm7, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vmovdqa	1216(%rsp), %ymm8       # 32-byte Reload
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	2464(%rsp), %ymm12      # 32-byte Reload
	vpaddd	%ymm1, %ymm12, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	2400(%rsp), %ymm10      # 32-byte Reload
	vpsubd	%ymm1, %ymm10, %ymm1
	vpbroadcastd	%xmm0, %ymm6
	vpaddd	%ymm15, %ymm6, %ymm0
	vmovdqa	2528(%rsp), %xmm7       # 16-byte Reload
	vpminsd	%xmm7, %xmm0, %xmm2
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm7, %xmm0, %xmm0
	vmovdqa	2512(%rsp), %xmm4       # 16-byte Reload
	vpmaxsd	%xmm4, %xmm2, %xmm2
	vpmaxsd	%xmm4, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vpunpckhbw	%xmm13, %xmm13, %xmm2 # xmm2 = xmm13[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm0, %ymm0
	vpinsrd	$3, %r9d, %xmm14, %xmm1
	vmovdqa	1920(%rsp), %ymm14      # 32-byte Reload
	vpaddd	%ymm0, %ymm14, %ymm3
	vmovq	%xmm3, %r13
	movslq	%r13d, %rax
	movq	%rax, 1728(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rcx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vinserti128	$1, %xmm9, %ymm1, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	%ymm8, %ymm9
	vpaddd	%ymm1, %ymm12, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm11, %ymm6, %ymm2
	vpminsd	%xmm7, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm4, %xmm6, %xmm6
	vpmaxsd	%xmm4, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm10, %ymm1
	vpmovzxbd	%xmm13, %ymm4   # ymm4 = xmm13[0],zero,zero,zero,xmm13[1],zero,zero,zero,xmm13[2],zero,zero,zero,xmm13[3],zero,zero,zero,xmm13[4],zero,zero,zero,xmm13[5],zero,zero,zero,xmm13[6],zero,zero,zero,xmm13[7],zero,zero,zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm2, %ymm1
	vpaddd	%ymm1, %ymm14, %ymm1
	vmovq	%xmm1, %rbx
	movslq	%ebx, %rax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vpextrq	$1, %xmm1, %rsi
	movslq	%esi, %rax
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rdi
	movslq	%edi, %rax
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpextrq	$1, %xmm1, %rdx
	movslq	%edx, %rax
	movq	%rax, 2688(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	sarq	$32, %r13
	movq	%r13, 1792(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm3, %r10
	movslq	%r10d, %r11
	movq	%r11, 1464(%rsp)        # 8-byte Spill
	sarq	$32, %r10
	movq	%r10, 1504(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm3, %xmm1
	vmovq	%xmm1, %r12
	movslq	%r12d, %r8
	movq	%r8, 1456(%rsp)         # 8-byte Spill
	sarq	$32, %r12
	movq	%r12, 1496(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r15
	movslq	%r15d, %r9
	movq	%r9, 1472(%rsp)         # 8-byte Spill
	sarq	$32, %r15
	movq	%r15, 1520(%rsp)        # 8-byte Spill
	movq	2720(%rsp), %rax        # 8-byte Reload
	andl	$1, %eax
	movl	%eax, 1760(%rsp)        # 4-byte Spill
	sete	%al
	andb	1824(%rsp), %al         # 1-byte Folded Reload
	movb	%al, 1824(%rsp)         # 1-byte Spill
	vpinsrw	$1, (%rcx,%r13,2), %xmm0, %xmm0
	vpinsrw	$2, (%rcx,%r11,2), %xmm0, %xmm0
	vpinsrw	$3, (%rcx,%r10,2), %xmm0, %xmm0
	vpinsrw	$4, (%rcx,%r8,2), %xmm0, %xmm0
	vpinsrw	$5, (%rcx,%r12,2), %xmm0, %xmm0
	vpinsrw	$6, (%rcx,%r9,2), %xmm0, %xmm0
	vpinsrw	$7, (%rcx,%r15,2), %xmm0, %xmm4
	jne	.LBB148_55
# BB#56:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vpxor	%ymm3, %ymm3, %ymm3
	movq	2656(%rsp), %r9         # 8-byte Reload
	movq	2592(%rsp), %rcx        # 8-byte Reload
	movq	2688(%rsp), %r8         # 8-byte Reload
	movq	2624(%rsp), %rax        # 8-byte Reload
	jmp	.LBB148_57
	.align	16, 0x90
.LBB148_55:                             #   in Loop: Header=BB148_54 Depth=2
	movq	2552(%rsp), %rcx        # 8-byte Reload
	movq	2592(%rsp), %r8         # 8-byte Reload
	movzwl	(%rcx,%r8,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rcx,%rbx,2), %xmm0, %xmm0
	movq	2656(%rsp), %r9         # 8-byte Reload
	vpinsrw	$2, (%rcx,%r9,2), %xmm0, %xmm0
	vpinsrw	$3, (%rcx,%rsi,2), %xmm0, %xmm0
	movq	2624(%rsp), %r10        # 8-byte Reload
	vpinsrw	$4, (%rcx,%r10,2), %xmm0, %xmm0
	vpinsrw	$5, (%rcx,%rdi,2), %xmm0, %xmm0
	movq	2688(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rcx,%rax,2), %xmm0, %xmm0
	vpinsrw	$7, (%rcx,%rdx,2), %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm3
	movq	%r8, %rcx
	movq	%rax, %r8
	movq	%r10, %rax
.LBB148_57:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	jne	.LBB148_58
# BB#59:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movq	%rcx, 2592(%rsp)        # 8-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 2656(%rsp)         # 8-byte Spill
	movq	%r8, 2688(%rsp)         # 8-byte Spill
	movq	%rbx, 1568(%rsp)        # 8-byte Spill
	movq	%rdi, 1600(%rsp)        # 8-byte Spill
	movq	%rsi, 1664(%rsp)        # 8-byte Spill
	movq	%rdx, 1696(%rsp)        # 8-byte Spill
	vpxor	%ymm4, %ymm4, %ymm4
	jmp	.LBB148_60
	.align	16, 0x90
.LBB148_58:                             #   in Loop: Header=BB148_54 Depth=2
	movq	%rcx, 2592(%rsp)        # 8-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 2656(%rsp)         # 8-byte Spill
	movq	%r8, 2688(%rsp)         # 8-byte Spill
	movq	%rbx, 1568(%rsp)        # 8-byte Spill
	movq	%rdi, 1600(%rsp)        # 8-byte Spill
	movq	%rsi, 1664(%rsp)        # 8-byte Spill
	movq	%rdx, 1696(%rsp)        # 8-byte Spill
	vpmovzxwd	%xmm4, %ymm0    # ymm0 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm0, %ymm4
.LBB148_60:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movq	1200(%rsp), %rax        # 8-byte Reload
	movq	2720(%rsp), %r13        # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm6
	vmovdqa	%ymm11, %ymm14
	vpaddd	%ymm14, %ymm6, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	1488(%rsp), %r12d       # 4-byte Reload
	idivl	%r12d
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	1512(%rsp), %r10d       # 4-byte Reload
	idivl	%r10d
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	2176(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	movl	1536(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %r15d
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	1448(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm7, %eax
	cltd
	idivl	1408(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	1400(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	1392(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vmovdqa	%ymm15, %ymm13
	vpaddd	%ymm13, %ymm6, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 1488(%rsp)        # 4-byte Spill
	vmovd	%r11d, %xmm1
	vpinsrd	$1, 1312(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	2240(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm2, %ymm7
	vmovdqa	.LCPI148_3(%rip), %ymm11 # ymm11 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm11, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2208(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm2, %ymm8
	vpshufb	%ymm11, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm12 # xmm12 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm12, %xmm8, %xmm2
	vpshufb	%xmm12, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm2, %xmm2
	vmovdqa	2048(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm7, %ymm7
	vpshufb	%ymm11, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2016(%rsp), %ymm8       # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm8, %ymm5
	vpshufb	%ymm11, %ymm5, %ymm5
	vpermq	$232, %ymm5, %ymm5      # ymm5 = ymm5[0,2,2,3]
	vpshufb	%xmm12, %xmm5, %xmm5
	vpshufb	%xmm12, %xmm7, %xmm7
	vpunpcklqdq	%xmm5, %xmm7, %xmm5 # xmm5 = xmm7[0],xmm5[0]
	vpor	%xmm2, %xmm5, %xmm5
	vpinsrd	$2, 1344(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r15d, %xmm1, %xmm1
	vmovd	%xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r10d
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %ebx, %xmm2, %xmm2
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ecx
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vpsrad	$31, %ymm0, %ymm1
	vmovdqa	%ymm9, %ymm8
	vpand	%ymm8, %ymm1, %ymm1
	vmovdqa	2464(%rsp), %ymm9       # 32-byte Reload
	vpaddd	%ymm0, %ymm9, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm1
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpsubd	%ymm0, %ymm10, %ymm0
	leal	-7(%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm7
	vpaddd	%ymm14, %ymm7, %ymm1
	vmovdqa	2528(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vmovdqa	2512(%rsp), %xmm15      # 16-byte Reload
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	2560(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vmovd	%r10d, %xmm1
	vpinsrd	$1, 1488(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%xmm6, %eax
	cltd
	idivl	1984(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vpinsrd	$2, %esi, %xmm1, %xmm1
	vpinsrd	$3, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	1528(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vpextrd	$3, %xmm6, %eax
	vmovd	%ebx, %xmm2
	vpinsrd	$1, %edi, %xmm2, %xmm2
	cltd
	idivl	1480(%rsp)              # 4-byte Folded Reload
	vpinsrd	$2, %ecx, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm8, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm13, %ymm7, %ymm2
	vpminsd	%xmm14, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm10, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm5 # xmm5 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm1, %ymm2, %ymm1
	vmovdqa	1920(%rsp), %ymm2       # 32-byte Reload
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm0, %ymm2, %ymm0
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdx
	vextracti128	$1, %ymm0, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rbx
	vextracti128	$1, %ymm1, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %rdi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%esi, %r15
	sarq	$32, %rsi
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%eax, %r13
	sarq	$32, %rax
	vpextrq	$1, %xmm1, 1536(%rsp)   # 8-byte Folded Spill
	movq	%rdi, %r11
	sarq	$32, %r11
	movslq	%r9d, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	movl	1952(%rsp), %r8d        # 4-byte Reload
	movq	2720(%rsp), %rcx        # 8-byte Reload
	andl	%ecx, %r8d
	setne	1512(%rsp)              # 1-byte Folded Spill
	vmovq	%xmm1, 2176(%rsp)       # 8-byte Folded Spill
	jne	.LBB148_61
# BB#62:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movq	%r10, 1312(%rsp)        # 8-byte Spill
	movq	%r15, 1344(%rsp)        # 8-byte Spill
	movq	%r12, 1392(%rsp)        # 8-byte Spill
	movq	%r13, 1400(%rsp)        # 8-byte Spill
	movq	%rbx, 1408(%rsp)        # 8-byte Spill
	movq	%rdx, 1448(%rsp)        # 8-byte Spill
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	movq	%rsi, 1488(%rsp)        # 8-byte Spill
	vxorps	%ymm5, %ymm5, %ymm5
	jmp	.LBB148_63
	.align	16, 0x90
.LBB148_61:                             #   in Loop: Header=BB148_54 Depth=2
	movq	%rdi, 1984(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rdi,%r10,2), %ecx
	movq	%r10, 1312(%rsp)        # 8-byte Spill
	vmovd	%ecx, %xmm0
	vpinsrw	$1, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	%rdx, 1448(%rsp)        # 8-byte Spill
	vpinsrw	$2, (%rdi,%r15,2), %xmm0, %xmm0
	movq	%r15, 1344(%rsp)        # 8-byte Spill
	vpinsrw	$3, (%rdi,%rsi,2), %xmm0, %xmm0
	movq	%rsi, 1488(%rsp)        # 8-byte Spill
	vpinsrw	$4, (%rdi,%r12,2), %xmm0, %xmm0
	movq	%r12, 1392(%rsp)        # 8-byte Spill
	vpinsrw	$5, (%rdi,%rbx,2), %xmm0, %xmm0
	movq	%rbx, 1408(%rsp)        # 8-byte Spill
	vpinsrw	$6, (%rdi,%r13,2), %xmm0, %xmm0
	movq	%r13, 1400(%rsp)        # 8-byte Spill
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movq	1984(%rsp), %rdi        # 8-byte Reload
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm5
.LBB148_63:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movq	2504(%rsp), %r12        # 8-byte Reload
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm12 # ymm12 = <4,u,5,u,6,u,7,u>
	vmovaps	.LCPI148_10(%rip), %ymm13 # ymm13 = <u,0,u,1,u,2,u,3>
	movq	2176(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 1984(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	1536(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rax
	movq	%rax, 2176(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	movslq	%edi, %r10
	movq	2552(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%r11,2), %r13d
	movq	2560(%rsp), %rax        # 8-byte Reload
	movzwl	(%rsi,%rax,2), %edx
	movzwl	(%rsi,%r9,2), %r15d
	movq	%rsi, %rdi
	testl	%r8d, %r8d
	jne	.LBB148_64
# BB#65:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movl	%r15d, 1528(%rsp)       # 4-byte Spill
	movl	%r13d, 1536(%rsp)       # 4-byte Spill
	movq	%r11, 1272(%rsp)        # 8-byte Spill
	movq	%r9, 1280(%rsp)         # 8-byte Spill
	vpxor	%ymm6, %ymm6, %ymm6
	jmp	.LBB148_66
	.align	16, 0x90
.LBB148_64:                             #   in Loop: Header=BB148_54 Depth=2
	movq	%r11, 1272(%rsp)        # 8-byte Spill
	movq	%r9, 1280(%rsp)         # 8-byte Spill
	movq	1984(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdi,%rcx,2), %xmm0, %xmm0
	movq	2176(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdi,%rbx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rdi,%r10,2), %xmm0, %xmm0
	vpinsrw	$5, %r13d, %xmm0, %xmm0
	movl	%r13d, 1536(%rsp)       # 4-byte Spill
	vpinsrw	$6, %edx, %xmm0, %xmm0
	vpinsrw	$7, %r15d, %xmm0, %xmm0
	movl	%r15d, 1528(%rsp)       # 4-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm6
.LBB148_66:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movl	%edx, %r13d
	movq	1400(%rsp), %r8         # 8-byte Reload
	movq	1392(%rsp), %r9         # 8-byte Reload
	movq	1344(%rsp), %rdx        # 8-byte Reload
	movq	1312(%rsp), %rax        # 8-byte Reload
	movq	%r10, %r11
	movq	%rdi, 2552(%rsp)        # 8-byte Spill
	vpermps	%ymm6, %ymm11, %ymm0
	vpermps	%ymm4, %ymm12, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm6, %ymm13, %ymm1
	vmovaps	.LCPI148_11(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vmovaps	%ymm2, %ymm6
	vpermps	%ymm4, %ymm6, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm5, %ymm11, %ymm2
	vpermps	%ymm3, %ymm12, %ymm4
	vblendps	$170, %ymm2, %ymm4, %ymm2 # ymm2 = ymm4[0],ymm2[1],ymm4[2],ymm2[3],ymm4[4],ymm2[5],ymm4[6],ymm2[7]
	vpermps	%ymm5, %ymm13, %ymm4
	vpermps	%ymm3, %ymm6, %ymm3
	vmovaps	%ymm6, %ymm14
	vblendps	$170, %ymm4, %ymm3, %ymm3 # ymm3 = ymm3[0],ymm4[1],ymm3[2],ymm4[3],ymm3[4],ymm4[5],ymm3[6],ymm4[7]
	movq	2720(%rsp), %r10        # 8-byte Reload
	movslq	%r10d, %rsi
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	addq	1208(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, (%r12,%rsi,4)
	vmovups	%ymm2, 32(%r12,%rsi,4)
	vmovups	%ymm1, 64(%r12,%rsi,4)
	vmovups	%ymm0, 96(%r12,%rsi,4)
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm0
	movq	1448(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$2, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	1488(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$4, (%rdi,%r9,2), %xmm0, %xmm0
	movq	1408(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdi,%r8,2), %xmm0, %xmm0
	movq	1480(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movq	1984(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	vpinsrw	$1, (%rdi,%rcx,2), %xmm1, %xmm1
	movq	2176(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	vpinsrw	$3, (%rdi,%rbx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdi,%r11,2), %xmm1, %xmm3
	movq	1728(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1792(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1464(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1504(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1456(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1496(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1472(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1520(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm5
	movq	2592(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1568(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2656(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1664(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1600(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2688(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1696(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm1
	movl	%r10d, %edx
	movq	1264(%rsp), %r9         # 8-byte Reload
	orl	%r9d, %edx
	andl	$1, %edx
	sete	%al
	vpmovzxwd	%xmm1, %ymm8    # ymm8 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm7
	vmovaps	%ymm7, %ymm4
	je	.LBB148_68
# BB#67:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_68:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vpmovzxwd	%xmm5, %ymm6    # ymm6 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm8, %ymm5
	orb	1512(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%ymm5, %ymm8
	movl	2172(%rsp), %r11d       # 4-byte Reload
	movl	1952(%rsp), %r8d        # 4-byte Reload
	movl	1528(%rsp), %edi        # 4-byte Reload
	jne	.LBB148_70
# BB#69:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm8, %ymm8, %ymm8
.LBB148_70:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vcvtdq2ps	%ymm6, %ymm6
	vmovaps	%ymm6, %ymm9
	movb	1856(%rsp), %al         # 1-byte Reload
	jne	.LBB148_72
# BB#71:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm9, %ymm9, %ymm9
.LBB148_72:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movl	1760(%rsp), %ebx        # 4-byte Reload
	andb	%bl, %al
	jne	.LBB148_74
# BB#73:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_74:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	jne	.LBB148_76
# BB#75:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_76:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	orb	1824(%rsp), %al         # 1-byte Folded Reload
	jne	.LBB148_78
# BB#77:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_78:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movl	1536(%rsp), %eax        # 4-byte Reload
	jne	.LBB148_79
# BB#80:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vpxor	%ymm10, %ymm10, %ymm10
	jmp	.LBB148_81
	.align	16, 0x90
.LBB148_79:                             #   in Loop: Header=BB148_54 Depth=2
	vpinsrw	$5, %eax, %xmm3, %xmm0
	vpinsrw	$6, %r13d, %xmm0, %xmm0
	vpinsrw	$7, %edi, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm10
.LBB148_81:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	movq	2392(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vpermps	%ymm10, %ymm11, %ymm0
	vpermps	%ymm9, %ymm12, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm10, %ymm13, %ymm1
	vpermps	%ymm9, %ymm14, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm7, %ymm11, %ymm2
	vpermps	%ymm8, %ymm12, %ymm9
	vblendps	$170, %ymm2, %ymm9, %ymm2 # ymm2 = ymm9[0],ymm2[1],ymm9[2],ymm2[3],ymm9[4],ymm2[5],ymm9[6],ymm2[7]
	vpermps	%ymm7, %ymm13, %ymm7
	vpermps	%ymm8, %ymm14, %ymm8
	vmovaps	%ymm14, %ymm9
	vblendps	$170, %ymm7, %ymm8, %ymm7 # ymm7 = ymm8[0],ymm7[1],ymm8[2],ymm7[3],ymm8[4],ymm7[5],ymm8[6],ymm7[7]
	vmovups	%ymm7, 12288(%r12,%rax,4)
	vmovups	%ymm2, 12320(%r12,%rax,4)
	vmovups	%ymm1, 12352(%r12,%rax,4)
	vmovups	%ymm0, 12384(%r12,%rax,4)
	testl	%edx, %edx
	je	.LBB148_82
# BB#83:                                # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vxorps	%ymm2, %ymm2, %ymm2
	jmp	.LBB148_84
	.align	16, 0x90
.LBB148_82:                             #   in Loop: Header=BB148_54 Depth=2
	movq	2552(%rsp), %rax        # 8-byte Reload
	movq	1272(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rax,%rdx,2), %xmm3, %xmm0
	movq	2560(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rax,%rdx,2), %xmm0, %xmm0
	movq	1280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rax,%rdx,2), %eax
	vpinsrw	$7, %eax, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm2
.LBB148_84:                             # %for deinterleaved$1.s0.v10.v10
                                        #   in Loop: Header=BB148_54 Depth=2
	vpermps	%ymm6, %ymm12, %ymm0
	vpermps	%ymm2, %ymm11, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	vpermps	%ymm6, %ymm9, %ymm1
	vpermps	%ymm2, %ymm13, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm4, %ymm11, %ymm2
	vpermps	%ymm5, %ymm12, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vpermps	%ymm4, %ymm13, %ymm3
	vpermps	%ymm5, %ymm9, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	addq	2384(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, 24576(%r12,%rsi,4)
	vmovups	%ymm2, 24608(%r12,%rsi,4)
	vmovups	%ymm1, 24640(%r12,%rsi,4)
	vmovups	%ymm0, 24672(%r12,%rsi,4)
	movq	%r12, 2504(%rsp)        # 8-byte Spill
	addl	$32, %r10d
	movl	1888(%rsp), %ecx        # 4-byte Reload
	addl	$-1, %ecx
	jne	.LBB148_54
.LBB148_85:                             #   in Loop: Header=BB148_52 Depth=1
	movl	%r9d, %eax
	addq	$1, %r9
	cmpl	1088(%rsp), %eax        # 4-byte Folded Reload
	movl	676(%rsp), %ecx         # 4-byte Reload
	movq	680(%rsp), %r12         # 8-byte Reload
	movl	956(%rsp), %r10d        # 4-byte Reload
	jne	.LBB148_52
# BB#86:                                # %produce gH
	leal	-2(%r12), %edx
	movl	%edx, 2592(%rsp)        # 4-byte Spill
	movl	604(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2784(%rsp)
	movl	556(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2788(%rsp)
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2792(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 2796(%rsp)
	movl	552(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2812(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2816(%rsp)
	movl	%ecx, 2820(%rsp)
	movl	%r12d, 2824(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 2828(%rsp)
	movl	%r11d, 2832(%rsp)
	vmovdqa	448(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, 2836(%rsp)
	vpextrd	$1, %xmm0, 2840(%rsp)
	vpextrd	$2, %xmm0, 2844(%rsp)
	vpextrd	$3, %xmm0, 2848(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 2852(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 2856(%rsp)
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 2872(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 2888(%rsp)
	movq	$0, 2896(%rsp)
	movq	440(%rsp), %rax         # 8-byte Reload
	movq	%rax, 2904(%rsp)
	movq	$0, 2912(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovq	%xmm0, 2920(%rsp)
	vpextrq	$1, %xmm0, 2928(%rsp)
	vextracti128	$1, %ymm0, %xmm0
	vmovdqa	%xmm0, 2624(%rsp)       # 16-byte Spill
	vmovq	%xmm0, 2936(%rsp)
	vpextrq	$1, %xmm0, 2944(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11(%rip), %rsi
	leaq	2784(%rsp), %r8
	movl	$6, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	%r11d, %r13d
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#87:                                # %produce gV
	vmovdqa	448(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, %ecx
	movl	%ecx, 548(%rsp)         # 4-byte Spill
	vpextrd	$1, %xmm0, %edx
	movl	%edx, 544(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 540(%rsp)         # 4-byte Spill
	vpextrd	$3, %xmm0, %edi
	movl	%edi, 536(%rsp)         # 4-byte Spill
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovq	%xmm0, %rbx
	movq	%rbx, 2720(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r8
	movq	%r8, 2688(%rsp)         # 8-byte Spill
	vmovdqa	2624(%rsp), %xmm0       # 16-byte Reload
	vmovq	%xmm0, %r9
	movq	%r9, 2656(%rsp)         # 8-byte Spill
	vpextrq	$1, %xmm0, %r10
	movq	%r10, 2624(%rsp)        # 8-byte Spill
	movl	604(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2952(%rsp)
	movl	556(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2956(%rsp)
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2960(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 2964(%rsp)
	movl	552(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2980(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2984(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 2988(%rsp)
	movl	%r12d, 2992(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 2996(%rsp)
	movl	%r13d, 3000(%rsp)
	movl	%ecx, 3004(%rsp)
	movl	%edx, 3008(%rsp)
	movl	%esi, 3012(%rsp)
	movl	%edi, 3016(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 3020(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3024(%rsp)
	vmovdqa	560(%rsp), %xmm0        # 16-byte Reload
	vmovdqu	%xmm0, 3040(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3056(%rsp)
	movq	$0, 3064(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3072(%rsp)
	movq	$0, 3080(%rsp)
	movq	%rbx, 3088(%rsp)
	movq	%r8, 3096(%rsp)
	movq	%r9, 3104(%rsp)
	movq	%r10, 3112(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11(%rip), %rsi
	leaq	2952(%rsp), %r8
	movl	$6, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	2592(%rsp), %edx        # 4-byte Reload
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#88:                                # %produce dV
	leal	-6(%r12), %ebx
	movl	604(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3120(%rsp)
	movl	556(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3124(%rsp)
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3128(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3132(%rsp)
	movl	552(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3148(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3152(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3156(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3160(%rsp)
	movl	%r12d, 3164(%rsp)
	movl	%r13d, 3168(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3172(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3176(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3180(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3184(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 3188(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3192(%rsp)
	vmovdqa	560(%rsp), %xmm0        # 16-byte Reload
	vmovdqu	%xmm0, 3208(%rsp)
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3224(%rsp)
	movq	$0, 3232(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3240(%rsp)
	movq	$0, 3248(%rsp)
	movq	2720(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3256(%rsp)
	movq	2688(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3264(%rsp)
	movq	2656(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3272(%rsp)
	movq	2624(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3280(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11(%rip), %rsi
	leaq	3120(%rsp), %r8
	movl	$14, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	%ebx, %edx
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#89:                                # %produce dh
	movl	604(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3288(%rsp)
	movl	556(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3292(%rsp)
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3296(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3300(%rsp)
	movl	552(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3316(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3320(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3324(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3328(%rsp)
	movl	%r12d, 3332(%rsp)
	movl	%r13d, 3336(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3340(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3344(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3348(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3352(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 3356(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3360(%rsp)
	vmovdqa	560(%rsp), %xmm0        # 16-byte Reload
	vmovdqu	%xmm0, 3376(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3392(%rsp)
	movq	$0, 3400(%rsp)
	movq	464(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3408(%rsp)
	movq	$0, 3416(%rsp)
	movq	2720(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3424(%rsp)
	movq	2688(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3432(%rsp)
	movq	2656(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3440(%rsp)
	movq	2624(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3448(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11(%rip), %rsi
	leaq	3288(%rsp), %r8
	movl	$14, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	%ebx, %edx
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#90:                                # %produce f4
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3456(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3460(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3464(%rsp)
	movl	%r13d, 3468(%rsp)
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3472(%rsp)
	movq	$0, 3480(%rsp)
	movq	464(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3488(%rsp)
	movq	$0, 3496(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3504(%rsp)
	movq	$0, 3512(%rsp)
	movq	440(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3520(%rsp)
	movq	$0, 3528(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3536(%rsp)
	movq	$0, 3544(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11(%rip), %rsi
	leaq	3456(%rsp), %r8
	movl	$6, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	2592(%rsp), %edx        # 4-byte Reload
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#91:                                # %produce f7
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3552(%rsp)
	vmovdqa	576(%rsp), %xmm0        # 16-byte Reload
	vpextrd	$2, %xmm0, 3556(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3560(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 3564(%rsp)
	movl	%r12d, 3568(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3572(%rsp)
	movq	392(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3576(%rsp)
	movq	472(%rsp), %rax         # 8-byte Reload
	movl	%eax, 3580(%rsp)
	movl	%r13d, 3584(%rsp)
	vmovaps	448(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3588(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 3604(%rsp)
	vmovdqa	640(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, 3608(%rsp)
	vpextrd	$1, %xmm0, 3612(%rsp)
	vpextrd	$2, %xmm0, 3616(%rsp)
	vpextrd	$3, %xmm0, 3620(%rsp)
	vmovdqa	560(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, 3624(%rsp)
	vpextrd	$3, %xmm0, 3628(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3632(%rsp)
	movq	$0, 3640(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3648(%rsp)
	movq	$0, 3656(%rsp)
	movq	744(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3664(%rsp)
	movq	$0, 3672(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 3680(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11(%rip), %rsi
	leaq	3552(%rsp), %r8
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	%r12d, %edx
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#92:                                # %produce f8
	vmovdqa	640(%rsp), %xmm1        # 16-byte Reload
	vmovd	%xmm1, %eax
	vpextrd	$1, %xmm1, %ebx
	vpextrd	$2, %xmm1, %ecx
	movl	604(%rsp), %edx         # 4-byte Reload
	movl	%edx, 3712(%rsp)
	vmovdqa	576(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, 3716(%rsp)
	vpextrd	$3, %xmm0, 3720(%rsp)
	movl	676(%rsp), %edx         # 4-byte Reload
	movl	%edx, 3724(%rsp)
	movl	%r12d, 3728(%rsp)
	movq	520(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 3732(%rsp)
	movq	472(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 3736(%rsp)
	movq	392(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 3740(%rsp)
	movl	%r13d, 3744(%rsp)
	vmovaps	448(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 3748(%rsp)
	movl	1660(%rsp), %edx        # 4-byte Reload
	movl	%edx, 3764(%rsp)
	movl	%eax, 3768(%rsp)
	vpextrd	$3, %xmm1, %eax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	movl	%ebx, 3772(%rsp)
	movl	%ecx, 3776(%rsp)
	vmovdqa	560(%rsp), %xmm0        # 16-byte Reload
	vmovd	%xmm0, %ecx
	movl	%ecx, 424(%rsp)         # 4-byte Spill
	movl	%eax, 3780(%rsp)
	movl	%ecx, 3784(%rsp)
	vpextrd	$1, %xmm0, 3788(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 3792(%rsp)
	movq	$0, 3800(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3808(%rsp)
	movq	$0, 3816(%rsp)
	movq	736(%rsp), %rax         # 8-byte Reload
	movq	%rax, 3824(%rsp)
	movq	$0, 3832(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 3840(%rsp)
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11(%rip), %rsi
	leaq	3712(%rsp), %r8
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	movl	%r12d, %edx
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#93:                                # %consume f8
	movq	%rbx, 1448(%rsp)        # 8-byte Spill
	movl	%r13d, 2172(%rsp)       # 4-byte Spill
	movq	472(%rsp), %rax         # 8-byte Reload
	movslq	%eax, %r8
	addq	$32, %r8
	movq	1064(%rsp), %r15        # 8-byte Reload
	movq	776(%rsp), %rsi         # 8-byte Reload
	imulq	%r15, %rsi
	movq	520(%rsp), %rax         # 8-byte Reload
	cltq
	movl	764(%rsp), %edi         # 4-byte Reload
	movl	732(%rsp), %ebx         # 4-byte Reload
	imull	%edi, %ebx
	addq	$3, %rax
	addq	768(%rsp), %rsi         # 8-byte Folded Reload
	addl	728(%rsp), %ebx         # 4-byte Folded Reload
	leaq	(%r15,%r15), %rcx
	subq	%rsi, %rcx
	vmovd	%ebx, %xmm0
	vmovaps	%ymm0, 2720(%rsp)       # 32-byte Spill
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 2688(%rsp)       # 32-byte Spill
	movq	384(%rsp), %rdi         # 8-byte Reload
	testl	%edi, %edi
	js	.LBB148_94
# BB#95:                                # %for f0.s0.v10.v10.preheader
	movq	376(%rsp), %rdi         # 8-byte Reload
	vbroadcastss	8(%rdi,%rcx,4), %ymm0
	vmovaps	%ymm0, 1824(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rdi,%rcx,4), %ymm0
	vmovaps	%ymm0, 1792(%rsp)       # 32-byte Spill
	vbroadcastss	(%rdi,%rcx,4), %ymm0
	vmovaps	%ymm0, 1760(%rsp)       # 32-byte Spill
	movq	%rcx, 336(%rsp)         # 8-byte Spill
	subq	%rsi, %r15
	movl	$2, %r9d
	movl	$2, %r13d
	subq	%rsi, %r13
	movl	$1, %r10d
	subq	%rsi, %r10
	shlq	$2, %rsi
	movq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 328(%rsp)         # 8-byte Spill
	negq	%rsi
	movq	984(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, %ecx
	notl	%ecx
	movslq	%ecx, %r12
	movl	$63, %edx
	andnl	%edx, %ebx, %edx
	movq	%rdx, %rbx
	imulq	%r8, %rbx
	movq	%rbx, 1504(%rsp)        # 8-byte Spill
	addq	$1, %r12
	movl	%r12d, %ebx
	andl	$63, %ebx
	movq	%rbx, %rcx
	imulq	%r8, %rcx
	movq	%rcx, 1488(%rsp)        # 8-byte Spill
	movq	%r8, 352(%rsp)          # 8-byte Spill
	vbroadcastss	(%rdi,%rsi), %ymm0
	vmovaps	%ymm0, 1728(%rsp)       # 32-byte Spill
	subq	720(%rsp), %r12         # 8-byte Folded Reload
	shlq	$5, %r12
	subq	2456(%rsp), %r9         # 8-byte Folded Reload
	movq	%r9, 1520(%rsp)         # 8-byte Spill
	imulq	%rax, %rdx
	movq	%rdx, 1512(%rsp)        # 8-byte Spill
	imulq	%rax, %rbx
	movq	%rbx, 1496(%rsp)        # 8-byte Spill
	movq	%rax, 344(%rsp)         # 8-byte Spill
	movq	976(%rsp), %r11         # 8-byte Reload
	notq	%r11
	cmpq	$-2, %r11
	movq	$-1, %r9
	cmovleq	%r9, %r11
	movq	888(%rsp), %rsi         # 8-byte Reload
	notq	%rsi
	cmpq	$-2, %rsi
	cmovleq	%r9, %rsi
	movq	992(%rsp), %rdx         # 8-byte Reload
	testl	%edx, %edx
	movl	$1, %eax
	cmovgl	%edx, %eax
	movl	$31, %edx
	subl	%eax, %edx
	leaq	8(%rdi,%r15,4), %rax
	movq	%rax, 320(%rsp)         # 8-byte Spill
	vbroadcastss	8(%rdi,%r15,4), %ymm0
	vmovaps	%ymm0, 1696(%rsp)       # 32-byte Spill
	movq	1080(%rsp), %rax        # 8-byte Reload
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	notl	%edx
	movslq	%edx, %rax
	leaq	1(%rsi,%rax), %rax
	movq	816(%rsp), %r8          # 8-byte Reload
	leaq	1(%r8), %rdx
	movq	%rdx, 1064(%rsp)        # 8-byte Spill
	addq	$1, %r8
	imulq	%r8, %rax
	addq	%r11, %rax
	leaq	4(%rdi,%r15,4), %rdx
	movq	%rdx, 312(%rsp)         # 8-byte Spill
	vbroadcastss	4(%rdi,%r15,4), %ymm0
	vmovaps	%ymm0, 1664(%rsp)       # 32-byte Spill
	vbroadcastss	(%rdi,%r15,4), %ymm0
	vmovaps	%ymm0, 1600(%rsp)       # 32-byte Spill
	leaq	(%rdi,%r15,4), %rdx
	movq	%rdx, 304(%rsp)         # 8-byte Spill
	vbroadcastss	(%rdi,%r13,4), %ymm0
	vmovaps	%ymm0, 1568(%rsp)       # 32-byte Spill
	leaq	(%rdi,%r13,4), %rdx
	movq	%rdx, 296(%rsp)         # 8-byte Spill
	vbroadcastss	(%rdi,%r10,4), %ymm0
	vmovaps	%ymm0, 1536(%rsp)       # 32-byte Spill
	leaq	(%rdi,%r10,4), %rdx
	movq	%rdx, 288(%rsp)         # 8-byte Spill
	movq	832(%rsp), %rdx         # 8-byte Reload
	leaq	1(%rdx,%rax), %rax
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	vbroadcastss	.LCPI148_12(%rip), %ymm9
	vxorps	%ymm10, %ymm10, %ymm10
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 720(%rsp)         # 4-byte Spill
	leaq	8(%r12), %rax
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	leaq	16(%r12), %rax
	movq	%rax, 1464(%rsp)        # 8-byte Spill
	leaq	24(%r12), %rax
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	movq	968(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	.align	16, 0x90
.LBB148_96:                             # %for f0.s0.v10.v10
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB148_97 Depth 2
                                        #       Child Loop BB148_98 Depth 3
	movq	%rax, 1528(%rsp)        # 8-byte Spill
	movl	%eax, %edx
	shll	$5, %edx
	movq	968(%rsp), %rsi         # 8-byte Reload
	addl	%esi, %edx
	movslq	%edx, %rsi
	movq	1520(%rsp), %rdx        # 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	1512(%rsp), %rax        # 8-byte Reload
	leaq	(%rdx,%rax), %rbx
	movq	752(%rsp), %rdi         # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm0
	vmovaps	%ymm0, 2656(%rsp)       # 32-byte Spill
	movl	%ecx, %r10d
	movq	1496(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdx,%rcx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm1
	leaq	8(%rax,%rdx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm2
	leaq	8(%rcx,%rdx), %rbx
	vmovups	(%rdi,%rbx,4), %ymm3
	leaq	16(%rax,%rdx), %rbx
	vmovups	(%rdi,%rbx,4), %ymm4
	leaq	16(%rcx,%rdx), %rbx
	vmovdqu	(%rdi,%rbx,4), %ymm5
	leaq	24(%rax,%rdx), %rbx
	vmovups	(%rdi,%rbx,4), %ymm6
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	leaq	24(%rcx,%rdx), %rdx
	vmovups	(%rdi,%rdx,4), %ymm7
	movq	1504(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rdx
	movq	744(%rsp), %rdi         # 8-byte Reload
	vmovups	(%rdi,%rdx,4), %ymm8
	vmovaps	%ymm8, 3872(%rsp)
	movq	736(%rsp), %rbx         # 8-byte Reload
	vmovdqu	(%rbx,%rdx,4), %ymm8
	movq	1488(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx), %rdx
	vmovups	(%rdi,%rdx,4), %ymm11
	vmovaps	%ymm11, 3872(%rsp,%r12,4)
	vmovups	(%rbx,%rdx,4), %ymm11
	leaq	8(%rax,%rsi), %rdx
	vmovups	(%rdi,%rdx,4), %ymm12
	vmovaps	%ymm12, 3904(%rsp)
	vmovups	(%rbx,%rdx,4), %ymm12
	leaq	8(%rsi,%rcx), %r9
	vmovups	(%rdi,%r9,4), %ymm13
	movq	1472(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm13, 3872(%rsp,%rdx,4)
	vmovdqu	(%rbx,%r9,4), %ymm13
	leaq	16(%rsi,%rax), %rdx
	vmovups	(%rdi,%rdx,4), %ymm14
	vmovaps	%ymm14, 3936(%rsp)
	vmovdqu	(%rbx,%rdx,4), %ymm14
	leaq	16(%rsi,%rcx), %r9
	vmovups	(%rdi,%r9,4), %ymm15
	movq	1464(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm15, 3872(%rsp,%rdx,4)
	vmovdqu	(%rbx,%r9,4), %ymm15
	leaq	24(%rsi,%rax), %rdx
	vmovups	(%rdi,%rdx,4), %ymm0
	vmovaps	%ymm0, 3968(%rsp)
	leaq	24(%rsi,%rcx), %rsi
	movl	%r10d, %ecx
	vmovups	(%rdi,%rsi,4), %ymm0
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovaps	%ymm0, 3872(%rsp,%rax,4)
	vmovaps	2656(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, 4128(%rsp)
	vmovdqa	%ymm1, 4128(%rsp,%r12,4)
	vmovdqa	%ymm2, 4160(%rsp)
	vmovaps	%ymm3, 4160(%rsp,%r12,4)
	vmovaps	%ymm4, 4192(%rsp)
	vmovdqa	%ymm5, 4192(%rsp,%r12,4)
	vmovaps	%ymm6, 4224(%rsp)
	vmovaps	%ymm7, 4224(%rsp,%r12,4)
	vmovdqa	%ymm8, 4384(%rsp)
	vmovaps	%ymm11, 4384(%rsp,%r12,4)
	vmovaps	%ymm12, 4416(%rsp)
	vmovdqa	%ymm13, 4416(%rsp,%r12,4)
	vmovdqa	%ymm14, 4448(%rsp)
	vmovdqa	%ymm15, 4448(%rsp,%r12,4)
	vmovups	(%rbx,%rdx,4), %ymm0
	movslq	%ecx, %rdx
	movq	1480(%rsp), %rax        # 8-byte Reload
	leaq	(%rdx,%rax), %r15
	vmovaps	%ymm0, 4480(%rsp)
	vmovups	(%rbx,%rsi,4), %ymm0
	vmovaps	%ymm0, 4480(%rsp,%r12,4)
	xorl	%r9d, %r9d
	movq	%r8, %rdi
	.align	16, 0x90
.LBB148_97:                             # %for f0.s0.v11.v13.yii
                                        #   Parent Loop BB148_96 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB148_98 Depth 3
	movq	%r9, %rdx
	shlq	$7, %rdx
	vmovaps	3872(%rsp,%rdx), %ymm3
	vmovaps	%ymm3, 1888(%rsp)       # 32-byte Spill
	vmovaps	3904(%rsp,%rdx), %ymm1
	vmovaps	%ymm1, 1920(%rsp)       # 32-byte Spill
	vmovaps	3936(%rsp,%rdx), %ymm0
	vmovaps	%ymm0, 1952(%rsp)       # 32-byte Spill
	vmovaps	3968(%rsp,%rdx), %ymm6
	vmovaps	%ymm6, 1856(%rsp)       # 32-byte Spill
	vmovaps	4128(%rsp,%rdx), %ymm14
	vmovaps	%ymm14, 1984(%rsp)      # 32-byte Spill
	vmovaps	4160(%rsp,%rdx), %ymm8
	vmovaps	4192(%rsp,%rdx), %ymm15
	vmovaps	4224(%rsp,%rdx), %ymm7
	vmovaps	4384(%rsp,%rdx), %ymm12
	vmovaps	4416(%rsp,%rdx), %ymm11
	vmovaps	4448(%rsp,%rdx), %ymm5
	vmovaps	1760(%rsp), %ymm2       # 32-byte Reload
	vmulps	%ymm2, %ymm0, %ymm0
	vmulps	%ymm2, %ymm1, %ymm1
	vmulps	%ymm2, %ymm3, %ymm3
	vmulps	%ymm2, %ymm6, %ymm4
	vmovaps	%ymm6, %ymm13
	vmovaps	1792(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm4, %ymm7, %ymm6
	vmovaps	%ymm2, %ymm4
	vfmadd213ps	%ymm3, %ymm14, %ymm4
	vmovaps	%ymm2, %ymm3
	vfmadd213ps	%ymm1, %ymm8, %ymm3
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm0, %ymm15, %ymm1
	vmovaps	1824(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm0
	vfmadd213ps	%ymm1, %ymm5, %ymm0
	vmovaps	%ymm0, 2624(%rsp)       # 32-byte Spill
	vmovaps	%ymm2, %ymm0
	vfmadd213ps	%ymm3, %ymm11, %ymm0
	vmovaps	%ymm0, 2592(%rsp)       # 32-byte Spill
	vmovaps	%ymm2, %ymm0
	vfmadd213ps	%ymm4, %ymm12, %ymm0
	vmovaps	%ymm0, 2560(%rsp)       # 32-byte Spill
	vmovaps	4480(%rsp,%rdx), %ymm0
	vfmadd213ps	%ymm6, %ymm0, %ymm2
	vmovaps	%ymm2, 2176(%rsp)       # 32-byte Spill
	vmovaps	1600(%rsp), %ymm2       # 32-byte Reload
	vmulps	%ymm2, %ymm13, %ymm3
	vmovaps	1664(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%ymm1, %ymm13
	vfmadd213ps	%ymm3, %ymm7, %ymm13
	vmovaps	%ymm7, %ymm14
	vmulps	1888(%rsp), %ymm2, %ymm4 # 32-byte Folded Reload
	vmovaps	%ymm1, %ymm7
	vmovaps	1984(%rsp), %ymm3       # 32-byte Reload
	vfmadd213ps	%ymm4, %ymm3, %ymm7
	vmulps	1920(%rsp), %ymm2, %ymm4 # 32-byte Folded Reload
	vmovaps	%ymm1, %ymm6
	vfmadd213ps	%ymm4, %ymm8, %ymm6
	vmulps	1952(%rsp), %ymm2, %ymm2 # 32-byte Folded Reload
	vmovaps	%ymm1, %ymm4
	vfmadd213ps	%ymm2, %ymm15, %ymm4
	vmovaps	1696(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%ymm1, %ymm2
	vfmadd213ps	%ymm4, %ymm5, %ymm2
	vmovaps	%ymm2, 2656(%rsp)       # 32-byte Spill
	vmovaps	%ymm1, %ymm4
	vfmadd213ps	%ymm6, %ymm11, %ymm4
	vmovaps	%ymm1, %ymm6
	vfmadd213ps	%ymm7, %ymm12, %ymm6
	vmovaps	%ymm1, %ymm7
	vfmadd213ps	%ymm13, %ymm0, %ymm7
	vmovaps	1728(%rsp), %ymm1       # 32-byte Reload
	vmulps	1856(%rsp), %ymm1, %ymm13 # 32-byte Folded Reload
	vmovaps	1536(%rsp), %ymm2       # 32-byte Reload
	vfmadd213ps	%ymm13, %ymm2, %ymm14
	vmulps	1888(%rsp), %ymm1, %ymm13 # 32-byte Folded Reload
	vfmadd213ps	%ymm13, %ymm2, %ymm3
	vmovaps	%ymm3, 1984(%rsp)       # 32-byte Spill
	vmulps	1920(%rsp), %ymm1, %ymm13 # 32-byte Folded Reload
	vfmadd213ps	%ymm13, %ymm2, %ymm8
	vmulps	1952(%rsp), %ymm1, %ymm13 # 32-byte Folded Reload
	vfmadd213ps	%ymm13, %ymm2, %ymm15
	vmovaps	1568(%rsp), %ymm1       # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm1, %ymm5
	vmovaps	%ymm5, %ymm3
	vfmadd213ps	%ymm8, %ymm1, %ymm11
	vfmadd213ps	1984(%rsp), %ymm1, %ymm12 # 32-byte Folded Reload
	vfmadd213ps	%ymm14, %ymm1, %ymm0
	xorl	%r10d, %r10d
	movl	$3, %r11d
	movq	%r15, %r8
	.align	16, 0x90
.LBB148_98:                             # %for f0.s0.v12
                                        #   Parent Loop BB148_96 Depth=1
                                        #     Parent Loop BB148_97 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovaps	%ymm6, %ymm5
	cmpl	$1, %r10d
	je	.LBB148_100
# BB#99:                                # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	2560(%rsp), %ymm5       # 32-byte Reload
.LBB148_100:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm4, %ymm8
	je	.LBB148_102
# BB#101:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	2592(%rsp), %ymm8       # 32-byte Reload
.LBB148_102:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	2656(%rsp), %ymm15      # 32-byte Reload
	je	.LBB148_104
# BB#103:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	2624(%rsp), %ymm15      # 32-byte Reload
.LBB148_104:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm7, %ymm13
	je	.LBB148_106
# BB#105:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	2176(%rsp), %ymm13      # 32-byte Reload
.LBB148_106:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm0, %ymm14
	testl	%r10d, %r10d
	je	.LBB148_108
# BB#107:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm13, %ymm14
.LBB148_108:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm3, %ymm13
	je	.LBB148_110
# BB#109:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm15, %ymm13
.LBB148_110:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm11, %ymm15
	je	.LBB148_112
# BB#111:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm8, %ymm15
.LBB148_112:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm12, %ymm8
	je	.LBB148_114
# BB#113:                               # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vmovaps	%ymm5, %ymm8
.LBB148_114:                            # %for f0.s0.v12
                                        #   in Loop: Header=BB148_98 Depth=3
	vminps	%ymm9, %ymm8, %ymm5
	vminps	%ymm9, %ymm15, %ymm8
	vminps	%ymm9, %ymm13, %ymm13
	vminps	%ymm9, %ymm14, %ymm14
	vmaxps	%ymm10, %ymm5, %ymm5
	vmaxps	%ymm10, %ymm8, %ymm8
	vmaxps	%ymm10, %ymm13, %ymm13
	vmaxps	%ymm10, %ymm14, %ymm14
	vcvttps2dq	%ymm5, %ymm5
	vmovdqa	.LCPI148_3(%rip), %ymm1 # ymm1 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm1, %ymm5, %ymm5
	vpermq	$232, %ymm5, %ymm5      # ymm5 = ymm5[0,2,2,3]
	vcvttps2dq	%ymm8, %ymm8
	vpshufb	%ymm1, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vcvttps2dq	%ymm13, %ymm13
	vpshufb	%ymm1, %ymm13, %ymm13
	vpermq	$232, %ymm13, %ymm13    # ymm13 = ymm13[0,2,2,3]
	vcvttps2dq	%ymm14, %ymm14
	vpshufb	%ymm1, %ymm14, %ymm14
	vpermq	$232, %ymm14, %ymm14    # ymm14 = ymm14[0,2,2,3]
	vpmovzxwd	%xmm14, %ymm14  # ymm14 = xmm14[0],zero,xmm14[1],zero,xmm14[2],zero,xmm14[3],zero,xmm14[4],zero,xmm14[5],zero,xmm14[6],zero,xmm14[7],zero
	vpmovzxwd	%xmm13, %ymm13  # ymm13 = xmm13[0],zero,xmm13[1],zero,xmm13[2],zero,xmm13[3],zero,xmm13[4],zero,xmm13[5],zero,xmm13[6],zero,xmm13[7],zero
	vpmovzxwd	%xmm8, %ymm8    # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero,xmm8[4],zero,xmm8[5],zero,xmm8[6],zero,xmm8[7],zero
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vmovdqa	2688(%rsp), %ymm1       # 32-byte Reload
	vpmulld	%ymm1, %ymm5, %ymm5
	vpmulld	%ymm1, %ymm8, %ymm8
	vpmulld	%ymm1, %ymm13, %ymm13
	vpmulld	%ymm1, %ymm14, %ymm14
	vmovd	%r10d, %xmm15
	vpsubd	2720(%rsp), %ymm15, %ymm15 # 32-byte Folded Reload
	vpbroadcastd	%xmm15, %ymm1
	vpaddd	%ymm14, %ymm1, %ymm14
	vpaddd	%ymm13, %ymm1, %ymm15
	vpaddd	%ymm8, %ymm1, %ymm8
	vpaddd	%ymm5, %ymm1, %ymm5
	vmovq	%xmm15, %rsi
	movslq	%esi, %rdx
	movzbl	(%r14,%rdx), %edx
	vmovd	%edx, %xmm1
	vpextrq	$1, %xmm15, %rdx
	sarq	$32, %rsi
	vpinsrb	$1, (%r14,%rsi), %xmm1, %xmm1
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vextracti128	$1, %ymm15, %xmm2
	vpinsrb	$2, (%r14,%rsi), %xmm1, %xmm1
	vmovq	%xmm2, %rsi
	vpinsrb	$3, (%r14,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$4, (%r14,%rdx), %xmm1, %xmm1
	vpextrq	$1, %xmm2, %rdx
	sarq	$32, %rsi
	vpinsrb	$5, (%r14,%rsi), %xmm1, %xmm1
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vpinsrb	$6, (%r14,%rsi), %xmm1, %xmm1
	vmovq	%xmm14, %rsi
	vpinsrb	$7, (%r14,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$8, (%r14,%rdx), %xmm1, %xmm1
	vpextrq	$1, %xmm14, %rdx
	sarq	$32, %rsi
	vpinsrb	$9, (%r14,%rsi), %xmm1, %xmm1
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vextracti128	$1, %ymm14, %xmm2
	vpinsrb	$10, (%r14,%rsi), %xmm1, %xmm1
	vmovq	%xmm2, %rsi
	vpinsrb	$11, (%r14,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	vpinsrb	$12, (%r14,%rdx), %xmm1, %xmm1
	vpextrq	$1, %xmm2, %rdx
	sarq	$32, %rsi
	vpinsrb	$13, (%r14,%rsi), %xmm1, %xmm1
	movslq	%edx, %rsi
	vpinsrb	$14, (%r14,%rsi), %xmm1, %xmm1
	vmovq	%xmm5, %rsi
	sarq	$32, %rdx
	vpinsrb	$15, (%r14,%rdx), %xmm1, %xmm1
	movslq	%esi, %rdx
	movzbl	(%r14,%rdx), %edx
	vmovd	%edx, %xmm2
	vpextrq	$1, %xmm5, %rdx
	sarq	$32, %rsi
	vpinsrb	$1, (%r14,%rsi), %xmm2, %xmm2
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vextracti128	$1, %ymm5, %xmm5
	vpinsrb	$2, (%r14,%rsi), %xmm2, %xmm2
	vmovq	%xmm5, %rsi
	vpinsrb	$3, (%r14,%rdx), %xmm2, %xmm2
	movslq	%esi, %rdx
	vpinsrb	$4, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm5, %rdx
	sarq	$32, %rsi
	vpinsrb	$5, (%r14,%rsi), %xmm2, %xmm2
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vpinsrb	$6, (%r14,%rsi), %xmm2, %xmm2
	vmovq	%xmm8, %rsi
	vpinsrb	$7, (%r14,%rdx), %xmm2, %xmm2
	movslq	%esi, %rdx
	vpinsrb	$8, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm8, %rdx
	sarq	$32, %rsi
	vpinsrb	$9, (%r14,%rsi), %xmm2, %xmm2
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vextracti128	$1, %ymm8, %xmm5
	vpinsrb	$10, (%r14,%rsi), %xmm2, %xmm2
	vmovq	%xmm5, %rsi
	vpinsrb	$11, (%r14,%rdx), %xmm2, %xmm2
	movslq	%esi, %rdx
	vpinsrb	$12, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm5, %rdx
	sarq	$32, %rsi
	vpinsrb	$13, (%r14,%rsi), %xmm2, %xmm2
	movslq	%edx, %rsi
	vpinsrb	$14, (%r14,%rsi), %xmm2, %xmm2
	sarq	$32, %rdx
	vpinsrb	$15, (%r14,%rdx), %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqu	%ymm1, (%r8)
	addq	2776(%rsp), %r8         # 8-byte Folded Reload
	addl	$1, %r10d
	addq	$-1, %r11
	jne	.LBB148_98
# BB#115:                               # %end for f0.s0.v12
                                        #   in Loop: Header=BB148_97 Depth=2
	addq	$1, %r9
	addq	%rdi, %r15
	cmpq	$2, %r9
	jne	.LBB148_97
# BB#116:                               # %end for f0.s0.v11.v13.yii
                                        #   in Loop: Header=BB148_96 Depth=1
	movq	%rdi, %r8
	movq	1528(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	addl	$32, %ecx
	cmpl	720(%rsp), %eax         # 4-byte Folded Reload
	jne	.LBB148_96
	jmp	.LBB148_117
.LBB148_2:                              # %assert failed
	leaq	.Lstr.164(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdi
	movq	%rax, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB148_22
.LBB148_21:                             # %assert failed1
	movq	%r12, %rdi
	callq	halide_error_out_of_memory@PLT
.LBB148_22:                             # %destructor_block.thread
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%r14d, %r14d
	xorl	%esi, %esi
	jmp	.LBB148_125
.LBB148_24:                             # %assert failed3
	leaq	.Lstr.165(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB148_27
.LBB148_26:                             # %assert failed5
	callq	halide_error_out_of_memory@PLT
.LBB148_27:                             # %destructor_block.thread
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%r14d, %r14d
	movq	2504(%rsp), %rsi        # 8-byte Reload
	jmp	.LBB148_125
.LBB148_29:                             # %assert failed9
	movq	%r12, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	movq	2504(%rsp), %rsi        # 8-byte Reload
	movq	440(%rsp), %r14         # 8-byte Reload
	jmp	.LBB148_125
.LBB148_31:                             # %assert failed11
	leaq	.Lstr.167(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB148_34
.LBB148_33:                             # %assert failed13
	callq	halide_error_out_of_memory@PLT
.LBB148_34:                             # %destructor_block.thread
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	xorl	%eax, %eax
	movq	2504(%rsp), %rsi        # 8-byte Reload
	movq	440(%rsp), %r14         # 8-byte Reload
	movq	488(%rsp), %rcx         # 8-byte Reload
	jmp	.LBB148_125
.LBB148_36:                             # %assert failed17
	movq	%r12, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	jmp	.LBB148_42
.LBB148_38:                             # %assert failed19
	leaq	.Lstr.169(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdi
	movq	%rax, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB148_41
.LBB148_40:                             # %assert failed21
	movq	%r12, %rdi
	callq	halide_error_out_of_memory@PLT
.LBB148_41:                             # %destructor_block.thread
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
.LBB148_42:                             # %destructor_block.thread
	movq	2504(%rsp), %rsi        # 8-byte Reload
	movq	440(%rsp), %r14         # 8-byte Reload
	movq	488(%rsp), %rcx         # 8-byte Reload
	movq	480(%rsp), %rax         # 8-byte Reload
	jmp	.LBB148_125
.LBB148_44:                             # %assert failed23
	leaq	.Lstr.170(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%r12, %rdi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB148_47
.LBB148_46:                             # %assert failed25
	callq	halide_error_out_of_memory@PLT
.LBB148_47:                             # %destructor_block.thread
	movl	%eax, %r15d
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	jmp	.LBB148_48
.LBB148_50:                             # %assert failed29
	movq	664(%rsp), %r12         # 8-byte Reload
	movq	%r12, %rdi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r15d
.LBB148_48:                             # %destructor_block.thread
	movq	2504(%rsp), %rsi        # 8-byte Reload
	movq	440(%rsp), %r14         # 8-byte Reload
	movq	488(%rsp), %rcx         # 8-byte Reload
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	752(%rsp), %rdx         # 8-byte Reload
	jmp	.LBB148_125
.LBB148_94:                             # %consume f8.for f0.s0.v11.v13.v13.preheader_crit_edge
	movq	%rcx, 336(%rsp)         # 8-byte Spill
	movq	%rax, 344(%rsp)         # 8-byte Spill
	movq	%r8, 352(%rsp)          # 8-byte Spill
	subq	%rsi, %r15
	movq	376(%rsp), %rcx         # 8-byte Reload
	leaq	8(%rcx,%r15,4), %rax
	movq	%rax, 320(%rsp)         # 8-byte Spill
	leaq	4(%rcx,%r15,4), %rax
	movq	%rax, 312(%rsp)         # 8-byte Spill
	leaq	(%rcx,%r15,4), %rax
	movq	%rax, 304(%rsp)         # 8-byte Spill
	movl	$2, %eax
	subq	%rsi, %rax
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 296(%rsp)         # 8-byte Spill
	movl	$1, %eax
	subq	%rsi, %rax
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 288(%rsp)         # 8-byte Spill
	shlq	$2, %rsi
	subq	%rsi, %rcx
	movq	%rcx, 328(%rsp)         # 8-byte Spill
	movq	816(%rsp), %rcx         # 8-byte Reload
	addq	$1, %rcx
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 720(%rsp)         # 4-byte Spill
	movq	%rcx, 1064(%rsp)        # 8-byte Spill
.LBB148_117:
	movq	680(%rsp), %rbx         # 8-byte Reload
	movq	392(%rsp), %rdx         # 8-byte Reload
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1660(%rsp), %r15d       # 4-byte Reload
	movl	956(%rsp), %r10d        # 4-byte Reload
	movl	%r10d, 956(%rsp)        # 4-byte Spill
	movq	%rdx, 392(%rsp)         # 8-byte Spill
	movq	1056(%rsp), %rsi        # 8-byte Reload
	leal	39(%rsi), %r12d
	movq	968(%rsp), %r8          # 8-byte Reload
	subl	%r8d, %r12d
	xorl	%ecx, %ecx
	sarl	$5, %r12d
	cmovsl	%ecx, %r12d
	cmpl	%r12d, %r10d
	movl	%r10d, %r11d
	cmovgl	%r12d, %r11d
	movl	%r11d, 768(%rsp)        # 4-byte Spill
	movq	1072(%rsp), %rcx        # 8-byte Reload
	subl	%r8d, %ecx
	movq	1048(%rsp), %rax        # 8-byte Reload
	subl	%r8d, %eax
	addl	%esi, %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	addl	$-24, %eax
	leal	-23(%rcx), %edi
	cmpl	%eax, %edi
	cmovlel	%edi, %eax
	leal	6(%rcx), %edi
	cmpl	%eax, %edi
	cmovlel	%edi, %eax
	addl	$7, %ecx
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	leal	47(%rdx), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	sarl	$5, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %r11d
	movl	%ecx, %edx
	cmovgel	%r11d, %edx
	movl	%edx, 764(%rsp)         # 4-byte Spill
	movl	$9, %r13d
	movq	984(%rsp), %rax         # 8-byte Reload
	subl	%eax, %r13d
	movl	$1, %r11d
	subl	%eax, %r11d
	movl	$2, %edx
	subq	2456(%rsp), %rdx        # 8-byte Folded Reload
	movq	%rdx, 688(%rsp)         # 8-byte Spill
	movl	$-7, %edx
	subl	%esi, %edx
	movq	%rdx, 1984(%rsp)        # 8-byte Spill
	movl	$-8, %edx
	subl	%esi, %edx
	movq	%rdx, 1952(%rsp)        # 8-byte Spill
	movl	%r10d, %edx
	notl	%edx
	notl	%r12d
	cmpl	%r12d, %edx
	cmovgel	%edx, %r12d
	movq	824(%rsp), %rdx         # 8-byte Reload
	movq	960(%rsp), %rdi         # 8-byte Reload
	imull	%edi, %edx
	leal	(%rdx,%rsi), %edx
	leal	-32(%r8), %edi
	movl	%edi, %eax
	subl	%edx, %edi
	movl	%r12d, %edx
	shll	$5, %edx
	subl	%edx, %eax
	movl	%eax, 708(%rsp)         # 4-byte Spill
	subl	%edx, %edi
	movq	%rdi, 360(%rsp)         # 8-byte Spill
	movl	%r12d, %eax
	notl	%eax
	movl	%eax, 704(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	1(%r12,%rcx), %eax
	movl	%eax, 700(%rsp)         # 4-byte Spill
	subl	%ecx, %r10d
	movl	%r10d, 696(%rsp)        # 4-byte Spill
	shll	$5, %ecx
	movq	%rcx, 1072(%rsp)        # 8-byte Spill
	movl	%ecx, %eax
	subl	%esi, %eax
	leal	-7(%rax), %edx
	movq	%rdx, 1048(%rsp)        # 8-byte Spill
	addl	$-8, %eax
	movq	%rax, 1056(%rsp)        # 8-byte Spill
	movq	976(%rsp), %r10         # 8-byte Reload
	notq	%r10
	cmpq	$-2, %r10
	movq	$-1, %rax
	cmovleq	%rax, %r10
	movq	888(%rsp), %rdx         # 8-byte Reload
	notq	%rdx
	cmpq	$-2, %rdx
	cmovleq	%rax, %rdx
	movq	992(%rsp), %r8          # 8-byte Reload
	testl	%r8d, %r8d
	movl	$1, %edi
	cmovlel	%edi, %r8d
	movl	$31, %eax
	subl	%r8d, %eax
	movq	1080(%rsp), %r8         # 8-byte Reload
	cmpl	%eax, %r8d
	cmovgel	%r8d, %eax
	notl	%eax
	cltq
	leaq	3(%rdx,%rax), %rax
	movq	1064(%rsp), %r8         # 8-byte Reload
	imulq	%r8, %rax
	addq	%r10, %rax
	movq	832(%rsp), %rdx         # 8-byte Reload
	leaq	1(%rdx,%rax), %rax
	movq	%rax, 712(%rsp)         # 8-byte Spill
	movq	512(%rsp), %r10         # 8-byte Reload
	movq	1448(%rsp), %rax        # 8-byte Reload
	leal	(%r10,%rax), %edx
	movl	%edx, 436(%rsp)         # 4-byte Spill
	leal	-3(%r10,%rax), %edx
	movl	%edx, 432(%rsp)         # 4-byte Spill
	movq	984(%rsp), %rax         # 8-byte Reload
	leal	-12(%rax), %edx
	movslq	%r11d, %rax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	vpabsd	1120(%rsp), %xmm0       # 16-byte Folded Reload
	vmovdqa	%ymm0, 832(%rsp)        # 32-byte Spill
	vinserti128	$1, %xmm0, %ymm0, %ymm0
	vmovdqa	%ymm0, 992(%rsp)        # 32-byte Spill
	vmovaps	2336(%rsp), %ymm0       # 32-byte Reload
	vextractf128	$1, %ymm0, 2176(%rsp) # 16-byte Folded Spill
	vmovdqa	576(%rsp), %xmm0        # 16-byte Reload
	vpextrd	$2, %xmm0, 284(%rsp)    # 4-byte Folded Spill
	vmovdqa	560(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$3, %xmm1, 280(%rsp)    # 4-byte Folded Spill
	vmovd	%xmm0, 276(%rsp)        # 4-byte Folded Spill
	vpextrd	$3, %xmm0, 272(%rsp)    # 4-byte Folded Spill
	vpextrd	$1, %xmm1, 268(%rsp)    # 4-byte Folded Spill
	leal	2(%r10), %eax
	movl	%eax, 428(%rsp)         # 4-byte Spill
	leal	-7(%rcx), %eax
	movq	%rax, 984(%rsp)         # 8-byte Spill
	leal	-8(%rcx), %eax
	movq	%rax, 976(%rsp)         # 8-byte Spill
	movl	%r13d, %ecx
	leaq	(%r8,%r8), %rax
	movq	%rax, 256(%rsp)         # 8-byte Spill
	movl	$1, %esi
	.align	16, 0x90
.LBB148_118:                            # %for f0.s0.v11.v13.v13
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB148_120 Depth 2
                                        #       Child Loop BB148_122 Depth 3
                                        #     Child Loop BB148_128 Depth 2
                                        #       Child Loop BB148_130 Depth 3
                                        #       Child Loop BB148_199 Depth 3
                                        #       Child Loop BB148_226 Depth 3
                                        #     Child Loop BB148_164 Depth 2
                                        #       Child Loop BB148_166 Depth 3
                                        #     Child Loop BB148_297 Depth 2
                                        #       Child Loop BB148_298 Depth 3
                                        #         Child Loop BB148_299 Depth 4
	movq	%rsi, 400(%rsp)         # 8-byte Spill
	movl	%edx, 408(%rsp)         # 4-byte Spill
	movq	%rdi, 496(%rsp)         # 8-byte Spill
	movl	%ecx, 508(%rsp)         # 4-byte Spill
	movq	%rbx, 680(%rsp)         # 8-byte Spill
	movq	824(%rsp), %rax         # 8-byte Reload
	cmpl	%ecx, %eax
	movl	%ecx, %esi
	cmovgel	%eax, %esi
	movl	%esi, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	%ecx, 1080(%rsp)        # 4-byte Spill
	movl	372(%rsp), %ecx         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	%esi, 412(%rsp)         # 4-byte Spill
	leal	8(%rbx,%rdi,2), %edx
	movl	%edx, 532(%rsp)         # 4-byte Spill
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovgel	%eax, %esi
	leal	10(%rbx,%rdi,2), %eax
	movl	%eax, 732(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	movl	%eax, %edi
	cmovgl	%esi, %edi
	movl	%edi, 888(%rsp)         # 4-byte Spill
	cmpl	%esi, %ecx
	cmovgel	%ecx, %esi
	movl	%esi, 816(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovgl	%esi, %eax
	movl	%eax, 728(%rsp)         # 4-byte Spill
	cmpl	%edi, %edx
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm14 # ymm14 = <4,u,5,u,6,u,7,u>
	vmovaps	.LCPI148_10(%rip), %ymm13 # ymm13 = <u,0,u,1,u,2,u,3>
	jge	.LBB148_126
# BB#119:                               # %for deinterleaved$1.s0.v1159.preheader
                                        #   in Loop: Header=BB148_118 Depth=1
	movslq	508(%rsp), %rax         # 4-byte Folded Reload
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	movl	532(%rsp), %eax         # 4-byte Reload
	.align	16, 0x90
.LBB148_120:                            # %for deinterleaved$1.s0.v1159
                                        #   Parent Loop BB148_118 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB148_122 Depth 3
	movl	%eax, 1088(%rsp)        # 4-byte Spill
	cmpl	$0, 956(%rsp)           # 4-byte Folded Reload
	jle	.LBB148_161
# BB#121:                               # %for deinterleaved$1.s0.v10.v1061.preheader
                                        #   in Loop: Header=BB148_120 Depth=2
	movq	1152(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	movq	824(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	804(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	800(%rsp), %eax         # 4-byte Folded Reload
	movq	808(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %edx
	leal	(%rdx,%rax), %ecx
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %ecx
	notl	%ecx
	cmovgl	%eax, %ecx
	movq	792(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %eax
	subl	%ecx, %eax
	cmpq	%rdi, %rdx
	movl	%edx, %ecx
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpq	%rdi, 784(%rsp)         # 8-byte Folded Reload
	cmovlel	%eax, %ecx
	cmpq	%rsi, %rdi
	cmovll	%eax, %ecx
	movq	960(%rsp), %rax         # 8-byte Reload
	imull	%eax, %ecx
	movl	%edi, %r8d
	andl	$1, %r8d
	movl	%r8d, 1760(%rsp)        # 4-byte Spill
	vmovd	%ecx, %xmm0
	vpsubd	896(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm0, 1728(%rsp)       # 32-byte Spill
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	movl	956(%rsp), %edx         # 4-byte Reload
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r10d
	.align	16, 0x90
.LBB148_122:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_120 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r10, 2656(%rsp)        # 8-byte Spill
	movl	%edx, 1696(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	1664(%rsp)              # 1-byte Folded Spill
	setne	1600(%rsp)              # 1-byte Folded Spill
	movq	1952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vmovdqa	.LCPI148_7(%rip), %ymm13 # ymm13 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm13, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vpextrd	$1, %xmm3, %eax
	vmovdqa	2176(%rsp), %xmm0       # 16-byte Reload
	vpextrd	$1, %xmm0, %r8d
	movl	%r8d, 1400(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	vmovd	%xmm3, %eax
	vmovd	%xmm0, %ebx
	movl	%ebx, 1792(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r15d
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm0, %ecx
	movl	%ecx, 1488(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm0, %edi
	movl	%edi, 1472(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$1, %xmm2, %eax
	vmovdqa	2336(%rsp), %ymm0       # 32-byte Reload
	vpextrd	$1, %xmm0, %esi
	movl	%esi, 1280(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vmovd	%r15d, %xmm3
	vmovd	%xmm2, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1272(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpinsrd	$1, %r9d, %xmm3, %xmm3
	vpextrd	$2, %xmm2, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpinsrd	$2, %r12d, %xmm3, %xmm4
	vpextrd	$3, %xmm2, %eax
	vpextrd	$3, %xmm0, %esi
	movl	%esi, 1216(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovdqa	.LCPI148_6(%rip), %ymm10 # ymm10 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm10, %ymm1, %ymm3
	vextracti128	$1, %ymm3, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$3, %r10d, %xmm4, %xmm9
	vmovd	%r13d, %xmm2
	vmovd	%xmm5, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpinsrd	$1, %r11d, %xmm2, %xmm2
	vpinsrd	$2, %r15d, %xmm2, %xmm12
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%ebx, %xmm4
	vpinsrd	$1, %esi, %xmm4, %xmm4
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpinsrd	$2, %ecx, %xmm4, %xmm4
	vpextrd	$1, %xmm3, %eax
	vpextrd	$1, %xmm0, %ecx
	movl	%ecx, 1496(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, %esi, %xmm4, %xmm5
	vmovd	%xmm3, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1480(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	vmovd	%edx, %xmm4
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1464(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm0, %ecx
	movl	%ecx, 1392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vpinsrd	$2, %esi, %xmm4, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm6
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm7
	vmovd	%ecx, %xmm3
	movq	%rcx, %rdi
	vpbroadcastd	%xmm3, %ymm4
	vmovdqa	2304(%rsp), %ymm0       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm0, %ymm3
	vmovdqa	.LCPI148_3(%rip), %ymm0 # ymm0 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vmovdqa	%ymm0, %ymm11
	vpshufb	%ymm11, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vmovdqa	2272(%rsp), %ymm0       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm0, %ymm8
	vpshufb	%ymm11, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm0 # xmm0 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vmovdqa	%xmm0, %xmm2
	vpshufb	%xmm2, %xmm8, %xmm0
	vpshufb	%xmm2, %xmm3, %xmm3
	vpunpcklqdq	%xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0]
	vmovdqa	2112(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm1, %ymm3
	vpshufb	%ymm11, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vmovdqa	2080(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm1, %ymm8
	vpshufb	%ymm11, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm2, %xmm8, %xmm1
	vpshufb	%xmm2, %xmm3, %xmm3
	vpunpcklqdq	%xmm1, %xmm3, %xmm1 # xmm1 = xmm3[0],xmm1[0]
	vpxor	.LCPI148_5(%rip), %xmm0, %xmm0
	vpor	%xmm0, %xmm1, %xmm3
	vinserti128	$1, %xmm5, %ymm6, %ymm0
	vpsrad	$31, %ymm0, %ymm1
	vmovdqa	992(%rsp), %ymm15       # 32-byte Reload
	vpand	%ymm1, %ymm15, %ymm1
	vmovdqa	2464(%rsp), %ymm8       # 32-byte Reload
	vpaddd	%ymm0, %ymm8, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm1
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovdqa	2400(%rsp), %ymm11      # 32-byte Reload
	vpsubd	%ymm0, %ymm11, %ymm0
	vpbroadcastd	%xmm7, %ymm5
	vpaddd	%ymm10, %ymm5, %ymm1
	vmovdqa	2528(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm1, %xmm6
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vmovdqa	2512(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm6, %ymm1
	vpunpckhbw	%xmm3, %xmm3, %xmm6 # xmm6 = xmm3[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm0, %ymm1, %ymm0
	vpinsrd	$3, %r9d, %xmm12, %xmm6
	vmovdqa	1728(%rsp), %ymm12      # 32-byte Reload
	vpaddd	%ymm0, %ymm12, %ymm2
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rax
	movq	%rcx, %r8
	movq	%rax, 1536(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rcx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vinserti128	$1, %xmm9, %ymm6, %ymm1
	vpsrad	$31, %ymm1, %ymm6
	vpand	%ymm6, %ymm15, %ymm6
	vpaddd	%ymm1, %ymm8, %ymm1
	vmovdqa	%ymm8, %ymm9
	vpaddd	%ymm6, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm6
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm6, %ymm1
	vpaddd	%ymm13, %ymm5, %ymm5
	vpminsd	%xmm14, %xmm5, %xmm6
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	vpsubd	%ymm1, %ymm11, %ymm1
	vpmovzxbd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	vpslld	$31, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm5, %ymm1
	vpaddd	%ymm1, %ymm12, %ymm1
	vmovq	%xmm1, %r9
	movslq	%r9d, %rax
	movq	%rax, 1824(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm1, %rbx
	movslq	%ebx, %rax
	movq	%rax, 1888(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rdi, %rax
	movslq	%esi, %rdx
	movq	%rdx, 1856(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm1, %rdi
	movslq	%edi, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	sarq	$32, %r8
	movq	%r8, 2592(%rsp)         # 8-byte Spill
	vpextrq	$1, %xmm2, %r12
	movslq	%r12d, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	movq	%r12, 1448(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm1
	vmovq	%xmm1, %r11
	movslq	%r11d, %rcx
	movq	%rcx, 1920(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	%r11, 1408(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r13
	movslq	%r13d, %r15
	movq	%r15, 1344(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movq	%r13, 1456(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	movq	2552(%rsp), %rdx        # 8-byte Reload
	andl	$1, %r8d
	sete	%al
	andb	1600(%rsp), %al         # 1-byte Folded Reload
	movq	2592(%rsp), %r10        # 8-byte Reload
	vpinsrw	$1, (%rdx,%r10,2), %xmm0, %xmm0
	movq	2560(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%r12,2), %xmm0, %xmm0
	movq	1920(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%r11,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdx,%r15,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%r13,2), %xmm0, %xmm3
	jne	.LBB148_123
# BB#132:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movb	%al, 1600(%rsp)         # 1-byte Spill
	vpxor	%ymm0, %ymm0, %ymm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	jmp	.LBB148_133
	.align	16, 0x90
.LBB148_123:                            #   in Loop: Header=BB148_122 Depth=3
	movb	%al, 1600(%rsp)         # 1-byte Spill
	movq	1824(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdx,%r9,2), %xmm0, %xmm0
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%rbx,2), %xmm0, %xmm0
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%rsi,2), %xmm0, %xmm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%rdi,2), %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
.LBB148_133:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	2656(%rsp), %r10        # 8-byte Reload
	jne	.LBB148_134
# BB#135:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vmovdqa	%ymm0, 1312(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1504(%rsp)         # 8-byte Spill
	movq	%rsi, 1512(%rsp)        # 8-byte Spill
	movq	%rbx, 1520(%rsp)        # 8-byte Spill
	movq	%rdi, 1528(%rsp)        # 8-byte Spill
	movl	%r8d, 1568(%rsp)        # 4-byte Spill
	vpxor	%ymm3, %ymm3, %ymm3
	jmp	.LBB148_136
	.align	16, 0x90
.LBB148_134:                            #   in Loop: Header=BB148_122 Depth=3
	vmovdqa	%ymm0, 1312(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1504(%rsp)         # 8-byte Spill
	movq	%rsi, 1512(%rsp)        # 8-byte Spill
	movq	%rbx, 1520(%rsp)        # 8-byte Spill
	movq	%rdi, 1528(%rsp)        # 8-byte Spill
	movl	%r8d, 1568(%rsp)        # 4-byte Spill
	vpmovzxwd	%xmm3, %ymm0    # ymm0 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vcvtdq2ps	%ymm0, %ymm3
.LBB148_136:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	%rdx, %rbx
	movq	1984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm5
	vmovdqa	%ymm13, %ymm12
	vpaddd	%ymm12, %ymm5, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	1400(%rsp), %r12d       # 4-byte Reload
	idivl	%r12d
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	1792(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	1488(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, 1208(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	movl	1472(%rsp), %r10d       # 4-byte Reload
	idivl	%r10d
	movl	%edx, %r15d
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vmovd	%xmm6, %eax
	cltd
	idivl	1272(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vpaddd	%ymm10, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 1400(%rsp)        # 4-byte Spill
	vmovd	%r11d, %xmm6
	vpinsrd	$1, 1200(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vmovdqa	2240(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm1, %ymm7
	vmovdqa	.LCPI148_3(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2208(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm1, %ymm8
	vpshufb	%ymm13, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm14 # xmm14 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm14, %xmm8, %xmm1
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0],xmm1[0]
	vpxor	.LCPI148_5(%rip), %xmm1, %xmm1
	vmovdqa	2048(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm7, %ymm7
	vpshufb	%ymm13, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2016(%rsp), %ymm8       # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm8, %ymm4
	vpshufb	%ymm13, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vpshufb	%xmm14, %xmm4, %xmm4
	vpshufb	%xmm14, %xmm7, %xmm7
	vpunpcklqdq	%xmm4, %xmm7, %xmm4 # xmm4 = xmm7[0],xmm4[0]
	vpor	%xmm1, %xmm4, %xmm4
	vpinsrd	$2, 1208(%rsp), %xmm6, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r15d, %xmm1, %xmm1
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r8d
	vmovd	%edi, %xmm6
	vpinsrd	$1, %r13d, %xmm6, %xmm6
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r9d
	vpinsrd	$2, %esi, %xmm6, %xmm6
	vpinsrd	$3, %ecx, %xmm6, %xmm6
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vinserti128	$1, %xmm1, %ymm6, %ymm0
	vpsrad	$31, %ymm0, %ymm1
	vpand	%ymm15, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm9, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm1
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpsubd	%ymm0, %ymm11, %ymm0
	movq	2656(%rsp), %rax        # 8-byte Reload
	leal	-7(%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm6
	vpaddd	%ymm12, %ymm6, %ymm1
	vmovdqa	2528(%rsp), %xmm2       # 16-byte Reload
	vpminsd	%xmm2, %xmm1, %xmm7
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm2, %xmm1, %xmm1
	vmovdqa	2512(%rsp), %xmm12      # 16-byte Reload
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm7, %ymm1
	vpmovzxbd	%xmm4, %ymm7    # ymm7 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	vpslld	$31, %ymm7, %ymm7
	vblendvps	%ymm7, %ymm0, %ymm1, %ymm0
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	1496(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%r8d, %xmm1
	vpinsrd	$1, 1400(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovd	%xmm5, %eax
	cltd
	idivl	1480(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpinsrd	$2, %r9d, %xmm1, %xmm1
	vpinsrd	$3, %edi, %xmm1, %xmm1
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	1464(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm5, %eax
	vmovd	%esi, %xmm5
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	cltd
	idivl	1392(%rsp)              # 4-byte Folded Reload
	vpinsrd	$2, %edi, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vinserti128	$1, %xmm1, %ymm5, %ymm1
	vpsrad	$31, %ymm1, %ymm5
	vpand	%ymm15, %ymm5, %ymm5
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm5, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm5
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm5, %ymm1
	vpaddd	%ymm10, %ymm6, %ymm5
	vpminsd	%xmm2, %xmm5, %xmm6
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm2, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm6, %ymm5
	vpsubd	%ymm1, %ymm11, %ymm1
	vpunpckhbw	%xmm4, %xmm4, %xmm4 # xmm4 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm5, %ymm1
	vmovdqa	1728(%rsp), %ymm2       # 32-byte Reload
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm0, %ymm2, %ymm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vextracti128	$1, %ymm0, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rdi
	vextracti128	$1, %ymm1, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %r10
	movq	%r10, 1208(%rsp)        # 8-byte Spill
	movslq	%esi, %r13
	sarq	$32, %rsi
	movslq	%edx, %r8
	sarq	$32, %rdx
	movslq	%edi, %r12
	sarq	$32, %rdi
	movslq	%eax, %r11
	sarq	$32, %rax
	vpextrq	$1, %xmm1, 1400(%rsp)   # 8-byte Folded Spill
	sarq	$32, %r10
	movslq	%r9d, %rcx
	movq	%rcx, 1792(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	movl	1760(%rsp), %r15d       # 4-byte Reload
	movq	2656(%rsp), %rcx        # 8-byte Reload
	andl	%ecx, %r15d
	setne	%cl
	vmovq	%xmm1, 1216(%rsp)       # 8-byte Folded Spill
	jne	.LBB148_137
# BB#138:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	%r13, 1264(%rsp)        # 8-byte Spill
	movq	%r8, 1272(%rsp)         # 8-byte Spill
	movq	%r12, 1280(%rsp)        # 8-byte Spill
	movq	%r11, 1392(%rsp)        # 8-byte Spill
	movq	%rdi, 1464(%rsp)        # 8-byte Spill
	movq	%rsi, 1472(%rsp)        # 8-byte Spill
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	movq	%rdx, 1488(%rsp)        # 8-byte Spill
	movb	%cl, 1496(%rsp)         # 1-byte Spill
	vxorps	%ymm4, %ymm4, %ymm4
	jmp	.LBB148_139
	.align	16, 0x90
.LBB148_137:                            #   in Loop: Header=BB148_122 Depth=3
	movb	%cl, 1496(%rsp)         # 1-byte Spill
	movzwl	(%rbx,%r13,2), %ecx
	movq	%r13, 1264(%rsp)        # 8-byte Spill
	vmovd	%ecx, %xmm0
	vpinsrw	$1, (%rbx,%rsi,2), %xmm0, %xmm0
	movq	%rsi, 1472(%rsp)        # 8-byte Spill
	vpinsrw	$2, (%rbx,%r8,2), %xmm0, %xmm0
	movq	%r8, 1272(%rsp)         # 8-byte Spill
	vpinsrw	$3, (%rbx,%rdx,2), %xmm0, %xmm0
	movq	%rdx, 1488(%rsp)        # 8-byte Spill
	vpinsrw	$4, (%rbx,%r12,2), %xmm0, %xmm0
	movq	%r12, 1280(%rsp)        # 8-byte Spill
	vpinsrw	$5, (%rbx,%rdi,2), %xmm0, %xmm0
	movq	%rdi, 1464(%rsp)        # 8-byte Spill
	vpinsrw	$6, (%rbx,%r11,2), %xmm0, %xmm0
	movq	%r11, 1392(%rsp)        # 8-byte Spill
	vpinsrw	$7, (%rbx,%rax,2), %xmm0, %xmm0
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm4
.LBB148_139:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	2504(%rsp), %r11        # 8-byte Reload
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm14 # ymm14 = <4,u,5,u,6,u,7,u>
	vmovaps	.LCPI148_10(%rip), %ymm13 # ymm13 = <u,0,u,1,u,2,u,3>
	vmovaps	1312(%rsp), %ymm2       # 32-byte Reload
	movq	1216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movq	1400(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %r13
	sarq	$32, %rdi
	movq	1208(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %r12
	movzwl	(%rbx,%r10,2), %edx
	movq	1792(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rbx,%rsi,2), %esi
	movzwl	(%rbx,%r9,2), %r8d
	testl	%r15d, %r15d
	jne	.LBB148_140
# BB#141:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movl	%r8d, 1200(%rsp)        # 4-byte Spill
	movl	%esi, 1208(%rsp)        # 4-byte Spill
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	movq	%r10, 1312(%rsp)        # 8-byte Spill
	movq	%r9, 1400(%rsp)         # 8-byte Spill
	vxorps	%ymm5, %ymm5, %ymm5
	movq	1280(%rsp), %r8         # 8-byte Reload
	movq	1272(%rsp), %rdx        # 8-byte Reload
	movq	%rax, %r9
	jmp	.LBB148_142
	.align	16, 0x90
.LBB148_140:                            #   in Loop: Header=BB148_122 Depth=3
	movq	%r10, 1312(%rsp)        # 8-byte Spill
	movq	%r9, 1400(%rsp)         # 8-byte Spill
	movq	%rax, %r9
	movzwl	(%rbx,%r9,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rbx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$2, (%rbx,%r13,2), %xmm0, %xmm0
	vpinsrw	$3, (%rbx,%rdi,2), %xmm0, %xmm0
	vpinsrw	$4, (%rbx,%r12,2), %xmm0, %xmm0
	vpinsrw	$5, %edx, %xmm0, %xmm0
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	vpinsrw	$6, %esi, %xmm0, %xmm0
	movl	%esi, 1208(%rsp)        # 4-byte Spill
	vpinsrw	$7, %r8d, %xmm0, %xmm0
	movl	%r8d, 1200(%rsp)        # 4-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm5
	movq	1280(%rsp), %r8         # 8-byte Reload
	movq	1272(%rsp), %rdx        # 8-byte Reload
.LBB148_142:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	1264(%rsp), %rax        # 8-byte Reload
	vpermps	%ymm5, %ymm11, %ymm0
	vpermps	%ymm3, %ymm14, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm5, %ymm13, %ymm1
	vmovaps	.LCPI148_11(%rip), %ymm5 # ymm5 = <0,u,1,u,2,u,3,u>
	vmovaps	%ymm5, %ymm6
	vpermps	%ymm3, %ymm6, %ymm3
	vblendps	$170, %ymm1, %ymm3, %ymm1 # ymm1 = ymm3[0],ymm1[1],ymm3[2],ymm1[3],ymm3[4],ymm1[5],ymm3[6],ymm1[7]
	vpermps	%ymm4, %ymm11, %ymm3
	vpermps	%ymm2, %ymm14, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm4, %ymm13, %ymm4
	vpermps	%ymm2, %ymm6, %ymm2
	vmovaps	%ymm6, %ymm10
	vblendps	$170, %ymm4, %ymm2, %ymm2 # ymm2 = ymm2[0],ymm4[1],ymm2[2],ymm4[3],ymm2[4],ymm4[5],ymm2[6],ymm4[7]
	movq	2656(%rsp), %r10        # 8-byte Reload
	movslq	%r10d, %rsi
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	addq	1120(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm2, (%r11,%rsi,4)
	vmovups	%ymm3, 32(%r11,%rsi,4)
	vmovups	%ymm1, 64(%r11,%rsi,4)
	vmovups	%ymm0, 96(%r11,%rsi,4)
	movzwl	(%rbx,%rax,2), %eax
	vmovd	%eax, %xmm0
	movq	1472(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rbx,%rax,2), %xmm0, %xmm0
	vpinsrw	$2, (%rbx,%rdx,2), %xmm0, %xmm0
	movq	1488(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rbx,%rax,2), %xmm0, %xmm0
	vpinsrw	$4, (%rbx,%r8,2), %xmm0, %xmm0
	movq	1464(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rbx,%rax,2), %xmm0, %xmm0
	movq	1392(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rbx,%rax,2), %xmm0, %xmm0
	movq	1480(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rbx,%rax,2), %xmm0, %xmm0
	movzwl	(%rbx,%r9,2), %eax
	vmovd	%eax, %xmm1
	vpinsrw	$1, (%rbx,%rcx,2), %xmm1, %xmm1
	vpinsrw	$2, (%rbx,%r13,2), %xmm1, %xmm1
	vpinsrw	$3, (%rbx,%rdi,2), %xmm1, %xmm1
	vpinsrw	$4, (%rbx,%r12,2), %xmm1, %xmm2
	movq	1536(%rsp), %rax        # 8-byte Reload
	movzwl	(%rbx,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	2592(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rbx,%rax,2), %xmm1, %xmm1
	movq	2560(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1448(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1920(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1408(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1344(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1456(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rbx,%rax,2), %xmm1, %xmm4
	movq	1824(%rsp), %rax        # 8-byte Reload
	movzwl	(%rbx,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1504(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1520(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1512(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rbx,%rax,2), %xmm1, %xmm1
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rbx,%rax,2), %xmm1, %xmm1
	movq	1528(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rbx,%rax,2), %xmm1, %xmm1
	movl	%r10d, %edx
	movq	1152(%rsp), %rax        # 8-byte Reload
	orl	%eax, %edx
	andl	$1, %edx
	sete	%al
	vpmovzxwd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm6
	vmovaps	%ymm6, %ymm3
	movb	1496(%rsp), %cl         # 1-byte Reload
	je	.LBB148_144
# BB#143:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm3, %ymm3, %ymm3
.LBB148_144:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vpmovzxwd	%xmm4, %ymm5    # ymm5 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm7, %ymm4
	orb	%cl, %al
	vmovaps	%ymm4, %ymm7
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1660(%rsp), %r15d       # 4-byte Reload
	movl	1760(%rsp), %r8d        # 4-byte Reload
	movb	1664(%rsp), %al         # 1-byte Reload
	jne	.LBB148_146
# BB#145:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_146:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vcvtdq2ps	%ymm5, %ymm5
	vmovaps	%ymm5, %ymm8
	jne	.LBB148_148
# BB#147:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm8, %ymm8, %ymm8
.LBB148_148:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movl	1568(%rsp), %edi        # 4-byte Reload
	andb	%dil, %al
	jne	.LBB148_150
# BB#149:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_150:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	jne	.LBB148_152
# BB#151:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_152:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	orb	1600(%rsp), %al         # 1-byte Folded Reload
	jne	.LBB148_154
# BB#153:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_154:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	jne	.LBB148_155
# BB#156:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vpxor	%ymm9, %ymm9, %ymm9
	jmp	.LBB148_157
	.align	16, 0x90
.LBB148_155:                            #   in Loop: Header=BB148_122 Depth=3
	vpinsrw	$5, 1216(%rsp), %xmm2, %xmm0 # 4-byte Folded Reload
	vpinsrw	$6, 1208(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrw	$7, 1200(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm9
.LBB148_157:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	2392(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vpermps	%ymm9, %ymm11, %ymm0
	vpermps	%ymm8, %ymm14, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm9, %ymm13, %ymm1
	vpermps	%ymm8, %ymm10, %ymm8
	vblendps	$170, %ymm1, %ymm8, %ymm1 # ymm1 = ymm8[0],ymm1[1],ymm8[2],ymm1[3],ymm8[4],ymm1[5],ymm8[6],ymm1[7]
	vpermps	%ymm6, %ymm11, %ymm8
	vpermps	%ymm7, %ymm14, %ymm9
	vblendps	$170, %ymm8, %ymm9, %ymm8 # ymm8 = ymm9[0],ymm8[1],ymm9[2],ymm8[3],ymm9[4],ymm8[5],ymm9[6],ymm8[7]
	vpermps	%ymm6, %ymm13, %ymm6
	vpermps	%ymm7, %ymm10, %ymm7
	vmovaps	%ymm10, %ymm9
	vblendps	$170, %ymm6, %ymm7, %ymm6 # ymm6 = ymm7[0],ymm6[1],ymm7[2],ymm6[3],ymm7[4],ymm6[5],ymm7[6],ymm6[7]
	vmovups	%ymm6, 12288(%r11,%rax,4)
	vmovups	%ymm8, 12320(%r11,%rax,4)
	vmovups	%ymm1, 12352(%r11,%rax,4)
	vmovups	%ymm0, 12384(%r11,%rax,4)
	testl	%edx, %edx
	je	.LBB148_158
# BB#159:                               # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	movq	%rbx, 2552(%rsp)        # 8-byte Spill
	movl	1696(%rsp), %edx        # 4-byte Reload
	vpxor	%ymm2, %ymm2, %ymm2
	jmp	.LBB148_160
	.align	16, 0x90
.LBB148_158:                            #   in Loop: Header=BB148_122 Depth=3
	movq	1312(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rbx,%rax,2), %xmm2, %xmm0
	movq	1792(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rbx,%rax,2), %xmm0, %xmm0
	movq	1400(%rsp), %rax        # 8-byte Reload
	movzwl	(%rbx,%rax,2), %eax
	movq	%rbx, 2552(%rsp)        # 8-byte Spill
	vpinsrw	$7, %eax, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm2
	movl	1696(%rsp), %edx        # 4-byte Reload
.LBB148_160:                            # %for deinterleaved$1.s0.v10.v1061
                                        #   in Loop: Header=BB148_122 Depth=3
	vpermps	%ymm5, %ymm14, %ymm0
	vpermps	%ymm2, %ymm11, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	vpermps	%ymm5, %ymm9, %ymm1
	vpermps	%ymm2, %ymm13, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm3, %ymm11, %ymm2
	vpermps	%ymm4, %ymm14, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm3, %ymm13, %ymm3
	vpermps	%ymm4, %ymm9, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	addq	2384(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, 24576(%r11,%rsi,4)
	vmovups	%ymm2, 24608(%r11,%rsi,4)
	vmovups	%ymm1, 24640(%r11,%rsi,4)
	vmovups	%ymm0, 24672(%r11,%rsi,4)
	movq	%r11, 2504(%rsp)        # 8-byte Spill
	addl	$32, %r10d
	addl	$-1, %edx
	jne	.LBB148_122
.LBB148_161:                            # %end for deinterleaved$1.s0.v10.v1062
                                        #   in Loop: Header=BB148_120 Depth=2
	movl	1088(%rsp), %eax        # 4-byte Reload
	addl	$1, %eax
	addq	$1, 1152(%rsp)          # 8-byte Folded Spill
	cmpl	888(%rsp), %eax         # 4-byte Folded Reload
	jne	.LBB148_120
.LBB148_126:                            # %end for deinterleaved$1.s0.v1160
                                        #   in Loop: Header=BB148_118 Depth=1
	movl	728(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, 888(%rsp)         # 4-byte Folded Reload
	movq	2504(%rsp), %rcx        # 8-byte Reload
	jge	.LBB148_162
# BB#127:                               #   in Loop: Header=BB148_118 Depth=1
	movl	1080(%rsp), %esi        # 4-byte Reload
	notl	%esi
	movq	960(%rsp), %rax         # 8-byte Reload
	imull	%esi, %eax
	movq	360(%rsp), %rdx         # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movl	%eax, 776(%rsp)         # 4-byte Spill
	movslq	%esi, %rax
	movq	%rax, 2592(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB148_128:                            # %for deinterleaved$1.s0.v1165
                                        #   Parent Loop BB148_118 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB148_130 Depth 3
                                        #       Child Loop BB148_199 Depth 3
                                        #       Child Loop BB148_226 Depth 3
	cmpl	$0, 768(%rsp)           # 4-byte Folded Reload
	jle	.LBB148_197
# BB#129:                               # %for deinterleaved$1.s0.v10.v1067.preheader
                                        #   in Loop: Header=BB148_128 Depth=2
	movq	2592(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	movq	960(%rsp), %rax         # 8-byte Reload
	imull	%ecx, %eax
	andl	$1, %r8d
	movl	%r8d, 1664(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm1
	vmovaps	832(%rsp), %ymm0        # 32-byte Reload
	vinsertf128	$1, %xmm0, %ymm0, %ymm0
	vmovaps	%ymm0, 1088(%rsp)       # 32-byte Spill
	vpsubd	896(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpbroadcastd	%xmm1, %ymm0
	vmovdqa	%ymm0, 1600(%rsp)       # 32-byte Spill
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1080(%rsp)        # 8-byte Spill
	movl	704(%rsp), %edx         # 4-byte Reload
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r10d
	.align	16, 0x90
.LBB148_130:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_128 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r10, 2656(%rsp)        # 8-byte Spill
	movl	%edx, 1568(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	1536(%rsp)              # 1-byte Folded Spill
	setne	1528(%rsp)              # 1-byte Folded Spill
	movq	1952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vmovdqa	.LCPI148_7(%rip), %ymm14 # ymm14 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm14, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vpextrd	$1, %xmm4, %eax
	vmovdqa	2176(%rsp), %xmm0       # 16-byte Reload
	vpextrd	$1, %xmm0, %r13d
	movl	%r13d, 1392(%rsp)       # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	vmovd	%xmm4, %eax
	vmovd	%xmm0, %ebx
	movl	%ebx, 1760(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm0, %ecx
	movl	%ecx, 1696(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm0, %edi
	movl	%edi, 1464(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$1, %xmm3, %eax
	vmovdqa	2336(%rsp), %ymm0       # 32-byte Reload
	vpextrd	$1, %xmm0, %esi
	movl	%esi, 1272(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vmovd	%r8d, %xmm4
	vmovd	%xmm3, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vpinsrd	$1, %r9d, %xmm4, %xmm4
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1216(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpinsrd	$2, %r12d, %xmm4, %xmm5
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm0, %esi
	movl	%esi, 1208(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovdqa	.LCPI148_6(%rip), %ymm12 # ymm12 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm12, %ymm2, %ymm4
	vextracti128	$1, %ymm4, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	vpinsrd	$3, %r10d, %xmm5, %xmm9
	vmovd	%r8d, %xmm3
	vmovd	%xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpinsrd	$1, %r11d, %xmm3, %xmm3
	vpinsrd	$2, %r15d, %xmm3, %xmm13
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%ebx, %xmm5
	vpinsrd	$1, %esi, %xmm5, %xmm5
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpinsrd	$2, %ecx, %xmm5, %xmm5
	vpextrd	$1, %xmm4, %eax
	vpextrd	$1, %xmm0, %ecx
	movl	%ecx, 1728(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, %esi, %xmm5, %xmm6
	vmovd	%xmm4, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1472(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	vmovd	%edx, %xmm5
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1456(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm0, %ecx
	movl	%ecx, 1344(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vpinsrd	$2, %esi, %xmm5, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm7
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm0
	vmovd	%ecx, %xmm4
	movq	%rcx, %rdi
	vpbroadcastd	%xmm4, %ymm10
	vmovdqa	2304(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm1, %ymm4
	vmovdqa	.LCPI148_3(%rip), %ymm1 # ymm1 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vmovdqa	%ymm1, %ymm5
	vpshufb	%ymm5, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2272(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm1, %ymm8
	vpshufb	%ymm5, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm1 # xmm1 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vmovdqa	%xmm1, %xmm3
	vpshufb	%xmm3, %xmm8, %xmm1
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm1, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm1[0]
	vmovdqa	2112(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm4
	vpshufb	%ymm5, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2080(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm8
	vpshufb	%ymm5, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm3, %xmm8, %xmm2
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm2, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm4
	vinserti128	$1, %xmm6, %ymm7, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vmovdqa	1088(%rsp), %ymm8       # 32-byte Reload
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	2464(%rsp), %ymm11      # 32-byte Reload
	vpaddd	%ymm1, %ymm11, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	2400(%rsp), %ymm15      # 32-byte Reload
	vpsubd	%ymm1, %ymm15, %ymm1
	vpbroadcastd	%xmm0, %ymm6
	vpaddd	%ymm12, %ymm6, %ymm0
	vmovdqa	2528(%rsp), %xmm5       # 16-byte Reload
	vpminsd	%xmm5, %xmm0, %xmm2
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm5, %xmm0, %xmm0
	vmovdqa	2512(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vpunpckhbw	%xmm4, %xmm4, %xmm2 # xmm2 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm0, %ymm0
	vpinsrd	$3, %r9d, %xmm13, %xmm1
	vmovdqa	1600(%rsp), %ymm13      # 32-byte Reload
	vpaddd	%ymm0, %ymm13, %ymm3
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rax
	movq	%rcx, %r8
	movq	%rax, 1512(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rcx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vinserti128	$1, %xmm9, %ymm1, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	%ymm8, %ymm9
	vpaddd	%ymm1, %ymm11, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm14, %ymm6, %ymm2
	vpminsd	%xmm5, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm5, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm15, %ymm1
	vpmovzxbd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm2, %ymm1
	vpaddd	%ymm1, %ymm13, %ymm1
	vmovq	%xmm1, %r9
	movslq	%r9d, %rax
	movq	%rax, 1792(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm1, %rbx
	movslq	%ebx, %rax
	movq	%rax, 1856(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rdi, %rax
	movslq	%esi, %rdx
	movq	%rdx, 1824(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm1, %rdi
	movslq	%edi, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	sarq	$32, %r8
	movq	%r8, 2560(%rsp)         # 8-byte Spill
	movq	%rcx, %r10
	vpextrq	$1, %xmm3, %r13
	movslq	%r13d, %rcx
	movq	%rcx, 1920(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movq	%r13, 1408(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm3, %xmm1
	vmovq	%xmm1, %r11
	movslq	%r11d, %rcx
	movq	%rcx, 1888(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	%r11, 1400(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r12
	movslq	%r12d, %r15
	movq	%r15, 1312(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	movq	%r12, 1448(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	movq	%r10, %rdx
	andl	$1, %r8d
	sete	%al
	andb	1528(%rsp), %al         # 1-byte Folded Reload
	movq	2560(%rsp), %r10        # 8-byte Reload
	vpinsrw	$1, (%rdx,%r10,2), %xmm0, %xmm0
	movq	1920(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%r13,2), %xmm0, %xmm0
	movq	1888(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%r11,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdx,%r15,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%r12,2), %xmm0, %xmm4
	jne	.LBB148_131
# BB#168:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movb	%al, 1528(%rsp)         # 1-byte Spill
	vpxor	%ymm0, %ymm0, %ymm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	jmp	.LBB148_169
	.align	16, 0x90
.LBB148_131:                            #   in Loop: Header=BB148_130 Depth=3
	movb	%al, 1528(%rsp)         # 1-byte Spill
	movq	1792(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdx,%r9,2), %xmm0, %xmm0
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%rbx,2), %xmm0, %xmm0
	movq	1824(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%rsi,2), %xmm0, %xmm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%rdi,2), %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
.LBB148_169:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movq	2656(%rsp), %r12        # 8-byte Reload
	jne	.LBB148_170
# BB#171:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vmovdqa	%ymm0, 1280(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1480(%rsp)         # 8-byte Spill
	movq	%rsi, 1488(%rsp)        # 8-byte Spill
	movq	%rbx, 1496(%rsp)        # 8-byte Spill
	movq	%rdi, 1504(%rsp)        # 8-byte Spill
	movl	%r8d, 1520(%rsp)        # 4-byte Spill
	vpxor	%ymm4, %ymm4, %ymm4
	jmp	.LBB148_172
	.align	16, 0x90
.LBB148_170:                            #   in Loop: Header=BB148_130 Depth=3
	vmovdqa	%ymm0, 1280(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1480(%rsp)         # 8-byte Spill
	movq	%rsi, 1488(%rsp)        # 8-byte Spill
	movq	%rbx, 1496(%rsp)        # 8-byte Spill
	movq	%rdi, 1504(%rsp)        # 8-byte Spill
	movl	%r8d, 1520(%rsp)        # 4-byte Spill
	vpmovzxwd	%xmm4, %ymm0    # ymm0 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm0, %ymm4
.LBB148_172:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movq	1984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm6
	vmovdqa	%ymm14, %ymm13
	vpaddd	%ymm13, %ymm6, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	1392(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 1152(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	1760(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	1696(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	movl	1464(%rsp), %r10d       # 4-byte Reload
	idivl	%r10d
	movl	%edx, %r15d
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	1272(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vmovd	%xmm7, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	1208(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vpaddd	%ymm12, %ymm6, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r11d, %xmm1
	vpinsrd	$1, 1152(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	2240(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm7
	vmovdqa	.LCPI148_3(%rip), %ymm14 # ymm14 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm14, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2208(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm8
	vpshufb	%ymm14, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm11 # xmm11 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm11, %xmm8, %xmm2
	vpshufb	%xmm11, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm2, %xmm2
	vmovdqa	2048(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm7, %ymm7
	vpshufb	%ymm14, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2016(%rsp), %ymm8       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm8, %ymm5
	vpshufb	%ymm14, %ymm5, %ymm5
	vpermq	$232, %ymm5, %ymm5      # ymm5 = ymm5[0,2,2,3]
	vpshufb	%xmm11, %xmm5, %xmm5
	vpshufb	%xmm11, %xmm7, %xmm7
	vpunpcklqdq	%xmm5, %xmm7, %xmm5 # xmm5 = xmm7[0],xmm5[0]
	vpor	%xmm2, %xmm5, %xmm5
	vpinsrd	$2, 1200(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r15d, %xmm1, %xmm1
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r8d
	vmovd	%edi, %xmm2
	vpinsrd	$1, %r13d, %xmm2, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpinsrd	$2, %ebx, %xmm2, %xmm2
	vpinsrd	$3, %ecx, %xmm2, %xmm2
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vpsrad	$31, %ymm0, %ymm1
	vmovdqa	%ymm9, %ymm8
	vpand	%ymm8, %ymm1, %ymm1
	vmovdqa	2464(%rsp), %ymm9       # 32-byte Reload
	vpaddd	%ymm0, %ymm9, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm1
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpsubd	%ymm0, %ymm15, %ymm0
	leal	-7(%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm7
	vpaddd	%ymm13, %ymm7, %ymm1
	vmovdqa	2528(%rsp), %xmm3       # 16-byte Reload
	vpminsd	%xmm3, %xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm3, %xmm1, %xmm1
	vmovdqa	2512(%rsp), %xmm10      # 16-byte Reload
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	1728(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%r8d, %xmm1
	vpinsrd	$1, %esi, %xmm1, %xmm1
	vmovd	%xmm6, %eax
	cltd
	idivl	1472(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	1456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm6, %eax
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	cltd
	idivl	1344(%rsp)              # 4-byte Folded Reload
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm8, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm12, %ymm7, %ymm2
	vpminsd	%xmm3, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm3, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm15, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm5 # xmm5 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm1, %ymm2, %ymm1
	vmovdqa	1600(%rsp), %ymm2       # 32-byte Reload
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm0, %ymm2, %ymm0
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdx
	vextracti128	$1, %ymm0, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rbx
	vextracti128	$1, %ymm1, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %rdi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%esi, %r12
	sarq	$32, %rsi
	movslq	%ebx, %r15
	sarq	$32, %rbx
	movslq	%eax, %r13
	sarq	$32, %rax
	vpextrq	$1, %xmm1, 1696(%rsp)   # 8-byte Folded Spill
	movq	%rdi, %r11
	sarq	$32, %r11
	movslq	%r9d, %rcx
	movq	%rcx, 1760(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	movl	1664(%rsp), %r8d        # 4-byte Reload
	movq	2656(%rsp), %rcx        # 8-byte Reload
	andl	%ecx, %r8d
	setne	%cl
	vmovq	%xmm1, 1728(%rsp)       # 8-byte Folded Spill
	jne	.LBB148_173
# BB#174:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movq	%r10, 1152(%rsp)        # 8-byte Spill
	movq	%r12, 1200(%rsp)        # 8-byte Spill
	movq	%r15, 1208(%rsp)        # 8-byte Spill
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	movq	%rbx, 1264(%rsp)        # 8-byte Spill
	movq	%rdx, 1272(%rsp)        # 8-byte Spill
	movq	%rax, 1344(%rsp)        # 8-byte Spill
	movq	%rsi, 1392(%rsp)        # 8-byte Spill
	movb	%cl, 1456(%rsp)         # 1-byte Spill
	vxorps	%ymm5, %ymm5, %ymm5
	jmp	.LBB148_175
	.align	16, 0x90
.LBB148_173:                            #   in Loop: Header=BB148_130 Depth=3
	movb	%cl, 1456(%rsp)         # 1-byte Spill
	movq	%rdi, 1472(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rdi,%r10,2), %ecx
	movq	%r10, 1152(%rsp)        # 8-byte Spill
	vmovd	%ecx, %xmm0
	vpinsrw	$1, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	%rdx, 1272(%rsp)        # 8-byte Spill
	vpinsrw	$2, (%rdi,%r12,2), %xmm0, %xmm0
	movq	%r12, 1200(%rsp)        # 8-byte Spill
	vpinsrw	$3, (%rdi,%rsi,2), %xmm0, %xmm0
	movq	%rsi, 1392(%rsp)        # 8-byte Spill
	vpinsrw	$4, (%rdi,%r15,2), %xmm0, %xmm0
	movq	%r15, 1208(%rsp)        # 8-byte Spill
	vpinsrw	$5, (%rdi,%rbx,2), %xmm0, %xmm0
	movq	%rbx, 1264(%rsp)        # 8-byte Spill
	vpinsrw	$6, (%rdi,%r13,2), %xmm0, %xmm0
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movq	1472(%rsp), %rdi        # 8-byte Reload
	movq	%rax, 1344(%rsp)        # 8-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm5
.LBB148_175:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movq	2504(%rsp), %rcx        # 8-byte Reload
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm14 # ymm14 = <4,u,5,u,6,u,7,u>
	vmovaps	.LCPI148_10(%rip), %ymm13 # ymm13 = <u,0,u,1,u,2,u,3>
	vmovaps	1280(%rsp), %ymm3       # 32-byte Reload
	movq	1728(%rsp), %rdx        # 8-byte Reload
	movslq	%edx, %r12
	sarq	$32, %rdx
	movq	1696(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rax
	movq	%rax, 1728(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	movslq	%edi, %rax
	movq	%rax, 1696(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%r11,2), %r13d
	movq	1760(%rsp), %rax        # 8-byte Reload
	movzwl	(%rsi,%rax,2), %r10d
	movzwl	(%rsi,%r9,2), %r15d
	movq	%rsi, %rdi
	testl	%r8d, %r8d
	jne	.LBB148_176
# BB#177:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movl	%r15d, 1464(%rsp)       # 4-byte Spill
	movl	%r13d, 1472(%rsp)       # 4-byte Spill
	movq	%r11, 1120(%rsp)        # 8-byte Spill
	movq	%r9, 1280(%rsp)         # 8-byte Spill
	vpxor	%ymm6, %ymm6, %ymm6
	jmp	.LBB148_178
	.align	16, 0x90
.LBB148_176:                            #   in Loop: Header=BB148_130 Depth=3
	movq	%r11, 1120(%rsp)        # 8-byte Spill
	movq	%r9, 1280(%rsp)         # 8-byte Spill
	movzwl	(%rdi,%r12,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	1728(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdi,%rbx,2), %xmm0, %xmm0
	movq	1696(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$5, %r13d, %xmm0, %xmm0
	movl	%r13d, 1472(%rsp)       # 4-byte Spill
	vpinsrw	$6, %r10d, %xmm0, %xmm0
	vpinsrw	$7, %r15d, %xmm0, %xmm0
	movl	%r15d, 1464(%rsp)       # 4-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm6
.LBB148_178:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movl	%r10d, %r13d
	movq	1216(%rsp), %r8         # 8-byte Reload
	movq	1208(%rsp), %r9         # 8-byte Reload
	movq	1200(%rsp), %r11        # 8-byte Reload
	movq	1152(%rsp), %rax        # 8-byte Reload
	vpermps	%ymm6, %ymm11, %ymm0
	vpermps	%ymm4, %ymm14, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm6, %ymm13, %ymm1
	vmovaps	.LCPI148_11(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vmovaps	%ymm2, %ymm6
	vpermps	%ymm4, %ymm6, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm5, %ymm11, %ymm2
	vpermps	%ymm3, %ymm14, %ymm4
	vblendps	$170, %ymm2, %ymm4, %ymm2 # ymm2 = ymm4[0],ymm2[1],ymm4[2],ymm2[3],ymm4[4],ymm2[5],ymm4[6],ymm2[7]
	vpermps	%ymm5, %ymm13, %ymm4
	vpermps	%ymm3, %ymm6, %ymm3
	vmovaps	%ymm6, %ymm12
	vblendps	$170, %ymm4, %ymm3, %ymm3 # ymm3 = ymm3[0],ymm4[1],ymm3[2],ymm4[3],ymm3[4],ymm4[5],ymm3[6],ymm4[7]
	movq	2656(%rsp), %r10        # 8-byte Reload
	movslq	%r10d, %rsi
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	addq	1080(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, (%rcx,%rsi,4)
	vmovups	%ymm2, 32(%rcx,%rsi,4)
	vmovups	%ymm1, 64(%rcx,%rsi,4)
	vmovups	%ymm0, 96(%rcx,%rsi,4)
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm0
	movq	1272(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$2, (%rdi,%r11,2), %xmm0, %xmm0
	movq	1392(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$4, (%rdi,%r9,2), %xmm0, %xmm0
	movq	1264(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdi,%r8,2), %xmm0, %xmm0
	movq	1344(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movzwl	(%rdi,%r12,2), %eax
	vmovd	%eax, %xmm1
	vpinsrw	$1, (%rdi,%rdx,2), %xmm1, %xmm1
	movq	1728(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	vpinsrw	$3, (%rdi,%rbx,2), %xmm1, %xmm1
	movq	1696(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm3
	movq	1512(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	2560(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1920(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1408(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1400(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1312(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1448(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm5
	movq	1792(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1480(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1496(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1824(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1488(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1504(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm1
	movl	%r10d, %edx
	movq	2592(%rsp), %rax        # 8-byte Reload
	orl	%eax, %edx
	andl	$1, %edx
	sete	%al
	vpmovzxwd	%xmm1, %ymm8    # ymm8 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm7
	vmovaps	%ymm7, %ymm4
	je	.LBB148_180
# BB#179:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_180:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vpmovzxwd	%xmm5, %ymm6    # ymm6 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm8, %ymm5
	orb	1456(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%ymm5, %ymm8
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1664(%rsp), %r8d        # 4-byte Reload
	movb	1536(%rsp), %al         # 1-byte Reload
	movl	1464(%rsp), %edi        # 4-byte Reload
	jne	.LBB148_182
# BB#181:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm8, %ymm8, %ymm8
.LBB148_182:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vcvtdq2ps	%ymm6, %ymm6
	vmovaps	%ymm6, %ymm9
	jne	.LBB148_184
# BB#183:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm9, %ymm9, %ymm9
.LBB148_184:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movl	1520(%rsp), %ebx        # 4-byte Reload
	andb	%bl, %al
	jne	.LBB148_186
# BB#185:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_186:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	jne	.LBB148_188
# BB#187:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_188:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	orb	1528(%rsp), %al         # 1-byte Folded Reload
	jne	.LBB148_190
# BB#189:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_190:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movl	1472(%rsp), %eax        # 4-byte Reload
	jne	.LBB148_191
# BB#192:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vpxor	%ymm10, %ymm10, %ymm10
	jmp	.LBB148_193
	.align	16, 0x90
.LBB148_191:                            #   in Loop: Header=BB148_130 Depth=3
	vpinsrw	$5, %eax, %xmm3, %xmm0
	vpinsrw	$6, %r13d, %xmm0, %xmm0
	vpinsrw	$7, %edi, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm10
.LBB148_193:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movq	2392(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vpermps	%ymm10, %ymm11, %ymm0
	vpermps	%ymm9, %ymm14, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm10, %ymm13, %ymm1
	vpermps	%ymm9, %ymm12, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm7, %ymm11, %ymm2
	vpermps	%ymm8, %ymm14, %ymm9
	vblendps	$170, %ymm2, %ymm9, %ymm2 # ymm2 = ymm9[0],ymm2[1],ymm9[2],ymm2[3],ymm9[4],ymm2[5],ymm9[6],ymm2[7]
	vpermps	%ymm7, %ymm13, %ymm7
	vpermps	%ymm8, %ymm12, %ymm8
	vmovaps	%ymm12, %ymm9
	vblendps	$170, %ymm7, %ymm8, %ymm7 # ymm7 = ymm8[0],ymm7[1],ymm8[2],ymm7[3],ymm8[4],ymm7[5],ymm8[6],ymm7[7]
	vmovups	%ymm7, 12288(%rcx,%rax,4)
	vmovups	%ymm2, 12320(%rcx,%rax,4)
	vmovups	%ymm1, 12352(%rcx,%rax,4)
	vmovups	%ymm0, 12384(%rcx,%rax,4)
	testl	%edx, %edx
	je	.LBB148_194
# BB#195:                               # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	movl	1568(%rsp), %edx        # 4-byte Reload
	vxorps	%ymm2, %ymm2, %ymm2
	jmp	.LBB148_196
	.align	16, 0x90
.LBB148_194:                            #   in Loop: Header=BB148_130 Depth=3
	movq	2552(%rsp), %rax        # 8-byte Reload
	movq	1120(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rax,%rdx,2), %xmm3, %xmm0
	movq	1760(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rax,%rdx,2), %xmm0, %xmm0
	movq	1280(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rax,%rdx,2), %eax
	vpinsrw	$7, %eax, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm2
	movl	1568(%rsp), %edx        # 4-byte Reload
.LBB148_196:                            # %for deinterleaved$1.s0.v10.v1067
                                        #   in Loop: Header=BB148_130 Depth=3
	vpermps	%ymm6, %ymm14, %ymm0
	vpermps	%ymm2, %ymm11, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	vpermps	%ymm6, %ymm9, %ymm1
	vpermps	%ymm2, %ymm13, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm4, %ymm11, %ymm2
	vpermps	%ymm5, %ymm14, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vpermps	%ymm4, %ymm13, %ymm3
	vpermps	%ymm5, %ymm9, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	addq	2384(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, 24576(%rcx,%rsi,4)
	vmovups	%ymm2, 24608(%rcx,%rsi,4)
	vmovups	%ymm1, 24640(%rcx,%rsi,4)
	vmovups	%ymm0, 24672(%rcx,%rsi,4)
	addl	$32, %r10d
	addl	$-1, %edx
	jne	.LBB148_130
.LBB148_197:                            # %end for deinterleaved$1.s0.v10.v1068
                                        #   in Loop: Header=BB148_128 Depth=2
	movl	%r9d, 2172(%rsp)        # 4-byte Spill
	movl	764(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, 768(%rsp)         # 4-byte Folded Reload
	vmovdqa	.LCPI148_13(%rip), %ymm12 # ymm12 = [0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15,16,17,20,21,24,25,28,29,18,19,22,23,26,27,30,31]
	vmovdqa	.LCPI148_14(%rip), %ymm15 # ymm15 = [2,3,6,7,10,11,14,15,0,1,4,5,8,9,12,13,18,19,22,23,26,27,30,31,16,17,20,21,24,25,28,29]
	jge	.LBB148_224
# BB#198:                               # %for deinterleaved$1.s0.v10.v1071.preheader
                                        #   in Loop: Header=BB148_128 Depth=2
	movq	2592(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %r9d
	andl	$1, %r9d
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 2656(%rsp)        # 8-byte Spill
	movl	700(%rsp), %r10d        # 4-byte Reload
	movl	776(%rsp), %r12d        # 4-byte Reload
	movl	708(%rsp), %edx         # 4-byte Reload
	.align	16, 0x90
.LBB148_199:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_128 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rcx, %r8
	testl	%r9d, %r9d
	sete	%bl
	movslq	%r12d, %rax
	movq	2552(%rsp), %rcx        # 8-byte Reload
	vmovdqu	-16(%rcx,%rax,2), %ymm0
	vpblendw	$170, 14(%rcx,%rax,2), %ymm0, %ymm0 # ymm0 = ymm0[0],mem[1],ymm0[2],mem[3],ymm0[4],mem[5],ymm0[6],mem[7],ymm0[8],mem[9],ymm0[10],mem[11],ymm0[12],mem[13],ymm0[14],mem[15]
	movq	%rcx, %rsi
	setne	%cl
	vpshufb	%ymm12, %ymm0, %ymm1
	vperm2i128	$35, %ymm0, %ymm0, %ymm0 # ymm0 = ymm0[2,3,0,1]
	vpshufb	%ymm15, %ymm0, %ymm0
	vpblendd	$60, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0,1],ymm0[2,3,4,5],ymm1[6,7]
	movl	%edx, %r13d
	andl	$1, %r13d
	sete	%r11b
	vextracti128	$1, %ymm0, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	vmovaps	%ymm0, %ymm4
	andb	%cl, %r11b
	jne	.LBB148_201
# BB#200:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_201:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vcvtdq2ps	%ymm1, %ymm1
	vmovaps	%ymm1, %ymm5
	jne	.LBB148_203
# BB#202:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_203:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vmovdqu	-14(%rsi,%rax,2), %ymm2
	vpblendw	$170, 16(%rsi,%rax,2), %ymm2, %ymm2 # ymm2 = ymm2[0],mem[1],ymm2[2],mem[3],ymm2[4],mem[5],ymm2[6],mem[7],ymm2[8],mem[9],ymm2[10],mem[11],ymm2[12],mem[13],ymm2[14],mem[15]
	vpshufb	%ymm12, %ymm2, %ymm3
	vperm2f128	$35, %ymm2, %ymm0, %ymm2 # ymm2 = ymm2[2,3,0,1]
	vpshufb	%ymm15, %ymm2, %ymm2
	vpblendd	$60, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0,1],ymm2[2,3,4,5],ymm3[6,7]
	movl	%r9d, %eax
	andl	%edx, %eax
	setne	%r15b
	vpmovzxwd	%xmm2, %ymm3    # ymm3 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm3, %ymm3
	vmovaps	%ymm3, %ymm6
	jne	.LBB148_205
# BB#204:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_205:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vextracti128	$1, %ymm2, %xmm2
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vcvtdq2ps	%ymm2, %ymm2
	vmovaps	%ymm2, %ymm7
	testl	%eax, %eax
	jne	.LBB148_207
# BB#206:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_207:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vpermps	%ymm7, %ymm11, %ymm8
	vpermps	%ymm5, %ymm14, %ymm9
	vblendps	$170, %ymm8, %ymm9, %ymm8 # ymm8 = ymm9[0],ymm8[1],ymm9[2],ymm8[3],ymm9[4],ymm8[5],ymm9[6],ymm8[7]
	vpermps	%ymm7, %ymm13, %ymm7
	vmovaps	.LCPI148_11(%rip), %ymm9 # ymm9 = <0,u,1,u,2,u,3,u>
	vmovaps	%ymm9, %ymm10
	vpermps	%ymm5, %ymm10, %ymm5
	vblendps	$170, %ymm7, %ymm5, %ymm5 # ymm5 = ymm5[0],ymm7[1],ymm5[2],ymm7[3],ymm5[4],ymm7[5],ymm5[6],ymm7[7]
	vpermps	%ymm6, %ymm11, %ymm7
	vpermps	%ymm4, %ymm14, %ymm9
	vblendps	$170, %ymm7, %ymm9, %ymm7 # ymm7 = ymm9[0],ymm7[1],ymm9[2],ymm7[3],ymm9[4],ymm7[5],ymm9[6],ymm7[7]
	vpermps	%ymm6, %ymm13, %ymm6
	vpermps	%ymm4, %ymm10, %ymm4
	vblendps	$170, %ymm6, %ymm4, %ymm4 # ymm4 = ymm4[0],ymm6[1],ymm4[2],ymm6[3],ymm4[4],ymm6[5],ymm4[6],ymm6[7]
	movslq	%edx, %rax
	subq	2456(%rsp), %rax        # 8-byte Folded Reload
	addq	2656(%rsp), %rax        # 8-byte Folded Reload
	vmovups	%ymm4, (%r8,%rax,4)
	vmovups	%ymm7, 32(%r8,%rax,4)
	vmovups	%ymm5, 64(%r8,%rax,4)
	vmovups	%ymm8, 96(%r8,%rax,4)
	movl	%edx, %edi
	movq	2592(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %edi
	andl	$1, %edi
	sete	%cl
	vmovaps	%ymm3, %ymm4
	je	.LBB148_209
# BB#208:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_209:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	orb	%r15b, %cl
	vmovaps	%ymm0, %ymm5
	jne	.LBB148_211
# BB#210:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_211:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vmovaps	%ymm1, %ymm6
	jne	.LBB148_213
# BB#212:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_213:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	andb	%r13b, %bl
	jne	.LBB148_215
# BB#214:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm0, %ymm0, %ymm0
.LBB148_215:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	jne	.LBB148_217
# BB#216:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm1, %ymm1, %ymm1
.LBB148_217:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	orb	%r11b, %bl
	jne	.LBB148_219
# BB#218:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm3, %ymm3, %ymm3
.LBB148_219:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vmovaps	%ymm2, %ymm7
	jne	.LBB148_221
# BB#220:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_221:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	movq	2392(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rcx
	vpermps	%ymm7, %ymm11, %ymm8
	vpermps	%ymm6, %ymm14, %ymm9
	vblendps	$170, %ymm8, %ymm9, %ymm8 # ymm8 = ymm9[0],ymm8[1],ymm9[2],ymm8[3],ymm9[4],ymm8[5],ymm9[6],ymm8[7]
	vpermps	%ymm7, %ymm13, %ymm7
	vpermps	%ymm6, %ymm10, %ymm6
	vblendps	$170, %ymm7, %ymm6, %ymm6 # ymm6 = ymm6[0],ymm7[1],ymm6[2],ymm7[3],ymm6[4],ymm7[5],ymm6[6],ymm7[7]
	vpermps	%ymm3, %ymm11, %ymm7
	vpermps	%ymm5, %ymm14, %ymm9
	vblendps	$170, %ymm7, %ymm9, %ymm7 # ymm7 = ymm9[0],ymm7[1],ymm9[2],ymm7[3],ymm9[4],ymm7[5],ymm9[6],ymm7[7]
	vpermps	%ymm3, %ymm13, %ymm3
	vpermps	%ymm5, %ymm10, %ymm5
	vmovaps	%ymm10, %ymm9
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vmovups	%ymm3, 12288(%r8,%rcx,4)
	vmovups	%ymm7, 12320(%r8,%rcx,4)
	vmovups	%ymm6, 12352(%r8,%rcx,4)
	vmovups	%ymm8, 12384(%r8,%rcx,4)
	movq	%r8, %rcx
	testl	%edi, %edi
	je	.LBB148_223
# BB#222:                               # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vxorps	%ymm2, %ymm2, %ymm2
.LBB148_223:                            # %for deinterleaved$1.s0.v10.v1071
                                        #   in Loop: Header=BB148_199 Depth=3
	vpermps	%ymm2, %ymm11, %ymm3
	vpermps	%ymm1, %ymm14, %ymm5
	vblendps	$170, %ymm3, %ymm5, %ymm3 # ymm3 = ymm5[0],ymm3[1],ymm5[2],ymm3[3],ymm5[4],ymm3[5],ymm5[6],ymm3[7]
	vpermps	%ymm2, %ymm13, %ymm2
	vpermps	%ymm1, %ymm9, %ymm1
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm4, %ymm11, %ymm2
	vpermps	%ymm0, %ymm14, %ymm5
	vblendps	$170, %ymm2, %ymm5, %ymm2 # ymm2 = ymm5[0],ymm2[1],ymm5[2],ymm2[3],ymm5[4],ymm2[5],ymm5[6],ymm2[7]
	vpermps	%ymm4, %ymm13, %ymm4
	vpermps	%ymm0, %ymm9, %ymm0
	vblendps	$170, %ymm4, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm4[1],ymm0[2],ymm4[3],ymm0[4],ymm4[5],ymm0[6],ymm4[7]
	addq	2384(%rsp), %rax        # 8-byte Folded Reload
	vmovups	%ymm0, 24576(%rcx,%rax,4)
	vmovups	%ymm2, 24608(%rcx,%rax,4)
	vmovups	%ymm1, 24640(%rcx,%rax,4)
	vmovups	%ymm3, 24672(%rcx,%rax,4)
	addl	$32, %edx
	addl	$32, %r12d
	addl	$-1, %r10d
	jne	.LBB148_199
.LBB148_224:                            # %end for deinterleaved$1.s0.v10.v1072
                                        #   in Loop: Header=BB148_128 Depth=2
	movl	764(%rsp), %eax         # 4-byte Reload
	cmpl	956(%rsp), %eax         # 4-byte Folded Reload
	jge	.LBB148_257
# BB#225:                               # %for deinterleaved$1.s0.v10.v1075.preheader
                                        #   in Loop: Header=BB148_128 Depth=2
	movq	2592(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %r8d
	movq	960(%rsp), %rax         # 8-byte Reload
	imull	%ecx, %eax
	andl	$1, %r8d
	movl	%r8d, 1600(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm0
	vmovaps	832(%rsp), %ymm1        # 32-byte Reload
	vinsertf128	$1, %xmm1, %ymm1, %ymm1
	vmovaps	%ymm1, 1152(%rsp)       # 32-byte Spill
	vpsubd	896(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpbroadcastd	%xmm0, %ymm15
	vmovdqa	%ymm15, 1120(%rsp)      # 32-byte Spill
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	movl	696(%rsp), %edx         # 4-byte Reload
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edi
	.align	16, 0x90
.LBB148_226:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_128 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%rdi, 2656(%rsp)        # 8-byte Spill
	movl	%edx, 1568(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	1536(%rsp)              # 1-byte Folded Spill
	setne	1520(%rsp)              # 1-byte Folded Spill
	movq	1056(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm2
	vmovdqa	.LCPI148_7(%rip), %ymm9 # ymm9 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm9, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	2176(%rsp), %xmm1       # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	movl	%ebx, 1696(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, 2624(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r9d
	movl	%r9d, 1664(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, 2560(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r10d
	movl	%r10d, 1728(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, 1920(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ecx
	movl	%ecx, 1760(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	vpextrd	$1, %xmm3, %eax
	vmovdqa	2336(%rsp), %ymm1       # 32-byte Reload
	vpextrd	$1, %xmm1, %esi
	movl	%esi, 1272(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vmovd	%xmm3, %eax
	vmovd	%xmm1, %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r13d
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm1, %esi
	movl	%esi, 1216(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r12d
	movq	976(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	vmovd	%eax, %xmm0
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm1, %esi
	movl	%esi, 1280(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vmovd	2560(%rsp), %xmm3       # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 2624(%rsp), %xmm3, %xmm3 # 4-byte Folded Reload
	vpinsrd	$2, 1920(%rsp), %xmm3, %xmm4 # 4-byte Folded Reload
	vmovdqa	.LCPI148_6(%rip), %ymm8 # ymm8 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm8, %ymm2, %ymm3
	vpinsrd	$3, %r11d, %xmm4, %xmm12
	vextracti128	$1, %ymm3, %xmm5
	vpextrd	$1, %xmm5, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%r13d, %xmm4
	vpinsrd	$1, %r15d, %xmm4, %xmm4
	movq	%rdi, %r11
	vmovd	%xmm5, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vpinsrd	$2, %r12d, %xmm4, %xmm4
	vpinsrd	$3, %r8d, %xmm4, %xmm13
	vpextrd	$2, %xmm5, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vmovd	%esi, %xmm6
	vpinsrd	$1, %ebx, %xmm6, %xmm6
	vpextrd	$3, %xmm5, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$2, %edi, %xmm6, %xmm5
	vpextrd	$1, %xmm3, %eax
	vpextrd	$1, %xmm1, %esi
	movl	%esi, 1400(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpinsrd	$3, %ecx, %xmm5, %xmm6
	vmovd	%xmm3, %eax
	vmovd	%xmm1, %ecx
	movl	%ecx, 1392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vmovd	%edx, %xmm5
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm1, %ecx
	movl	%ecx, 1344(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$1, %esi, %xmm5, %xmm5
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm1, %esi
	movl	%esi, 1312(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	vpinsrd	$2, %ecx, %xmm5, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm7
	movq	1072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r11), %r13d
	movl	%r13d, 1528(%rsp)       # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm0
	vmovdqa	2528(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm3
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm10, %xmm0, %xmm0
	vmovdqa	2512(%rsp), %xmm11      # 16-byte Reload
	vpmaxsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm3, %ymm0
	vmovd	%r13d, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vinserti128	$1, %xmm6, %ymm7, %ymm6
	vpsrad	$31, %ymm6, %ymm7
	vmovdqa	1152(%rsp), %ymm14      # 32-byte Reload
	vpand	%ymm7, %ymm14, %ymm7
	vmovdqa	2464(%rsp), %ymm8       # 32-byte Reload
	vpaddd	%ymm6, %ymm8, %ymm6
	vpaddd	%ymm7, %ymm6, %ymm6
	vpabsd	%xmm6, %xmm7
	vextracti128	$1, %ymm6, %xmm6
	vpabsd	%xmm6, %xmm6
	vinserti128	$1, %xmm6, %ymm7, %ymm6
	vmovdqa	2272(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm3, %ymm1, %ymm7
	vmovdqa	2400(%rsp), %ymm1       # 32-byte Reload
	vpsubd	%ymm6, %ymm1, %ymm6
	vblendvps	%ymm7, %ymm0, %ymm6, %ymm0
	vpaddd	%ymm0, %ymm15, %ymm0
	vmovq	%xmm0, %rsi
	movslq	%esi, %rax
	movq	%rax, 1504(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm6
	vpaddd	%ymm9, %ymm5, %ymm5
	vpminsd	%xmm10, %xmm5, %xmm7
	vextracti128	$1, %ymm5, %xmm5
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm11, %xmm5, %xmm5
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vinserti128	$1, %xmm12, %ymm13, %ymm2
	vpsrad	$31, %ymm2, %ymm4
	vpand	%ymm4, %ymm14, %ymm4
	vmovdqa	%ymm14, %ymm12
	vpaddd	%ymm2, %ymm8, %ymm2
	vmovdqa	%ymm1, %ymm7
	vmovdqa	%ymm8, %ymm14
	vpaddd	%ymm4, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vmovdqa	2304(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm3, %ymm1, %ymm4
	vpsubd	%ymm2, %ymm7, %ymm2
	vblendvps	%ymm4, %ymm5, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm15, %ymm2
	vmovq	%xmm2, %r15
	movslq	%r15d, %rax
	movq	%rax, 1792(%rsp)        # 8-byte Spill
	sarq	$32, %r15
	vpextrq	$1, %xmm2, %r12
	movslq	%r12d, %rax
	movq	%rax, 1856(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rdi
	movslq	%edi, %rax
	movq	%rax, 1824(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	vpextrq	$1, %xmm2, %rbx
	movslq	%ebx, %rax
	movq	%rax, 1888(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	sarq	$32, %rsi
	movq	%rsi, 2624(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %r8
	movslq	%r8d, %rax
	movq	%rax, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %r8
	movq	%r8, 1456(%rsp)         # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r11
	movslq	%r11d, %rax
	movq	%rax, 1920(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	%r11, 1448(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm0, %rsi
	movslq	%esi, %r10
	movq	%r10, 1408(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	andl	$1, %r13d
	sete	%al
	andb	1520(%rsp), %al         # 1-byte Folded Reload
	movq	2624(%rsp), %r9         # 8-byte Reload
	vpinsrw	$1, (%rdx,%r9,2), %xmm6, %xmm0
	movq	2560(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%r8,2), %xmm0, %xmm0
	movq	1920(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%r11,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdx,%r10,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%rsi,2), %xmm0, %xmm0
	jne	.LBB148_227
# BB#228:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	%r15, 1472(%rsp)        # 8-byte Spill
	movb	%al, 1520(%rsp)         # 1-byte Spill
	vpxor	%ymm13, %ymm13, %ymm13
	jmp	.LBB148_229
	.align	16, 0x90
.LBB148_227:                            #   in Loop: Header=BB148_226 Depth=3
	movb	%al, 1520(%rsp)         # 1-byte Spill
	movq	2552(%rsp), %rdx        # 8-byte Reload
	movq	1792(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm4
	vpinsrw	$1, (%rdx,%r15,2), %xmm4, %xmm4
	movq	%r15, 1472(%rsp)        # 8-byte Spill
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm4, %xmm4
	vpinsrw	$3, (%rdx,%r12,2), %xmm4, %xmm4
	movq	1824(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm4, %xmm4
	vpinsrw	$5, (%rdx,%rdi,2), %xmm4, %xmm4
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm4, %xmm4
	vpinsrw	$7, (%rdx,%rbx,2), %xmm4, %xmm4
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm4, %ymm13
.LBB148_229:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	jne	.LBB148_230
# BB#231:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	%rsi, 1464(%rsp)        # 8-byte Spill
	movq	%rdi, 1480(%rsp)        # 8-byte Spill
	movq	%r12, 1488(%rsp)        # 8-byte Spill
	movq	%rbx, 1496(%rsp)        # 8-byte Spill
	movl	%r13d, 1512(%rsp)       # 4-byte Spill
	vxorps	%ymm5, %ymm5, %ymm5
	jmp	.LBB148_232
	.align	16, 0x90
.LBB148_230:                            #   in Loop: Header=BB148_226 Depth=3
	movq	%rsi, 1464(%rsp)        # 8-byte Spill
	movq	%rdi, 1480(%rsp)        # 8-byte Spill
	movq	%r12, 1488(%rsp)        # 8-byte Spill
	movq	%rbx, 1496(%rsp)        # 8-byte Spill
	movl	%r13d, 1512(%rsp)       # 4-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm5
.LBB148_232:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	1048(%rsp), %rax        # 8-byte Reload
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm6
	vpaddd	%ymm9, %ymm6, %ymm0
	vextracti128	$1, %ymm0, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	movl	1696(%rsp), %r13d       # 4-byte Reload
	idivl	%r13d
	movl	%edx, %r8d
	vmovd	%xmm7, %eax
	cltd
	movl	1664(%rsp), %r12d       # 4-byte Reload
	idivl	%r12d
	movl	%edx, %r9d
	vpextrd	$2, %xmm7, %eax
	cltd
	movl	1728(%rsp), %r11d       # 4-byte Reload
	idivl	%r11d
	movl	%edx, %r10d
	vpextrd	$3, %xmm7, %eax
	cltd
	movl	1760(%rsp), %r15d       # 4-byte Reload
	idivl	%r15d
	movl	%edx, %ebx
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	1272(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	movq	984(%rsp), %rax         # 8-byte Reload
	movq	2656(%rsp), %rdx        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	vmovd	%eax, %xmm7
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm8
	vpminsd	%xmm10, %xmm8, %xmm2
	vextracti128	$1, %ymm8, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vmovdqa	%xmm11, %xmm9
	vpmaxsd	%xmm9, %xmm2, %xmm2
	vpmaxsd	%xmm9, %xmm4, %xmm4
	vinserti128	$1, %xmm4, %ymm2, %ymm8
	vmovdqa	.LCPI148_6(%rip), %ymm11 # ymm11 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm11, %ymm6, %ymm6
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vmovd	%esi, %xmm2
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vextracti128	$1, %ymm6, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %esi, %xmm2, %xmm2
	vmovd	%xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %esi
	vinserti128	$1, %xmm1, %ymm2, %ymm2
	vpsrad	$31, %ymm2, %ymm1
	vpand	%ymm12, %ymm1, %ymm1
	vpaddd	%ymm2, %ymm14, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	2240(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm3, %ymm2, %ymm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vmovdqa	2400(%rsp), %ymm15      # 32-byte Reload
	vpsubd	%ymm1, %ymm15, %ymm1
	vblendvps	%ymm2, %ymm8, %ymm1, %ymm1
	vpaddd	%ymm11, %ymm7, %ymm2
	vpextrd	$3, %xmm0, %eax
	vpminsd	%xmm10, %xmm2, %xmm0
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	vpmaxsd	%xmm9, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm2, %xmm2
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	1400(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vinserti128	$1, %xmm2, %ymm0, %ymm0
	vmovd	%esi, %xmm2
	vmovd	%xmm6, %eax
	cltd
	idivl	1392(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	1344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpinsrd	$3, %ebx, %xmm2, %xmm2
	vpextrd	$3, %xmm6, %eax
	vmovd	%esi, %xmm4
	cltd
	idivl	1312(%rsp)              # 4-byte Folded Reload
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edi, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vpsrad	$31, %ymm2, %ymm4
	vpand	%ymm12, %ymm4, %ymm4
	vpaddd	%ymm2, %ymm14, %ymm2
	vpaddd	%ymm4, %ymm2, %ymm2
	vpabsd	%xmm2, %xmm4
	vextracti128	$1, %ymm2, %xmm2
	vpabsd	%xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vmovdqa	2208(%rsp), %ymm4       # 32-byte Reload
	vpcmpgtd	%ymm3, %ymm4, %ymm3
	vpsubd	%ymm2, %ymm15, %ymm2
	vblendvps	%ymm3, %ymm0, %ymm2, %ymm0
	vmovdqa	1120(%rsp), %ymm15      # 32-byte Reload
	vpaddd	%ymm0, %ymm15, %ymm0
	vpaddd	%ymm1, %ymm15, %ymm1
	vpextrq	$1, %xmm1, %rbx
	vmovq	%xmm1, %r10
	vextracti128	$1, %ymm1, %xmm1
	vpextrq	$1, %xmm1, %r8
	vmovq	%xmm1, %r9
	vextracti128	$1, %ymm0, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rdx
	movslq	%r10d, %r13
	sarq	$32, %r10
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%r9d, %rdi
	sarq	$32, %r9
	movslq	%r8d, %r15
	sarq	$32, %r8
	vpextrq	$1, %xmm0, 1400(%rsp)   # 8-byte Folded Spill
	movq	%rdx, %rcx
	sarq	$32, %rcx
	movq	%rcx, 1728(%rsp)        # 8-byte Spill
	movslq	%eax, %rcx
	movq	%rcx, 1696(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	movl	1600(%rsp), %esi        # 4-byte Reload
	movl	1528(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %esi
	setne	%al
	vmovq	%xmm0, %r11
	jne	.LBB148_233
# BB#234:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	movq	%r12, 1264(%rsp)        # 8-byte Spill
	movq	%rbx, 1272(%rsp)        # 8-byte Spill
	movq	%r15, 1280(%rsp)        # 8-byte Spill
	movb	%al, 1312(%rsp)         # 1-byte Spill
	vpxor	%ymm0, %ymm0, %ymm0
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm14 # ymm14 = <4,u,5,u,6,u,7,u>
	movq	%rdi, 1664(%rsp)        # 8-byte Spill
	jmp	.LBB148_235
	.align	16, 0x90
.LBB148_233:                            #   in Loop: Header=BB148_226 Depth=3
	movb	%al, 1312(%rsp)         # 1-byte Spill
	movq	%rdi, 1664(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rdi,%r13,2), %eax
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdi,%r10,2), %xmm0, %xmm0
	vpinsrw	$2, (%rdi,%r12,2), %xmm0, %xmm0
	movq	%r12, 1264(%rsp)        # 8-byte Spill
	vpinsrw	$3, (%rdi,%rbx,2), %xmm0, %xmm0
	movq	%rbx, 1272(%rsp)        # 8-byte Spill
	movq	1664(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdi,%r9,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdi,%r15,2), %xmm0, %xmm0
	movq	%r15, 1280(%rsp)        # 8-byte Spill
	vpinsrw	$7, (%rdi,%r8,2), %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm14 # ymm14 = <4,u,5,u,6,u,7,u>
.LBB148_235:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	%r8, 1208(%rsp)         # 8-byte Spill
	movq	%r9, 1200(%rsp)         # 8-byte Spill
	movslq	%r11d, %r9
	sarq	$32, %r11
	movq	1400(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %r8
	sarq	$32, %rbx
	movslq	%edx, %r13
	movq	2552(%rsp), %r12        # 8-byte Reload
	movq	1728(%rsp), %rdx        # 8-byte Reload
	movzwl	(%r12,%rdx,2), %eax
	movq	1696(%rsp), %rdx        # 8-byte Reload
	movzwl	(%r12,%rdx,2), %edi
	movq	1760(%rsp), %rdx        # 8-byte Reload
	movzwl	(%r12,%rdx,2), %r15d
	movq	%r12, %rdx
	testl	%esi, %esi
	jne	.LBB148_236
# BB#237:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movl	%r15d, 1344(%rsp)       # 4-byte Spill
	movl	%edi, 1392(%rsp)        # 4-byte Spill
	movl	%eax, 1400(%rsp)        # 4-byte Spill
	vxorps	%ymm3, %ymm3, %ymm3
	jmp	.LBB148_238
	.align	16, 0x90
.LBB148_236:                            #   in Loop: Header=BB148_226 Depth=3
	movzwl	(%rdx,%r9,2), %esi
	vmovd	%esi, %xmm1
	vpinsrw	$1, (%rdx,%r11,2), %xmm1, %xmm1
	vpinsrw	$2, (%rdx,%r8,2), %xmm1, %xmm1
	vpinsrw	$3, (%rdx,%rbx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdx,%r13,2), %xmm1, %xmm1
	vpinsrw	$5, %eax, %xmm1, %xmm1
	movl	%eax, 1400(%rsp)        # 4-byte Spill
	vpinsrw	$6, %edi, %xmm1, %xmm1
	movl	%edi, 1392(%rsp)        # 4-byte Spill
	vpinsrw	$7, %r15d, %xmm1, %xmm1
	movl	%r15d, 1344(%rsp)       # 4-byte Spill
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm3
.LBB148_238:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vpermps	%ymm3, %ymm11, %ymm1
	vpermps	%ymm5, %ymm14, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vmovaps	.LCPI148_10(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vmovaps	%ymm2, %ymm6
	vpermps	%ymm3, %ymm6, %ymm2
	vmovaps	.LCPI148_11(%rip), %ymm12 # ymm12 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm5, %ymm12, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vpermps	%ymm0, %ymm11, %ymm3
	vpermps	%ymm13, %ymm14, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	vpermps	%ymm0, %ymm6, %ymm0
	vmovaps	%ymm6, %ymm9
	vpermps	%ymm13, %ymm12, %ymm4
	vblendps	$170, %ymm0, %ymm4, %ymm0 # ymm0 = ymm4[0],ymm0[1],ymm4[2],ymm0[3],ymm4[4],ymm0[5],ymm4[6],ymm0[7]
	movslq	%ecx, %rsi
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	addq	1088(%rsp), %rsi        # 8-byte Folded Reload
	movq	2504(%rsp), %r12        # 8-byte Reload
	vmovups	%ymm0, (%r12,%rsi,4)
	vmovups	%ymm3, 32(%r12,%rsi,4)
	vmovups	%ymm2, 64(%r12,%rsi,4)
	vmovups	%ymm1, 96(%r12,%rsi,4)
	movl	%ecx, %r12d
	movq	1216(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %ecx
	vmovd	%ecx, %xmm0
	vpinsrw	$1, (%rdx,%r10,2), %xmm0, %xmm0
	movq	1264(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm0, %xmm0
	movq	1272(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rax,2), %xmm0, %xmm0
	movq	1664(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm0, %xmm0
	movq	1200(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rax,2), %xmm0, %xmm0
	movq	1280(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm0, %xmm0
	movq	1208(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rax,2), %xmm0, %xmm0
	movzwl	(%rdx,%r9,2), %ecx
	vmovd	%ecx, %xmm1
	vpinsrw	$1, (%rdx,%r11,2), %xmm1, %xmm1
	vpinsrw	$2, (%rdx,%r8,2), %xmm1, %xmm1
	vpinsrw	$3, (%rdx,%rbx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdx,%r13,2), %xmm1, %xmm3
	movq	1504(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rax,2), %xmm1, %xmm1
	movq	2560(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1456(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1920(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1448(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1408(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1464(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rax,2), %xmm1, %xmm5
	movq	1792(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1472(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1488(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1824(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1480(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm1, %xmm1
	movq	1496(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdx,%rax,2), %xmm1, %xmm1
	movq	2592(%rsp), %rax        # 8-byte Reload
	orl	%eax, %r12d
	andl	$1, %r12d
	sete	%al
	vpmovzxwd	%xmm1, %ymm7    # ymm7 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
	vmovaps	%ymm0, %ymm4
	je	.LBB148_240
# BB#239:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_240:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vpmovzxwd	%xmm5, %ymm6    # ymm6 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm7, %ymm5
	orb	1312(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%ymm5, %ymm7
	movl	1600(%rsp), %r8d        # 4-byte Reload
	movl	1568(%rsp), %edx        # 4-byte Reload
	movq	2656(%rsp), %rdi        # 8-byte Reload
	movl	1392(%rsp), %r9d        # 4-byte Reload
	movl	1344(%rsp), %r10d       # 4-byte Reload
	jne	.LBB148_242
# BB#241:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_242:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vcvtdq2ps	%ymm6, %ymm6
	vmovaps	%ymm6, %ymm8
	movq	2504(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm9, %ymm13
	movb	1536(%rsp), %al         # 1-byte Reload
	jne	.LBB148_244
# BB#243:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm8, %ymm8, %ymm8
.LBB148_244:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movl	1512(%rsp), %ebx        # 4-byte Reload
	andb	%bl, %al
	jne	.LBB148_246
# BB#245:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_246:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	jne	.LBB148_248
# BB#247:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_248:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	orb	1520(%rsp), %al         # 1-byte Folded Reload
	jne	.LBB148_250
# BB#249:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm0, %ymm0, %ymm0
.LBB148_250:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movl	1400(%rsp), %eax        # 4-byte Reload
	jne	.LBB148_251
# BB#252:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vpxor	%ymm10, %ymm10, %ymm10
	jmp	.LBB148_253
	.align	16, 0x90
.LBB148_251:                            #   in Loop: Header=BB148_226 Depth=3
	vpinsrw	$5, %eax, %xmm3, %xmm1
	vpinsrw	$6, %r9d, %xmm1, %xmm1
	vpinsrw	$7, %r10d, %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vcvtdq2ps	%ymm1, %ymm10
.LBB148_253:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	movq	2392(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vpermps	%ymm10, %ymm11, %ymm1
	vpermps	%ymm8, %ymm14, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm10, %ymm13, %ymm2
	vpermps	%ymm8, %ymm12, %ymm8
	vblendps	$170, %ymm2, %ymm8, %ymm2 # ymm2 = ymm8[0],ymm2[1],ymm8[2],ymm2[3],ymm8[4],ymm2[5],ymm8[6],ymm2[7]
	vpermps	%ymm0, %ymm11, %ymm8
	vpermps	%ymm7, %ymm14, %ymm10
	vblendps	$170, %ymm8, %ymm10, %ymm8 # ymm8 = ymm10[0],ymm8[1],ymm10[2],ymm8[3],ymm10[4],ymm8[5],ymm10[6],ymm8[7]
	vpermps	%ymm0, %ymm13, %ymm0
	vpermps	%ymm7, %ymm12, %ymm7
	vblendps	$170, %ymm0, %ymm7, %ymm0 # ymm0 = ymm7[0],ymm0[1],ymm7[2],ymm0[3],ymm7[4],ymm0[5],ymm7[6],ymm0[7]
	vmovups	%ymm0, 12288(%rcx,%rax,4)
	vmovups	%ymm8, 12320(%rcx,%rax,4)
	vmovups	%ymm2, 12352(%rcx,%rax,4)
	vmovups	%ymm1, 12384(%rcx,%rax,4)
	testl	%r12d, %r12d
	je	.LBB148_254
# BB#255:                               # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vxorps	%ymm2, %ymm2, %ymm2
	jmp	.LBB148_256
	.align	16, 0x90
.LBB148_254:                            #   in Loop: Header=BB148_226 Depth=3
	movq	2552(%rsp), %rax        # 8-byte Reload
	movq	1728(%rsp), %rbx        # 8-byte Reload
	vpinsrw	$5, (%rax,%rbx,2), %xmm3, %xmm0
	movq	1696(%rsp), %rbx        # 8-byte Reload
	vpinsrw	$6, (%rax,%rbx,2), %xmm0, %xmm0
	movq	1760(%rsp), %rbx        # 8-byte Reload
	movzwl	(%rax,%rbx,2), %eax
	vpinsrw	$7, %eax, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm2
.LBB148_256:                            # %for deinterleaved$1.s0.v10.v1075
                                        #   in Loop: Header=BB148_226 Depth=3
	vpermps	%ymm6, %ymm14, %ymm0
	vpermps	%ymm2, %ymm11, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	vpermps	%ymm6, %ymm12, %ymm1
	vpermps	%ymm2, %ymm13, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm4, %ymm11, %ymm2
	vpermps	%ymm5, %ymm14, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vpermps	%ymm4, %ymm13, %ymm3
	vpermps	%ymm5, %ymm12, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	addq	2384(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, 24576(%rcx,%rsi,4)
	vmovups	%ymm2, 24608(%rcx,%rsi,4)
	vmovups	%ymm1, 24640(%rcx,%rsi,4)
	vmovups	%ymm0, 24672(%rcx,%rsi,4)
	addl	$32, %edi
	addl	$-1, %edx
	jne	.LBB148_226
.LBB148_257:                            # %end for deinterleaved$1.s0.v10.v1076
                                        #   in Loop: Header=BB148_128 Depth=2
	movl	888(%rsp), %edx         # 4-byte Reload
	addl	$1, %edx
	movl	%edx, 888(%rsp)         # 4-byte Spill
	addq	$1, 2592(%rsp)          # 8-byte Folded Spill
	movq	960(%rsp), %rax         # 8-byte Reload
	addl	%eax, 776(%rsp)         # 4-byte Folded Spill
	cmpl	728(%rsp), %edx         # 4-byte Folded Reload
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1660(%rsp), %r15d       # 4-byte Reload
	jne	.LBB148_128
.LBB148_162:                            # %end for deinterleaved$1.s0.v1166
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	680(%rsp), %rax         # 8-byte Reload
	movq	496(%rsp), %rcx         # 8-byte Reload
	leal	(%rax,%rcx,2), %eax
	movq	%rax, 888(%rsp)         # 8-byte Spill
	movl	732(%rsp), %eax         # 4-byte Reload
	cmpl	816(%rsp), %eax         # 4-byte Folded Reload
	jle	.LBB148_288
# BB#163:                               #   in Loop: Header=BB148_118 Depth=1
	movl	412(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cltq
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	.align	16, 0x90
.LBB148_164:                            # %for deinterleaved$1.s0.v1179
                                        #   Parent Loop BB148_118 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB148_166 Depth 3
	cmpl	$0, 956(%rsp)           # 4-byte Folded Reload
	jle	.LBB148_287
# BB#165:                               # %for deinterleaved$1.s0.v10.v1082.preheader
                                        #   in Loop: Header=BB148_164 Depth=2
	movq	792(%rsp), %rdi         # 8-byte Reload
	movq	1120(%rsp), %rbx        # 8-byte Reload
	cmpq	%rbx, %rdi
	movl	%edi, %ecx
	cmovgl	%ebx, %ecx
	movq	824(%rsp), %rdx         # 8-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	%ebx, %eax
	subl	%edx, %eax
	cltd
	idivl	804(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	800(%rsp), %eax         # 4-byte Folded Reload
	movq	808(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %edx
	leal	(%rdx,%rax), %esi
	leal	1(%rdx,%rax), %eax
	cmpl	$-2, %esi
	notl	%esi
	cmovgl	%eax, %esi
	movl	%edi, %eax
	subl	%esi, %eax
	cmpq	%rbx, 784(%rsp)         # 8-byte Folded Reload
	cmovgl	%ecx, %eax
	movl	%ebx, %r8d
	movq	960(%rsp), %rcx         # 8-byte Reload
	imull	%ecx, %eax
	andl	$1, %r8d
	movl	%r8d, 1696(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm1
	vmovaps	832(%rsp), %ymm0        # 32-byte Reload
	vinsertf128	$1, %xmm0, %ymm0, %ymm0
	vmovaps	%ymm0, 1088(%rsp)       # 32-byte Spill
	vpsubd	896(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpbroadcastd	%xmm1, %ymm0
	vmovdqa	%ymm0, 1664(%rsp)       # 32-byte Spill
	movq	936(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rbx), %rax
	imulq	944(%rsp), %rax         # 8-byte Folded Reload
	movq	%rax, 1080(%rsp)        # 8-byte Spill
	movl	956(%rsp), %edx         # 4-byte Reload
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r10d
	.align	16, 0x90
.LBB148_166:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_164 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	%r10, 2656(%rsp)        # 8-byte Spill
	movl	%edx, 1600(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	1568(%rsp)              # 1-byte Folded Spill
	setne	1536(%rsp)              # 1-byte Folded Spill
	movq	1952(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vmovdqa	.LCPI148_7(%rip), %ymm14 # ymm14 = [0,2,4,6,8,10,12,14]
	vpaddd	%ymm14, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vpextrd	$1, %xmm4, %eax
	vmovdqa	2176(%rsp), %xmm0       # 16-byte Reload
	vpextrd	$1, %xmm0, %r13d
	movl	%r13d, 1400(%rsp)       # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	vmovd	%xmm4, %eax
	vmovd	%xmm0, %ebx
	movl	%ebx, 1792(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm0, %ecx
	movl	%ecx, 1728(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm0, %edi
	movl	%edi, 1472(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$1, %xmm3, %eax
	vmovdqa	2336(%rsp), %ymm0       # 32-byte Reload
	vpextrd	$1, %xmm0, %esi
	movl	%esi, 1280(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r11d
	vmovd	%r8d, %xmm4
	vmovd	%xmm3, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1272(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vpinsrd	$1, %r9d, %xmm4, %xmm4
	vpextrd	$2, %xmm3, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r15d
	vpinsrd	$2, %r12d, %xmm4, %xmm5
	vpextrd	$3, %xmm3, %eax
	vpextrd	$3, %xmm0, %esi
	movl	%esi, 1216(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovdqa	.LCPI148_6(%rip), %ymm12 # ymm12 = [16,18,20,22,24,26,28,30]
	vpaddd	%ymm12, %ymm2, %ymm4
	vextracti128	$1, %ymm4, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	vpinsrd	$3, %r10d, %xmm5, %xmm9
	vmovd	%r8d, %xmm3
	vmovd	%xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpinsrd	$1, %r11d, %xmm3, %xmm3
	vpinsrd	$2, %r15d, %xmm3, %xmm13
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%ebx, %xmm5
	vpinsrd	$1, %esi, %xmm5, %xmm5
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	vpinsrd	$2, %ecx, %xmm5, %xmm5
	vpextrd	$1, %xmm4, %eax
	vpextrd	$1, %xmm0, %ecx
	movl	%ecx, 1760(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpinsrd	$3, %esi, %xmm5, %xmm6
	vmovd	%xmm4, %eax
	vmovd	%xmm0, %esi
	movl	%esi, 1480(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	vmovd	%edx, %xmm5
	vpextrd	$2, %xmm4, %eax
	vpextrd	$2, %xmm0, %esi
	movl	%esi, 1464(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpextrd	$3, %xmm4, %eax
	vpextrd	$3, %xmm0, %ecx
	movl	%ecx, 1392(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	vpinsrd	$2, %esi, %xmm5, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm7
	movq	2656(%rsp), %rcx        # 8-byte Reload
	leal	-8(%rcx), %eax
	vmovd	%eax, %xmm0
	vmovd	%ecx, %xmm4
	movq	%rcx, %rdi
	vpbroadcastd	%xmm4, %ymm10
	vmovdqa	2304(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm1, %ymm4
	vmovdqa	.LCPI148_3(%rip), %ymm1 # ymm1 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vmovdqa	%ymm1, %ymm5
	vpshufb	%ymm5, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2272(%rsp), %ymm1       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm1, %ymm8
	vpshufb	%ymm5, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm1 # xmm1 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vmovdqa	%xmm1, %xmm3
	vpshufb	%xmm3, %xmm8, %xmm1
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm1, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm1[0]
	vmovdqa	2112(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm4
	vpshufb	%ymm5, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vmovdqa	2080(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm8
	vpshufb	%ymm5, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vpshufb	%xmm3, %xmm8, %xmm2
	vpshufb	%xmm3, %xmm4, %xmm4
	vpunpcklqdq	%xmm2, %xmm4, %xmm2 # xmm2 = xmm4[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm1, %xmm1
	vpor	%xmm1, %xmm2, %xmm4
	vinserti128	$1, %xmm6, %ymm7, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vmovdqa	1088(%rsp), %ymm8       # 32-byte Reload
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	2464(%rsp), %ymm11      # 32-byte Reload
	vpaddd	%ymm1, %ymm11, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vmovdqa	2400(%rsp), %ymm15      # 32-byte Reload
	vpsubd	%ymm1, %ymm15, %ymm1
	vpbroadcastd	%xmm0, %ymm6
	vpaddd	%ymm12, %ymm6, %ymm0
	vmovdqa	2528(%rsp), %xmm5       # 16-byte Reload
	vpminsd	%xmm5, %xmm0, %xmm2
	vextracti128	$1, %ymm0, %xmm0
	vpminsd	%xmm5, %xmm0, %xmm0
	vmovdqa	2512(%rsp), %xmm7       # 16-byte Reload
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vpunpckhbw	%xmm4, %xmm4, %xmm2 # xmm2 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm0, %ymm0
	vpinsrd	$3, %r9d, %xmm13, %xmm1
	vmovdqa	1664(%rsp), %ymm13      # 32-byte Reload
	vpaddd	%ymm0, %ymm13, %ymm3
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rax
	movq	%rcx, %r8
	movq	%rax, 1520(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rcx        # 8-byte Reload
	movzwl	(%rcx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vinserti128	$1, %xmm9, %ymm1, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm2, %ymm8, %ymm2
	vmovdqa	%ymm8, %ymm9
	vpaddd	%ymm1, %ymm11, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm14, %ymm6, %ymm2
	vpminsd	%xmm5, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm5, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm15, %ymm1
	vpmovzxbd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	vpslld	$31, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm2, %ymm1
	vpaddd	%ymm1, %ymm13, %ymm1
	vmovq	%xmm1, %r9
	movslq	%r9d, %rax
	movq	%rax, 1824(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	vpextrq	$1, %xmm1, %rbx
	movslq	%ebx, %rax
	movq	%rax, 1888(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm1, %rsi
	movq	%rdi, %rax
	movslq	%esi, %rdx
	movq	%rdx, 1856(%rsp)        # 8-byte Spill
	sarq	$32, %rsi
	vpextrq	$1, %xmm1, %rdi
	movslq	%edi, %rdx
	movq	%rdx, 2624(%rsp)        # 8-byte Spill
	sarq	$32, %rdi
	sarq	$32, %r8
	movq	%r8, 2592(%rsp)         # 8-byte Spill
	vpextrq	$1, %xmm3, %r13
	movslq	%r13d, %rcx
	movq	%rcx, 2560(%rsp)        # 8-byte Spill
	sarq	$32, %r13
	movq	%r13, 1448(%rsp)        # 8-byte Spill
	vextracti128	$1, %ymm3, %xmm1
	vmovq	%xmm1, %r11
	movslq	%r11d, %rcx
	movq	%rcx, 1920(%rsp)        # 8-byte Spill
	sarq	$32, %r11
	movq	%r11, 1408(%rsp)        # 8-byte Spill
	vpextrq	$1, %xmm1, %r12
	movslq	%r12d, %r15
	movq	%r15, 1344(%rsp)        # 8-byte Spill
	sarq	$32, %r12
	movq	%r12, 1456(%rsp)        # 8-byte Spill
	movl	%eax, %r8d
	movq	2552(%rsp), %rdx        # 8-byte Reload
	andl	$1, %r8d
	sete	%al
	andb	1536(%rsp), %al         # 1-byte Folded Reload
	movq	2592(%rsp), %r10        # 8-byte Reload
	vpinsrw	$1, (%rdx,%r10,2), %xmm0, %xmm0
	movq	2560(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%r13,2), %xmm0, %xmm0
	movq	1920(%rsp), %rcx        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rcx,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%r11,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdx,%r15,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%r12,2), %xmm0, %xmm4
	jne	.LBB148_167
# BB#258:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movb	%al, 1536(%rsp)         # 1-byte Spill
	vpxor	%ymm0, %ymm0, %ymm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	jmp	.LBB148_259
	.align	16, 0x90
.LBB148_167:                            #   in Loop: Header=BB148_166 Depth=3
	movb	%al, 1536(%rsp)         # 1-byte Spill
	movq	1824(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdx,%rax,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdx,%r9,2), %xmm0, %xmm0
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdx,%rbx,2), %xmm0, %xmm0
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$5, (%rdx,%rsi,2), %xmm0, %xmm0
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdx,%rax,2), %xmm0, %xmm0
	vpinsrw	$7, (%rdx,%rdi,2), %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm0
.LBB148_259:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movq	2656(%rsp), %r12        # 8-byte Reload
	jne	.LBB148_260
# BB#261:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vmovdqa	%ymm0, 1312(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1488(%rsp)         # 8-byte Spill
	movq	%rsi, 1496(%rsp)        # 8-byte Spill
	movq	%rbx, 1504(%rsp)        # 8-byte Spill
	movq	%rdi, 1512(%rsp)        # 8-byte Spill
	movl	%r8d, 1528(%rsp)        # 4-byte Spill
	vpxor	%ymm4, %ymm4, %ymm4
	jmp	.LBB148_262
	.align	16, 0x90
.LBB148_260:                            #   in Loop: Header=BB148_166 Depth=3
	vmovdqa	%ymm0, 1312(%rsp)       # 32-byte Spill
	movq	%rax, 2624(%rsp)        # 8-byte Spill
	movq	%r9, 1488(%rsp)         # 8-byte Spill
	movq	%rsi, 1496(%rsp)        # 8-byte Spill
	movq	%rbx, 1504(%rsp)        # 8-byte Spill
	movq	%rdi, 1512(%rsp)        # 8-byte Spill
	movl	%r8d, 1528(%rsp)        # 4-byte Spill
	vpmovzxwd	%xmm4, %ymm0    # ymm0 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vcvtdq2ps	%ymm0, %ymm4
.LBB148_262:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movq	1984(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm6
	vmovdqa	%ymm14, %ymm13
	vpaddd	%ymm13, %ymm6, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	movl	1400(%rsp), %esi        # 4-byte Reload
	idivl	%esi
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	movl	1792(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	movl	1728(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, 1208(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	movl	1472(%rsp), %r10d       # 4-byte Reload
	idivl	%r10d
	movl	%edx, %r15d
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %r13d
	vmovd	%xmm7, %eax
	cltd
	idivl	1272(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebx
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vpaddd	%ymm12, %ymm6, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	vmovd	%r11d, %xmm1
	vpinsrd	$1, 1200(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vmovdqa	2240(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm7
	vmovdqa	.LCPI148_3(%rip), %ymm14 # ymm14 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm14, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2208(%rsp), %ymm2       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm2, %ymm8
	vpshufb	%ymm14, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vmovdqa	.LCPI148_4(%rip), %xmm11 # xmm11 = <0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u>
	vpshufb	%xmm11, %xmm8, %xmm2
	vpshufb	%xmm11, %xmm7, %xmm7
	vpunpcklqdq	%xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0],xmm2[0]
	vpxor	.LCPI148_5(%rip), %xmm2, %xmm2
	vmovdqa	2048(%rsp), %ymm7       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm7, %ymm7
	vpshufb	%ymm14, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vmovdqa	2016(%rsp), %ymm8       # 32-byte Reload
	vpcmpgtd	%ymm10, %ymm8, %ymm5
	vpshufb	%ymm14, %ymm5, %ymm5
	vpermq	$232, %ymm5, %ymm5      # ymm5 = ymm5[0,2,2,3]
	vpshufb	%xmm11, %xmm5, %xmm5
	vpshufb	%xmm11, %xmm7, %xmm7
	vpunpcklqdq	%xmm5, %xmm7, %xmm5 # xmm5 = xmm7[0],xmm5[0]
	vpor	%xmm2, %xmm5, %xmm5
	vpinsrd	$2, 1208(%rsp), %xmm1, %xmm1 # 4-byte Folded Reload
	vpinsrd	$3, %r15d, %xmm1, %xmm1
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r8d
	vmovd	%edi, %xmm2
	vpinsrd	$1, %r13d, %xmm2, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	vpinsrd	$2, %ebx, %xmm2, %xmm2
	vpinsrd	$3, %ecx, %xmm2, %xmm2
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vinserti128	$1, %xmm1, %ymm2, %ymm0
	vpsrad	$31, %ymm0, %ymm1
	vmovdqa	%ymm9, %ymm8
	vpand	%ymm8, %ymm1, %ymm1
	vmovdqa	2464(%rsp), %ymm9       # 32-byte Reload
	vpaddd	%ymm0, %ymm9, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpabsd	%xmm0, %xmm1
	vextracti128	$1, %ymm0, %xmm0
	vpabsd	%xmm0, %xmm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpsubd	%ymm0, %ymm15, %ymm0
	leal	-7(%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm7
	vpaddd	%ymm13, %ymm7, %ymm1
	vmovdqa	2528(%rsp), %xmm3       # 16-byte Reload
	vpminsd	%xmm3, %xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpminsd	%xmm3, %xmm1, %xmm1
	vmovdqa	2512(%rsp), %xmm10      # 16-byte Reload
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpmovzxbd	%xmm5, %ymm2    # ymm2 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	vpslld	$31, %ymm2, %ymm2
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	1760(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%r8d, %xmm1
	vpinsrd	$1, %esi, %xmm1, %xmm1
	vmovd	%xmm6, %eax
	cltd
	idivl	1480(%rsp)              # 4-byte Folded Reload
	movl	%edx, %esi
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %ebx, %xmm1, %xmm1
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	1464(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$3, %xmm6, %eax
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	cltd
	idivl	1392(%rsp)              # 4-byte Folded Reload
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpsrad	$31, %ymm1, %ymm2
	vpand	%ymm8, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpabsd	%xmm1, %xmm2
	vextracti128	$1, %ymm1, %xmm1
	vpabsd	%xmm1, %xmm1
	vinserti128	$1, %xmm1, %ymm2, %ymm1
	vpaddd	%ymm12, %ymm7, %ymm2
	vpminsd	%xmm3, %xmm2, %xmm6
	vextracti128	$1, %ymm2, %xmm2
	vpminsd	%xmm3, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm6, %xmm6
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vinserti128	$1, %xmm2, %ymm6, %ymm2
	vpsubd	%ymm1, %ymm15, %ymm1
	vpunpckhbw	%xmm5, %xmm5, %xmm5 # xmm5 = xmm5[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
	vpmovzxwd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpslld	$31, %ymm5, %ymm5
	vblendvps	%ymm5, %ymm1, %ymm2, %ymm1
	vmovdqa	1664(%rsp), %ymm2       # 32-byte Reload
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm0, %ymm2, %ymm0
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdx
	vextracti128	$1, %ymm0, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rbx
	vextracti128	$1, %ymm1, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %rdi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%esi, %r12
	sarq	$32, %rsi
	movslq	%ebx, %r15
	sarq	$32, %rbx
	movslq	%eax, %r13
	sarq	$32, %rax
	vpextrq	$1, %xmm1, 1480(%rsp)   # 8-byte Folded Spill
	movq	%rdi, %r11
	sarq	$32, %r11
	movslq	%r9d, %rcx
	movq	%rcx, 1792(%rsp)        # 8-byte Spill
	sarq	$32, %r9
	movl	1696(%rsp), %r8d        # 4-byte Reload
	movq	2656(%rsp), %rcx        # 8-byte Reload
	andl	%ecx, %r8d
	setne	%cl
	vmovq	%xmm1, 1760(%rsp)       # 8-byte Folded Spill
	jne	.LBB148_263
# BB#264:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movq	%r10, 1200(%rsp)        # 8-byte Spill
	movq	%r12, 1208(%rsp)        # 8-byte Spill
	movq	%r15, 1216(%rsp)        # 8-byte Spill
	movq	%r13, 1264(%rsp)        # 8-byte Spill
	movq	%rbx, 1272(%rsp)        # 8-byte Spill
	movq	%rdx, 1280(%rsp)        # 8-byte Spill
	movq	%rax, 1392(%rsp)        # 8-byte Spill
	movq	%rsi, 1400(%rsp)        # 8-byte Spill
	movb	%cl, 1464(%rsp)         # 1-byte Spill
	vxorps	%ymm5, %ymm5, %ymm5
	jmp	.LBB148_265
	.align	16, 0x90
.LBB148_263:                            #   in Loop: Header=BB148_166 Depth=3
	movb	%cl, 1464(%rsp)         # 1-byte Spill
	movq	%rdi, 1728(%rsp)        # 8-byte Spill
	movq	2552(%rsp), %rdi        # 8-byte Reload
	movzwl	(%rdi,%r10,2), %ecx
	movq	%r10, 1200(%rsp)        # 8-byte Spill
	vmovd	%ecx, %xmm0
	vpinsrw	$1, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	%rdx, 1280(%rsp)        # 8-byte Spill
	vpinsrw	$2, (%rdi,%r12,2), %xmm0, %xmm0
	movq	%r12, 1208(%rsp)        # 8-byte Spill
	vpinsrw	$3, (%rdi,%rsi,2), %xmm0, %xmm0
	movq	%rsi, 1400(%rsp)        # 8-byte Spill
	vpinsrw	$4, (%rdi,%r15,2), %xmm0, %xmm0
	movq	%r15, 1216(%rsp)        # 8-byte Spill
	vpinsrw	$5, (%rdi,%rbx,2), %xmm0, %xmm0
	movq	%rbx, 1272(%rsp)        # 8-byte Spill
	vpinsrw	$6, (%rdi,%r13,2), %xmm0, %xmm0
	movq	%r13, 1264(%rsp)        # 8-byte Spill
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movq	1728(%rsp), %rdi        # 8-byte Reload
	movq	%rax, 1392(%rsp)        # 8-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm5
.LBB148_265:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movq	2504(%rsp), %r12        # 8-byte Reload
	vmovaps	.LCPI148_8(%rip), %ymm11 # ymm11 = <u,4,u,5,u,6,u,7>
	vmovaps	.LCPI148_9(%rip), %ymm12 # ymm12 = <4,u,5,u,6,u,7,u>
	vmovaps	.LCPI148_10(%rip), %ymm13 # ymm13 = <u,0,u,1,u,2,u,3>
	vmovaps	1312(%rsp), %ymm3       # 32-byte Reload
	movq	1760(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, 1728(%rsp)        # 8-byte Spill
	sarq	$32, %rcx
	movq	1480(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rax
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	sarq	$32, %rbx
	movslq	%edi, %r10
	movq	2552(%rsp), %rsi        # 8-byte Reload
	movzwl	(%rsi,%r11,2), %r13d
	movq	1792(%rsp), %rax        # 8-byte Reload
	movzwl	(%rsi,%rax,2), %edx
	movzwl	(%rsi,%r9,2), %r15d
	movq	%rsi, %rdi
	testl	%r8d, %r8d
	jne	.LBB148_266
# BB#267:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movl	%r15d, 1472(%rsp)       # 4-byte Spill
	movl	%r13d, 1480(%rsp)       # 4-byte Spill
	movq	%r11, 1152(%rsp)        # 8-byte Spill
	movq	%r9, 1312(%rsp)         # 8-byte Spill
	vpxor	%ymm6, %ymm6, %ymm6
	jmp	.LBB148_268
	.align	16, 0x90
.LBB148_266:                            #   in Loop: Header=BB148_166 Depth=3
	movq	%r11, 1152(%rsp)        # 8-byte Spill
	movq	%r9, 1312(%rsp)         # 8-byte Spill
	movq	1728(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm0
	vpinsrw	$1, (%rdi,%rcx,2), %xmm0, %xmm0
	movq	1760(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$3, (%rdi,%rbx,2), %xmm0, %xmm0
	vpinsrw	$4, (%rdi,%r10,2), %xmm0, %xmm0
	vpinsrw	$5, %r13d, %xmm0, %xmm0
	movl	%r13d, 1480(%rsp)       # 4-byte Spill
	vpinsrw	$6, %edx, %xmm0, %xmm0
	vpinsrw	$7, %r15d, %xmm0, %xmm0
	movl	%r15d, 1472(%rsp)       # 4-byte Spill
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm6
.LBB148_268:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movl	%edx, %r13d
	movq	1264(%rsp), %r8         # 8-byte Reload
	movq	1216(%rsp), %r9         # 8-byte Reload
	movq	1208(%rsp), %rdx        # 8-byte Reload
	movq	1200(%rsp), %rax        # 8-byte Reload
	movq	%r10, %r11
	vpermps	%ymm6, %ymm11, %ymm0
	vpermps	%ymm4, %ymm12, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm6, %ymm13, %ymm1
	vmovaps	.LCPI148_11(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vmovaps	%ymm2, %ymm6
	vpermps	%ymm4, %ymm6, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm5, %ymm11, %ymm2
	vpermps	%ymm3, %ymm12, %ymm4
	vblendps	$170, %ymm2, %ymm4, %ymm2 # ymm2 = ymm4[0],ymm2[1],ymm4[2],ymm2[3],ymm4[4],ymm2[5],ymm4[6],ymm2[7]
	vpermps	%ymm5, %ymm13, %ymm4
	vpermps	%ymm3, %ymm6, %ymm3
	vmovaps	%ymm6, %ymm14
	vblendps	$170, %ymm4, %ymm3, %ymm3 # ymm3 = ymm3[0],ymm4[1],ymm3[2],ymm4[3],ymm3[4],ymm4[5],ymm3[6],ymm4[7]
	movq	2656(%rsp), %r10        # 8-byte Reload
	movslq	%r10d, %rsi
	subq	2456(%rsp), %rsi        # 8-byte Folded Reload
	addq	1080(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, (%r12,%rsi,4)
	vmovups	%ymm2, 32(%r12,%rsi,4)
	vmovups	%ymm1, 64(%r12,%rsi,4)
	vmovups	%ymm0, 96(%r12,%rsi,4)
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm0
	movq	1280(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$2, (%rdi,%rdx,2), %xmm0, %xmm0
	movq	1400(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$4, (%rdi,%r9,2), %xmm0, %xmm0
	movq	1272(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm0, %xmm0
	vpinsrw	$6, (%rdi,%r8,2), %xmm0, %xmm0
	movq	1392(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm0, %xmm0
	movq	1728(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	vpinsrw	$1, (%rdi,%rcx,2), %xmm1, %xmm1
	movq	1760(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	vpinsrw	$3, (%rdi,%rbx,2), %xmm1, %xmm1
	vpinsrw	$4, (%rdi,%r11,2), %xmm1, %xmm3
	movq	1520(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	2592(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2560(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1448(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1920(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1408(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1344(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1456(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm5
	movq	1824(%rsp), %rax        # 8-byte Reload
	movzwl	(%rdi,%rax,2), %eax
	vmovd	%eax, %xmm1
	movq	1488(%rsp), %rax        # 8-byte Reload
	vpinsrw	$1, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1888(%rsp), %rax        # 8-byte Reload
	vpinsrw	$2, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1504(%rsp), %rax        # 8-byte Reload
	vpinsrw	$3, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1856(%rsp), %rax        # 8-byte Reload
	vpinsrw	$4, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1496(%rsp), %rax        # 8-byte Reload
	vpinsrw	$5, (%rdi,%rax,2), %xmm1, %xmm1
	movq	2624(%rsp), %rax        # 8-byte Reload
	vpinsrw	$6, (%rdi,%rax,2), %xmm1, %xmm1
	movq	1512(%rsp), %rax        # 8-byte Reload
	vpinsrw	$7, (%rdi,%rax,2), %xmm1, %xmm1
	movl	%r10d, %edx
	movq	1120(%rsp), %rax        # 8-byte Reload
	orl	%eax, %edx
	andl	$1, %edx
	sete	%al
	vpmovzxwd	%xmm1, %ymm8    # ymm8 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm7
	vmovaps	%ymm7, %ymm4
	je	.LBB148_270
# BB#269:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm4, %ymm4, %ymm4
.LBB148_270:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vpmovzxwd	%xmm5, %ymm6    # ymm6 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vcvtdq2ps	%ymm8, %ymm5
	orb	1464(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%ymm5, %ymm8
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1660(%rsp), %r15d       # 4-byte Reload
	movl	1696(%rsp), %r8d        # 4-byte Reload
	movb	1568(%rsp), %al         # 1-byte Reload
	movl	1472(%rsp), %edi        # 4-byte Reload
	jne	.LBB148_272
# BB#271:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm8, %ymm8, %ymm8
.LBB148_272:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vcvtdq2ps	%ymm6, %ymm6
	vmovaps	%ymm6, %ymm9
	jne	.LBB148_274
# BB#273:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm9, %ymm9, %ymm9
.LBB148_274:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movl	1528(%rsp), %ebx        # 4-byte Reload
	andb	%bl, %al
	jne	.LBB148_276
# BB#275:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm5, %ymm5, %ymm5
.LBB148_276:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	jne	.LBB148_278
# BB#277:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm6, %ymm6, %ymm6
.LBB148_278:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	orb	1536(%rsp), %al         # 1-byte Folded Reload
	jne	.LBB148_280
# BB#279:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vxorps	%ymm7, %ymm7, %ymm7
.LBB148_280:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movl	1480(%rsp), %eax        # 4-byte Reload
	jne	.LBB148_281
# BB#282:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vpxor	%ymm10, %ymm10, %ymm10
	jmp	.LBB148_283
	.align	16, 0x90
.LBB148_281:                            #   in Loop: Header=BB148_166 Depth=3
	vpinsrw	$5, %eax, %xmm3, %xmm0
	vpinsrw	$6, %r13d, %xmm0, %xmm0
	vpinsrw	$7, %edi, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm10
.LBB148_283:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movq	2392(%rsp), %rax        # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	vpermps	%ymm10, %ymm11, %ymm0
	vpermps	%ymm9, %ymm12, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	vpermps	%ymm10, %ymm13, %ymm1
	vpermps	%ymm9, %ymm14, %ymm2
	vblendps	$170, %ymm1, %ymm2, %ymm1 # ymm1 = ymm2[0],ymm1[1],ymm2[2],ymm1[3],ymm2[4],ymm1[5],ymm2[6],ymm1[7]
	vpermps	%ymm7, %ymm11, %ymm2
	vpermps	%ymm8, %ymm12, %ymm9
	vblendps	$170, %ymm2, %ymm9, %ymm2 # ymm2 = ymm9[0],ymm2[1],ymm9[2],ymm2[3],ymm9[4],ymm2[5],ymm9[6],ymm2[7]
	vpermps	%ymm7, %ymm13, %ymm7
	vpermps	%ymm8, %ymm14, %ymm8
	vmovaps	%ymm14, %ymm9
	vblendps	$170, %ymm7, %ymm8, %ymm7 # ymm7 = ymm8[0],ymm7[1],ymm8[2],ymm7[3],ymm8[4],ymm7[5],ymm8[6],ymm7[7]
	vmovups	%ymm7, 12288(%r12,%rax,4)
	vmovups	%ymm2, 12320(%r12,%rax,4)
	vmovups	%ymm1, 12352(%r12,%rax,4)
	vmovups	%ymm0, 12384(%r12,%rax,4)
	testl	%edx, %edx
	je	.LBB148_284
# BB#285:                               # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	movl	1600(%rsp), %edx        # 4-byte Reload
	vxorps	%ymm2, %ymm2, %ymm2
	jmp	.LBB148_286
	.align	16, 0x90
.LBB148_284:                            #   in Loop: Header=BB148_166 Depth=3
	movq	2552(%rsp), %rax        # 8-byte Reload
	movq	1152(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$5, (%rax,%rdx,2), %xmm3, %xmm0
	movq	1792(%rsp), %rdx        # 8-byte Reload
	vpinsrw	$6, (%rax,%rdx,2), %xmm0, %xmm0
	movq	1312(%rsp), %rdx        # 8-byte Reload
	movzwl	(%rax,%rdx,2), %eax
	vpinsrw	$7, %eax, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0    # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vcvtdq2ps	%ymm0, %ymm2
	movl	1600(%rsp), %edx        # 4-byte Reload
.LBB148_286:                            # %for deinterleaved$1.s0.v10.v1082
                                        #   in Loop: Header=BB148_166 Depth=3
	vpermps	%ymm6, %ymm12, %ymm0
	vpermps	%ymm2, %ymm11, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	vpermps	%ymm6, %ymm9, %ymm1
	vpermps	%ymm2, %ymm13, %ymm2
	vblendps	$170, %ymm2, %ymm1, %ymm1 # ymm1 = ymm1[0],ymm2[1],ymm1[2],ymm2[3],ymm1[4],ymm2[5],ymm1[6],ymm2[7]
	vpermps	%ymm4, %ymm11, %ymm2
	vpermps	%ymm5, %ymm12, %ymm3
	vblendps	$170, %ymm2, %ymm3, %ymm2 # ymm2 = ymm3[0],ymm2[1],ymm3[2],ymm2[3],ymm3[4],ymm2[5],ymm3[6],ymm2[7]
	vpermps	%ymm4, %ymm13, %ymm3
	vpermps	%ymm5, %ymm9, %ymm4
	vblendps	$170, %ymm3, %ymm4, %ymm3 # ymm3 = ymm4[0],ymm3[1],ymm4[2],ymm3[3],ymm4[4],ymm3[5],ymm4[6],ymm3[7]
	addq	2384(%rsp), %rsi        # 8-byte Folded Reload
	vmovups	%ymm3, 24576(%r12,%rsi,4)
	vmovups	%ymm2, 24608(%r12,%rsi,4)
	vmovups	%ymm1, 24640(%r12,%rsi,4)
	vmovups	%ymm0, 24672(%r12,%rsi,4)
	addl	$32, %r10d
	addl	$-1, %edx
	jne	.LBB148_166
.LBB148_287:                            # %end for deinterleaved$1.s0.v10.v1083
                                        #   in Loop: Header=BB148_164 Depth=2
	movl	816(%rsp), %eax         # 4-byte Reload
	addl	$1, %eax
	movl	%eax, 816(%rsp)         # 4-byte Spill
	addq	$1, 1120(%rsp)          # 8-byte Folded Spill
	cmpl	732(%rsp), %eax         # 4-byte Folded Reload
	jne	.LBB148_164
.LBB148_288:                            # %produce gH87
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	888(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rdx
	leal	2(%rdx), %ebx
	movl	%ebx, 2656(%rsp)        # 4-byte Spill
	movq	512(%rsp), %rcx         # 8-byte Reload
	cmpl	%ebx, %ecx
	movl	%ebx, %eax
	cmovgel	%ecx, %eax
	leal	4(%rdx), %r12d
	cmpl	%eax, %r12d
	movl	%r12d, %ecx
	cmovgl	%eax, %ecx
	movl	436(%rsp), %edx         # 4-byte Reload
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	movl	604(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4664(%rsp)
	movl	556(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4668(%rsp)
	movl	600(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4672(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4676(%rsp)
	movl	552(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4692(%rsp)
	movl	596(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4696(%rsp)
	movl	676(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4700(%rsp)
	movq	680(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 4704(%rsp)
	movq	392(%rsp), %r13         # 8-byte Reload
	movl	%r13d, 4708(%rsp)
	movl	%eax, 4712(%rsp)
	movl	%ecx, 4716(%rsp)
	movl	%r9d, 4720(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4724(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4728(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4732(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4736(%rsp)
	movl	%r15d, 4740(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4744(%rsp)
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4760(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 4776(%rsp)
	movq	$0, 4784(%rsp)
	movq	440(%rsp), %rax         # 8-byte Reload
	movq	%rax, 4792(%rsp)
	movq	$0, 4800(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 4808(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172(%rip), %rsi
	movl	%ebx, %edx
	leaq	4664(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#289:                               # %produce gV92
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	512(%rsp), %rcx         # 8-byte Reload
	movq	888(%rsp), %rdx         # 8-byte Reload
	cmpl	%edx, %ecx
	movl	%edx, %eax
	cmovgel	%ecx, %eax
	addl	$2, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	leal	3(%rdx), %ecx
	movq	%rdx, %rbx
	movl	432(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	movl	%edx, %r12d
	addl	$1, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	604(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4840(%rsp)
	movl	556(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4844(%rsp)
	movl	600(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4848(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4852(%rsp)
	movl	552(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4868(%rsp)
	movl	596(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4872(%rsp)
	movl	676(%rsp), %edx         # 4-byte Reload
	movl	%edx, 4876(%rsp)
	movq	680(%rsp), %rdx         # 8-byte Reload
	movl	%edx, 4880(%rsp)
	movl	%r13d, 4884(%rsp)
	movl	%ecx, 4888(%rsp)
	movl	%eax, 4892(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 4896(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4900(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4904(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4908(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 4912(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 4916(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4920(%rsp)
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 4936(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 4952(%rsp)
	movq	$0, 4960(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movq	%rax, 4968(%rsp)
	movq	$0, 4976(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 4984(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173(%rip), %rsi
	movl	2656(%rsp), %edx        # 4-byte Reload
	leaq	4840(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#290:                               # %produce dV97
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	%rbx, %rdx
	leal	6(%rdx), %ebx
	movl	428(%rsp), %ecx         # 4-byte Reload
	cmpl	%ebx, %ecx
	movl	%ebx, %eax
	cmovgel	%ecx, %eax
	movl	532(%rsp), %ecx         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	7(%rdx), %ecx
	cmpl	%r12d, %ecx
	cmovgl	%r12d, %ecx
	addl	$1, %ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	604(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5016(%rsp)
	movl	556(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5020(%rsp)
	movl	600(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5024(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5028(%rsp)
	movl	552(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5044(%rsp)
	movl	596(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5048(%rsp)
	movl	%r13d, 5052(%rsp)
	movl	%ecx, 5056(%rsp)
	movl	%eax, 5060(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5064(%rsp)
	movq	680(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5068(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5072(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5076(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5080(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5084(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5088(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5092(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5096(%rsp)
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5112(%rsp)
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5128(%rsp)
	movq	$0, 5136(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 5144(%rsp)
	movq	$0, 5152(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 5160(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174(%rip), %rsi
	movl	%ebx, %edx
	leaq	5016(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#291:                               # %produce dh102
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	512(%rsp), %rcx         # 8-byte Reload
	cmpl	%ebx, %ecx
	movl	%ebx, %eax
	cmovgel	%ecx, %eax
	movl	532(%rsp), %esi         # 4-byte Reload
	cmpl	%eax, %esi
	movl	%esi, %ecx
	cmovgl	%eax, %ecx
	movl	436(%rsp), %edx         # 4-byte Reload
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	movl	604(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5192(%rsp)
	movl	556(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5196(%rsp)
	movl	600(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5200(%rsp)
	vmovaps	576(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5204(%rsp)
	movl	552(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5220(%rsp)
	movl	596(%rsp), %edx         # 4-byte Reload
	movl	%edx, 5224(%rsp)
	movl	%r13d, 5228(%rsp)
	movl	%eax, 5232(%rsp)
	movl	%ecx, 5236(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5240(%rsp)
	movq	680(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5244(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5248(%rsp)
	movl	548(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5252(%rsp)
	movl	544(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5256(%rsp)
	movl	540(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5260(%rsp)
	movl	536(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5264(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5268(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5272(%rsp)
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5288(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 5304(%rsp)
	movq	$0, 5312(%rsp)
	movq	464(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5320(%rsp)
	movq	$0, 5328(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 5336(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175(%rip), %rsi
	movl	%ebx, %edx
	leaq	5192(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#292:                               # %produce f4107
                                        #   in Loop: Header=BB148_118 Depth=1
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5368(%rsp)
	movl	%r13d, 5372(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5376(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5380(%rsp)
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5384(%rsp)
	movq	$0, 5392(%rsp)
	movq	464(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5400(%rsp)
	movq	$0, 5408(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5416(%rsp)
	movq	$0, 5424(%rsp)
	movq	440(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5432(%rsp)
	movq	$0, 5440(%rsp)
	movq	488(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5448(%rsp)
	movq	$0, 5456(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176(%rip), %rsi
	movl	2656(%rsp), %edx        # 4-byte Reload
	leaq	5368(%rsp), %r8
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#293:                               # %produce f7112
                                        #   in Loop: Header=BB148_118 Depth=1
	movl	428(%rsp), %eax         # 4-byte Reload
	movq	888(%rsp), %rdx         # 8-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %r12d
	cmovgel	%eax, %r12d
	movl	2656(%rsp), %eax        # 4-byte Reload
	cmpl	%r12d, %eax
	cmovlel	%eax, %r12d
	leal	1(%rdx), %ebx
	movl	432(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovgel	%eax, %ebx
	addl	$1, %ebx
	cmpl	%ebx, %r12d
	cmovgel	%r12d, %ebx
	movl	600(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5464(%rsp)
	movl	284(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5468(%rsp)
	movl	596(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5472(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5476(%rsp)
	movq	680(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5480(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5484(%rsp)
	movl	%r13d, 5488(%rsp)
	movl	%ebx, 5492(%rsp)
	movl	%r12d, 5496(%rsp)
	movq	472(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5500(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5504(%rsp)
	vmovaps	448(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5508(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5524(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5528(%rsp)
	movl	424(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5544(%rsp)
	movl	280(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5548(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 5552(%rsp)
	movq	$0, 5560(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5568(%rsp)
	movq	$0, 5576(%rsp)
	movq	744(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5584(%rsp)
	movq	$0, 5592(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 5600(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177(%rip), %rsi
	leaq	5464(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#294:                               # %produce f8117
                                        #   in Loop: Header=BB148_118 Depth=1
	movl	604(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5632(%rsp)
	movl	276(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5636(%rsp)
	movl	272(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5640(%rsp)
	movl	676(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5644(%rsp)
	movq	680(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5648(%rsp)
	movq	520(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5652(%rsp)
	movq	472(%rsp), %rax         # 8-byte Reload
	movl	%eax, 5656(%rsp)
	movl	%r13d, 5660(%rsp)
	movl	%ebx, 5664(%rsp)
	movl	%r12d, 5668(%rsp)
	movl	2172(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5672(%rsp)
	vmovaps	448(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5676(%rsp)
	movl	1660(%rsp), %eax        # 4-byte Reload
	movl	%eax, 5692(%rsp)
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vmovups	%xmm0, 5696(%rsp)
	movl	424(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5712(%rsp)
	movl	268(%rsp), %eax         # 4-byte Reload
	movl	%eax, 5716(%rsp)
	movq	2504(%rsp), %rax        # 8-byte Reload
	movq	%rax, 5720(%rsp)
	movq	$0, 5728(%rsp)
	movq	752(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5736(%rsp)
	movq	$0, 5744(%rsp)
	movq	736(%rsp), %rax         # 8-byte Reload
	movq	%rax, 5752(%rsp)
	movq	$0, 5760(%rsp)
	vmovdqa	608(%rsp), %ymm0        # 32-byte Reload
	vmovdqu	%ymm0, 5768(%rsp)
	movl	$2, %ecx
	movq	664(%rsp), %rdi         # 8-byte Reload
	leaq	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178(%rip), %rsi
	movq	888(%rsp), %rdx         # 8-byte Reload
	leaq	5632(%rsp), %r8
	vzeroupper
	callq	halide_do_par_for@PLT
	movl	%eax, %r15d
	testl	%r15d, %r15d
	jne	.LBB148_3
# BB#295:                               # %consume f8121
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	384(%rsp), %rax         # 8-byte Reload
	testl	%eax, %eax
	js	.LBB148_318
# BB#296:                               # %for f0.s0.v10.v10122.preheader
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	888(%rsp), %rax         # 8-byte Reload
	cltq
	movq	376(%rsp), %rcx         # 8-byte Reload
	movq	336(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	8(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 1824(%rsp)       # 32-byte Spill
	vbroadcastss	4(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 1792(%rsp)       # 32-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %ymm0
	vmovaps	%ymm0, 1760(%rsp)       # 32-byte Spill
	movq	320(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rcx), %ymm0
	vmovaps	%ymm0, 1728(%rsp)       # 32-byte Spill
	movq	312(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rcx), %ymm0
	vmovaps	%ymm0, 1696(%rsp)       # 32-byte Spill
	movq	304(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rcx), %ymm0
	vmovaps	%ymm0, 1664(%rsp)       # 32-byte Spill
	movq	296(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rcx), %ymm0
	vmovaps	%ymm0, 1600(%rsp)       # 32-byte Spill
	movq	288(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rcx), %ymm0
	vmovaps	%ymm0, 1568(%rsp)       # 32-byte Spill
	movq	416(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %esi
	andl	$63, %esi
	movq	%rsi, %rdi
	movq	352(%rsp), %rcx         # 8-byte Reload
	imulq	%rcx, %rdi
	movq	%rdi, 1512(%rsp)        # 8-byte Spill
	leaq	1(%rdx), %rbx
	andl	$63, %ebx
	movq	%rbx, %rdi
	imulq	%rcx, %rdi
	movq	%rdi, 1496(%rsp)        # 8-byte Spill
	movq	328(%rsp), %rcx         # 8-byte Reload
	vpbroadcastd	(%rcx), %ymm0
	vmovdqa	%ymm0, 1536(%rsp)       # 32-byte Spill
	negq	%rax
	leaq	1(%rdx,%rax), %r12
	shlq	$5, %r12
	leaq	8(%r12), %rax
	movq	%rax, 1488(%rsp)        # 8-byte Spill
	leaq	16(%r12), %rax
	movq	%rax, 1480(%rsp)        # 8-byte Spill
	movq	344(%rsp), %rax         # 8-byte Reload
	imulq	%rax, %rsi
	movq	%rsi, 1520(%rsp)        # 8-byte Spill
	leaq	24(%r12), %rcx
	movq	%rcx, 1472(%rsp)        # 8-byte Spill
	imulq	%rax, %rbx
	movq	%rbx, 1504(%rsp)        # 8-byte Spill
	movq	968(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ecx
	xorl	%eax, %eax
	.align	16, 0x90
.LBB148_297:                            # %for f0.s0.v10.v10122
                                        #   Parent Loop BB148_118 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB148_298 Depth 3
                                        #         Child Loop BB148_299 Depth 4
	movq	%rax, 1528(%rsp)        # 8-byte Spill
	shll	$5, %eax
	movq	968(%rsp), %rdx         # 8-byte Reload
	addl	%edx, %eax
	cltq
	movq	688(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %rsi
	movl	%ecx, %r9d
	movq	1520(%rsp), %rcx        # 8-byte Reload
	leaq	(%rsi,%rcx), %rdx
	movq	752(%rsp), %rbx         # 8-byte Reload
	vmovups	(%rbx,%rdx,4), %ymm0
	movq	1504(%rsp), %rdi        # 8-byte Reload
	leaq	(%rsi,%rdi), %rdx
	vmovups	(%rbx,%rdx,4), %ymm1
	leaq	8(%rcx,%rsi), %rdx
	vmovdqu	(%rbx,%rdx,4), %ymm2
	leaq	8(%rdi,%rsi), %rdx
	vmovdqu	(%rbx,%rdx,4), %ymm3
	leaq	16(%rcx,%rsi), %rdx
	vmovdqu	(%rbx,%rdx,4), %ymm4
	leaq	16(%rdi,%rsi), %rdx
	vmovups	(%rbx,%rdx,4), %ymm5
	leaq	24(%rcx,%rsi), %rdx
	vmovups	(%rbx,%rdx,4), %ymm6
	subq	2456(%rsp), %rax        # 8-byte Folded Reload
	leaq	24(%rdi,%rsi), %rdx
	vmovdqu	(%rbx,%rdx,4), %ymm7
	movq	1512(%rsp), %rcx        # 8-byte Reload
	leaq	(%rax,%rcx), %rdx
	movq	744(%rsp), %rsi         # 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm8
	vmovaps	%ymm8, 3872(%rsp)
	movq	736(%rsp), %rbx         # 8-byte Reload
	vmovdqu	(%rbx,%rdx,4), %ymm8
	movq	1496(%rsp), %rdi        # 8-byte Reload
	leaq	(%rax,%rdi), %rdx
	vmovups	(%rsi,%rdx,4), %ymm9
	vmovaps	%ymm9, 3872(%rsp,%r12,4)
	vmovups	(%rbx,%rdx,4), %ymm9
	leaq	8(%rcx,%rax), %rdx
	vmovups	(%rsi,%rdx,4), %ymm10
	vmovaps	%ymm10, 3904(%rsp)
	vmovups	(%rbx,%rdx,4), %ymm10
	leaq	8(%rax,%rdi), %r8
	vmovups	(%rsi,%r8,4), %ymm11
	movq	1488(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm11, 3872(%rsp,%rdx,4)
	vmovups	(%rbx,%r8,4), %ymm11
	leaq	16(%rax,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm12
	vmovaps	%ymm12, 3936(%rsp)
	vmovdqu	(%rbx,%rdx,4), %ymm12
	leaq	16(%rax,%rdi), %r8
	vmovups	(%rsi,%r8,4), %ymm13
	movq	1480(%rsp), %rdx        # 8-byte Reload
	vmovaps	%ymm13, 3872(%rsp,%rdx,4)
	vmovdqu	(%rbx,%r8,4), %ymm13
	leaq	24(%rax,%rcx), %rdx
	vmovups	(%rsi,%rdx,4), %ymm14
	vmovaps	%ymm14, 3968(%rsp)
	leaq	24(%rax,%rdi), %rax
	vmovups	(%rsi,%rax,4), %ymm14
	movq	1472(%rsp), %rcx        # 8-byte Reload
	vmovaps	%ymm14, 3872(%rsp,%rcx,4)
	movl	%r9d, %ecx
	vmovaps	%ymm0, 4128(%rsp)
	vmovaps	%ymm1, 4128(%rsp,%r12,4)
	vmovdqa	%ymm2, 4160(%rsp)
	vmovdqa	%ymm3, 4160(%rsp,%r12,4)
	vmovdqa	%ymm4, 4192(%rsp)
	vmovaps	%ymm5, 4192(%rsp,%r12,4)
	vmovaps	%ymm6, 4224(%rsp)
	vmovdqa	%ymm7, 4224(%rsp,%r12,4)
	vmovdqa	%ymm8, 4384(%rsp)
	vmovaps	%ymm9, 4384(%rsp,%r12,4)
	vmovaps	%ymm10, 4416(%rsp)
	vmovaps	%ymm11, 4416(%rsp,%r12,4)
	vmovdqa	%ymm12, 4448(%rsp)
	vmovdqa	%ymm13, 4448(%rsp,%r12,4)
	vmovups	(%rbx,%rdx,4), %ymm0
	movslq	%ecx, %rdx
	movq	712(%rsp), %rsi         # 8-byte Reload
	leaq	(%rdx,%rsi), %r10
	vmovaps	%ymm0, 4480(%rsp)
	vmovdqu	(%rbx,%rax,4), %ymm0
	vmovdqa	%ymm0, 4480(%rsp,%r12,4)
	xorl	%eax, %eax
	.align	16, 0x90
.LBB148_298:                            # %for f0.s0.v11.v13.yii146
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_297 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB148_299 Depth 4
	movq	%rax, %rsi
	shlq	$7, %rsi
	vmovaps	3872(%rsp,%rsi), %ymm14
	vmovaps	%ymm14, 1856(%rsp)      # 32-byte Spill
	vmovaps	3904(%rsp,%rsi), %ymm1
	vmovaps	%ymm1, 1888(%rsp)       # 32-byte Spill
	vmovaps	3936(%rsp,%rsi), %ymm0
	vmovaps	%ymm0, 1920(%rsp)       # 32-byte Spill
	vmovaps	3968(%rsp,%rsi), %ymm8
	vmovaps	4128(%rsp,%rsi), %ymm3
	vmovaps	4160(%rsp,%rsi), %ymm7
	vmovaps	4192(%rsp,%rsi), %ymm12
	vmovaps	4224(%rsp,%rsi), %ymm4
	vmovaps	4384(%rsp,%rsi), %ymm9
	vmovaps	4416(%rsp,%rsi), %ymm10
	vmovaps	4448(%rsp,%rsi), %ymm11
	vmovaps	1760(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm0, %ymm0
	vmulps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm5, %ymm14, %ymm2
	vmulps	%ymm5, %ymm8, %ymm5
	vmovaps	1792(%rsp), %ymm13      # 32-byte Reload
	vmovaps	%ymm13, %ymm6
	vfmadd213ps	%ymm5, %ymm4, %ymm6
	vmovaps	%ymm13, %ymm5
	vfmadd213ps	%ymm2, %ymm3, %ymm5
	vmovaps	%ymm13, %ymm2
	vfmadd213ps	%ymm1, %ymm7, %ymm2
	vmovaps	%ymm13, %ymm1
	vfmadd213ps	%ymm0, %ymm12, %ymm1
	vmovaps	1824(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm13
	vfmadd213ps	%ymm1, %ymm11, %ymm13
	vmovaps	%ymm13, 2656(%rsp)      # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm2, %ymm10, %ymm1
	vmovaps	%ymm1, 2624(%rsp)       # 32-byte Spill
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm5, %ymm9, %ymm1
	vmovaps	%ymm1, 2592(%rsp)       # 32-byte Spill
	vmovaps	4480(%rsp,%rsi), %ymm15
	vfmadd213ps	%ymm6, %ymm15, %ymm0
	vmovaps	%ymm0, 2560(%rsp)       # 32-byte Spill
	vmovaps	1664(%rsp), %ymm5       # 32-byte Reload
	vmulps	%ymm5, %ymm8, %ymm0
	vmovaps	1696(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%ymm2, %ymm13
	vfmadd213ps	%ymm0, %ymm4, %ymm13
	vmulps	%ymm5, %ymm14, %ymm0
	vmovaps	%ymm2, %ymm6
	vfmadd213ps	%ymm0, %ymm3, %ymm6
	vmulps	1888(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm1
	vfmadd213ps	%ymm0, %ymm7, %ymm1
	vmulps	1920(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
	vmovaps	%ymm2, %ymm5
	vfmadd213ps	%ymm0, %ymm12, %ymm5
	vmovaps	1728(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%ymm0, %ymm14
	vfmadd213ps	%ymm5, %ymm11, %ymm14
	vmovaps	%ymm0, %ymm5
	vfmadd213ps	%ymm1, %ymm10, %ymm5
	vmovaps	%ymm0, %ymm1
	vfmadd213ps	%ymm6, %ymm9, %ymm1
	vmovaps	%ymm0, %ymm6
	vfmadd213ps	%ymm13, %ymm15, %ymm6
	vmovaps	1536(%rsp), %ymm2       # 32-byte Reload
	vmulps	%ymm2, %ymm8, %ymm8
	vmovaps	1568(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm4
	vmulps	1856(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm3
	vmulps	1888(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm7
	vmulps	1920(%rsp), %ymm2, %ymm8 # 32-byte Folded Reload
	vfmadd213ps	%ymm8, %ymm0, %ymm12
	vmovaps	1600(%rsp), %ymm0       # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm0, %ymm11
	vfmadd213ps	%ymm7, %ymm0, %ymm10
	vfmadd213ps	%ymm3, %ymm0, %ymm9
	vfmadd213ps	%ymm4, %ymm0, %ymm15
	xorl	%r8d, %r8d
	movl	$3, %r9d
	movq	%r10, %rsi
	.align	16, 0x90
.LBB148_299:                            # %for f0.s0.v12149
                                        #   Parent Loop BB148_118 Depth=1
                                        #     Parent Loop BB148_297 Depth=2
                                        #       Parent Loop BB148_298 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovaps	%ymm1, %ymm3
	cmpl	$1, %r8d
	je	.LBB148_301
# BB#300:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	2592(%rsp), %ymm3       # 32-byte Reload
.LBB148_301:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm5, %ymm4
	je	.LBB148_303
# BB#302:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	2624(%rsp), %ymm4       # 32-byte Reload
.LBB148_303:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm14, %ymm8
	je	.LBB148_305
# BB#304:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	2656(%rsp), %ymm8       # 32-byte Reload
.LBB148_305:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm6, %ymm12
	je	.LBB148_307
# BB#306:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	2560(%rsp), %ymm12      # 32-byte Reload
.LBB148_307:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm15, %ymm7
	testl	%r8d, %r8d
	je	.LBB148_309
# BB#308:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm12, %ymm7
.LBB148_309:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm11, %ymm12
	je	.LBB148_311
# BB#310:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm8, %ymm12
.LBB148_311:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm10, %ymm8
	je	.LBB148_313
# BB#312:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm4, %ymm8
.LBB148_313:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm9, %ymm4
	je	.LBB148_315
# BB#314:                               # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vmovaps	%ymm3, %ymm4
.LBB148_315:                            # %for f0.s0.v12149
                                        #   in Loop: Header=BB148_299 Depth=4
	vbroadcastss	.LCPI148_12(%rip), %ymm3
	vminps	%ymm3, %ymm4, %ymm4
	vminps	%ymm3, %ymm8, %ymm8
	vminps	%ymm3, %ymm12, %ymm12
	vminps	%ymm3, %ymm7, %ymm3
	vxorps	%ymm0, %ymm0, %ymm0
	vmaxps	%ymm0, %ymm4, %ymm4
	vmaxps	%ymm0, %ymm8, %ymm7
	vmaxps	%ymm0, %ymm12, %ymm8
	vmaxps	%ymm0, %ymm3, %ymm3
	vcvttps2dq	%ymm4, %ymm4
	vmovdqa	.LCPI148_3(%rip), %ymm0 # ymm0 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	vpshufb	%ymm0, %ymm4, %ymm4
	vpermq	$232, %ymm4, %ymm4      # ymm4 = ymm4[0,2,2,3]
	vcvttps2dq	%ymm7, %ymm7
	vpshufb	%ymm0, %ymm7, %ymm7
	vpermq	$232, %ymm7, %ymm7      # ymm7 = ymm7[0,2,2,3]
	vcvttps2dq	%ymm8, %ymm8
	vpshufb	%ymm0, %ymm8, %ymm8
	vpermq	$232, %ymm8, %ymm8      # ymm8 = ymm8[0,2,2,3]
	vcvttps2dq	%ymm3, %ymm3
	vpshufb	%ymm0, %ymm3, %ymm3
	vpermq	$232, %ymm3, %ymm3      # ymm3 = ymm3[0,2,2,3]
	vpmovzxwd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm8, %ymm8    # ymm8 = xmm8[0],zero,xmm8[1],zero,xmm8[2],zero,xmm8[3],zero,xmm8[4],zero,xmm8[5],zero,xmm8[6],zero,xmm8[7],zero
	vpmovzxwd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpmovzxwd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vmovdqa	2688(%rsp), %ymm0       # 32-byte Reload
	vpmulld	%ymm0, %ymm4, %ymm12
	vpmulld	%ymm0, %ymm7, %ymm7
	vpmulld	%ymm0, %ymm8, %ymm8
	vpmulld	%ymm0, %ymm3, %ymm3
	vmovd	%r8d, %xmm4
	vpsubd	2720(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpbroadcastd	%xmm4, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm4
	vpaddd	%ymm8, %ymm13, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	%ymm12, %ymm13, %ymm3
	vmovq	%xmm8, %r11
	movslq	%r11d, %r15
	movzbl	(%r14,%r15), %edx
	vmovd	%edx, %xmm2
	vpextrq	$1, %xmm8, %rdx
	sarq	$32, %r11
	vpinsrb	$1, (%r14,%r11), %xmm2, %xmm2
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vextracti128	$1, %ymm8, %xmm0
	vpinsrb	$2, (%r14,%rbx), %xmm2, %xmm2
	vmovq	%xmm0, %rbx
	vpinsrb	$3, (%r14,%rdx), %xmm2, %xmm2
	movslq	%ebx, %rdx
	vpinsrb	$4, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm0, %rdx
	sarq	$32, %rbx
	vpinsrb	$5, (%r14,%rbx), %xmm2, %xmm0
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vpinsrb	$6, (%r14,%rbx), %xmm0, %xmm0
	vmovq	%xmm4, %rbx
	vpinsrb	$7, (%r14,%rdx), %xmm0, %xmm0
	movslq	%ebx, %rdx
	vpinsrb	$8, (%r14,%rdx), %xmm0, %xmm0
	vpextrq	$1, %xmm4, %rdx
	sarq	$32, %rbx
	vpinsrb	$9, (%r14,%rbx), %xmm0, %xmm0
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vextracti128	$1, %ymm4, %xmm2
	vpinsrb	$10, (%r14,%rbx), %xmm0, %xmm0
	vmovq	%xmm2, %rbx
	vpinsrb	$11, (%r14,%rdx), %xmm0, %xmm0
	movslq	%ebx, %rdx
	vpinsrb	$12, (%r14,%rdx), %xmm0, %xmm0
	vpextrq	$1, %xmm2, %rdx
	sarq	$32, %rbx
	vpinsrb	$13, (%r14,%rbx), %xmm0, %xmm0
	movslq	%edx, %rbx
	vpinsrb	$14, (%r14,%rbx), %xmm0, %xmm0
	vmovq	%xmm3, %rbx
	sarq	$32, %rdx
	vpinsrb	$15, (%r14,%rdx), %xmm0, %xmm0
	movslq	%ebx, %rdx
	movzbl	(%r14,%rdx), %edx
	vmovd	%edx, %xmm2
	vpextrq	$1, %xmm3, %rdx
	sarq	$32, %rbx
	vpinsrb	$1, (%r14,%rbx), %xmm2, %xmm2
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vextracti128	$1, %ymm3, %xmm3
	vpinsrb	$2, (%r14,%rbx), %xmm2, %xmm2
	vmovq	%xmm3, %rbx
	vpinsrb	$3, (%r14,%rdx), %xmm2, %xmm2
	movslq	%ebx, %rdx
	vpinsrb	$4, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm3, %rdx
	sarq	$32, %rbx
	vpinsrb	$5, (%r14,%rbx), %xmm2, %xmm2
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vpinsrb	$6, (%r14,%rbx), %xmm2, %xmm2
	vmovq	%xmm7, %rbx
	vpinsrb	$7, (%r14,%rdx), %xmm2, %xmm2
	movslq	%ebx, %rdx
	vpinsrb	$8, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm7, %rdx
	sarq	$32, %rbx
	vpinsrb	$9, (%r14,%rbx), %xmm2, %xmm2
	movslq	%edx, %rbx
	sarq	$32, %rdx
	vextracti128	$1, %ymm7, %xmm3
	vpinsrb	$10, (%r14,%rbx), %xmm2, %xmm2
	vmovq	%xmm3, %rbx
	vpinsrb	$11, (%r14,%rdx), %xmm2, %xmm2
	movslq	%ebx, %rdx
	vpinsrb	$12, (%r14,%rdx), %xmm2, %xmm2
	vpextrq	$1, %xmm3, %rdx
	sarq	$32, %rbx
	vpinsrb	$13, (%r14,%rbx), %xmm2, %xmm2
	movslq	%edx, %rbx
	vpinsrb	$14, (%r14,%rbx), %xmm2, %xmm2
	sarq	$32, %rdx
	vpinsrb	$15, (%r14,%rdx), %xmm2, %xmm2
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vmovdqu	%ymm0, (%rsi)
	addq	2776(%rsp), %rsi        # 8-byte Folded Reload
	addl	$1, %r8d
	addq	$-1, %r9
	jne	.LBB148_299
# BB#316:                               # %end for f0.s0.v12150
                                        #   in Loop: Header=BB148_298 Depth=3
	addq	$1, %rax
	addq	1064(%rsp), %r10        # 8-byte Folded Reload
	cmpq	$2, %rax
	jne	.LBB148_298
# BB#317:                               # %end for f0.s0.v11.v13.yii147
                                        #   in Loop: Header=BB148_297 Depth=2
	movq	1528(%rsp), %rax        # 8-byte Reload
	addq	$1, %rax
	addl	$32, %ecx
	cmpl	720(%rsp), %eax         # 4-byte Folded Reload
	jne	.LBB148_297
.LBB148_318:                            # %end for f0.s0.v10.v10123
                                        #   in Loop: Header=BB148_118 Depth=1
	movq	400(%rsp), %rsi         # 8-byte Reload
	addq	$1, %rsi
	movq	496(%rsp), %rdi         # 8-byte Reload
	addl	$1, %edi
	movl	508(%rsp), %ecx         # 4-byte Reload
	addl	$2, %ecx
	movl	408(%rsp), %edx         # 4-byte Reload
	addl	$-2, %edx
	addq	$2, 416(%rsp)           # 8-byte Folded Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	addq	%rax, 712(%rsp)         # 8-byte Folded Spill
	cmpl	$16, %esi
	movq	680(%rsp), %rbx         # 8-byte Reload
	movl	2172(%rsp), %r9d        # 4-byte Reload
	movl	1660(%rsp), %r15d       # 4-byte Reload
	jne	.LBB148_118
# BB#124:                               # %call_destructor.exit208
	movq	664(%rsp), %r12         # 8-byte Reload
	movq	%r12, %rdi
	movq	2504(%rsp), %rsi        # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	440(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	488(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	480(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	464(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	752(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	744(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	movq	%r12, %rdi
	movq	736(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	xorl	%eax, %eax
	movq	%rax, 744(%rsp)         # 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%r14d, %r14d
	xorl	%esi, %esi
	xorl	%r15d, %r15d
.LBB148_125:                            # %destructor_block.thread
	testl	%r15d, %r15d
	sete	%bl
.LBB148_4:                              # %call_destructor.exit
	movq	%rsi, 2504(%rsp)        # 8-byte Spill
	movq	%rax, 480(%rsp)         # 8-byte Spill
	movq	%rcx, 488(%rsp)         # 8-byte Spill
	testb	%bl, %bl
	jne	.LBB148_7
# BB#5:                                 # %call_destructor.exit
	cmpq	$0, 744(%rsp)           # 8-byte Folded Reload
	je	.LBB148_7
# BB#6:                                 # %if.then.i.174
	movq	%r12, %rdi
	movq	744(%rsp), %rsi         # 8-byte Reload
	movq	%rdx, %r13
	callq	halide_free@PLT
	movq	%r13, %rdx
.LBB148_7:                              # %call_destructor.exit175
	testq	%rdx, %rdx
	sete	%al
	orb	%bl, %al
	jne	.LBB148_9
# BB#8:                                 # %if.then.i.179
	movq	%r12, %rdi
	movq	%rdx, %rsi
	callq	halide_free@PLT
.LBB148_9:                              # %call_destructor.exit180
	movq	464(%rsp), %rsi         # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB148_11
# BB#10:                                # %if.then.i.184
	movq	%r12, %rdi
	callq	halide_free@PLT
.LBB148_11:                             # %call_destructor.exit185
	movq	480(%rsp), %rsi         # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB148_13
# BB#12:                                # %if.then.i.189
	movq	%r12, %rdi
	callq	halide_free@PLT
.LBB148_13:                             # %call_destructor.exit190
	movq	488(%rsp), %rsi         # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB148_15
# BB#14:                                # %if.then.i.194
	movq	%r12, %rdi
	callq	halide_free@PLT
.LBB148_15:                             # %call_destructor.exit195
	testq	%r14, %r14
	sete	%al
	orb	%bl, %al
	jne	.LBB148_17
# BB#16:                                # %if.then.i.199
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	halide_free@PLT
.LBB148_17:                             # %call_destructor.exit200
	movq	2504(%rsp), %rsi        # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%al, %bl
	jne	.LBB148_19
# BB#18:                                # %if.then.i.204
	movq	%r12, %rdi
	callq	halide_free@PLT
.LBB148_19:                             # %call_destructor.exit205
	movl	%r15d, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB148_3:                              # %if.then.i
	movq	664(%rsp), %r12         # 8-byte Reload
	movq	%r12, %rdi
	movq	736(%rsp), %rsi         # 8-byte Reload
	callq	halide_free@PLT
	xorl	%ebx, %ebx
	movq	2504(%rsp), %rsi        # 8-byte Reload
	movq	440(%rsp), %r14         # 8-byte Reload
	movq	488(%rsp), %rcx         # 8-byte Reload
	movq	480(%rsp), %rax         # 8-byte Reload
	movq	752(%rsp), %rdx         # 8-byte Reload
	jmp	.LBB148_4
.Lfunc_end148:
	.size	par_for___sharpi_f0.s0.v11.v14, .Lfunc_end148-par_for___sharpi_f0.s0.v11.v14

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI149_0:
	.long	1199570688              # float 65535
.LCPI149_3:
	.long	1065353216              # float 1
.LCPI149_4:
	.long	1073741824              # float 2
.LCPI149_5:
	.long	1048576000              # float 0.25
.LCPI149_6:
	.long	1056964608              # float 0.5
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI149_1:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI149_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI149_9:
	.zero	16,255
	.section	.rodata,"a",@progbits
	.align	32
.LCPI149_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI149_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1064, %rsp             # imm = 0x428
	movq	%rdx, %r12
	movl	44(%r12), %eax
	addl	$43, %eax
	sarl	$3, %eax
	movl	%eax, 956(%rsp)         # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB149_31
# BB#1:                                 # %for gH.s0.v10.v10.preheader
	vmovss	(%r12), %xmm10          # xmm10 = mem[0],zero,zero,zero
	vmovss	4(%r12), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	8(%r12), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovss	24(%r12), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 880(%rsp)        # 4-byte Spill
	vmovss	28(%r12), %xmm13        # xmm13 = mem[0],zero,zero,zero
	vmovss	32(%r12), %xmm12        # xmm12 = mem[0],zero,zero,zero
	movslq	48(%r12), %rax
	movq	%rax, 928(%rsp)         # 8-byte Spill
	movl	52(%r12), %edi
	movq	%rdi, 672(%rsp)         # 8-byte Spill
	movl	56(%r12), %eax
	movq	%rax, 960(%rsp)         # 8-byte Spill
	movl	60(%r12), %ebp
	movl	64(%r12), %eax
	movq	%rax, 800(%rsp)         # 8-byte Spill
	movl	68(%r12), %eax
	movl	%eax, 1008(%rsp)        # 4-byte Spill
	movl	72(%r12), %r11d
	movl	76(%r12), %eax
	movq	%rax, 704(%rsp)         # 8-byte Spill
	movl	80(%r12), %r9d
	movl	84(%r12), %eax
	movq	%rax, 696(%rsp)         # 8-byte Spill
	movslq	88(%r12), %rax
	movq	%rax, 784(%rsp)         # 8-byte Spill
	vmovss	92(%r12), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 848(%rsp)        # 4-byte Spill
	vmovss	96(%r12), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vmovss	100(%r12), %xmm11       # xmm11 = mem[0],zero,zero,zero
	movq	104(%r12), %rax
	movq	%rax, 736(%rsp)         # 8-byte Spill
	movl	$2, %eax
	subl	%r9d, %eax
	movq	120(%r12), %rcx
	movq	%rcx, 432(%rsp)         # 8-byte Spill
	movq	136(%r12), %r13
	movq	152(%r12), %rcx
	movq	%rcx, 864(%rsp)         # 8-byte Spill
	leal	(%r11,%r11), %ecx
	movl	%ecx, 656(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%r11d, %ebx
	sarl	$31, %ebx
	andnl	%ecx, %ebx, %r8d
	movl	%ecx, %r15d
	andl	%eax, %ebx
	orl	%r8d, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	movq	%rsi, 448(%rsp)         # 8-byte Spill
	leal	-1(%r11,%r11), %ecx
	movl	%ecx, 1024(%rsp)        # 4-byte Spill
	subl	%eax, %ecx
	cmpl	%eax, %r11d
	cmovgl	%eax, %ecx
	addl	%r9d, %ecx
	leal	-1(%r9,%r11), %r14d
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	movl	%ecx, 832(%rsp)         # 4-byte Spill
	leal	(%r9,%r11), %edx
	movl	%edx, 912(%rsp)         # 4-byte Spill
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	$3, %edx
	cmovll	%ecx, %eax
	movl	%eax, 816(%rsp)         # 4-byte Spill
	leal	(%rdi,%rdi), %edx
	movl	%edx, 992(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%edi, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %r8d
	movl	%edx, %esi
	andl	%eax, %ecx
	movl	$2, %eax
	subl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%r8d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %r10d
	movl	%r10d, %edx
	subl	%eax, %edx
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%ebp, %edx
	movq	%rdi, %rsi
	leal	-1(%rbp,%rsi), %edi
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, 732(%rsp)         # 4-byte Spill
	leal	(%rbp,%rsi), %r8d
	movl	%r8d, 896(%rsp)         # 4-byte Spill
	cmpl	$3, %r8d
	movl	$2, %eax
	cmovll	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	$3, %r8d
	cmovll	%edx, %eax
	movl	%eax, 728(%rsp)         # 4-byte Spill
	movl	%r9d, %eax
	negl	%eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	movl	1024(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r11d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r14d
	cmovlel	%r14d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 724(%rsp)         # 4-byte Spill
	xorl	%r15d, %r15d
	movl	912(%rsp), %r8d         # 4-byte Reload
	testl	%r8d, %r8d
	movl	$0, %eax
	cmovlel	%r14d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	testl	%r8d, %r8d
	cmovlel	%edx, %eax
	movl	%eax, 720(%rsp)         # 4-byte Spill
	movl	%ebp, %eax
	negl	%eax
	cltd
	idivl	992(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%r10d, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %esi
	cmovgl	%eax, %r8d
	addl	%ebp, %r8d
	cmpl	%r8d, %edi
	cmovlel	%edi, %r8d
	cmpl	%ebp, %r8d
	cmovll	%ebp, %r8d
	movl	896(%rsp), %edx         # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	testl	%edx, %edx
	cmovlel	%r8d, %eax
	movl	%eax, 976(%rsp)         # 4-byte Spill
	movl	$1, %eax
	subl	%r9d, %eax
	cltd
	idivl	656(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	movl	1024(%rsp), %esi        # 4-byte Reload
	subl	%eax, %esi
	cmpl	%eax, %r11d
	cmovgl	%eax, %esi
	addl	%r9d, %esi
	cmpl	%esi, %r14d
	cmovlel	%r14d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	movl	%esi, 1024(%rsp)        # 4-byte Spill
	movl	912(%rsp), %edx         # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r15d, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%r9d, %r14d
	cmovll	%r9d, %r14d
	cmpl	$2, %edx
	cmovll	%esi, %r14d
	movl	$1, %eax
	subl	%ebp, %eax
	cltd
	idivl	992(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %r10d
	movq	672(%rsp), %rcx         # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %r10d
	addl	%ebp, %r10d
	cmpl	%r10d, %edi
	cmovlel	%edi, %r10d
	cmpl	%ebp, %r10d
	cmovll	%ebp, %r10d
	movl	896(%rsp), %ecx         # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	cmovgel	%r15d, %edi
	movzbl	%al, %eax
	orl	%eax, %edi
	cmpl	%ebp, %edi
	cmovll	%ebp, %edi
	cmpl	$2, %ecx
	vmovss	20(%r12), %xmm15        # xmm15 = mem[0],zero,zero,zero
	vmovss	12(%r12), %xmm14        # xmm14 = mem[0],zero,zero,zero
	vmovss	16(%r12), %xmm3         # xmm3 = mem[0],zero,zero,zero
	cmovll	%r10d, %edi
	movl	%edi, 992(%rsp)         # 4-byte Spill
	movq	448(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	movq	696(%rsp), %rdi         # 8-byte Reload
	subl	%edi, %eax
	movl	36(%r12), %ecx
	movq	%rcx, 456(%rsp)         # 8-byte Spill
	movq	704(%rsp), %rbx         # 8-byte Reload
	leal	(%rbx,%rbx), %ecx
	cltd
	idivl	%ecx
	movl	%ebx, %eax
	movq	%rbx, %r11
	sarl	$31, %eax
	andnl	%ecx, %eax, %ebx
	negl	%ecx
	andl	%eax, %ecx
	orl	%ebx, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movq	928(%rsp), %rdx         # 8-byte Reload
	movq	%rdx, %rbx
	sarq	$63, %rbx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 440(%rsp)         # 8-byte Spill
	movl	%esi, %ecx
	andl	$1, %ecx
	movl	%ecx, 76(%rsp)          # 4-byte Spill
	movq	%r11, %rcx
	leal	-1(%rcx,%rcx), %edx
	subl	%eax, %edx
	cmpl	%eax, %ecx
	cmovgl	%eax, %edx
	leal	(%rdi,%rcx), %r11d
	leal	-1(%rdi,%rcx), %ecx
	addl	%edi, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%esi, %r11d
	cmovlel	%edx, %ecx
	cmpl	%esi, %edi
	movq	%rdi, %r11
	cmovgl	%edx, %ecx
	movq	960(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %eax
	vmovd	%eax, %xmm5
	vmovss	.LCPI149_0(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm0, %xmm6
	vmulss	%xmm13, %xmm6, %xmm1
	vdivss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 896(%rsp)        # 16-byte Spill
	movq	800(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi,%rdx), %eax
	vmovd	%eax, %xmm1
	movq	784(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r11d
	addl	%r9d, %r11d
	movq	456(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %edi
	sarl	$5, %edi
	movq	%rdi, 704(%rsp)         # 8-byte Spill
	cmpl	$1, %ebp
	movl	992(%rsp), %edx         # 4-byte Reload
	cmovgl	%r10d, %edx
	movl	%edx, 992(%rsp)         # 4-byte Spill
	movslq	%ecx, %rdx
	imulq	%rax, %rdx
	vsubss	%xmm13, %xmm3, %xmm3
	vmulss	%xmm3, %xmm6, %xmm3
	vdivss	%xmm3, %xmm7, %xmm2
	vmovaps	%xmm2, 912(%rsp)        # 16-byte Spill
	movl	1008(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm6
	movq	%rsi, %rcx
	imull	%ecx, %eax
	movl	%eax, %esi
	vsubss	%xmm4, %xmm0, %xmm13
	vmulss	%xmm12, %xmm13, %xmm3
	vdivss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm2
	vmovaps	%xmm2, 784(%rsp)        # 16-byte Spill
	leal	2(%rcx), %eax
	movq	%rcx, %r10
	vmovd	%eax, %xmm8
	addl	%ebp, %esi
	movl	%esi, 1008(%rsp)        # 4-byte Spill
	cmpl	$1, %r9d
	cmovgl	1024(%rsp), %r14d       # 4-byte Folded Reload
	movslq	%r11d, %rcx
	movq	%rdx, %rax
	subq	%rcx, %rax
	testl	%ebp, %ebp
	movl	976(%rsp), %esi         # 4-byte Reload
	cmovgl	%r8d, %esi
	movl	%esi, 976(%rsp)         # 4-byte Spill
	testl	%r9d, %r9d
	movl	720(%rsp), %esi         # 4-byte Reload
	cmovgl	724(%rsp), %esi         # 4-byte Folded Reload
	movslq	%esi, %r11
	subq	%rcx, %r11
	addq	%rdx, %r11
	cmpl	$2, %ebp
	movl	728(%rsp), %ebp         # 4-byte Reload
	cmovgl	732(%rsp), %ebp         # 4-byte Folded Reload
	cmpl	$2, %r9d
	movl	816(%rsp), %edx         # 4-byte Reload
	cmovgl	832(%rsp), %edx         # 4-byte Folded Reload
	movslq	%r14d, %rcx
	leaq	(%rcx,%rax), %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	movslq	%edx, %rcx
	addq	%rax, %rcx
	movq	%rcx, 1024(%rsp)        # 8-byte Spill
	movslq	%edi, %rax
	shlq	$5, %rax
	addq	$40, %rax
	movq	448(%rsp), %rcx         # 8-byte Reload
	andl	$63, %ecx
	imulq	%rax, %rcx
	andq	928(%rsp), %rbx         # 8-byte Folded Reload
	subq	%rbx, %rcx
	movq	%rcx, 424(%rsp)         # 8-byte Spill
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	8(%rax), %eax
	subl	40(%r12), %eax
	movq	456(%rsp), %rcx         # 8-byte Reload
	andl	$-32, %ecx
	addl	$64, %ecx
	imull	%eax, %ecx
	movq	%rcx, 456(%rsp)         # 8-byte Spill
	vsubss	%xmm12, %xmm15, %xmm15
	movq	960(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm9
	movq	%r10, %rdi
	leal	(%rdi,%rax), %ecx
	movl	%ecx, 732(%rsp)         # 4-byte Spill
	leal	-1(%rdi,%rax), %r14d
	leal	1(%rdi,%rax), %r12d
	leal	3(%rdi,%rax), %ecx
	movl	%ecx, 728(%rsp)         # 4-byte Spill
	leal	4(%rdi,%rax), %eax
	movl	%eax, 960(%rsp)         # 4-byte Spill
	vmulss	%xmm15, %xmm13, %xmm13
	movq	440(%rsp), %r10         # 8-byte Reload
	movl	%r10d, %edx
	subl	%edi, %edx
	movq	%rdx, 416(%rsp)         # 8-byte Spill
	vmovd	%edi, %xmm15
	leal	1(%rdi), %ebx
	leal	3(%rdi), %r8d
	leal	4(%rdi), %r9d
	leal	-1(%rdi), %edi
	vpbroadcastd	%xmm5, %xmm3
	vmovdqa	%xmm3, 400(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm2
	vmovaps	%xmm2, 816(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vdivss	%xmm13, %xmm11, %xmm11
	vsubss	%xmm10, %xmm0, %xmm0
	vmovss	880(%rsp), %xmm2        # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmulss	%xmm2, %xmm0, %xmm6
	vmovss	848(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vdivss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm6, %xmm10, %xmm4
	movl	1008(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm6
	vsubss	%xmm2, %xmm14, %xmm5
	movl	992(%rsp), %esi         # 4-byte Reload
	vmovd	%esi, %xmm2
	vpsubd	%xmm6, %xmm2, %xmm2
	vmulss	%xmm5, %xmm0, %xmm0
	vdivss	%xmm0, %xmm7, %xmm14
	movl	976(%rsp), %ecx         # 4-byte Reload
	vmovd	%ecx, %xmm5
	vpsubd	%xmm6, %xmm5, %xmm5
	vmovd	%ebp, %xmm7
	vpsubd	%xmm6, %xmm7, %xmm7
	vmovdqa	.LCPI149_1(%rip), %xmm0 # xmm0 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm10
	vbroadcastss	%xmm9, %xmm1
	vmovaps	%xmm1, 928(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %xmm6
	vmovdqa	%xmm6, 880(%rsp)        # 16-byte Spill
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 352(%rsp)        # 16-byte Spill
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 336(%rsp)        # 16-byte Spill
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 592(%rsp)        # 16-byte Spill
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 320(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm1
	vmovdqa	%xmm1, 1008(%rsp)       # 16-byte Spill
	vbroadcastss	912(%rsp), %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vbroadcastss	896(%rsp), %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	movl	728(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 304(%rsp)        # 16-byte Spill
	vmovd	%r8d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 288(%rsp)        # 16-byte Spill
	movl	732(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm1
	movq	736(%rsp), %r14         # 8-byte Reload
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 272(%rsp)        # 16-byte Spill
	movl	960(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 256(%rsp)        # 16-byte Spill
	vmovd	%r9d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 240(%rsp)        # 16-byte Spill
	vmovd	%edi, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 224(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm1
	vmovdqa	%xmm1, 208(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm6, %xmm0
	vmovdqa	%xmm0, 192(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 48(%rsp)         # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vmovdqa	%xmm0, 32(%rsp)         # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 16(%rsp)         # 16-byte Spill
	vbroadcastss	784(%rsp), %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, (%rsp)           # 16-byte Spill
	vmovd	%ebp, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, -16(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vmovdqa	%xmm0, -32(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, -48(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, -64(%rsp)        # 16-byte Spill
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movl	%eax, %ecx
	shll	$10, %ecx
	movq	456(%rsp), %rsi         # 8-byte Reload
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 184(%rsp)         # 8-byte Spill
	shll	$9, %eax
	leal	(%rax,%rsi), %eax
	movq	%rax, 176(%rsp)         # 8-byte Spill
	leal	1(%r10), %eax
	movq	%rax, 168(%rsp)         # 8-byte Spill
	leal	-4(%r10), %eax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	leal	-3(%r10), %eax
	movq	%rax, 152(%rsp)         # 8-byte Spill
	leal	-1(%r10), %eax
	movq	%rax, 144(%rsp)         # 8-byte Spill
	leal	-2(%r10), %eax
	movq	%rax, 136(%rsp)         # 8-byte Spill
	leal	1(%rdx), %eax
	movq	%rax, 128(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %eax
	movq	%rax, 120(%rsp)         # 8-byte Spill
	leal	-3(%rdx), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %eax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %eax
	movq	%rax, 96(%rsp)          # 8-byte Spill
	movq	864(%rsp), %rax         # 8-byte Reload
	movq	832(%rsp), %rcx         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, -80(%rsp)        # 16-byte Spill
	movq	1024(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, -96(%rsp)        # 16-byte Spill
	vpabsd	%xmm3, %xmm0
	vmovdqa	%xmm0, 864(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm3, %xmm0
	vmovdqa	%xmm0, 848(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI149_3(%rip), %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI149_4(%rip), %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI149_5(%rip), %xmm0
	vmovaps	%xmm0, 784(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI149_6(%rip), %xmm0
	vmovaps	%xmm0, 832(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB149_2:                              # %for gH.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	96(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI149_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	400(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %ebx
	movl	%ebx, 720(%rsp)         # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %ebp
	movl	%ebp, 724(%rsp)         # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%edx, %edi
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %esi
	movl	%esi, 732(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r12d
	movl	%r12d, 728(%rsp)        # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r10d
	cltd
	idivl	%r10d
	vpinsrd	$1, %r8d, %xmm1, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	864(%rsp), %xmm4        # 16-byte Reload
	vpand	%xmm4, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm11
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	368(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	928(%rsp), %xmm14       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	848(%rsp), %xmm13       # 16-byte Reload
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vmovdqa	880(%rsp), %xmm8        # 16-byte Reload
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	movq	104(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm8, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovdqa	816(%rsp), %xmm6        # 16-byte Reload
	vpmulld	%xmm6, %xmm0, %xmm15
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm3
	vmovd	%xmm2, %eax
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vpand	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm4, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	352(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm2
	vpcmpeqd	%xmm12, %xmm12, %xmm12
	vpxor	%xmm12, %xmm2, %xmm2
	vmovdqa	336(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm5
	vpor	%xmm2, %xmm5, %xmm2
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm13, %xmm7
	vblendvps	%xmm5, %xmm3, %xmm7, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm8, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpminsd	%xmm8, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vblendvps	%xmm2, %xmm3, %xmm5, %xmm2
	vpmulld	%xmm6, %xmm2, %xmm1
	vmovdqa	%xmm1, 1024(%rsp)       # 16-byte Spill
	vpaddd	1008(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r13,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 960(%rsp)        # 16-byte Spill
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r11
	vmovups	12312(%r14,%r11,4), %xmm2
	vmovups	12328(%r14,%r11,4), %xmm5
	vmovdqa	304(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm3
	vpxor	%xmm12, %xmm3, %xmm3
	vmovdqa	288(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm7
	vpor	%xmm3, %xmm7, %xmm7
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm13, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	movq	152(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovups	12320(%r14,%r11,4), %xmm3
	vmovaps	%xmm3, 976(%rsp)        # 16-byte Spill
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm1, %xmm1
	vpminsd	%xmm8, %xmm1, %xmm1
	vmovd	%xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vmovdqa	%xmm6, %xmm12
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 736(%rsp)        # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm4, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovdqa	592(%rsp), %xmm0        # 16-byte Reload
	vpsubd	%xmm0, %xmm15, %xmm1
	vmovdqa	%xmm1, 704(%rsp)        # 16-byte Spill
	vmovdqa	%xmm0, %xmm15
	vpaddd	320(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovdqa	272(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpxor	.LCPI149_9(%rip), %xmm1, %xmm1
	vmovdqa	208(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm6, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpcmpgtd	%xmm3, %xmm14, %xmm6
	vpsubd	%xmm3, %xmm13, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm8, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm11, %xmm7
	vpminsd	%xmm8, %xmm7, %xmm7
	vmovd	%xmm6, %eax
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm1, %xmm3, %xmm7, %xmm1
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	movq	168(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %r12d
	vmovd	%edi, %xmm3
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r10d
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %ecx, %xmm3, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm6
	vpand	%xmm4, %xmm6, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm3
	vpcmpgtd	%xmm3, %xmm14, %xmm6
	vpsubd	%xmm3, %xmm13, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vmovdqa	192(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm6, %xmm6
	vpxor	.LCPI149_9(%rip), %xmm6, %xmm6
	vmovdqa	224(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm10, %xmm3, %xmm3
	vpminsd	%xmm8, %xmm3, %xmm3
	vpmaxsd	%xmm10, %xmm3, %xmm3
	vmovd	%r12d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm9, %xmm7, %xmm7
	vpminsd	%xmm8, %xmm7, %xmm7
	vpmaxsd	%xmm10, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vmovaps	896(%rsp), %xmm13       # 16-byte Reload
	vmulps	960(%rsp), %xmm13, %xmm6 # 16-byte Folded Reload
	vshufps	$221, %xmm5, %xmm2, %xmm7 # xmm7 = xmm2[1,3],xmm5[1,3]
	vmovaps	992(%rsp), %xmm9        # 16-byte Reload
	vsubps	%xmm9, %xmm7, %xmm7
	vmovaps	912(%rsp), %xmm14       # 16-byte Reload
	vmulps	%xmm7, %xmm14, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vpmulld	736(%rsp), %xmm12, %xmm8 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm5[0,2]
	vpxor	%xmm4, %xmm4, %xmm4
	vmovaps	80(%rsp), %xmm7         # 16-byte Reload
	vminps	%xmm7, %xmm6, %xmm5
	vmovaps	%xmm9, %xmm6
	vmaxps	%xmm4, %xmm5, %xmm4
	vmovups	%ymm4, 736(%rsp)        # 32-byte Spill
	vmovaps	%xmm13, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vpmulld	%xmm12, %xmm1, %xmm1
	vmovdqa	%xmm1, 672(%rsp)        # 16-byte Spill
	vpaddd	1008(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rbp
	vmovq	%xmm1, %rbx
	movq	%rbx, %rsi
	sarq	$32, %rsi
	movq	%rbp, %rdi
	sarq	$32, %rdi
	movq	456(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovaps	%xmm4, %xmm1
	movl	76(%rsp), %r9d          # 4-byte Reload
	testl	%r12d, %r9d
	jne	.LBB149_4
# BB#3:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
.LBB149_4:                              # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm1, 624(%rsp)        # 16-byte Spill
	vpmulld	%xmm12, %xmm3, %xmm13
	vpsubd	%xmm15, %xmm8, %xmm9
	vmovdqa	%xmm8, 656(%rsp)        # 16-byte Spill
	vmovdqa	1024(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm15, %xmm1, %xmm12
	movl	%r12d, %r8d
	vminps	%xmm7, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vmovups	%ymm0, 1024(%rsp)       # 32-byte Spill
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vmovss	(%r13,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r13,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r13,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	976(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 12336(%r14,%r11,4), %xmm2, %xmm3 # xmm3 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm14, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm1
	vmovups	(%r14,%rdx,4), %xmm2
	vmovups	32(%r14,%rax,4), %xmm6
	vmovups	48(%r14,%rax,4), %xmm5
	vmovups	(%r14,%rcx,4), %xmm3
	vmovups	40(%r14,%rax,4), %xmm14
	vmulps	832(%rsp), %xmm1, %xmm8 # 16-byte Folded Reload
	andl	$1, %r8d
	vmovaps	%xmm7, %xmm15
	je	.LBB149_5
# BB#6:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovdqa	%xmm9, 640(%rsp)        # 16-byte Spill
	vmovdqa	%xmm13, 464(%rsp)       # 16-byte Spill
	vmovdqa	%xmm12, 544(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, %xmm7
	vmovaps	%xmm2, 480(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 512(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 528(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 560(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 576(%rsp)        # 16-byte Spill
	movq	%r11, 696(%rsp)         # 8-byte Spill
	movl	%r12d, 960(%rsp)        # 4-byte Spill
	movl	%r10d, 620(%rsp)        # 4-byte Spill
	vmovaps	624(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm0, %xmm8
	jmp	.LBB149_7
	.align	16, 0x90
.LBB149_5:                              #   in Loop: Header=BB149_2 Depth=1
	movq	%r11, 696(%rsp)         # 8-byte Spill
	movl	%r12d, 960(%rsp)        # 4-byte Spill
	movl	%r10d, 620(%rsp)        # 4-byte Spill
	vmovdqa	48(%rsp), %xmm0         # 16-byte Reload
	vmovaps	%xmm3, %xmm7
	vmovaps	%xmm7, 576(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm9, %xmm3
	vmovdqa	%xmm9, 640(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm3, %rdx
	vmovq	%xmm3, %rax
	movslq	%eax, %rcx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %rcx
	movq	%rcx, -104(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	vpaddd	32(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vmovdqa	%xmm13, 464(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm3, %rsi
	vmovq	%xmm3, %rbp
	movslq	%ebp, %r12
	sarq	$32, %rbp
	movslq	%esi, %rcx
	sarq	$32, %rsi
	vpaddd	%xmm0, %xmm12, %xmm3
	vmovdqa	%xmm12, 544(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm3, %rdi
	vmovq	%xmm3, %rbx
	movq	%r15, %r11
	movl	%r9d, %r15d
	movq	%r14, %r9
	movslq	%ebx, %r14
	sarq	$32, %rbx
	movslq	%edi, %r10
	sarq	$32, %rdi
	movq	496(%rsp), %r8          # 8-byte Reload
	vmovss	(%r13,%r8,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	-104(%rsp), %rax        # 8-byte Reload
	vinsertps	$32, (%r13,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	-80(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm3, %xmm0, %xmm3
	vmovaps	%xmm2, 480(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, %xmm13
	vmovaps	%xmm13, 512(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm6, %xmm2, %xmm5 # xmm5 = xmm2[1,3],xmm6[1,3]
	vmovaps	%xmm14, 560(%rsp)       # 16-byte Spill
	vmovaps	(%rsp), %xmm4           # 16-byte Reload
	vsubps	%xmm4, %xmm5, %xmm5
	vmovaps	16(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm3, %xmm5, %xmm3
	vmovss	(%r13,%r12,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r13,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r13,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm0, %xmm5
	vmovaps	%xmm6, 528(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm13, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm13[1,3]
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm5, %xmm6, %xmm5
	vmovss	(%r13,%r14,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	movq	%r9, %r14
	movl	%r15d, %r9d
	movq	%r11, %r15
	vinsertps	$16, (%r13,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r13,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$221, %xmm14, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm14[1,3]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vminps	%xmm15, %xmm5, %xmm5
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm5, %xmm5
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm1, %xmm0
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm2, %xmm3, %xmm3
	vmovaps	%xmm8, %xmm7
	vsubps	%xmm3, %xmm0, %xmm8
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm0, %xmm8
	vmovaps	624(%rsp), %xmm0        # 16-byte Reload
.LBB149_7:                              # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	movq	184(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	testl	%r9d, %r9d
	movl	960(%rsp), %ecx         # 4-byte Reload
	jne	.LBB149_9
# BB#8:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm0, %xmm8
.LBB149_9:                              # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	cltq
	vmovups	24592(%r14,%rax,4), %xmm2
	vmovups	24608(%r14,%rax,4), %xmm3
	vmovups	24624(%r14,%rax,4), %xmm13
	vmovups	24600(%r14,%rax,4), %xmm14
	vmovups	24616(%r14,%rax,4), %xmm5
	andl	$1, %ecx
	vmovdqa	640(%rsp), %xmm0        # 16-byte Reload
	jne	.LBB149_10
# BB#11:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 496(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 640(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, %xmm12
	vmovdqa	816(%rsp), %xmm4        # 16-byte Reload
	vmovaps	992(%rsp), %xmm9        # 16-byte Reload
	movl	732(%rsp), %ebx         # 4-byte Reload
	movl	620(%rsp), %esi         # 4-byte Reload
	movl	728(%rsp), %ecx         # 4-byte Reload
	movl	724(%rsp), %edi         # 4-byte Reload
	movl	720(%rsp), %ebp         # 4-byte Reload
	vmovups	736(%rsp), %ymm14       # 32-byte Reload
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB149_12
	.align	16, 0x90
.LBB149_10:                             #   in Loop: Header=BB149_2 Depth=1
	vmovdqa	-16(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm0, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	%rcx, 640(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %rcx
	movq	%rcx, -104(%rsp)        # 8-byte Spill
	sarq	$32, %rdx
	vmovdqa	464(%rsp), %xmm0        # 16-byte Reload
	vpaddd	-32(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbp
	movslq	%ebp, %r11
	sarq	$32, %rbp
	movq	%r15, %r8
	movl	%r9d, %r15d
	movq	%r14, %r9
	movslq	%esi, %r14
	sarq	$32, %rsi
	vpaddd	544(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbx
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%edi, %rcx
	sarq	$32, %rdi
	movq	640(%rsp), %r10         # 8-byte Reload
	vmovss	(%r13,%r10,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	-104(%rsp), %rax        # 8-byte Reload
	vinsertps	$32, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	-96(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm3[1,3]
	vmovaps	-64(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	-48(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vmovss	(%r13,%r11,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r13,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%r9, %r14
	movl	%r15d, %r9d
	movq	%r8, %r15
	vinsertps	$48, (%r13,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm3, 496(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm13, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm13[1,3]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vmovss	(%r13,%r12,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r13,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r13,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vshufps	$221, %xmm5, %xmm14, %xmm6 # xmm6 = xmm14[1,3],xmm5[1,3]
	vmovaps	%xmm5, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 640(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm3, %xmm6, %xmm3
	vminps	%xmm15, %xmm2, %xmm2
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm2, %xmm2
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm1, %xmm3
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm12
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm0, %xmm12
	vmovdqa	816(%rsp), %xmm4        # 16-byte Reload
	vmovaps	992(%rsp), %xmm9        # 16-byte Reload
	movl	732(%rsp), %ebx         # 4-byte Reload
	movl	620(%rsp), %esi         # 4-byte Reload
	movl	728(%rsp), %ecx         # 4-byte Reload
	movl	724(%rsp), %edi         # 4-byte Reload
	movl	720(%rsp), %ebp         # 4-byte Reload
	vmovups	736(%rsp), %ymm14       # 32-byte Reload
.LBB149_12:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovdqa	656(%rsp), %xmm0        # 16-byte Reload
	vpaddd	1008(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r12
	vmovq	%xmm0, %r11
	testl	%r9d, %r9d
	je	.LBB149_14
# BB#13:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm8, %xmm12
.LBB149_14:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm13, %xmm8
	movq	120(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI149_2(%rip), %xmm5 # xmm5 = [0,2,4,6]
	vpaddd	%xmm5, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%esi
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	%r11, %rax
	sarq	$32, %rax
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	864(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	928(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm1
	vmovdqa	848(%rsp), %xmm2        # 16-byte Reload
	vpsubd	%xmm0, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	movq	160(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	vmovd	%ecx, %xmm1
	movq	%r12, %rcx
	sarq	$32, %rcx
	vmovdqa	256(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpxor	.LCPI149_9(%rip), %xmm2, %xmm2
	vmovdqa	240(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm3, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpaddd	%xmm10, %xmm0, %xmm0
	vmovdqa	880(%rsp), %xmm7        # 16-byte Reload
	vpminsd	%xmm7, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm5, %xmm1, %xmm1
	vpminsd	%xmm7, %xmm1, %xmm1
	vpmaxsd	%xmm10, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm0, %xmm1, %xmm1
	vmovups	1024(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%xmm0, %xmm3
	testl	%ebx, %r9d
	jne	.LBB149_16
# BB#15:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
.LBB149_16:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vpmulld	%xmm4, %xmm1, %xmm11
	movl	%ebx, %edx
	movslq	%r11d, %rsi
	movslq	%r12d, %rdi
	vmovss	(%r13,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	movq	696(%rsp), %rax         # 8-byte Reload
	vmovups	12304(%r14,%rax,4), %xmm1
	vshufps	$221, 976(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	912(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm0
	vaddps	%xmm0, %xmm14, %xmm0
	vmulps	832(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
	andl	$1, %edx
	je	.LBB149_17
# BB#18:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm9, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, %xmm5
	jmp	.LBB149_19
	.align	16, 0x90
.LBB149_17:                             #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm9, 992(%rsp)        # 16-byte Spill
	vpaddd	32(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	vmovdqa	672(%rsp), %xmm0        # 16-byte Reload
	vpsubd	592(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	48(%rsp), %xmm4         # 16-byte Reload
	vpaddd	%xmm4, %xmm0, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rbp
	vpaddd	704(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movl	%ebx, %r10d
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r13,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movl	%r10d, %ebx
	vinsertps	$16, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	-80(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	528(%rsp), %xmm6        # 16-byte Reload
	vmovaps	480(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm6, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm6[0,2]
	vmovaps	(%rsp), %xmm1           # 16-byte Reload
	vsubps	%xmm1, %xmm5, %xmm5
	vmovaps	16(%rsp), %xmm4         # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	movslq	%ebp, %rax
	sarq	$32, %rbp
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, 512(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm6[0,2],mem[0,2]
	vxorps	%xmm9, %xmm9, %xmm9
	vmovss	(%r13,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbp,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm2, %xmm6
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	576(%rsp), %xmm6        # 16-byte Reload
	vshufps	$136, 560(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[0,2],mem[0,2]
	vmovss	(%r13,%rax,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r13,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r13,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm2, %xmm7
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vminps	%xmm15, %xmm6, %xmm6
	vmaxps	%xmm9, %xmm6, %xmm6
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm1, %xmm6
	vminps	%xmm15, %xmm5, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm0, %xmm6, %xmm5
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm13, %xmm0, %xmm5
.LBB149_19:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	movq	448(%rsp), %r10         # 8-byte Reload
	movl	960(%rsp), %r12d        # 4-byte Reload
	vmovaps	624(%rsp), %xmm2        # 16-byte Reload
	testl	%r9d, %r9d
	jne	.LBB149_21
# BB#20:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm3, %xmm5
.LBB149_21:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	movl	%ebx, %eax
	andl	$1, %eax
	jne	.LBB149_22
# BB#23:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm5, %xmm2
	jmp	.LBB149_24
	.align	16, 0x90
.LBB149_22:                             #   in Loop: Header=BB149_2 Depth=1
	vmovdqa	-32(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm11, %xmm1, %xmm0
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	672(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vmovdqa	704(%rsp), %xmm0        # 16-byte Reload
	vpaddd	-16(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movl	%ebx, %r11d
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovss	(%r13,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movl	%r11d, %ebx
	vinsertps	$16, (%r13,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	-96(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	496(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vmovaps	-64(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	-48(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	movslq	%esi, %rax
	sarq	$32, %rsi
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vshufps	$136, %xmm8, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm8[0,2]
	vmovss	(%r13,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r13,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	640(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 544(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmovss	(%r13,%rax,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r13,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm7, %xmm6
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vminps	%xmm15, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm1, %xmm3
	vsubps	%xmm2, %xmm3, %xmm2
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm13, %xmm0, %xmm2
.LBB149_24:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	movl	956(%rsp), %ecx         # 4-byte Reload
	testl	%r9d, %r9d
	je	.LBB149_26
# BB#25:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm5, %xmm2
.LBB149_26:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	movl	%ebx, %eax
	orl	%r10d, %eax
	testb	$1, %al
	je	.LBB149_28
# BB#27:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm2, %xmm0
	vmovups	%ymm0, 1024(%rsp)       # 32-byte Spill
.LBB149_28:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	orl	%r10d, %r12d
	testb	$1, %r12b
	je	.LBB149_30
# BB#29:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	%xmm12, %xmm14
.LBB149_30:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB149_2 Depth=1
	vmovaps	.LCPI149_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm14, %ymm0, %ymm0
	vmovaps	.LCPI149_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	1024(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%ebx, %rax
	movq	424(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	432(%rsp), %rdx         # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r15d
	addl	$-1, %ecx
	movl	%ecx, 956(%rsp)         # 4-byte Spill
	jne	.LBB149_2
.LBB149_31:                             # %destructor_block
	xorl	%eax, %eax
	addq	$1064, %rsp             # imm = 0x428
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end149:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11, .Lfunc_end149-par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI150_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI150_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI150_9:
	.zero	16,255
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI150_1:
	.long	1199570688              # float 65535
.LCPI150_3:
	.long	1065353216              # float 1
.LCPI150_4:
	.long	1073741824              # float 2
.LCPI150_5:
	.long	1048576000              # float 0.25
.LCPI150_6:
	.long	1056964608              # float 0.5
	.section	.rodata,"a",@progbits
	.align	32
.LCPI150_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI150_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$904, %rsp              # imm = 0x388
	movq	%rsi, 520(%rsp)         # 8-byte Spill
	movl	44(%rdx), %eax
	addl	$43, %eax
	sarl	$3, %eax
	movl	%eax, 876(%rsp)         # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB150_22
# BB#1:                                 # %for gV.s0.v10.v10.preheader
	vmovss	(%rdx), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	4(%rdx), %xmm14         # xmm14 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovss	24(%rdx), %xmm9         # xmm9 = mem[0],zero,zero,zero
	vmovss	28(%rdx), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm15        # xmm15 = mem[0],zero,zero,zero
	movslq	48(%rdx), %rax
	movq	%rax, 640(%rsp)         # 8-byte Spill
	movl	52(%rdx), %ebx
	movq	%rbx, 352(%rsp)         # 8-byte Spill
	movl	56(%rdx), %eax
	movq	%rax, 688(%rsp)         # 8-byte Spill
	movl	60(%rdx), %r8d
	movq	%r8, 344(%rsp)          # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 880(%rsp)         # 8-byte Spill
	movl	68(%rdx), %eax
	movl	%eax, 816(%rsp)         # 4-byte Spill
	movl	72(%rdx), %esi
	movq	%rsi, 424(%rsp)         # 8-byte Spill
	movl	76(%rdx), %r9d
	movq	%r9, 448(%rsp)          # 8-byte Spill
	movl	80(%rdx), %edi
	movq	%rdi, 576(%rsp)         # 8-byte Spill
	movl	84(%rdx), %eax
	movq	%rax, 320(%rsp)         # 8-byte Spill
	movslq	88(%rdx), %rax
	movq	%rax, 672(%rsp)         # 8-byte Spill
	vmovss	92(%rdx), %xmm10        # xmm10 = mem[0],zero,zero,zero
	vmovss	96(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	100(%rdx), %xmm11       # xmm11 = mem[0],zero,zero,zero
	movq	104(%rdx), %rax
	movq	%rax, 72(%rsp)          # 8-byte Spill
	movl	%edi, %eax
	negl	%eax
	movq	120(%rdx), %rcx
	movq	%rcx, 504(%rsp)         # 8-byte Spill
	movq	136(%rdx), %r13
	movq	152(%rdx), %rcx
	movq	%rcx, 656(%rsp)         # 8-byte Spill
	leal	(%rsi,%rsi), %r14d
	movl	%r14d, 768(%rsp)        # 4-byte Spill
	movq	%rdx, 464(%rsp)         # 8-byte Spill
	cltd
	idivl	%r14d
	movl	%r14d, %eax
	negl	%eax
	movl	%esi, %ecx
	sarl	$31, %ecx
	andnl	%r14d, %ecx, %r11d
	movl	%r14d, %r15d
	andl	%eax, %ecx
	orl	%r11d, %ecx
	movl	%ecx, 512(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%ecx, %r14d
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %edx
	movl	%edx, 832(%rsp)         # 4-byte Spill
	movl	%edx, %ecx
	movl	%edx, %r11d
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	leal	-1(%rdi,%rsi), %r12d
	movl	%r12d, 704(%rsp)        # 4-byte Spill
	cmpl	%ecx, %r12d
	cmovlel	%r12d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 560(%rsp)         # 4-byte Spill
	leal	(%rdi,%rsi), %r10d
	movl	%r10d, 408(%rsp)        # 4-byte Spill
	xorl	%ebp, %ebp
	testl	%r10d, %r10d
	movl	$0, %eax
	cmovlel	%r12d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%r10d, %r10d
	cmovlel	%ecx, %eax
	movl	%eax, 544(%rsp)         # 4-byte Spill
	movl	$2, %eax
	subl	%edi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	addl	%edx, %eax
	movl	%r11d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	cmpl	%ecx, %r12d
	cmovlel	%r12d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 528(%rsp)         # 4-byte Spill
	cmpl	$3, %r10d
	movl	$2, %eax
	cmovll	%r12d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %r10d
	cmovll	%ecx, %eax
	movl	%eax, 736(%rsp)         # 4-byte Spill
	movl	$2, %eax
	subl	%r8d, %eax
	leal	(%rbx,%rbx), %ecx
	movl	%ecx, 392(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%ebx, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %r10d
	andl	%eax, %esi
	orl	%r10d, %esi
	movl	%esi, 400(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %edi
	movl	%edi, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	movq	%r8, %rsi
	addl	%esi, %ecx
	leal	-1(%rsi,%rbx), %edx
	movl	%edx, 800(%rsp)         # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 480(%rsp)         # 4-byte Spill
	leal	(%rsi,%rbx), %r8d
	movl	%r8d, 368(%rsp)         # 4-byte Spill
	cmpl	$3, %r8d
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %r8d
	cmovll	%ecx, %eax
	movl	%eax, 784(%rsp)         # 4-byte Spill
	movq	520(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, %r15d
	movq	%rbx, %r12
	movq	320(%rsp), %r8          # 8-byte Reload
	subl	%r8d, %r15d
	movq	%r15, 432(%rsp)         # 8-byte Spill
	leal	(%r9,%r9), %ebx
	movl	%ebx, 592(%rsp)         # 4-byte Spill
	leal	-1(%r15), %eax
	cltd
	idivl	%ebx
	movl	%ebx, %eax
	negl	%eax
	movl	%r9d, %esi
	sarl	$31, %esi
	andnl	%ebx, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 608(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	movl	%esi, %r11d
	addl	%edx, %eax
	leal	-1(%r9,%r9), %r10d
	movl	%r10d, 624(%rsp)        # 4-byte Spill
	movl	%r10d, %ebx
	subl	%eax, %ebx
	cmpl	%eax, %r9d
	cmovgl	%eax, %ebx
	addl	%r8d, %ebx
	movq	%r9, %r14
	leal	-1(%r8,%r14), %ecx
	movl	%ecx, 416(%rsp)         # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%ecx, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	leal	(%r8,%r14), %r9d
	cmpl	%r12d, %r9d
	movl	%r9d, %eax
	cmovgl	%r12d, %eax
	addl	$-1, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%r12d, %r9d
	cmovll	%ebx, %eax
	movl	%eax, 848(%rsp)         # 4-byte Spill
	movl	%r15d, %eax
	cltd
	movl	592(%rsp), %esi         # 4-byte Reload
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movl	%r10d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r11d
	addl	%r8d, %r11d
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	cmpl	%r8d, %r11d
	cmovll	%r8d, %r11d
	movq	%r12, %r10
	cmpl	%r10d, %ecx
	movl	%ecx, %eax
	movl	%ecx, %r12d
	cmovgl	%r10d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	%r10d, %r9d
	movq	%r10, %r9
	cmovlel	%r11d, %eax
	movl	%eax, %ecx
	leal	1(%r15), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	608(%rsp), %eax         # 4-byte Folded Reload
	addl	%edx, %eax
	movl	624(%rsp), %r10d        # 4-byte Reload
	subl	%eax, %r10d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r10d
	addl	%r8d, %r10d
	cmpl	%r10d, %r12d
	cmovlel	%r12d, %r10d
	cmpl	%r8d, %r10d
	cmovll	%r8d, %r10d
	movq	%r9, %rax
	leal	1(%rax), %r9d
	cmpl	%r9d, %r12d
	cmovlel	%r12d, %r9d
	cmpl	%r8d, %r9d
	cmovll	%r8d, %r9d
	cmpl	%eax, %r8d
	cmovgl	%r11d, %ecx
	movl	%ecx, 384(%rsp)         # 4-byte Spill
	movl	848(%rsp), %edx         # 4-byte Reload
	cmovgel	%ebx, %edx
	movl	%edx, 848(%rsp)         # 4-byte Spill
	cmpl	%eax, %r12d
	cmovlel	%r10d, %r9d
	movl	$1, %eax
	movq	576(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	768(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	512(%rsp), %eax         # 4-byte Folded Reload
	addl	%edx, %eax
	movl	832(%rsp), %edx         # 4-byte Reload
	subl	%eax, %edx
	movq	424(%rsp), %rcx         # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %edx
	addl	%esi, %edx
	movl	704(%rsp), %ecx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movl	%edx, 832(%rsp)         # 4-byte Spill
	movl	408(%rsp), %r12d        # 4-byte Reload
	cmpl	$1, %r12d
	setg	%al
	cmpl	$2, %r12d
	cmovgel	%ebp, %ecx
	movzbl	%al, %eax
	orl	%eax, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	$2, %r12d
	cmovll	%edx, %ecx
	movl	%ecx, 704(%rsp)         # 4-byte Spill
	movl	$1, %eax
	movq	344(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %eax
	cltd
	movl	392(%rsp), %r14d        # 4-byte Reload
	idivl	%r14d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	400(%rsp), %r15d        # 4-byte Reload
	andl	%r15d, %eax
	addl	%edx, %eax
	movl	%edi, %ebx
	subl	%eax, %ebx
	movq	352(%rsp), %r11         # 8-byte Reload
	cmpl	%eax, %r11d
	cmovgl	%eax, %ebx
	addl	%ecx, %ebx
	movl	800(%rsp), %esi         # 4-byte Reload
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%ecx, %ebx
	cmovll	%ecx, %ebx
	movl	368(%rsp), %edx         # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%edx, %r12d
	movl	$0, %edx
	cmovll	%esi, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	$2, %r12d
	cmovll	%ebx, %edx
	movl	%edx, 768(%rsp)         # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	%r14d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%edx, %eax
	subl	%eax, %edi
	cmpl	%eax, %r11d
	cmovgl	%eax, %edi
	addl	%ecx, %edi
	movl	%esi, %r14d
	cmpl	%edi, %r14d
	cmovlel	%r14d, %edi
	cmpl	%ecx, %edi
	cmovll	%ecx, %edi
	testl	%r12d, %r12d
	cmovgl	%ebp, %r14d
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	movq	%rcx, %rsi
	testl	%r12d, %r12d
	cmovlel	%edi, %r14d
	movl	%r14d, %edx
	movq	640(%rsp), %r14         # 8-byte Reload
	movq	%r14, %rax
	sarq	$63, %rax
	movq	%rax, 424(%rsp)         # 8-byte Spill
	movl	%r14d, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	movq	520(%rsp), %r12         # 8-byte Reload
	movl	%r12d, %r14d
	andl	$1, %r14d
	testl	%esi, %esi
	cmovgl	%edi, %edx
	movl	%edx, 800(%rsp)         # 4-byte Spill
	movl	816(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm13
	movq	880(%rsp), %rdx         # 8-byte Reload
	imull	%edx, %eax
	addl	%esi, %eax
	movl	%eax, 816(%rsp)         # 4-byte Spill
	cmpl	$1, %esi
	movl	768(%rsp), %eax         # 4-byte Reload
	cmovgl	%ebx, %eax
	movl	%eax, 768(%rsp)         # 4-byte Spill
	movq	576(%rsp), %r15         # 8-byte Reload
	cmpl	$1, %r15d
	movl	704(%rsp), %edx         # 4-byte Reload
	cmovgl	832(%rsp), %edx         # 4-byte Folded Reload
	movq	672(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %eax
	imull	%r8d, %eax
	movq	464(%rsp), %rbx         # 8-byte Reload
	movl	36(%rbx), %ecx
	movl	%ecx, 408(%rsp)         # 4-byte Spill
	sarl	$5, %ecx
	movl	%ecx, 400(%rsp)         # 4-byte Spill
	addl	%r15d, %eax
	cltq
	movq	%rax, 392(%rsp)         # 8-byte Spill
	movslq	384(%rsp), %rcx         # 4-byte Folded Reload
	movslq	%edx, %r11
	imulq	%rdi, %rcx
	subq	%rax, %r11
	leal	-1(%r8), %eax
	cmpl	%r12d, %eax
	cmovgl	%r10d, %r9d
	movslq	848(%rsp), %rax         # 4-byte Folded Reload
	imulq	%rdi, %rax
	leaq	(%rax,%r11), %rax
	movq	656(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	movslq	%r9d, %rax
	imulq	%rdi, %rax
	leaq	(%rax,%r11), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm1
	vmovaps	%xmm1, 832(%rsp)        # 16-byte Spill
	cmpl	$2, %esi
	movl	784(%rsp), %eax         # 4-byte Reload
	cmovgl	480(%rsp), %eax         # 4-byte Folded Reload
	movl	%eax, 784(%rsp)         # 4-byte Spill
	cmpl	$2, %r15d
	movl	736(%rsp), %eax         # 4-byte Reload
	cmovgl	528(%rsp), %eax         # 4-byte Folded Reload
	movl	%eax, 736(%rsp)         # 4-byte Spill
	movq	%rbx, %rax
	vmovss	20(%rax), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmovss	12(%rax), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vmovss	16(%rax), %xmm7         # xmm7 = mem[0],zero,zero,zero
	movl	40(%rax), %eax
	movl	%eax, 528(%rsp)         # 4-byte Spill
	movq	432(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	592(%rsp), %edi         # 4-byte Reload
	idivl	%edi
	movl	%edx, %r9d
	vmovss	.LCPI150_1(%rip), %xmm6 # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm14, %xmm6, %xmm4
	vmulss	%xmm3, %xmm4, %xmm2
	vdivss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm2, %xmm14, %xmm14
	vsubss	%xmm3, %xmm7, %xmm2
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%edi
	vmulss	%xmm2, %xmm4, %xmm2
	vdivss	%xmm2, %xmm0, %xmm0
	vsubss	%xmm12, %xmm6, %xmm2
	vmulss	%xmm15, %xmm2, %xmm3
	vdivss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm12, %xmm12
	movq	880(%rsp), %rsi         # 8-byte Reload
	movq	688(%rsp), %rax         # 8-byte Reload
	leal	2(%rsi,%rax), %eax
	vmovd	%eax, %xmm7
	vsubss	%xmm15, %xmm1, %xmm4
	leal	2(%rsi), %eax
	vmovd	%eax, %xmm1
	movl	%r9d, %edi
	sarl	$31, %edi
	movl	608(%rsp), %esi         # 4-byte Reload
	andl	%esi, %edi
	addl	%r9d, %edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	testl	%r15d, %r15d
	movl	544(%rsp), %r9d         # 4-byte Reload
	cmovgl	560(%rsp), %r9d         # 4-byte Folded Reload
	movq	%rcx, %rdx
	leaq	(%r11,%rdx), %rcx
	movq	%rcx, 608(%rsp)         # 8-byte Spill
	movq	392(%rsp), %r15         # 8-byte Reload
	subq	%r15, %rdx
	movq	%rdx, 704(%rsp)         # 8-byte Spill
	movl	624(%rsp), %r10d        # 4-byte Reload
	movl	%r10d, %edx
	subl	%edi, %edx
	movq	448(%rsp), %r11         # 8-byte Reload
	cmpl	%edi, %r11d
	cmovgl	%edi, %edx
	addl	%r8d, %edx
	movl	416(%rsp), %ecx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movq	%r12, %rsi
	leal	2(%rsi), %edi
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	leal	-2(%r8,%r11), %ebx
	movq	%r11, %r12
	cmpl	%esi, %ebx
	cmovlel	%edx, %edi
	leal	-2(%r8), %ebx
	cmpl	%esi, %ebx
	movq	%rsi, %r11
	cmovgl	%edx, %edi
	movslq	%r9d, %rdx
	movslq	%edi, %rdi
	movq	672(%rsp), %rbx         # 8-byte Reload
	imulq	%rbx, %rdi
	movq	%rdi, %rsi
	subq	%r15, %rsi
	movq	%r15, %r9
	addq	%rdx, %rsi
	movq	%rsi, 592(%rsp)         # 8-byte Spill
	subl	%eax, %r10d
	cmpl	%eax, %r12d
	cmovgl	%eax, %r10d
	addl	%r8d, %r10d
	cmpl	%r10d, %ecx
	cmovlel	%ecx, %r10d
	cmpl	%r8d, %r10d
	cmovll	%r8d, %r10d
	leal	-2(%r11), %eax
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	leal	2(%r8,%r12), %eax
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	%r11d, %eax
	leal	2(%r8), %eax
	cmovlel	%r10d, %ecx
	cmpl	%r11d, %eax
	cmovgl	%r10d, %ecx
	movslq	%ecx, %rax
	imulq	%rbx, %rax
	vmulss	%xmm4, %xmm2, %xmm2
	movq	688(%rsp), %rbx         # 8-byte Reload
	movq	880(%rsp), %rsi         # 8-byte Reload
	leal	1(%rsi,%rbx), %ecx
	vmovd	%ecx, %xmm3
	vdivss	%xmm2, %xmm11, %xmm11
	vsubss	%xmm8, %xmm6, %xmm6
	vmulss	%xmm9, %xmm6, %xmm4
	vdivss	%xmm10, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm8
	leal	1(%rsi), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm7, %xmm7
	vsubss	%xmm9, %xmm5, %xmm5
	vmovdqa	.LCPI150_0(%rip), %xmm4 # xmm4 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm4, %xmm7, %xmm7
	vmovdqa	%xmm7, 480(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	vmovdqa	%xmm1, 464(%rsp)        # 16-byte Spill
	vmulss	%xmm5, %xmm6, %xmm1
	vmovd	816(%rsp), %xmm7        # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm10, %xmm5
	vmovd	800(%rsp), %xmm1        # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm4, %xmm3, %xmm3
	vmovdqa	%xmm3, 448(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 432(%rsp)        # 16-byte Spill
	vmovd	768(%rsp), %xmm2        # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpsubd	%xmm7, %xmm1, %xmm1
	vpsubd	%xmm7, %xmm2, %xmm6
	vmovd	784(%rsp), %xmm2        # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpsubd	%xmm7, %xmm2, %xmm7
	movq	%rax, %r15
	movq	%r9, %rsi
	subq	%rsi, %r15
	addq	%rdx, %r15
	movq	704(%rsp), %rcx         # 8-byte Reload
	leaq	(%rcx,%rdx), %rdx
	movq	%rdx, 672(%rsp)         # 8-byte Spill
	movslq	736(%rsp), %rdx         # 4-byte Folded Reload
	addq	%rdx, %rcx
	movq	%rcx, 704(%rsp)         # 8-byte Spill
	subq	%rsi, %rdx
	leaq	(%rax,%rdx), %rax
	movq	%rax, 624(%rsp)         # 8-byte Spill
	leaq	(%rdi,%rdx), %rax
	movq	%rax, 736(%rsp)         # 8-byte Spill
	movslq	400(%rsp), %rsi         # 4-byte Folded Reload
	movq	%rsi, %rdx
	shlq	$5, %rdx
	addq	$40, %rdx
	movl	%r11d, %eax
	andl	$63, %eax
	leal	6(%r11), %r10d
	movl	528(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %r10d
	movl	408(%rsp), %r12d        # 4-byte Reload
	andl	$-32, %r12d
	addl	$64, %r12d
	imull	%r12d, %r10d
	movq	%r10, 416(%rsp)         # 8-byte Spill
	imulq	%rdx, %rax
	leal	10(%r11), %r9d
	subl	%ecx, %r9d
	imull	%r12d, %r9d
	movq	%r9, 408(%rsp)          # 8-byte Spill
	leal	7(%r11), %r8d
	subl	%ecx, %r8d
	movl	%ecx, %edx
	imull	%r12d, %r8d
	movq	424(%rsp), %rcx         # 8-byte Reload
	andq	640(%rsp), %rcx         # 8-byte Folded Reload
	subq	%rcx, %rax
	movq	%rax, 424(%rsp)         # 8-byte Spill
	leal	9(%r11), %edi
	subl	%edx, %edi
	imull	%r12d, %edi
	leal	8(%r11), %ecx
	subl	%edx, %ecx
	imull	%r12d, %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	leal	(%rbx,%rbx), %edx
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vmovdqa	%xmm3, 368(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm2
	vmovaps	%xmm2, 352(%rsp)        # 16-byte Spill
	movq	512(%rsp), %r11         # 8-byte Reload
	movl	%r11d, %eax
	movq	880(%rsp), %rdx         # 8-byte Reload
	subl	%edx, %eax
	leal	-1(%rdx,%rbx), %r12d
	vmovd	%edx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 320(%rsp)        # 16-byte Spill
	vmovd	%ebx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 304(%rsp)        # 16-byte Spill
	vmovd	%r12d, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 288(%rsp)        # 16-byte Spill
	movl	816(%rsp), %edx         # 4-byte Reload
	vmovd	%edx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 272(%rsp)        # 16-byte Spill
	movl	800(%rsp), %edx         # 4-byte Reload
	vmovd	%edx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 256(%rsp)        # 16-byte Spill
	movl	768(%rsp), %edx         # 4-byte Reload
	vmovd	%edx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	movl	784(%rsp), %edx         # 4-byte Reload
	vmovd	%edx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, -112(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vmovdqa	%xmm0, -128(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm0
	vmovdqa	%xmm0, 224(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vmovdqa	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 48(%rsp)         # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 32(%rsp)         # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 16(%rsp)         # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, (%rsp)           # 16-byte Spill
	movl	%esi, %edx
	shll	$10, %edx
	leal	(%rdx,%rdx,2), %edx
	shll	$9, %esi
	leal	(%rsi,%rsi,2), %esi
	addl	%esi, %r8d
	movq	%r8, 400(%rsp)          # 8-byte Spill
	addl	%esi, %edi
	movq	%rdi, 392(%rsp)         # 8-byte Spill
	leal	(%r10,%rdx), %edi
	movq	%rdi, 200(%rsp)         # 8-byte Spill
	leal	(%r9,%rdx), %edi
	movq	%rdi, 192(%rsp)         # 8-byte Spill
	leal	(%rdx,%rcx), %edx
	movq	%rdx, 184(%rsp)         # 8-byte Spill
	leal	(%rsi,%rcx), %ecx
	movq	%rcx, 176(%rsp)         # 8-byte Spill
	leal	-2(%rax), %ecx
	movq	%rcx, 168(%rsp)         # 8-byte Spill
	addl	$-1, %eax
	movq	%rax, 344(%rsp)         # 8-byte Spill
	movq	%r11, %rcx
	leal	-1(%rcx), %eax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %eax
	movq	%rax, 152(%rsp)         # 8-byte Spill
	movq	656(%rsp), %rdx         # 8-byte Reload
	movq	608(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 128(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdx,%r15,4), %xmm0
	vmovaps	%xmm0, -16(%rsp)        # 16-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -32(%rsp)        # 16-byte Spill
	movq	672(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -48(%rsp)        # 16-byte Spill
	movq	624(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -64(%rsp)        # 16-byte Spill
	movq	736(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -80(%rsp)        # 16-byte Spill
	movq	704(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -96(%rsp)        # 16-byte Spill
	vpabsd	%xmm3, %xmm0
	vmovdqa	%xmm0, 112(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm3, %xmm0
	vmovdqa	%xmm0, 96(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI150_3(%rip), %xmm0
	vmovaps	%xmm0, 880(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI150_4(%rip), %xmm0
	vmovaps	%xmm0, 784(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI150_5(%rip), %xmm0
	vmovaps	%xmm0, 768(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI150_6(%rip), %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	.align	16, 0x90
.LBB150_2:                              # %for gV.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movl	%r14d, %r15d
	testl	%r15d, %r15d
	setne	%r12b
	sete	704(%rsp)               # 1-byte Folded Spill
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %esi
	movl	%esi, %eax
	andl	$1, %eax
	movl	%eax, 688(%rsp)         # 4-byte Spill
	sete	736(%rsp)               # 1-byte Folded Spill
	movq	344(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI150_2(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	368(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %edi
	cltd
	idivl	%edi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r11d
	movq	168(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	movl	%esi, %r8d
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %r11d, %xmm0, %xmm0
	vmovd	%edi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpsrad	$31, %xmm0, %xmm3
	vmovdqa	112(%rsp), %xmm1        # 16-byte Reload
	vpand	%xmm1, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm9
	vpinsrd	$2, %ebx, %xmm2, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm1, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vmovd	%r8d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	480(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm3, %xmm3
	vmovdqa	464(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	304(%rsp), %xmm10       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm10, %xmm4
	vmovdqa	96(%rsp), %xmm13        # 16-byte Reload
	vpsubd	%xmm2, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm2, %xmm5, %xmm2
	vmovdqa	320(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm2, %xmm2
	vmovdqa	288(%rsp), %xmm7        # 16-byte Reload
	vpminsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm1, %xmm2, %xmm2
	movq	152(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm7, %xmm4, %xmm4
	vpmaxsd	%xmm1, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovdqa	352(%rsp), %xmm5        # 16-byte Reload
	vpmulld	%xmm5, %xmm2, %xmm2
	vpsubd	272(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmovdqa	%xmm4, 624(%rsp)        # 16-byte Spill
	vpaddd	256(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r13,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%r13,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 608(%rsp)        # 16-byte Spill
	vpaddd	240(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r13,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%r13,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm2, %xmm3 # xmm3 = xmm2[0,1,2],mem[0]
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	movq	72(%rsp), %rsi          # 8-byte Reload
	vmovups	12312(%rsi,%rax,4), %xmm15
	vmovups	12328(%rsi,%rax,4), %xmm12
	movq	392(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	12312(%rsi,%rax,4), %xmm11
	vmovaps	%xmm11, 640(%rsp)       # 16-byte Spill
	vmovups	12328(%rsi,%rax,4), %xmm8
	vmovaps	%xmm8, 656(%rsp)        # 16-byte Spill
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	12312(%rsi,%rax,4), %xmm6
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm4
	vpxor	.LCPI150_9(%rip), %xmm4, %xmm4
	vmovdqa	432(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm0
	vpor	%xmm4, %xmm0, %xmm0
	vpcmpgtd	%xmm9, %xmm10, %xmm4
	vpsubd	%xmm9, %xmm13, %xmm2
	vblendvps	%xmm4, %xmm9, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm2
	vpminsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm1, %xmm2, %xmm2
	movq	160(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rbp), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm7, %xmm4, %xmm4
	vpmaxsd	%xmm1, %xmm4, %xmm4
	vblendvps	%xmm0, %xmm2, %xmm4, %xmm0
	vpmulld	%xmm5, %xmm0, %xmm1
	vmovdqa	%xmm1, 672(%rsp)        # 16-byte Spill
	vmovups	12328(%rsi,%rax,4), %xmm0
	vpaddd	224(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r13,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm4, %xmm2 # xmm2 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r13,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	sarq	$32, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm2, %xmm13 # xmm13 = xmm2[0,1,2],mem[0]
	movl	%r15d, %eax
	movl	%r15d, %r14d
	andl	%r8d, %eax
	vmulps	848(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm12, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm12[0,2]
	vmovaps	800(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmovaps	816(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	880(%rsp), %xmm9        # 16-byte Reload
	vminps	%xmm9, %xmm2, %xmm2
	vmulps	832(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm11, %xmm7 # xmm7 = xmm11[0,2],xmm8[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm9, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vaddps	%xmm4, %xmm2, %xmm14
	vmovaps	128(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm2
	vshufps	$136, %xmm0, %xmm6, %xmm3 # xmm3 = xmm6[0,2],xmm0[0,2]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm13, %xmm4, %xmm3
	vshufps	$221, %xmm0, %xmm6, %xmm0 # xmm0 = xmm6[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm1
	vmovaps	%xmm1, %xmm11
	jne	.LBB150_4
# BB#3:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
.LBB150_4:                              # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmulps	%xmm3, %xmm0, %xmm9
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rsi,%rax,4), %xmm5
	orq	$6, %rax
	vmovups	(%rsi,%rax,4), %xmm6
	movq	408(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rsi,%rax,4), %xmm4
	orq	$6, %rax
	vmovups	(%rsi,%rax,4), %xmm3
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rsi,%rax,4), %xmm7
	orq	$6, %rax
	vmovups	(%rsi,%rax,4), %xmm8
	vmovaps	80(%rsp), %xmm0         # 16-byte Reload
	vmulps	%xmm0, %xmm14, %xmm10
	vmovaps	%xmm0, %xmm14
	andb	736(%rsp), %r12b        # 1-byte Folded Reload
	jne	.LBB150_5
# BB#6:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm6, 528(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 560(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 576(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 608(%rsp)        # 16-byte Spill
	vmovups	%ymm1, 736(%rsp)        # 32-byte Spill
	movb	704(%rsp), %bl          # 1-byte Reload
	movl	688(%rsp), %r9d         # 4-byte Reload
	vmovaps	640(%rsp), %xmm8        # 16-byte Reload
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB150_7
	.align	16, 0x90
.LBB150_5:                              #   in Loop: Header=BB150_2 Depth=1
	vmovups	%ymm1, 736(%rsp)        # 32-byte Spill
	vmovaps	608(%rsp), %xmm0        # 16-byte Reload
	vmulps	-16(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmovaps	%xmm8, 576(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm6, %xmm2 # xmm2 = xmm6[0,2],xmm5[0,2]
	vmovaps	%xmm6, 528(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, %xmm5
	vmovaps	%xmm3, %xmm9
	vmovaps	%xmm9, 560(%rsp)        # 16-byte Spill
	vmovaps	32(%rsp), %xmm3         # 16-byte Reload
	vsubps	%xmm3, %xmm2, %xmm2
	vmovaps	48(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	-32(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm4, 592(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm4, %xmm9, %xmm4 # xmm4 = xmm9[0,2],xmm4[0,2]
	vmovaps	%xmm5, %xmm9
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	-48(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm7, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm7[0,2]
	vmovaps	%xmm7, 608(%rsp)        # 16-byte Spill
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vminps	%xmm3, %xmm4, %xmm4
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm4, %xmm4
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	784(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm4, %xmm1, %xmm0
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm2, %xmm0, %xmm11
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm0, %xmm11
	movb	704(%rsp), %bl          # 1-byte Reload
	movl	688(%rsp), %r9d         # 4-byte Reload
	vmovaps	640(%rsp), %xmm8        # 16-byte Reload
.LBB150_7:                              # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	656(%rsp), %xmm4        # 16-byte Reload
	vminps	%xmm3, %xmm9, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm1
	vmovaps	%xmm1, %xmm9
	testb	%r12b, %r12b
	jne	.LBB150_9
# BB#8:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB150_9:                              # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	movl	%r14d, %eax
	vmulps	848(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm12, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm12[1,3]
	vmovaps	800(%rsp), %xmm15       # 16-byte Reload
	vsubps	%xmm15, %xmm2, %xmm2
	vmovaps	816(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmulps	832(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
	vshufps	$221, %xmm4, %xmm8, %xmm4 # xmm4 = xmm8[1,3],xmm4[1,3]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vaddps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm14, %xmm0, %xmm8
	andl	%r8d, %eax
	jne	.LBB150_10
# BB#11:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 656(%rsp)       # 16-byte Spill
	vmovups	%ymm1, 704(%rsp)        # 32-byte Spill
	vmovdqa	672(%rsp), %xmm10       # 16-byte Reload
	jmp	.LBB150_12
	.align	16, 0x90
.LBB150_10:                             #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm10, 656(%rsp)       # 16-byte Spill
	vmovups	%ymm1, 704(%rsp)        # 32-byte Spill
	vmovdqa	672(%rsp), %xmm10       # 16-byte Reload
	vpaddd	-128(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	528(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 544(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	%xmm3, %xmm6
	vmovaps	%xmm6, 880(%rsp)        # 16-byte Spill
	vmulps	-16(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	32(%rsp), %xmm5         # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	48(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	560(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 592(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	-32(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	576(%rsp), %xmm4        # 16-byte Reload
	vshufps	$221, 608(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	-48(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm6, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	784(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm0
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm2, %xmm0, %xmm9
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	%xmm8, %xmm0, %xmm9
.LBB150_12:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vpaddd	208(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rdi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	200(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rsi,%rax,4), %xmm13
	vmovups	24616(%rsi,%rax,4), %xmm14
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rsi,%rax,4), %xmm6
	vmovups	24616(%rsi,%rax,4), %xmm15
	movq	184(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rsi,%rax,4), %xmm10
	vmovups	24616(%rsi,%rax,4), %xmm12
	andb	%r9b, %bl
	jne	.LBB150_13
# BB#14:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 640(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, %xmm13
	vmovaps	%xmm8, 688(%rsp)        # 16-byte Spill
	jmp	.LBB150_15
	.align	16, 0x90
.LBB150_13:                             #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 688(%rsp)        # 16-byte Spill
	vmovdqa	624(%rsp), %xmm0        # 16-byte Reload
	vpaddd	-112(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rdi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r13,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r13,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	-64(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm13, %xmm4 # xmm4 = xmm13[0,2],xmm14[0,2]
	vmovaps	%xmm13, 640(%rsp)       # 16-byte Spill
	vmovaps	(%rsp), %xmm0           # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	16(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm5, %xmm8
	vmulps	-80(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm13
	vshufps	$136, %xmm15, %xmm13, %xmm6 # xmm6 = xmm13[0,2],xmm15[0,2]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmulps	-96(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm12, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm12[0,2]
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmovaps	880(%rsp), %xmm1        # 16-byte Reload
	vminps	%xmm1, %xmm5, %xmm5
	vmaxps	%xmm7, %xmm5, %xmm5
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm0, %xmm3
	vminps	%xmm1, %xmm8, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vsubps	%xmm4, %xmm3, %xmm11
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	656(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
.LBB150_15:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	movl	876(%rsp), %edx         # 4-byte Reload
	vmovups	736(%rsp), %ymm6        # 32-byte Reload
	movl	%r8d, %eax
	movq	520(%rsp), %rcx         # 8-byte Reload
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB150_17
# BB#16:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm11, %xmm6
.LBB150_17:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	testl	%eax, %eax
	jne	.LBB150_19
# BB#18:                                #   in Loop: Header=BB150_2 Depth=1
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vmovaps	672(%rsp), %xmm2        # 16-byte Reload
	vmulps	-64(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmovaps	(%rsp), %xmm5           # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	16(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vshufps	$221, %xmm15, %xmm13, %xmm3 # xmm3 = xmm13[1,3],xmm15[1,3]
	vmulps	-80(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$221, %xmm12, %xmm10, %xmm4 # xmm4 = xmm10[1,3],xmm12[1,3]
	vmulps	-96(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	880(%rsp), %xmm1        # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vmovaps	784(%rsp), %xmm1        # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm2
	vsubps	%xmm0, %xmm2, %xmm9
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vfmadd213ps	688(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB150_19:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovups	704(%rsp), %ymm1        # 32-byte Reload
	testb	%bl, %bl
	jne	.LBB150_21
# BB#20:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	%xmm9, %xmm1
.LBB150_21:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB150_2 Depth=1
	vmovaps	.LCPI150_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI150_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm6, %ymm1, %ymm2
	vblendps	$170, %ymm0, %ymm2, %ymm0 # ymm0 = ymm2[0],ymm0[1],ymm2[2],ymm0[3],ymm2[4],ymm0[5],ymm2[6],ymm0[7]
	movslq	%r8d, %rax
	movq	424(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	504(%rsp), %rcx         # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %ebp
	addl	$-1, %edx
	movl	%edx, 876(%rsp)         # 4-byte Spill
	jne	.LBB150_2
.LBB150_22:                             # %destructor_block
	xorl	%eax, %eax
	addq	$904, %rsp              # imm = 0x388
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end150:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11, .Lfunc_end150-par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI151_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI151_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI151_9:
	.zero	16,255
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI151_1:
	.long	1199570688              # float 65535
.LCPI151_3:
	.long	1065353216              # float 1
.LCPI151_4:
	.long	2147483647              # 0x7fffffff
.LCPI151_5:
	.long	1056964608              # float 0.5
.LCPI151_6:
	.long	1073741824              # float 2
	.section	.rodata,"a",@progbits
	.align	32
.LCPI151_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI151_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1000, %rsp             # imm = 0x3E8
	movq	%rsi, 504(%rsp)         # 8-byte Spill
	movl	36(%rdx), %eax
	addl	$51, %eax
	sarl	$3, %eax
	movl	%eax, 924(%rsp)         # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB151_22
# BB#1:                                 # %for dV.s0.v10.v10.preheader
	vmovss	(%rdx), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 672(%rsp)        # 4-byte Spill
	vmovss	4(%rdx), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmovss	24(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 656(%rsp)        # 4-byte Spill
	vmovss	28(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm12        # xmm12 = mem[0],zero,zero,zero
	movslq	48(%rdx), %rax
	movq	%rax, 784(%rsp)         # 8-byte Spill
	movl	52(%rdx), %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movl	56(%rdx), %eax
	movq	%rax, 880(%rsp)         # 8-byte Spill
	movl	60(%rdx), %ebx
	movq	%rbx, 688(%rsp)         # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 800(%rsp)         # 8-byte Spill
	movl	68(%rdx), %eax
	movl	%eax, 864(%rsp)         # 4-byte Spill
	movq	%rdx, %rcx
	movq	%rcx, 472(%rsp)         # 8-byte Spill
	movl	72(%rcx), %esi
	movq	%rsi, 448(%rsp)         # 8-byte Spill
	movl	76(%rcx), %eax
	movq	%rax, 736(%rsp)         # 8-byte Spill
	movl	80(%rcx), %ebp
	movq	%rbp, 816(%rsp)         # 8-byte Spill
	movl	84(%rcx), %r10d
	movslq	88(%rcx), %rax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	vmovss	92(%rcx), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmovss	%xmm1, 624(%rsp)        # 4-byte Spill
	vmovss	96(%rcx), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vmovss	100(%rcx), %xmm11       # xmm11 = mem[0],zero,zero,zero
	movq	104(%rcx), %rax
	movq	%rax, 488(%rsp)         # 8-byte Spill
	movl	%ebp, %eax
	negl	%eax
	movq	120(%rcx), %r15
	movq	136(%rcx), %r14
	movq	152(%rcx), %rcx
	movq	%rcx, 640(%rsp)         # 8-byte Spill
	leal	(%rsi,%rsi), %edi
	movl	%edi, 496(%rsp)         # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edi, %eax
	negl	%eax
	movl	%esi, %r11d
	sarl	$31, %r11d
	andnl	%edi, %r11d, %r9d
	andl	%eax, %r11d
	orl	%r9d, %r11d
	movl	%r11d, 456(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %r8d
	movl	%r8d, 848(%rsp)         # 4-byte Spill
	movl	%r8d, %ecx
	movl	%r8d, %r13d
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%ebp, %ecx
	leal	-1(%rbp,%rsi), %eax
	movl	%eax, 416(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	%eax, %r8d
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	movl	%ecx, 608(%rsp)         # 4-byte Spill
	leal	(%rbp,%rsi), %r9d
	movl	%r9d, 440(%rsp)         # 4-byte Spill
	xorl	%r12d, %r12d
	testl	%r9d, %r9d
	movl	$0, %eax
	cmovlel	%r8d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	testl	%r9d, %r9d
	cmovlel	%ecx, %eax
	movl	%eax, 592(%rsp)         # 4-byte Spill
	movl	$2, %eax
	subl	%ebp, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%ebp, %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	movl	%ecx, 584(%rsp)         # 4-byte Spill
	cmpl	$3, %r9d
	movl	$2, %eax
	cmovll	%r8d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	$3, %r9d
	cmovll	%ecx, %eax
	movl	%eax, 560(%rsp)         # 4-byte Spill
	movl	$2, %eax
	movq	%rbx, %rdi
	subl	%edi, %eax
	movq	832(%rsp), %rbx         # 8-byte Reload
	leal	(%rbx,%rbx), %ecx
	movl	%ecx, 720(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%ebx, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 360(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %ecx
	movl	%ecx, 368(%rsp)         # 4-byte Spill
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	leal	-1(%rdi,%rbx), %esi
	movl	%esi, 976(%rsp)         # 4-byte Spill
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 528(%rsp)         # 4-byte Spill
	leal	(%rdi,%rbx), %edx
	movl	%edx, 704(%rsp)         # 4-byte Spill
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %edx
	cmovll	%ecx, %eax
	movl	%eax, 928(%rsp)         # 4-byte Spill
	movq	504(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %r11d
	subl	%r10d, %r11d
	movq	%r11, 768(%rsp)         # 8-byte Spill
	movq	736(%rsp), %r9          # 8-byte Reload
	leal	(%r9,%r9), %r8d
	movl	%r8d, 400(%rsp)         # 4-byte Spill
	leal	-1(%r11), %eax
	cltd
	idivl	%r8d
	movl	%r8d, %eax
	negl	%eax
	movl	%r9d, %r13d
	sarl	$31, %r13d
	andnl	%r8d, %r13d, %ecx
	andl	%eax, %r13d
	orl	%ecx, %r13d
	movl	%r13d, 464(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	leal	-1(%r9,%r9), %edi
	movl	%edi, 752(%rsp)         # 4-byte Spill
	subl	%eax, %edi
	cmpl	%eax, %r9d
	cmovgl	%eax, %edi
	addl	%r10d, %edi
	leal	-1(%r10,%r9), %ecx
	cmpl	%edi, %ecx
	cmovlel	%ecx, %edi
	cmpl	%r10d, %edi
	cmovll	%r10d, %edi
	leal	(%r10,%r9), %ebx
	cmpl	%esi, %ebx
	movl	%ebx, %eax
	cmovgl	%esi, %eax
	addl	$-1, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%esi, %ebx
	cmovll	%edi, %eax
	movl	%eax, 896(%rsp)         # 4-byte Spill
	movl	%r11d, %eax
	cltd
	idivl	%r8d
	movl	%r8d, %r11d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	%r13d, %r8d
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	752(%rsp), %r13d        # 4-byte Reload
	movl	%r13d, %ebp
	subl	%eax, %ebp
	cmpl	%eax, %r9d
	cmovgl	%eax, %ebp
	addl	%r10d, %ebp
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	cmpl	%r10d, %ebp
	cmovll	%r10d, %ebp
	cmpl	%esi, %ecx
	movl	%ecx, %eax
	cmovgl	%esi, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%esi, %ebx
	movq	%rsi, %rbx
	cmovlel	%ebp, %eax
	movl	%eax, %esi
	movq	768(%rsp), %rax         # 8-byte Reload
	leal	1(%rax), %eax
	cltd
	idivl	%r11d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%r13d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r9d
	cmovgl	%eax, %edx
	addl	%r10d, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 512(%rsp)         # 4-byte Spill
	leal	1(%rbx), %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	%ebx, %r10d
	cmovgl	%ebp, %esi
	movl	%esi, 384(%rsp)         # 4-byte Spill
	movl	896(%rsp), %esi         # 4-byte Reload
	cmovgel	%edi, %esi
	movl	%esi, 896(%rsp)         # 4-byte Spill
	cmpl	%ebx, %ecx
	cmovlel	%edx, %eax
	movl	%eax, 480(%rsp)         # 4-byte Spill
	movq	688(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	cltd
	idivl	720(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	movl	360(%rsp), %r8d         # 4-byte Reload
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	368(%rsp), %r9d         # 4-byte Reload
	movl	%r9d, %edi
	subl	%eax, %edi
	movq	832(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %edi
	movq	%rsi, %rax
	addl	%eax, %edi
	movl	976(%rsp), %esi         # 4-byte Reload
	cmpl	%edi, %esi
	cmovlel	%esi, %edi
	cmpl	%eax, %edi
	cmovll	%eax, %edi
	movl	704(%rsp), %ebx         # 4-byte Reload
	testl	%ebx, %ebx
	movl	$0, %edx
	cmovlel	%esi, %edx
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movq	%rax, %r11
	testl	%ebx, %ebx
	cmovlel	%edi, %edx
	movl	%edx, 960(%rsp)         # 4-byte Spill
	movl	$1, %eax
	movq	816(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	496(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	456(%rsp), %eax         # 4-byte Folded Reload
	addl	%edx, %eax
	movl	848(%rsp), %ebx         # 4-byte Reload
	subl	%eax, %ebx
	movq	448(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ebx
	movq	%rsi, %rdx
	addl	%edx, %ebx
	movl	416(%rsp), %r13d        # 4-byte Reload
	cmpl	%ebx, %r13d
	cmovlel	%r13d, %ebx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	movl	%ebx, 848(%rsp)         # 4-byte Spill
	movl	440(%rsp), %esi         # 4-byte Reload
	cmpl	$1, %esi
	setg	%al
	cmpl	$2, %esi
	cmovgel	%r12d, %r13d
	movzbl	%al, %eax
	orl	%eax, %r13d
	cmpl	%edx, %r13d
	cmovll	%edx, %r13d
	cmpl	$2, %esi
	cmovll	%ebx, %r13d
	movl	$1, %eax
	subl	%r11d, %eax
	cltd
	idivl	720(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	subl	%eax, %r9d
	movq	832(%rsp), %rsi         # 8-byte Reload
	cmpl	%eax, %esi
	cmovgl	%eax, %r9d
	movq	%r11, %rbp
	addl	%ebp, %r9d
	movl	976(%rsp), %r11d        # 4-byte Reload
	cmpl	%r9d, %r11d
	cmovlel	%r11d, %r9d
	cmpl	%ebp, %r9d
	cmovll	%ebp, %r9d
	movl	704(%rsp), %edx         # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r12d, %r11d
	movzbl	%al, %eax
	orl	%eax, %r11d
	cmpl	%ebp, %r11d
	cmovll	%ebp, %r11d
	movq	%rbp, %r8
	movl	%r11d, %ebp
	cmpl	$2, %edx
	cmovll	%r9d, %ebp
	movq	784(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rdx
	sarq	$63, %rdx
	movq	%rdx, 720(%rsp)         # 8-byte Spill
	movl	%eax, %edx
	sarl	$31, %edx
	andl	%eax, %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	movq	504(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r11d
	andl	$1, %r11d
	movl	%r11d, 44(%rsp)         # 4-byte Spill
	cmpl	$1, %r8d
	cmovgl	%r9d, %ebp
	movl	%ebp, 976(%rsp)         # 4-byte Spill
	movl	864(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm1
	vmovaps	%xmm1, 832(%rsp)        # 16-byte Spill
	movq	800(%rsp), %r9          # 8-byte Reload
	imull	%r9d, %eax
	addl	%r8d, %eax
	movq	%r8, %rbx
	movl	%eax, 864(%rsp)         # 4-byte Spill
	movq	816(%rsp), %r8          # 8-byte Reload
	cmpl	$1, %r8d
	cmovgl	848(%rsp), %r13d        # 4-byte Folded Reload
	movq	544(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %eax
	imull	%r10d, %eax
	movq	472(%rsp), %rbp         # 8-byte Reload
	movl	40(%rbp), %esi
	movl	%esi, 848(%rsp)         # 4-byte Spill
	sarl	$5, %esi
	movl	%esi, 704(%rsp)         # 4-byte Spill
	addl	%r8d, %eax
	cltq
	movq	%rax, 456(%rsp)         # 8-byte Spill
	movslq	384(%rsp), %rsi         # 4-byte Folded Reload
	movslq	%r13d, %r8
	imulq	%rdx, %rsi
	movq	%rsi, 448(%rsp)         # 8-byte Spill
	movq	%rdx, %r13
	subq	%rax, %r8
	testl	%ebx, %ebx
	movl	960(%rsp), %eax         # 4-byte Reload
	cmovgl	%edi, %eax
	movl	%eax, 960(%rsp)         # 4-byte Spill
	movq	%rbp, %rax
	vmovss	20(%rax), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vmovss	12(%rax), %xmm15        # xmm15 = mem[0],zero,zero,zero
	vmovss	16(%rax), %xmm2         # xmm2 = mem[0],zero,zero,zero
	movl	44(%rax), %eax
	movl	%eax, 472(%rsp)         # 4-byte Spill
	movq	768(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	400(%rsp), %ebp         # 4-byte Reload
	idivl	%ebp
	movl	%edx, %edi
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%ebp
	vmovss	.LCPI151_1(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm4, %xmm1
	vmulss	%xmm0, %xmm1, %xmm3
	vdivss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm14
	movq	880(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%rsi), %eax
	vmovd	%eax, %xmm10
	vsubss	%xmm0, %xmm2, %xmm2
	leal	6(%r9,%rsi), %eax
	vmovd	%eax, %xmm3
	movl	%edi, %eax
	sarl	$31, %eax
	movl	464(%rsp), %esi         # 4-byte Reload
	andl	%esi, %eax
	addl	%edi, %eax
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	752(%rsp), %r9d         # 4-byte Reload
	movl	%r9d, %edx
	subl	%eax, %edx
	movq	736(%rsp), %rbx         # 8-byte Reload
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	addl	%r10d, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movq	504(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi), %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	leal	-2(%r10,%rbx), %ebp
	cmpl	%esi, %ebp
	cmovlel	%edx, %eax
	leal	-2(%r10), %ebp
	cmpl	%esi, %ebp
	cmovgl	%edx, %eax
	movslq	%eax, %rbp
	imulq	%r13, %rbp
	movl	%r9d, %eax
	subl	%edi, %eax
	cmpl	%edi, %ebx
	cmovgl	%edi, %eax
	addl	%r10d, %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	leal	-2(%rsi), %edx
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	leal	2(%r10,%rbx), %edx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	cmpl	%esi, %edx
	cmovlel	%eax, %ecx
	leal	2(%r10), %edx
	cmpl	%esi, %edx
	movq	%rsi, %r9
	cmovgl	%eax, %ecx
	vmulss	%xmm2, %xmm1, %xmm0
	vdivss	%xmm0, %xmm5, %xmm9
	movslq	%ecx, %rbx
	imulq	%r13, %rbx
	vsubss	%xmm13, %xmm4, %xmm0
	vmulss	%xmm12, %xmm0, %xmm1
	vdivss	%xmm11, %xmm1, %xmm1
	vaddss	%xmm1, %xmm13, %xmm13
	movq	800(%rsp), %rdi         # 8-byte Reload
	leal	6(%rdi), %edx
	vmovd	%edx, %xmm1
	addl	$-1, %r10d
	cmpl	%r9d, %r10d
	movl	480(%rsp), %eax         # 4-byte Reload
	cmovgl	512(%rsp), %eax         # 4-byte Folded Reload
	movslq	%eax, %rdx
	imulq	%r13, %rdx
	movslq	896(%rsp), %rsi         # 4-byte Folded Reload
	imulq	%r13, %rsi
	movq	688(%rsp), %rax         # 8-byte Reload
	cmpl	$2, %eax
	movl	928(%rsp), %eax         # 4-byte Reload
	cmovgl	528(%rsp), %eax         # 4-byte Folded Reload
	movl	%eax, 928(%rsp)         # 4-byte Spill
	movq	816(%rsp), %rax         # 8-byte Reload
	cmpl	$2, %eax
	movl	560(%rsp), %r10d        # 4-byte Reload
	cmovgl	584(%rsp), %r10d        # 4-byte Folded Reload
	testl	%eax, %eax
	movl	592(%rsp), %eax         # 4-byte Reload
	cmovgl	608(%rsp), %eax         # 4-byte Folded Reload
	leaq	(%rdx,%r8), %rcx
	movq	%rcx, 768(%rsp)         # 8-byte Spill
	leaq	(%rsi,%r8), %rcx
	movq	%rcx, 816(%rsp)         # 8-byte Spill
	movq	448(%rsp), %rcx         # 8-byte Reload
	leaq	(%r8,%rcx), %rdx
	movq	%rdx, 736(%rsp)         # 8-byte Spill
	leaq	(%r8,%rbp), %rdx
	movq	%rdx, 752(%rsp)         # 8-byte Spill
	leaq	(%r8,%rbx), %rdx
	movq	%rdx, 688(%rsp)         # 8-byte Spill
	movq	456(%rsp), %rdx         # 8-byte Reload
	subq	%rdx, %rbp
	subq	%rdx, %rbx
	subq	%rdx, %rcx
	movslq	%eax, %rdx
	movslq	%r10d, %rsi
	leaq	(%rbp,%rdx), %rax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	addq	%rsi, %rbp
	movq	%rbp, 608(%rsp)         # 8-byte Spill
	leaq	(%rbx,%rdx), %rax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	addq	%rsi, %rbx
	movq	%rbx, 592(%rsp)         # 8-byte Spill
	leaq	(%rdx,%rcx), %rax
	movq	%rax, 584(%rsp)         # 8-byte Spill
	leaq	(%rsi,%rcx), %rax
	movq	%rax, 528(%rsp)         # 8-byte Spill
	vsubss	%xmm12, %xmm6, %xmm2
	movq	880(%rsp), %rax         # 8-byte Reload
	leal	5(%rdi,%rax), %esi
	movq	%rdi, %rcx
	vmovd	%esi, %xmm12
	movslq	704(%rsp), %rsi         # 4-byte Folded Reload
	movq	%rsi, 704(%rsp)         # 8-byte Spill
	shlq	$5, %rsi
	addq	$48, %rsi
	movl	%r9d, %ebp
	andl	$63, %ebp
	imulq	%rsi, %rbp
	vmulss	%xmm2, %xmm0, %xmm0
	leal	7(%r9), %edi
	movl	472(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %edi
	movl	848(%rsp), %eax         # 4-byte Reload
	andl	$-32, %eax
	addl	$64, %eax
	imull	%eax, %edi
	movl	%eax, %r10d
	leal	5(%rcx), %esi
	vmovd	%esi, %xmm8
	movq	720(%rsp), %rax         # 8-byte Reload
	andq	784(%rsp), %rax         # 8-byte Folded Reload
	leal	9(%r9), %edx
	movl	%ebx, %esi
	subl	%esi, %edx
	imull	%r10d, %edx
	leal	6(%r9), %r8d
	subl	%esi, %r8d
	imull	%r10d, %r8d
	movq	%r8, 456(%rsp)          # 8-byte Spill
	subq	%rax, %rbp
	movq	%rbp, 480(%rsp)         # 8-byte Spill
	leal	10(%r9), %ebp
	subl	%esi, %ebp
	leal	8(%r9), %eax
	subl	%esi, %eax
	imull	%r10d, %ebp
	movq	%rbp, 448(%rsp)         # 8-byte Spill
	imull	%r10d, %eax
	movq	%rax, 440(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vdivss	%xmm0, %xmm11, %xmm11
	vmovss	672(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm4, %xmm4
	vmovss	656(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm4, %xmm7
	vmovss	624(%rsp), %xmm2        # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm2, %xmm7, %xmm7
	vaddss	%xmm7, %xmm0, %xmm7
	vmovdqa	.LCPI151_0(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm6, %xmm3, %xmm0
	vmovdqa	%xmm0, 416(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vsubss	%xmm5, %xmm15, %xmm1
	movl	864(%rsp), %r10d        # 4-byte Reload
	vmovd	%r10d, %xmm3
	vmulss	%xmm1, %xmm4, %xmm1
	movl	976(%rsp), %r9d         # 4-byte Reload
	vmovd	%r9d, %xmm4
	vdivss	%xmm1, %xmm2, %xmm1
	vmovd	960(%rsp), %xmm0        # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm12, %xmm5
	vpaddd	%xmm6, %xmm5, %xmm2
	vmovdqa	%xmm2, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vmovdqa	%xmm2, 368(%rsp)        # 16-byte Spill
	movl	928(%rsp), %r13d        # 4-byte Reload
	vmovd	%r13d, %xmm2
	vpsubd	%xmm3, %xmm4, %xmm4
	vpsubd	%xmm3, %xmm0, %xmm0
	vpsubd	%xmm3, %xmm2, %xmm2
	movq	496(%rsp), %rbx         # 8-byte Reload
	movq	%rcx, %rsi
	subl	%esi, %ebx
	vmovd	%esi, %xmm3
	movq	880(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rsi,%rcx), %esi
	vpbroadcastd	%xmm10, %xmm5
	vmovdqa	%xmm5, 336(%rsp)        # 16-byte Spill
	vbroadcastss	832(%rsp), %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm6, 896(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 320(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vmovd	%esi, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 304(%rsp)        # 16-byte Spill
	vmovd	%r10d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 288(%rsp)        # 16-byte Spill
	vmovd	%r9d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 272(%rsp)        # 16-byte Spill
	movl	960(%rsp), %ecx         # 4-byte Reload
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm3
	vmovaps	%xmm3, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm3
	vmovaps	%xmm3, 224(%rsp)        # 16-byte Spill
	vmovd	%r13d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 208(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm3
	vmovdqa	%xmm3, 192(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 176(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm0
	vmovdqa	%xmm0, 160(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 16(%rsp)         # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, (%rsp)           # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, -16(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm7, %xmm0
	vmovaps	%xmm0, -32(%rsp)        # 16-byte Spill
	movq	704(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %esi
	shll	$9, %esi
	leal	(%rsi,%rsi,2), %esi
	addl	%esi, %edi
	movq	%rdi, 472(%rsp)         # 8-byte Spill
	addl	%esi, %edx
	movq	%rdx, 464(%rsp)         # 8-byte Spill
	shll	$10, %ecx
	leal	(%rcx,%rcx,2), %edi
	leal	(%r8,%rdi), %edx
	movq	%rdx, 152(%rsp)         # 8-byte Spill
	leal	(%rbp,%rdi), %edx
	movq	%rdx, 144(%rsp)         # 8-byte Spill
	leal	(%rdi,%rax), %edx
	movq	%rdx, 136(%rsp)         # 8-byte Spill
	leal	(%r8,%rsi), %ecx
	movq	%rcx, 128(%rsp)         # 8-byte Spill
	leal	(%rbp,%rsi), %ecx
	movq	%rcx, 120(%rsp)         # 8-byte Spill
	leal	(%rsi,%rax), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	leal	-6(%rbx), %eax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	addl	$-5, %ebx
	movq	%rbx, 360(%rsp)         # 8-byte Spill
	movq	496(%rsp), %rcx         # 8-byte Reload
	leal	-5(%rcx), %eax
	movq	%rax, 96(%rsp)          # 8-byte Spill
	leal	-6(%rcx), %eax
	movq	%rax, 88(%rsp)          # 8-byte Spill
	movq	640(%rsp), %rdx         # 8-byte Reload
	movq	736(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 864(%rsp)        # 16-byte Spill
	movq	688(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 848(%rsp)        # 16-byte Spill
	movq	752(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 832(%rsp)        # 16-byte Spill
	movq	816(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	movq	768(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 64(%rsp)         # 16-byte Spill
	movq	584(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -48(%rsp)        # 16-byte Spill
	movq	544(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -64(%rsp)        # 16-byte Spill
	movq	560(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -80(%rsp)        # 16-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -96(%rsp)        # 16-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -112(%rsp)       # 16-byte Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, -128(%rsp)       # 16-byte Spill
	vpabsd	%xmm5, %xmm0
	vmovdqa	%xmm0, 48(%rsp)         # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm5, %xmm0
	vmovdqa	%xmm0, 800(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI151_3(%rip), %xmm12
	vbroadcastss	.LCPI151_4(%rip), %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI151_5(%rip), %xmm0
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI151_6(%rip), %xmm0
	vmovaps	%xmm0, 784(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB151_2:                              # %for dV.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	testl	%r11d, %r11d
	setne	768(%rsp)               # 1-byte Folded Spill
	sete	592(%rsp)               # 1-byte Folded Spill
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %ecx
	movl	%ecx, %eax
	movl	%ecx, %ebx
	andl	$1, %eax
	movl	%eax, 584(%rsp)         # 4-byte Spill
	sete	928(%rsp)               # 1-byte Folded Spill
	movq	360(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI151_2(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	336(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ebp
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	movq	104(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %r10d
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vmovdqa	48(%rsp), %xmm3         # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 720(%rsp)        # 16-byte Spill
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r13d
	vmovd	%ecx, %xmm0
	vpinsrd	$1, %r10d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%ebx, %xmm1
	movl	%ebx, %r10d
	vpbroadcastd	%xmm1, %xmm11
	vmovdqa	416(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	400(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	880(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vmovdqa	800(%rsp), %xmm3        # 16-byte Reload
	vpsubd	%xmm0, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	320(%rsp), %xmm13       # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	304(%rsp), %xmm10       # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	movq	88(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	896(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpsubd	288(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 624(%rsp)        # 16-byte Spill
	vpaddd	272(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r14,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r14,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r14,%rax,4), %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],mem[0]
	vpaddd	256(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %r9
	vmulps	864(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 736(%rsp)        # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 704(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	224(%rsp), %xmm15       # 16-byte Reload
	vsubps	%xmm15, %xmm1, %xmm1
	vmovaps	240(%rsp), %xmm8        # 16-byte Reload
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm6
	vmulps	848(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 688(%rsp)        # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 672(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm8, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vmovaps	960(%rsp), %xmm4        # 16-byte Reload
	vandps	%xmm4, %xmm2, %xmm2
	vmulps	832(%rsp), %xmm7, %xmm5 # 16-byte Folded Reload
	movq	120(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 640(%rsp)        # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 656(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm3 # xmm3 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm8, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm6, %xmm3, %xmm0
	vandps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 752(%rsp)        # 16-byte Spill
	vmulps	816(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	movq	472(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm5
	vmovaps	%xmm5, 608(%rsp)        # 16-byte Spill
	vmovdqa	720(%rsp), %xmm1        # 16-byte Reload
	vmovdqa	880(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm2
	vmovdqa	800(%rsp), %xmm3        # 16-byte Reload
	vpsubd	%xmm1, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm2
	vmovups	12312(%r15,%rdx,4), %xmm4
	vshufps	$136, %xmm4, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm4[0,2]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm8, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	64(%rsp), %xmm5         # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm3
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovdqa	384(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm6, %xmm6
	vpxor	.LCPI151_9(%rip), %xmm6, %xmm6
	vmovdqa	368(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	movq	96(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%r12), %esi
	vmovd	%esi, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm14, %xmm7, %xmm7
	vpminsd	%xmm10, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vmovups	12296(%r15,%rdx,4), %xmm14
	vmovups	12312(%r15,%rdx,4), %xmm11
	vshufps	$136, %xmm11, %xmm14, %xmm6 # xmm6 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm8, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vpxor	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vpxor	%xmm10, %xmm10, %xmm10
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 720(%rsp)        # 16-byte Spill
	vmovdqa	624(%rsp), %xmm0        # 16-byte Reload
	vpaddd	208(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rdi
	vpmulld	896(%rsp), %xmm2, %xmm9 # 16-byte Folded Reload
	vpaddd	192(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbx
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vpaddd	176(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbp
	vmovss	(%r14,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rax
	vinsertps	$16, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%ebx, %rax
	vinsertps	$32, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rbx
	vinsertps	$48, (%r14,%rbx,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rax
	vmovaps	736(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 704(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	864(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm2, %xmm0, %xmm2
	vmovss	(%r14,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %r9
	vinsertps	$16, (%r14,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r8d, %rax
	vinsertps	$32, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %r8
	vinsertps	$48, (%r14,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 624(%rsp)        # 16-byte Spill
	movslq	%edi, %rax
	vmovaps	688(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	848(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm1, %xmm0, %xmm6
	vmovss	(%r14,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rdi
	vinsertps	$16, (%r14,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%edx, %rax
	vinsertps	$32, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rdx
	vinsertps	$48, (%r14,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 688(%rsp)        # 16-byte Spill
	movslq	%ebp, %rax
	sarq	$32, %rbp
	vmovaps	640(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 656(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	832(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm3, %xmm0, %xmm0
	vmovss	(%r14,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	sarq	$32, %rsi
	vinsertps	$16, (%r14,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r14,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r14,%rsi,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm1, 704(%rsp)        # 16-byte Spill
	movl	%r10d, %esi
	vpaddd	160(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	vmovss	(%r14,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r14,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r14,%rcx,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vminps	%xmm12, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vminps	%xmm12, %xmm6, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm7
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm6
	vmovaps	608(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm4, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm4[1,3]
	vmulps	816(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm8, %xmm4
	vmulps	%xmm13, %xmm5, %xmm0
	vshufps	$221, %xmm11, %xmm14, %xmm5 # xmm5 = xmm14[1,3],xmm11[1,3]
	vsubps	%xmm15, %xmm5, %xmm5
	vmulps	%xmm5, %xmm8, %xmm5
	sarq	$32, %rax
	vinsertps	$48, (%r14,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 736(%rsp)        # 16-byte Spill
	vmovaps	752(%rsp), %xmm1        # 16-byte Reload
	vmulps	976(%rsp), %xmm1, %xmm15 # 16-byte Folded Reload
	movl	44(%rsp), %r11d         # 4-byte Reload
	movl	%r11d, %eax
	vmovaps	%xmm15, %xmm13
	andl	%esi, %eax
	jne	.LBB151_4
# BB#3:                                 # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB151_4:                              # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmulps	%xmm3, %xmm4, %xmm10
	vmulps	%xmm0, %xmm5, %xmm11
	vmovaps	960(%rsp), %xmm0        # 16-byte Reload
	vandps	720(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm7, %xmm5
	vsubps	%xmm2, %xmm6, %xmm4
	movq	440(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm1
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm9
	movq	456(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm6
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm7
	movq	448(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm3
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm2
	movb	768(%rsp), %cl          # 1-byte Reload
	andb	928(%rsp), %cl          # 1-byte Folded Reload
	movb	%cl, %dil
	jne	.LBB151_5
# BB#6:                                 # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm14, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 752(%rsp)        # 16-byte Spill
	vmovups	%ymm15, 928(%rsp)       # 32-byte Spill
	jmp	.LBB151_7
	.align	16, 0x90
.LBB151_5:                              #   in Loop: Header=BB151_2 Depth=1
	vmovups	%ymm15, 928(%rsp)       # 32-byte Spill
	vmovaps	%xmm4, %xmm8
	vmovaps	624(%rsp), %xmm4        # 16-byte Reload
	vmulps	-48(%rsp), %xmm4, %xmm13 # 16-byte Folded Reload
	vmovaps	%xmm5, 608(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm9, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm9[0,2]
	vmovaps	%xmm9, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, %xmm15
	vmovaps	(%rsp), %xmm0           # 16-byte Reload
	vsubps	%xmm0, %xmm5, %xmm5
	vmovaps	16(%rsp), %xmm1         # 16-byte Reload
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm13, %xmm13
	vmulps	-64(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm6, 672(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm7[0,2]
	vmovaps	%xmm7, 656(%rsp)        # 16-byte Spill
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmulps	-80(%rsp), %xmm4, %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm8, %xmm4
	vshufps	$136, %xmm2, %xmm3, %xmm7 # xmm7 = xmm3[0,2],xmm2[0,2]
	vmovaps	%xmm2, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 640(%rsp)        # 16-byte Spill
	vsubps	%xmm0, %xmm7, %xmm7
	vmovaps	%xmm15, %xmm0
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm12, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vminps	%xmm12, %xmm6, %xmm6
	vmaxps	%xmm1, %xmm6, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vminps	%xmm12, %xmm13, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vmovaps	784(%rsp), %xmm1        # 16-byte Reload
	vfnmadd213ps	%xmm5, %xmm1, %xmm2
	vmovaps	608(%rsp), %xmm5        # 16-byte Reload
	vandps	%xmm0, %xmm2, %xmm2
	vaddps	%xmm2, %xmm14, %xmm2
	vmovaps	%xmm14, 624(%rsp)       # 16-byte Spill
	vmulps	976(%rsp), %xmm2, %xmm13 # 16-byte Folded Reload
.LBB151_7:                              # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	movb	592(%rsp), %bl          # 1-byte Reload
	movl	584(%rsp), %edx         # 4-byte Reload
	vminps	%xmm12, %xmm10, %xmm14
	vminps	%xmm12, %xmm11, %xmm8
	vandps	%xmm0, %xmm5, %xmm15
	vandps	%xmm0, %xmm4, %xmm11
	movq	136(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm6
	vmovups	24600(%r15,%rcx,4), %xmm7
	movq	152(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm2
	vmovups	24600(%r15,%rcx,4), %xmm5
	movq	144(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm1
	vmovups	24600(%r15,%rcx,4), %xmm9
	andb	%dl, %bl
	jne	.LBB151_8
# BB#9:                                 # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm7, 512(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 528(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 560(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB151_10
	.align	16, 0x90
.LBB151_8:                              #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm5, 528(%rsp)        # 16-byte Spill
	vmovaps	688(%rsp), %xmm4        # 16-byte Reload
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 560(%rsp)        # 16-byte Spill
	vmulps	-96(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm6, 544(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 512(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vmovaps	-32(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, %xmm10
	vmovaps	-16(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm7
	vmulps	%xmm7, %xmm3, %xmm13
	vmulps	-112(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	-128(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmovaps	%xmm10, %xmm0
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm13, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vmovaps	784(%rsp), %xmm2        # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm2, %xmm1
	vandps	%xmm0, %xmm1, %xmm1
	vaddps	624(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	976(%rsp), %xmm1, %xmm13 # 16-byte Folded Reload
.LBB151_10:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmaxps	%xmm6, %xmm14, %xmm2
	vmaxps	%xmm6, %xmm8, %xmm1
	vaddps	%xmm15, %xmm11, %xmm5
	movl	%esi, %ecx
	movq	504(%rsp), %rdx         # 8-byte Reload
	orl	%edx, %ecx
	andl	$1, %ecx
	vxorps	%xmm11, %xmm11, %xmm11
	je	.LBB151_12
# BB#11:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm13, %xmm3
	vmovups	%ymm3, 928(%rsp)        # 32-byte Spill
.LBB151_12:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	976(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, %xmm7
	testb	%dil, %dil
	vmovaps	768(%rsp), %xmm13       # 16-byte Reload
	jne	.LBB151_14
# BB#13:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vxorps	%xmm7, %xmm7, %xmm7
.LBB151_14:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vandps	%xmm0, %xmm1, %xmm8
	testl	%eax, %eax
	jne	.LBB151_15
# BB#16:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	jmp	.LBB151_17
	.align	16, 0x90
.LBB151_15:                             #   in Loop: Header=BB151_2 Depth=1
	vmovaps	752(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 720(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	704(%rsp), %xmm7        # 16-byte Reload
	vmulps	-48(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm5
	vmovaps	%xmm5, 960(%rsp)        # 16-byte Spill
	vmovaps	(%rsp), %xmm0           # 16-byte Reload
	vsubps	%xmm0, %xmm1, %xmm1
	vmovaps	16(%rsp), %xmm6         # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	672(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 656(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	-64(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	640(%rsp), %xmm4        # 16-byte Reload
	vshufps	$221, %xmm13, %xmm4, %xmm4 # xmm4 = xmm4[1,3],xmm13[1,3]
	vmulps	-80(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vminps	%xmm12, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm0, %xmm1
	vandps	%xmm5, %xmm1, %xmm1
	vaddps	%xmm1, %xmm8, %xmm1
	vmulps	976(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
.LBB151_17:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovups	928(%rsp), %ymm15       # 32-byte Reload
	testl	%ecx, %ecx
	jne	.LBB151_19
# BB#18:                                #   in Loop: Header=BB151_2 Depth=1
	vmovaps	544(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 512(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	736(%rsp), %xmm4        # 16-byte Reload
	vmulps	-96(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovaps	-32(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	-16(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	592(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 528(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	-112(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	608(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 560(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	-128(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm12, %xmm0, %xmm0
	vmaxps	%xmm11, %xmm0, %xmm0
	vminps	%xmm12, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vminps	%xmm12, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmovaps	784(%rsp), %xmm3        # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm3, %xmm0
	vandps	960(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm8, %xmm0
	vmulps	976(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
.LBB151_19:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	movl	924(%rsp), %ecx         # 4-byte Reload
	testb	%bl, %bl
	jne	.LBB151_21
# BB#20:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	%xmm7, %xmm2
.LBB151_21:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB151_2 Depth=1
	vmovaps	.LCPI151_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm2, %ymm0, %ymm0
	vmovaps	.LCPI151_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm15, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%esi, %rax
	movq	480(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	488(%rsp), %rdx         # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r12d
	addl	$-1, %ecx
	movl	%ecx, 924(%rsp)         # 4-byte Spill
	jne	.LBB151_2
.LBB151_22:                             # %destructor_block
	xorl	%eax, %eax
	addq	$1000, %rsp             # imm = 0x3E8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end151:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11, .Lfunc_end151-par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI152_0:
	.long	1199570688              # float 65535
.LCPI152_3:
	.long	1065353216              # float 1
.LCPI152_4:
	.long	2147483647              # 0x7fffffff
.LCPI152_5:
	.long	1056964608              # float 0.5
.LCPI152_6:
	.long	1073741824              # float 2
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI152_1:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI152_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.section	.rodata,"a",@progbits
	.align	32
.LCPI152_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI152_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1032, %rsp             # imm = 0x408
	movq	%rdx, %r15
	movq	%rsi, 576(%rsp)         # 8-byte Spill
	movl	36(%r15), %eax
	addl	$51, %eax
	sarl	$3, %eax
	movl	%eax, 956(%rsp)         # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB152_21
# BB#1:                                 # %for dh.s0.v10.v10.preheader
	vmovss	(%r15), %xmm11          # xmm11 = mem[0],zero,zero,zero
	vmovss	4(%r15), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vmovss	8(%r15), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmovss	24(%r15), %xmm9         # xmm9 = mem[0],zero,zero,zero
	vmovss	28(%r15), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vmovss	32(%r15), %xmm12        # xmm12 = mem[0],zero,zero,zero
	movslq	48(%r15), %rax
	movq	%rax, 896(%rsp)         # 8-byte Spill
	movl	52(%r15), %r14d
	movl	56(%r15), %eax
	movq	%rax, 928(%rsp)         # 8-byte Spill
	movl	60(%r15), %ebx
	movl	64(%r15), %eax
	movq	%rax, 720(%rsp)         # 8-byte Spill
	movl	68(%r15), %eax
	movl	%eax, 1004(%rsp)        # 4-byte Spill
	movl	72(%r15), %r13d
	movl	76(%r15), %eax
	movq	%rax, 640(%rsp)         # 8-byte Spill
	movl	80(%r15), %r8d
	movl	84(%r15), %eax
	movq	%rax, 656(%rsp)         # 8-byte Spill
	movslq	88(%r15), %rax
	movq	%rax, 912(%rsp)         # 8-byte Spill
	vmovss	92(%r15), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 816(%rsp)        # 4-byte Spill
	vmovss	96(%r15), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	100(%r15), %xmm13       # xmm13 = mem[0],zero,zero,zero
	movq	104(%r15), %rax
	movq	%rax, 568(%rsp)         # 8-byte Spill
	movl	$2, %eax
	subl	%r8d, %eax
	movq	120(%r15), %rcx
	movq	%rcx, 560(%rsp)         # 8-byte Spill
	movq	136(%r15), %r10
	movq	152(%r15), %rcx
	movq	%rcx, 832(%rsp)         # 8-byte Spill
	leal	(%r13,%r13), %ecx
	movl	%ecx, 960(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%r13d, %r12d
	sarl	$31, %r12d
	andnl	%ecx, %r12d, %ecx
	andl	%eax, %r12d
	orl	%ecx, %r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	leal	-1(%r13,%r13), %ecx
	movl	%ecx, 1008(%rsp)        # 4-byte Spill
	subl	%eax, %ecx
	cmpl	%eax, %r13d
	cmovgl	%eax, %ecx
	addl	%r8d, %ecx
	leal	-1(%r8,%r13), %ebp
	cmpl	%ecx, %ebp
	cmovlel	%ebp, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	movl	%ecx, 800(%rsp)         # 4-byte Spill
	leal	(%r8,%r13), %edi
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%ebp, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 768(%rsp)         # 4-byte Spill
	leal	(%r14,%r14), %edx
	movl	%edx, 888(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r14d, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %r9d
	movl	%edx, %esi
	andl	%eax, %ecx
	movl	$2, %eax
	subl	%ebx, %eax
	cltd
	idivl	%esi
	orl	%r9d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %r11d
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	leal	-1(%rbx,%r14), %esi
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movl	%edx, 752(%rsp)         # 4-byte Spill
	leal	(%rbx,%r14), %r9d
	cmpl	$3, %r9d
	movl	$2, %eax
	cmovll	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	$3, %r9d
	cmovll	%edx, %eax
	movl	%eax, 736(%rsp)         # 4-byte Spill
	movl	%r8d, %eax
	negl	%eax
	cltd
	idivl	960(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1008(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r13d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	cmpl	%edx, %ebp
	cmovlel	%ebp, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 704(%rsp)         # 4-byte Spill
	testl	%edi, %edi
	movl	$0, %eax
	cmovlel	%ebp, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	testl	%edi, %edi
	cmovlel	%edx, %eax
	movl	%eax, 688(%rsp)         # 4-byte Spill
	movl	%ebx, %eax
	negl	%eax
	cltd
	idivl	888(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movl	%edx, 672(%rsp)         # 4-byte Spill
	testl	%r9d, %r9d
	movl	$0, %eax
	cmovlel	%esi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	testl	%r9d, %r9d
	cmovlel	%edx, %eax
	movl	%eax, 976(%rsp)         # 4-byte Spill
	movl	$1, %eax
	subl	%r8d, %eax
	cltd
	idivl	960(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	1008(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r13d
	movl	$0, %r13d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	cmpl	%edx, %ebp
	cmovlel	%ebp, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 1008(%rsp)        # 4-byte Spill
	cmpl	$1, %edi
	setg	%al
	cmpl	$2, %edi
	cmovgel	%r13d, %ebp
	movzbl	%al, %eax
	orl	%eax, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	cmpl	$2, %edi
	cmovll	%edx, %ebp
	movl	$1, %eax
	subl	%ebx, %eax
	cltd
	idivl	888(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %r11d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	cmpl	$1, %r9d
	setg	%al
	cmpl	$2, %r9d
	cmovgel	%r13d, %esi
	movzbl	%al, %eax
	orl	%eax, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	cmpl	$2, %r9d
	vmovss	20(%r15), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	12(%r15), %xmm14        # xmm14 = mem[0],zero,zero,zero
	vmovss	16(%r15), %xmm4         # xmm4 = mem[0],zero,zero,zero
	cmovll	%r11d, %esi
	movl	%esi, 960(%rsp)         # 4-byte Spill
	movq	576(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %eax
	movq	656(%rsp), %rsi         # 8-byte Reload
	subl	%esi, %eax
	movl	40(%r15), %ecx
	movq	%rcx, 584(%rsp)         # 8-byte Spill
	movq	640(%rsp), %r14         # 8-byte Reload
	leal	(%r14,%r14), %ecx
	cltd
	idivl	%ecx
	movl	%r14d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %r9d
	negl	%ecx
	andl	%eax, %ecx
	orl	%r9d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movq	896(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rdx
	sarq	$63, %rdx
	movq	%rdx, 888(%rsp)         # 8-byte Spill
	movl	%ecx, %r9d
	sarl	$31, %r9d
	andl	%ecx, %r9d
	movq	%r9, 552(%rsp)          # 8-byte Spill
	movl	%edi, %r12d
	andl	$1, %r12d
	movl	%r12d, 28(%rsp)         # 4-byte Spill
	leal	-1(%r14,%r14), %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	leal	(%rsi,%r14), %eax
	leal	-1(%rsi,%r14), %ecx
	addl	%esi, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%edi, %eax
	cmovlel	%edx, %ecx
	cmpl	%edi, %esi
	cmovgl	%edx, %ecx
	movq	912(%rsp), %rax         # 8-byte Reload
	imull	%eax, %esi
	movq	%rsi, %rdi
	movq	928(%rsp), %rax         # 8-byte Reload
	movq	720(%rsp), %rdx         # 8-byte Reload
	leal	6(%rdx,%rax), %eax
	vmovd	%eax, %xmm5
	addl	%r8d, %edi
	movq	584(%rsp), %rax         # 8-byte Reload
	sarl	$5, %eax
	movq	%rax, 640(%rsp)         # 8-byte Spill
	cmpl	$1, %ebx
	movl	960(%rsp), %esi         # 4-byte Reload
	cmovgl	%r11d, %esi
	movl	%esi, 960(%rsp)         # 4-byte Spill
	vmovss	.LCPI152_0(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm15, %xmm1, %xmm6
	vmulss	%xmm7, %xmm6, %xmm3
	vdivss	%xmm0, %xmm3, %xmm3
	vaddss	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm3, 656(%rsp)        # 16-byte Spill
	vsubss	%xmm7, %xmm4, %xmm4
	vmulss	%xmm4, %xmm6, %xmm4
	vdivss	%xmm4, %xmm0, %xmm15
	movl	1004(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm10
	imull	%edx, %eax
	addl	%ebx, %eax
	movl	%eax, 1004(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm4
	vmovd	%esi, %xmm7
	vpsubd	%xmm4, %xmm7, %xmm7
	vsubss	%xmm8, %xmm1, %xmm3
	vmulss	%xmm12, %xmm3, %xmm4
	vdivss	%xmm13, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm6
	movq	%rdx, %rsi
	leal	6(%rsi), %eax
	vmovd	%eax, %xmm4
	movslq	%ecx, %rdx
	imulq	912(%rsp), %rdx         # 8-byte Folded Reload
	vsubss	%xmm12, %xmm2, %xmm2
	movq	928(%rsp), %rcx         # 8-byte Reload
	leal	4(%rsi,%rcx), %eax
	vmovd	%eax, %xmm12
	vmulss	%xmm2, %xmm3, %xmm2
	leal	4(%rsi), %eax
	vmovd	%eax, %xmm3
	vdivss	%xmm2, %xmm13, %xmm13
	vsubss	%xmm11, %xmm1, %xmm2
	vmulss	%xmm9, %xmm2, %xmm1
	vmovss	816(%rsp), %xmm8        # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vdivss	%xmm8, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	leal	8(%rsi,%rcx), %eax
	vmovd	%eax, %xmm11
	vsubss	%xmm9, %xmm14, %xmm9
	leal	8(%rsi), %eax
	vmovd	%eax, %xmm14
	vmulss	%xmm9, %xmm2, %xmm2
	leal	5(%rsi,%rcx), %eax
	movq	%rcx, %r11
	vmovd	%eax, %xmm9
	vpbroadcastd	%xmm5, %xmm0
	vdivss	%xmm2, %xmm8, %xmm5
	vmovdqa	.LCPI152_1(%rip), %xmm2 # xmm2 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 528(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 512(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 496(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 480(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm11, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 464(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 448(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 432(%rsp)        # 16-byte Spill
	leal	5(%rsi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 416(%rsp)        # 16-byte Spill
	leal	7(%rsi,%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	leal	7(%rsi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	leal	3(%rsi,%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	leal	3(%rsi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	cmpl	$1, %r8d
	cmovgl	1008(%rsp), %ebp        # 4-byte Folded Reload
	movslq	%edi, %rax
	movslq	%ebp, %rcx
	subq	%rax, %rcx
	addq	%rdx, %rcx
	testl	%ebx, %ebx
	movl	976(%rsp), %ebp         # 4-byte Reload
	cmovgl	672(%rsp), %ebp         # 4-byte Folded Reload
	movl	%ebp, 976(%rsp)         # 4-byte Spill
	testl	%r8d, %r8d
	movl	688(%rsp), %ebp         # 4-byte Reload
	cmovgl	704(%rsp), %ebp         # 4-byte Folded Reload
	subq	%rax, %rdx
	cmpl	$2, %ebx
	movl	736(%rsp), %r14d        # 4-byte Reload
	cmovgl	752(%rsp), %r14d        # 4-byte Folded Reload
	cmpl	$2, %r8d
	movl	768(%rsp), %edi         # 4-byte Reload
	cmovgl	800(%rsp), %edi         # 4-byte Folded Reload
	movslq	%ebp, %rax
	movslq	%edi, %rbp
	leaq	(%rax,%rdx), %r8
	leaq	(%rbp,%rdx), %rbp
	movq	640(%rsp), %rdi         # 8-byte Reload
	movslq	%edi, %rax
	shlq	$5, %rax
	addq	$48, %rax
	movq	576(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, %edx
	andl	$63, %edx
	imulq	%rax, %rdx
	movq	888(%rsp), %rax         # 8-byte Reload
	andq	896(%rsp), %rax         # 8-byte Folded Reload
	subq	%rax, %rdx
	movq	%rdx, 344(%rsp)         # 8-byte Spill
	leal	8(%rbx), %eax
	subl	44(%r15), %eax
	movq	584(%rsp), %rdx         # 8-byte Reload
	andl	$-32, %edx
	addl	$64, %edx
	imull	%eax, %edx
	movq	%rdx, 584(%rsp)         # 8-byte Spill
	movq	%rdx, %r15
	leal	(%r11,%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm2
	vmovdqa	%xmm2, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	movl	%r9d, %ebx
	subl	%esi, %ebx
	leal	-1(%rsi,%r11), %eax
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, (%rsp)           # 16-byte Spill
	vmovd	%r11d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	movl	1004(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	movl	960(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm15, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	656(%rsp), %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vmovdqa	%xmm0, 928(%rsp)        # 16-byte Spill
	movl	976(%rsp), %eax         # 4-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, -16(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, -32(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, -48(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, -64(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, -80(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, -96(%rsp)        # 16-byte Spill
	leal	(%rdi,%rdi,2), %eax
	movl	%eax, %edx
	shll	$10, %edx
	leal	(%rdx,%r15), %edx
	movq	%rdx, 200(%rsp)         # 8-byte Spill
	shll	$9, %eax
	leal	(%rax,%r15), %eax
	movq	%rax, 192(%rsp)         # 8-byte Spill
	leal	-3(%rbx), %eax
	movq	%rax, 184(%rsp)         # 8-byte Spill
	leal	-7(%rbx), %eax
	movq	%rax, 176(%rsp)         # 8-byte Spill
	leal	-8(%rbx), %eax
	movq	%rax, 168(%rsp)         # 8-byte Spill
	leal	-4(%rbx), %eax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	leal	-6(%rbx), %eax
	movq	%rax, 152(%rsp)         # 8-byte Spill
	addl	$-5, %ebx
	movq	%rbx, 296(%rsp)         # 8-byte Spill
	leal	-3(%r9), %eax
	movq	%rax, 144(%rsp)         # 8-byte Spill
	leal	-7(%r9), %eax
	movq	%rax, 136(%rsp)         # 8-byte Spill
	leal	-5(%r9), %eax
	movq	%rax, 128(%rsp)         # 8-byte Spill
	leal	-8(%r9), %eax
	movq	%rax, 120(%rsp)         # 8-byte Spill
	leal	-4(%r9), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	leal	-6(%r9), %eax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	movq	832(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, -112(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbp,4), %xmm0
	vmovaps	%xmm0, -128(%rsp)       # 16-byte Spill
	vpabsd	%xmm2, %xmm0
	vmovdqa	%xmm0, 64(%rsp)         # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 48(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI152_3(%rip), %xmm0
	vmovaps	%xmm0, 1008(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI152_4(%rip), %xmm0
	vmovaps	%xmm0, 32(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI152_5(%rip), %xmm0
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI152_6(%rip), %xmm0
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB152_2:                              # %for dh.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	testl	%r12d, %r12d
	setne	960(%rsp)               # 1-byte Folded Spill
	sete	888(%rsp)               # 1-byte Folded Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 1004(%rsp)        # 4-byte Spill
	sete	832(%rsp)               # 1-byte Folded Spill
	movq	296(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI152_2(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	320(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 800(%rsp)         # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r15d
	cltd
	idivl	%r15d
	movl	%edx, 768(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 704(%rsp)         # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, 752(%rsp)         # 4-byte Spill
	movq	152(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, 816(%rsp)         # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebp
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, 736(%rsp)         # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%ecx, %ebx
	movl	%edx, %esi
	vmovd	%ebp, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebp
	vpinsrd	$1, 816(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 736(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, %r11d, %xmm0, %xmm0
	movq	168(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r11d
	movl	%edx, %ecx
	movq	104(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vmovd	%r14d, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %esi, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r8d
	movl	%edx, %ebx
	vpinsrd	$3, %ebp, %xmm3, %xmm3
	vmovd	%edi, %xmm4
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	movl	%r9d, %ebp
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	64(%rsp), %xmm13        # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r12d, %xmm1
	movl	%r12d, %r9d
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	528(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	512(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm6, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vmovdqa	272(%rsp), %xmm9        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm7
	vmovdqa	48(%rsp), %xmm15        # 16-byte Reload
	vpsubd	%xmm0, %xmm15, %xmm6
	vblendvps	%xmm7, %xmm0, %xmm6, %xmm0
	vmovdqa	(%rsp), %xmm12          # 16-byte Reload
	vpaddd	%xmm12, %xmm0, %xmm0
	vmovdqa	256(%rsp), %xmm14       # 16-byte Reload
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm0, %xmm2, %xmm0
	vmovdqa	304(%rsp), %xmm11       # 16-byte Reload
	vpmulld	%xmm11, %xmm0, %xmm2
	vpsrad	$31, %xmm3, %xmm0
	vpand	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm3
	vpinsrd	$2, %ebx, %xmm4, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	496(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	480(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm11, %xmm3, %xmm4
	vmovdqa	%xmm4, 816(%rsp)        # 16-byte Spill
	vpsubd	912(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm2, 592(%rsp)        # 16-byte Spill
	vpaddd	240(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r10,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%r10,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r10,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 736(%rsp)        # 16-byte Spill
	vmovdqa	928(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm8
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rax
	vmovss	(%r10,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rax
	sarq	$32, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%r10,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovdqa	464(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	448(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm9, %xmm4
	vpsubd	%xmm0, %xmm15, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	movq	120(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm0, 720(%rsp)        # 16-byte Spill
	vinsertps	$48, (%r10,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 688(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%r10,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r10,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1],mem[0],xmm0[3]
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	568(%rsp), %r14         # 8-byte Reload
	vmovups	12296(%r14,%rax,4), %xmm0
	vmovups	12312(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 608(%rsp)        # 16-byte Spill
	vmovups	12304(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 672(%rsp)        # 16-byte Spill
	vmovups	12320(%r14,%rax,4), %xmm3
	vmovaps	%xmm3, 640(%rsp)        # 16-byte Spill
	sarq	$32, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vmovups	12288(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 656(%rsp)        # 16-byte Spill
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	768(%rsp), %xmm7        # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovd	%xmm3, %eax
	cltd
	idivl	%r15d
	movl	%edx, %edi
	vpinsrd	$1, 800(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$2, 704(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$3, 752(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpsrad	$31, %xmm7, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebp
	vpinsrd	$2, %esi, %xmm7, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm7
	vpand	%xmm13, %xmm7, %xmm7
	vpaddd	%xmm3, %xmm7, %xmm3
	vpcmpgtd	%xmm6, %xmm9, %xmm7
	vpsubd	%xmm6, %xmm15, %xmm4
	vblendvps	%xmm7, %xmm6, %xmm4, %xmm4
	vmovdqa	432(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	416(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vpmulld	%xmm11, %xmm4, %xmm2
	vmovdqa	%xmm2, 752(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm4
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	vpcmpgtd	%xmm3, %xmm9, %xmm4
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovdqa	400(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	384(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	movq	136(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r13), %edx
	vmovd	%edx, %xmm6
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovss	(%r10,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r10,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r10,%rax,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	vpmulld	%xmm11, %xmm3, %xmm2
	vmovdqa	%xmm2, 704(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm3
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r10,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r10,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r10,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm2, 768(%rsp)        # 16-byte Spill
	movq	184(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	vpextrd	$2, %xmm4, %eax
	vpextrd	$3, %xmm4, %edi
	cltd
	idivl	%r8d
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edx, %xmm4, %xmm4
	movl	%edi, %eax
	cltd
	idivl	%ebp
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm4, %xmm9, %xmm6
	vpsubd	%xmm4, %xmm15, %xmm9
	vblendvps	%xmm6, %xmm4, %xmm9, %xmm4
	vmovdqa	368(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	352(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm1, %xmm4, %xmm6, %xmm9
	vmovaps	80(%rsp), %xmm14        # 16-byte Reload
	vmulps	736(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm3
	vmovaps	608(%rsp), %xmm7        # 16-byte Reload
	vshufps	$136, %xmm7, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm7[0,2]
	vmovaps	208(%rsp), %xmm13       # 16-byte Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmovaps	224(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm2, %xmm12
	vmulps	688(%rsp), %xmm14, %xmm6 # 16-byte Folded Reload
	vmovaps	640(%rsp), %xmm10       # 16-byte Reload
	vmovaps	672(%rsp), %xmm8        # 16-byte Reload
	vshufps	$136, %xmm10, %xmm8, %xmm2 # xmm2 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	624(%rsp), %xmm14, %xmm6 # 16-byte Folded Reload
	vmovaps	656(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, %xmm8, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm8[0,2]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	1008(%rsp), %xmm15      # 16-byte Reload
	vminps	%xmm15, %xmm12, %xmm6
	vxorps	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm6, %xmm6
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vminps	%xmm15, %xmm5, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vsubps	%xmm6, %xmm5, %xmm5
	vshufps	$221, %xmm7, %xmm3, %xmm6 # xmm6 = xmm3[1,3],xmm7[1,3]
	vmulps	800(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	768(%rsp), %xmm14, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm0, %xmm7 # xmm7 = xmm0[1,3],xmm8[1,3]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm3, %xmm7, %xmm3
	vpmulld	%xmm11, %xmm9, %xmm9
	vpaddd	928(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	vshufps	$221, %xmm10, %xmm8, %xmm0 # xmm0 = xmm8[1,3],xmm10[1,3]
	vmovss	(%r10,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	sarq	$32, %rax
	vinsertps	$48, (%r10,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movl	28(%rsp), %r12d         # 4-byte Reload
	movl	%r12d, %eax
	movl	%r9d, %ecx
	andl	%ecx, %eax
	movl	%ecx, %r15d
	vmovaps	32(%rsp), %xmm10        # 16-byte Reload
	vandps	%xmm10, %xmm2, %xmm7
	vmovaps	%xmm7, 800(%rsp)        # 16-byte Spill
	vandps	%xmm10, %xmm5, %xmm2
	vaddps	%xmm2, %xmm7, %xmm2
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vminps	%xmm15, %xmm6, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm4, %xmm14, %xmm4
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm6
	vsubps	%xmm3, %xmm5, %xmm0
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm10, %xmm3, %xmm4
	vminps	%xmm15, %xmm6, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm10, %xmm3, %xmm5
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm2
	vmovups	%ymm2, 768(%rsp)        # 32-byte Spill
	vmovaps	%xmm2, %xmm14
	jne	.LBB152_4
# BB#3:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB152_4:                              # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	movq	584(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rcx
	vmovups	(%r14,%rax,4), %xmm6
	movq	%rax, %rdx
	vmovups	32(%r14,%rax,4), %xmm8
	orq	$2, %rax
	vmovups	(%r14,%rax,4), %xmm7
	vandps	%xmm10, %xmm0, %xmm3
	vaddps	%xmm5, %xmm4, %xmm5
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm4
	vmovdqa	912(%rsp), %xmm0        # 16-byte Reload
	vmovdqa	720(%rsp), %xmm2        # 16-byte Reload
	vpsubd	%xmm0, %xmm2, %xmm12
	orq	$4, %rdx
	vmovups	(%r14,%rdx,4), %xmm13
	vmovdqa	816(%rsp), %xmm2        # 16-byte Reload
	vpsubd	%xmm0, %xmm2, %xmm2
	movb	960(%rsp), %al          # 1-byte Reload
	andb	832(%rsp), %al          # 1-byte Folded Reload
	movb	%al, %bl
	vmovdqa	%xmm0, %xmm11
	jne	.LBB152_5
# BB#6:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 688(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, %xmm15
	vmovdqa	%xmm2, 816(%rsp)        # 16-byte Spill
	vmovdqa	752(%rsp), %xmm2        # 16-byte Reload
	vmovdqa	704(%rsp), %xmm4        # 16-byte Reload
	vmovaps	%xmm14, %xmm7
	jmp	.LBB152_7
	.align	16, 0x90
.LBB152_5:                              #   in Loop: Header=BB152_2 Depth=1
	vmovdqa	%xmm2, %xmm15
	vmovdqa	%xmm15, 816(%rsp)       # 16-byte Spill
	vmovdqa	-16(%rsp), %xmm2        # 16-byte Reload
	vmovaps	%xmm7, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, %xmm14
	vmovaps	%xmm14, 688(%rsp)       # 16-byte Spill
	vpaddd	592(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r10,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	-112(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm4, %xmm7, %xmm7 # xmm7 = xmm7[0,2],xmm4[0,2]
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	-48(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm7, %xmm7
	vmovaps	%xmm5, 832(%rsp)        # 16-byte Spill
	vmovaps	-32(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm6, %xmm7
	vpaddd	%xmm2, %xmm12, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r10,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm13, %xmm14, %xmm3 # xmm3 = xmm14[0,2],xmm13[0,2]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm2, %xmm15, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r10,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm8, %xmm13, %xmm2 # xmm2 = xmm13[0,2],xmm8[0,2]
	vmovaps	%xmm8, %xmm15
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	832(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm3, %xmm3
	vminps	%xmm0, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vaddps	%xmm2, %xmm3, %xmm2
	vminps	%xmm0, %xmm7, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vmovaps	896(%rsp), %xmm0        # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm3
	vandps	%xmm10, %xmm3, %xmm2
	vmovaps	960(%rsp), %xmm3        # 16-byte Reload
	vaddps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm7
	vmovdqa	752(%rsp), %xmm2        # 16-byte Reload
	vmovdqa	704(%rsp), %xmm4        # 16-byte Reload
.LBB152_7:                              # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmulps	%xmm1, %xmm5, %xmm0
	vmovaps	%xmm0, %xmm14
	testb	%bl, %bl
	jne	.LBB152_9
# BB#8:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB152_9:                              # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vpsubd	%xmm11, %xmm2, %xmm5
	vpsubd	%xmm11, %xmm4, %xmm1
	vpsubd	%xmm11, %xmm9, %xmm4
	movl	%r12d, %eax
	andl	%r15d, %eax
	vmovdqa	%xmm12, %xmm8
	jne	.LBB152_10
# BB#11:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovdqa	%xmm4, 720(%rsp)        # 16-byte Spill
	vmovdqa	%xmm5, 736(%rsp)        # 16-byte Spill
	vmovdqa	%xmm1, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	vmovups	%ymm0, 832(%rsp)        # 32-byte Spill
	jmp	.LBB152_12
	.align	16, 0x90
.LBB152_10:                             #   in Loop: Header=BB152_2 Depth=1
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	vmovups	%ymm0, 832(%rsp)        # 32-byte Spill
	vmovdqa	-16(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm5, %xmm2
	vmovdqa	%xmm5, 736(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rsi
	vpaddd	%xmm3, %xmm1, %xmm2
	vmovdqa	%xmm1, 752(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	vpaddd	%xmm3, %xmm4, %xmm2
	vmovdqa	%xmm4, 720(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r9
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	672(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 656(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovss	(%r10,%r8,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r10,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r10,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	-112(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	-48(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	-32(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vmovss	(%r10,%rbx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r10,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r10,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	688(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm13, %xmm0, %xmm4 # xmm4 = xmm0[1,3],xmm13[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm15[1,3]
	vmovss	(%r10,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r10,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r10,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vmovaps	1008(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm3, %xmm3
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm2
	vandps	%xmm10, %xmm2, %xmm0
	vaddps	800(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	976(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
.LBB152_12:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	movq	200(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24584(%r14,%rax,4), %xmm9
	vmovups	24600(%r14,%rax,4), %xmm13
	vmovups	24576(%r14,%rax,4), %xmm12
	vmovups	24592(%r14,%rax,4), %xmm3
	vmovups	24608(%r14,%rax,4), %xmm15
	movb	888(%rsp), %r14b        # 1-byte Reload
	movb	%r14b, %al
	movl	1004(%rsp), %ecx        # 4-byte Reload
	andb	%cl, %al
	je	.LBB152_14
# BB#13:                                #   in Loop: Header=BB152_2 Depth=1
	vmovdqa	-64(%rsp), %xmm7        # 16-byte Reload
	vpaddd	592(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	-128(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vshufps	$136, %xmm13, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm13[0,2]
	vmovaps	-96(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vmovaps	-80(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm8, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r10,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vshufps	$136, %xmm3, %xmm12, %xmm5 # xmm5 = xmm12[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vpaddd	816(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r10,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vshufps	$136, %xmm15, %xmm3, %xmm7 # xmm7 = xmm3[0,2],xmm15[0,2]
	vsubps	%xmm2, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmovaps	1008(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm5, %xmm5
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm5, %xmm5
	vminps	%xmm1, %xmm6, %xmm6
	vmaxps	%xmm2, %xmm6, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vfnmadd213ps	%xmm5, %xmm1, %xmm0
	vandps	%xmm10, %xmm0, %xmm0
	vaddps	960(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	976(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
.LBB152_14:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovups	768(%rsp), %ymm8        # 32-byte Reload
	movl	%r15d, %eax
	movq	576(%rsp), %rcx         # 8-byte Reload
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB152_16
# BB#15:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovaps	%xmm7, %xmm8
.LBB152_16:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	testl	%eax, %eax
	jne	.LBB152_18
# BB#17:                                #   in Loop: Header=BB152_2 Depth=1
	vmovdqa	-64(%rsp), %xmm5        # 16-byte Reload
	vpaddd	736(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	752(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	720(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r9
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vshufps	$221, %xmm13, %xmm9, %xmm0 # xmm0 = xmm9[1,3],xmm13[1,3]
	vmovss	(%r10,%r8,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r10,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r10,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	-128(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	-96(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	-80(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%r10,%rbx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r10,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r10,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vshufps	$221, %xmm3, %xmm12, %xmm4 # xmm4 = xmm12[1,3],xmm3[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm15, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm15[1,3]
	vmovss	(%r10,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r10,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r10,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	1008(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vmovaps	896(%rsp), %xmm2        # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	%xmm10, %xmm0, %xmm0
	vaddps	800(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	976(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
.LBB152_18:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	movl	956(%rsp), %ecx         # 4-byte Reload
	vmovups	832(%rsp), %ymm1        # 32-byte Reload
	movl	1004(%rsp), %eax        # 4-byte Reload
	andb	%al, %r14b
	jne	.LBB152_20
# BB#19:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovaps	%xmm14, %xmm1
.LBB152_20:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB152_2 Depth=1
	vmovaps	.LCPI152_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI152_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm8, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	344(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	560(%rsp), %rdx         # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r13d
	addl	$-1, %ecx
	movl	%ecx, 956(%rsp)         # 4-byte Spill
	jne	.LBB152_2
.LBB152_21:                             # %destructor_block
	xorl	%eax, %eax
	addq	$1032, %rsp             # imm = 0x408
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end152:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11, .Lfunc_end152-par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI153_0:
	.long	1                       # 0x1
.LCPI153_1:
	.long	40                      # 0x28
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	4(%rdx), %r11d
	addl	$43, %r11d
	sarl	$3, %r11d
	testl	%r11d, %r11d
	jle	.LBB153_5
# BB#1:                                 # %for f4.s0.v10.v10.preheader
	movslq	12(%rdx), %rbx
	movq	16(%rdx), %rcx
	movq	32(%rdx), %rdi
	movq	48(%rdx), %rax
	movq	%rax, -8(%rsp)          # 8-byte Spill
	movq	64(%rdx), %rax
	movq	%rax, -16(%rsp)         # 8-byte Spill
	movq	80(%rdx), %rax
	movq	%rax, -24(%rsp)         # 8-byte Spill
	movq	%rbx, %rbp
	sarq	$63, %rbp
	movl	(%rdx), %r10d
	andl	$-32, %r10d
	leal	40(%r10), %r14d
	movl	%esi, %r12d
	andl	$63, %r12d
	imull	%r12d, %r14d
	movl	%ebx, %r15d
	sarl	$31, %r15d
	andl	%ebx, %r15d
	addl	$48, %r10d
	movslq	8(%rdx), %rdx
	addq	$3, %rdx
	imulq	%rdx, %r12
	andq	%rbx, %rbp
	subq	%rbp, %r12
	addl	$60, %esi
	xorl	%r13d, %r13d
	vbroadcastss	.LCPI153_0(%rip), %ymm0
	vpbroadcastd	.LCPI153_1(%rip), %ymm1
	.align	16, 0x90
.LBB153_2:                              # %for f4.s0.v10.v10
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB153_3 Depth 2
	leal	(,%r13,8), %ebp
	vpxor	%ymm2, %ymm2, %ymm2
	movl	$9, %ebx
	movl	%esi, %edx
	.align	16, 0x90
.LBB153_3:                              # %for sum.s1.r4$y
                                        #   Parent Loop BB153_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%edx, %r8d
	andl	$63, %r8d
	imull	%r10d, %r8d
	leal	(%rbp,%r8), %eax
	movslq	%eax, %r9
	vmovups	(%rcx,%r9,4), %ymm3
	vcmpnltps	(%rdi,%r9,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$1, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$2, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$3, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$4, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$5, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$6, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	orl	$7, %r9d
	movslq	%r9d, %rax
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	leal	8(%rbp,%r8), %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	addl	$1, %edx
	addl	$-1, %ebx
	jne	.LBB153_3
# BB#4:                                 # %consume sum
                                        #   in Loop: Header=BB153_2 Depth=1
	leal	(%rbp,%r15), %eax
	addl	%r14d, %ebp
	vpcmpgtd	%ymm2, %ymm1, %ymm2
	movslq	%ebp, %rdx
	movq	-16(%rsp), %rbp         # 8-byte Reload
	vmovups	(%rbp,%rdx,4), %ymm3
	movq	-24(%rsp), %rbp         # 8-byte Reload
	vblendvps	%ymm2, (%rbp,%rdx,4), %ymm3, %ymm2
	cltq
	leaq	(%rax,%r12), %rax
	movq	-8(%rsp), %rdx          # 8-byte Reload
	vmovups	%ymm2, (%rdx,%rax,4)
	addq	$1, %r13
	cmpl	%r11d, %r13d
	jne	.LBB153_2
.LBB153_5:                              # %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end153:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11, .Lfunc_end153-par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI154_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI154_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI154_9:
	.zero	16,255
.LCPI154_10:
	.zero	16
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI154_1:
	.long	1199570688              # float 65535
.LCPI154_3:
	.long	1065353216              # float 1
.LCPI154_4:
	.long	1048576000              # float 0.25
.LCPI154_5:
	.long	1042983595              # float 0.166666672
.LCPI154_6:
	.long	1045220557              # float 0.200000003
	.section	.rodata,"a",@progbits
	.align	32
.LCPI154_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI154_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1464, %rsp             # imm = 0x5B8
	movl	24(%rdx), %r9d
	addl	$39, %r9d
	sarl	$3, %r9d
	testl	%r9d, %r9d
	jle	.LBB154_34
# BB#1:                                 # %for f7.s0.v10.v10.preheader
	vmovss	(%rdx), %xmm10          # xmm10 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm9          # xmm9 = mem[0],zero,zero,zero
	movslq	32(%rdx), %rax
	movq	%rax, 1392(%rsp)        # 8-byte Spill
	movl	36(%rdx), %r15d
	movl	40(%rdx), %eax
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	movl	44(%rdx), %eax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	movl	48(%rdx), %eax
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	movl	52(%rdx), %eax
	movl	%eax, 1408(%rsp)        # 4-byte Spill
	movl	56(%rdx), %r14d
	movl	60(%rdx), %ebx
	movq	%rbx, 1280(%rsp)        # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 1360(%rsp)        # 8-byte Spill
	movl	68(%rdx), %ebp
	movslq	72(%rdx), %rax
	movq	%rax, 1328(%rsp)        # 8-byte Spill
	vmovss	76(%rdx), %xmm2         # xmm2 = mem[0],zero,zero,zero
	movq	80(%rdx), %rax
	movq	%rax, 360(%rsp)         # 8-byte Spill
	movq	%rsi, %r10
	movq	%r10, 1032(%rsp)        # 8-byte Spill
	movl	%r10d, %ecx
	subl	%ebp, %ecx
	movq	%rcx, 1200(%rsp)        # 8-byte Spill
	movq	96(%rdx), %rax
	movq	%rax, 352(%rsp)         # 8-byte Spill
	movq	112(%rdx), %rax
	movq	%rax, 336(%rsp)         # 8-byte Spill
	movq	128(%rdx), %rax
	movq	%rax, 1024(%rsp)        # 8-byte Spill
	movq	144(%rdx), %rax
	movq	%rax, 1344(%rsp)        # 8-byte Spill
	leal	(%rbx,%rbx), %esi
	movl	%esi, 1184(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	movq	%rdx, 1376(%rsp)        # 8-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r8d
	leal	-1(%rcx), %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	leal	1(%rcx), %eax
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%ebx, %edi
	sarl	$31, %edi
	andnl	%esi, %edi, %ecx
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edi, 1264(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%r8d, %eax
	movl	%r11d, %esi
	sarl	$31, %esi
	andl	%edi, %esi
	addl	%r11d, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edi, %ecx
	addl	%edx, %ecx
	leal	-1(%rbx,%rbx), %r11d
	movl	%r11d, 1136(%rsp)       # 4-byte Spill
	movl	%r11d, %edx
	subl	%ecx, %edx
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %edx
	addl	%ebp, %edx
	leal	-1(%rbp,%rbx), %r12d
	cmpl	%edx, %r12d
	cmovlel	%r12d, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, 1232(%rsp)        # 4-byte Spill
	movl	%edx, %edi
	leal	1(%r10), %ecx
	movq	%rcx, 376(%rsp)         # 8-byte Spill
	cmpl	%ecx, %r12d
	movl	%r12d, %edx
	cmovgl	%ecx, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, %ecx
	movl	%r11d, %r13d
	subl	%esi, %r13d
	cmpl	%esi, %ebx
	cmovgl	%esi, %r13d
	addl	%ebp, %r13d
	cmpl	%r13d, %r12d
	cmovlel	%r12d, %r13d
	cmpl	%ebp, %r13d
	cmovll	%ebp, %r13d
	subl	%eax, %r11d
	cmpl	%eax, %ebx
	movq	%rbx, %rdx
	cmovgl	%eax, %r11d
	addl	%ebp, %r11d
	cmpl	%r11d, %r12d
	cmovlel	%r12d, %r11d
	cmpl	%ebp, %r11d
	cmovll	%ebp, %r11d
	movq	%r10, %rbx
	cmpl	%ebx, %r12d
	cmovlel	%edi, %ecx
	movl	%ecx, 1216(%rsp)        # 4-byte Spill
	movl	%r12d, %ecx
	cmovgl	%ebx, %ecx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	leal	(%rbp,%rdx), %eax
	cmpl	%ebx, %eax
	movl	%eax, %edx
	cmovgl	%ebx, %edx
	cmovlel	%r11d, %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	addl	$-1, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	cmpl	%r10d, %eax
	cmovll	%r13d, %edx
	movl	%edx, 1168(%rsp)        # 4-byte Spill
	movq	1360(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	leal	(%r14,%r14), %ecx
	cltd
	idivl	%ecx
	movl	%r14d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	negl	%ecx
	andl	%eax, %ecx
	orl	%edi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ecx
	movq	%rsi, %rdx
	leal	(%rdx,%r14), %eax
	leal	-1(%rdx,%r14), %r8d
	addl	%edx, %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	xorl	%esi, %esi
	testl	%eax, %eax
	cmovgl	%esi, %r8d
	cmpl	%edx, %r8d
	cmovll	%edx, %r8d
	testl	%eax, %eax
	cmovlel	%ecx, %r8d
	movq	1152(%rsp), %rsi        # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	leal	(%r15,%r15), %ebx
	cltd
	idivl	%ebx
	movl	%r15d, %eax
	sarl	$31, %eax
	andnl	%ebx, %eax, %edi
	negl	%ebx
	andl	%eax, %ebx
	orl	%edi, %ebx
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%ebx, %edi
	addl	%edx, %edi
	leal	-1(%r15,%r15), %eax
	subl	%edi, %eax
	cmpl	%edi, %r15d
	cmovgl	%edi, %eax
	movq	%rsi, %rdi
	leal	(%rdi,%r15), %edx
	leal	-1(%rdi,%r15), %r14d
	addl	%edi, %eax
	cmpl	%eax, %r14d
	cmovlel	%r14d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%edx, %edx
	movl	$0, %esi
	cmovgl	%esi, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	movq	%rdi, %r15
	testl	%edx, %edx
	cmovlel	%eax, %r14d
	movq	1392(%rsp), %rdx        # 8-byte Reload
	movq	%rdx, %rdi
	sarq	$63, %rdi
	movq	%rdi, 1312(%rsp)        # 8-byte Spill
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%edx, %edi
	movq	%rdi, 344(%rsp)         # 8-byte Spill
	movl	%r10d, %edx
	andl	$1, %edx
	movl	%edx, 332(%rsp)         # 4-byte Spill
	movl	%r10d, %edx
	andl	$63, %edx
	movq	%rdx, 1296(%rsp)        # 8-byte Spill
	testl	%r15d, %r15d
	cmovgl	%eax, %r14d
	movl	1408(%rsp), %ebx        # 4-byte Reload
	movq	1440(%rsp), %rax        # 8-byte Reload
	imull	%eax, %ebx
	addl	%r15d, %ebx
	movq	1360(%rsp), %r10        # 8-byte Reload
	testl	%r10d, %r10d
	cmovgl	%ecx, %r8d
	movq	1376(%rsp), %rax        # 8-byte Reload
	vmovss	4(%rax), %xmm11         # xmm11 = mem[0],zero,zero,zero
	movl	20(%rax), %ecx
	movq	%rcx, 368(%rsp)         # 8-byte Spill
	movl	16(%rax), %ecx
	movl	%ecx, 1152(%rsp)        # 4-byte Spill
	movl	12(%rax), %r15d
	movq	1200(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx), %eax
	cltd
	movl	1184(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, %esi
	movq	%rcx, %rax
	addl	$-2, %eax
	cltd
	idivl	%edi
	movl	1408(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm15
	movq	1424(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx), %eax
	vmovd	%eax, %xmm1
	vmovd	%ebx, %xmm4
	vmovd	%ebx, %xmm0
	movq	1440(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%rcx), %eax
	vmovd	%eax, %xmm6
	leal	-1(%rbx,%rcx), %eax
	vmovd	%eax, %xmm12
	vmovd	%r14d, %xmm7
	vmovd	%r14d, %xmm14
	leal	-2(%rbx,%rcx), %eax
	movq	%rbx, %rcx
	vmovd	%eax, %xmm13
	movq	1328(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %eax
	imull	%ebp, %eax
	addl	%r10d, %eax
	movl	%esi, %r10d
	sarl	$31, %r10d
	movl	1264(%rsp), %edi        # 4-byte Reload
	andl	%edi, %r10d
	addl	%esi, %r10d
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	leal	-2(%rcx), %edx
	vmovd	%edx, %xmm8
	andl	$-32, %r15d
	movl	%r15d, 1408(%rsp)       # 4-byte Spill
	leal	-1(%rbp), %edx
	movq	1032(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %edx
	movl	1216(%rsp), %edi        # 4-byte Reload
	cmovgl	1232(%rsp), %edi        # 4-byte Folded Reload
	movslq	%eax, %rdx
	movslq	%r8d, %r8
	subq	%rdx, %r8
	movslq	%edi, %rax
	movq	%r14, %rdx
	imulq	%rdx, %rax
	movq	%rax, 1360(%rsp)        # 8-byte Spill
	cmpl	%ecx, %ebp
	movl	1168(%rsp), %esi        # 4-byte Reload
	cmovgel	%r13d, %esi
	movl	1248(%rsp), %eax        # 4-byte Reload
	cmovgl	%r11d, %eax
	movslq	%esi, %r14
	imulq	%rdx, %r14
	movq	%rdx, %r11
	movq	1424(%rsp), %rdx        # 8-byte Reload
	movq	1440(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi,%rdx), %esi
	vmovd	%esi, %xmm5
	movl	1136(%rsp), %r15d       # 4-byte Reload
	movl	%r15d, %esi
	subl	%ebx, %esi
	movq	1280(%rsp), %r13        # 8-byte Reload
	cmpl	%ebx, %r13d
	cmovgl	%ebx, %esi
	addl	%ebp, %esi
	cmpl	%esi, %r12d
	cmovlel	%r12d, %esi
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	leal	-2(%rcx), %edx
	cmpl	%edx, %r12d
	movl	%r12d, %edi
	cmovgl	%edx, %edi
	cmpl	%ebp, %edi
	cmovll	%ebp, %edi
	leal	2(%rbp,%r13), %ebx
	cmpl	%ecx, %ebx
	cmovlel	%esi, %edi
	leal	2(%rbp), %ebx
	cmpl	%ecx, %ebx
	cmovgl	%esi, %edi
	movslq	%edi, %rsi
	movq	%r11, %rbx
	imulq	%rbx, %rsi
	movq	%rsi, 1264(%rsp)        # 8-byte Spill
	movslq	%eax, %r11
	imulq	%rbx, %r11
	movq	%rbx, %rsi
	movl	%r15d, %eax
	subl	%r10d, %eax
	cmpl	%r10d, %r13d
	cmovgl	%r10d, %eax
	leal	-2(%rbp,%r13), %r10d
	addl	%ebp, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	movl	%eax, %edi
	movq	%rcx, %rax
	leal	2(%rax), %ebx
	cmpl	%ebx, %r12d
	cmovgl	%ebx, %r12d
	cmpl	%ebp, %r12d
	cmovll	%ebp, %r12d
	cmpl	%eax, %r10d
	leal	-2(%rbp), %ecx
	cmovlel	%edi, %r12d
	cmpl	%eax, %ecx
	cmovgl	%edi, %r12d
	movslq	%r12d, %rcx
	imulq	%rsi, %rcx
	vpsubd	%xmm4, %xmm7, %xmm3
	vmovss	.LCPI154_1(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm10, %xmm4, %xmm4
	vmulss	%xmm9, %xmm4, %xmm7
	vdivss	%xmm2, %xmm7, %xmm7
	vaddss	%xmm7, %xmm10, %xmm10
	movq	368(%rsp), %rsi         # 8-byte Reload
	addl	$3, %esi
	andl	$63, %ebx
	imull	%esi, %ebx
	movq	%rbx, 312(%rsp)         # 8-byte Spill
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 320(%rsp)         # 8-byte Spill
	leal	63(%rax), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 304(%rsp)         # 8-byte Spill
	movq	376(%rsp), %rdx         # 8-byte Reload
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 376(%rsp)         # 8-byte Spill
	movq	1440(%rsp), %r10        # 8-byte Reload
	leal	2(%r10), %ebx
	vmovd	%ebx, %xmm7
	movq	1296(%rsp), %rdx        # 8-byte Reload
	imull	%edx, %esi
	movq	%rsi, 368(%rsp)         # 8-byte Spill
	movq	1376(%rsp), %rsi        # 8-byte Reload
	movslq	28(%rsi), %rbx
	addq	$32, %rbx
	movq	%rdx, %r13
	imulq	%rbx, %r13
	movq	1360(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%r8), %r15
	leaq	(%r14,%r8), %rdx
	movq	%rdx, 1360(%rsp)        # 8-byte Spill
	leal	10(%rax), %ebx
	movl	1152(%rsp), %ebp        # 4-byte Reload
	subl	%ebp, %ebx
	movl	1408(%rsp), %edx        # 4-byte Reload
	addl	$64, %edx
	imull	%edx, %ebx
	movq	%rbx, 288(%rsp)         # 8-byte Spill
	leaq	(%r11,%r8), %r14
	movq	1264(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%r8), %r11
	movq	%rax, %rdi
	leal	8(%rdi), %esi
	subl	%ebp, %esi
	imull	%edx, %esi
	movq	%rsi, 280(%rsp)         # 8-byte Spill
	leal	6(%rdi), %esi
	subl	%ebp, %esi
	imull	%edx, %esi
	movq	%rsi, 272(%rsp)         # 8-byte Spill
	leaq	(%rcx,%r8), %r12
	movq	1312(%rsp), %rax        # 8-byte Reload
	andq	1392(%rsp), %rax        # 8-byte Folded Reload
	leal	7(%rdi), %ecx
	subl	%ebp, %ecx
	imull	%edx, %ecx
	movq	%rcx, 264(%rsp)         # 8-byte Spill
	leal	9(%rdi), %ecx
	subl	%ebp, %ecx
	imull	%edx, %ecx
	movq	%rcx, 256(%rsp)         # 8-byte Spill
	subq	%rax, %r13
	movq	%r13, 296(%rsp)         # 8-byte Spill
	vsubss	%xmm9, %xmm11, %xmm9
	movq	1424(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm11
	leal	1(%r10,%rax), %ecx
	leal	-3(%r10,%rax), %ebx
	vmulss	%xmm9, %xmm4, %xmm9
	movq	344(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ebp
	subl	%r10d, %ebp
	movq	%rbp, 248(%rsp)         # 8-byte Spill
	vmovd	%r10d, %xmm4
	leal	1(%r10), %edx
	leal	-1(%r10), %edi
	leal	-3(%r10), %esi
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm15, %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm0
	vdivss	%xmm9, %xmm2, %xmm2
	vmovdqa	.LCPI154_0(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 192(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm15
	vmovdqa	%xmm15, -96(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm11, %xmm4
	vmovaps	%xmm4, 1296(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm4
	vmovdqa	%xmm4, 1008(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 160(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 144(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vmovdqa	%xmm0, 1280(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 128(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 112(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 96(%rsp)         # 16-byte Spill
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 80(%rsp)         # 16-byte Spill
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 64(%rsp)         # 16-byte Spill
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 48(%rsp)         # 16-byte Spill
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 32(%rsp)         # 16-byte Spill
	vpaddd	%xmm6, %xmm15, %xmm0
	vmovdqa	%xmm0, 16(%rsp)         # 16-byte Spill
	vpaddd	%xmm6, %xmm4, %xmm0
	vmovdqa	%xmm0, (%rsp)           # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	movq	%rax, %rdx
	leal	3(%rdx), %ecx
	movq	%rcx, -8(%rsp)          # 8-byte Spill
	leal	1(%rdx), %ecx
	movq	%rcx, -16(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %ecx
	movq	%rcx, -24(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, -32(%rsp)         # 8-byte Spill
	leal	2(%rdx), %ecx
	movq	%rcx, -40(%rsp)         # 8-byte Spill
	leal	3(%rbp), %ecx
	movq	%rcx, -48(%rsp)         # 8-byte Spill
	leal	1(%rbp), %ecx
	movq	%rcx, -56(%rsp)         # 8-byte Spill
	leal	-1(%rbp), %ecx
	movq	%rcx, -64(%rsp)         # 8-byte Spill
	leal	-2(%rbp), %ecx
	movq	%rcx, -72(%rsp)         # 8-byte Spill
	leal	2(%rbp), %ecx
	movq	%rcx, -80(%rsp)         # 8-byte Spill
	movq	1344(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rcx,%r15,4), %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	movl	$0, %ebp
	movq	352(%rsp), %rbx         # 8-byte Reload
	movq	360(%rsp), %r15         # 8-byte Reload
	movq	1360(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r14,4), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r11,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r12,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 992(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 976(%rsp)        # 16-byte Spill
	vmovdqa	.LCPI154_2(%rip), %xmm12 # xmm12 = [0,2,4,6]
	vbroadcastss	.LCPI154_3(%rip), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI154_4(%rip), %xmm0
	vmovaps	%xmm0, -112(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI154_5(%rip), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI154_6(%rip), %xmm0
	vmovaps	%xmm0, -128(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB154_2:                              # %for f7.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	%rbp, 928(%rsp)         # 8-byte Spill
	movl	%r9d, 940(%rsp)         # 4-byte Spill
	movq	248(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm12, %xmm6
	vpaddd	%xmm6, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	224(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r9d
	movl	%r9d, 916(%rsp)         # 4-byte Spill
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %ecx
	movl	%ecx, 712(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %r10d
	movl	%edx, %edi
	movq	344(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %esi
	movl	%esi, 924(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r14d
	movl	%r14d, 716(%rsp)        # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r12d
	movl	%r12d, 920(%rsp)        # 4-byte Spill
	cltd
	idivl	%r12d
	vpinsrd	$1, %r8d, %xmm1, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	992(%rsp), %xmm14       # 16-byte Reload
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm5
	vmovdqa	%xmm5, 720(%rsp)        # 16-byte Spill
	vmovdqa	192(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	16(%rsp), %xmm2         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1296(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm4
	vmovdqa	976(%rsp), %xmm10       # 16-byte Reload
	vpsubd	%xmm0, %xmm10, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vmovdqa	1008(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vpaddd	%xmm6, %xmm5, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1312(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm7
	vpsubd	208(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	176(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1024(%rsp), %r11        # 8-byte Reload
	vmovss	(%r11,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r11,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 880(%rsp)        # 16-byte Spill
	movq	-80(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r12d
	vpinsrd	$1, %r8d, %xmm1, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	160(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpxor	%xmm8, %xmm1, %xmm1
	vmovdqa	144(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm4, %xmm3
	vmovdqa	%xmm4, %xmm8
	vpsubd	%xmm0, %xmm10, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	-40(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm15, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	1280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r11,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r11,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	movq	-64(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r12d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	96(%rsp), %xmm0         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm0, %xmm0
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vpxor	%xmm7, %xmm0, %xmm0
	vmovdqa	80(%rsp), %xmm2         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm3
	vpor	%xmm0, %xmm3, %xmm0
	vpcmpgtd	%xmm1, %xmm8, %xmm3
	vpsubd	%xmm1, %xmm10, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	movq	-24(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	movq	-56(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm6, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vmovd	%xmm4, %eax
	cltd
	idivl	%r10d
	movl	%r10d, %r11d
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm3, %xmm3
	vblendvps	%xmm0, %xmm1, %xmm3, %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r12d
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	(%rsp), %xmm1           # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm1
	vpxor	%xmm7, %xmm1, %xmm1
	vmovdqa	64(%rsp), %xmm2         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vmovdqa	%xmm1, 832(%rsp)        # 16-byte Spill
	vpcmpgtd	%xmm0, %xmm8, %xmm1
	vpsubd	%xmm0, %xmm10, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm2
	movq	376(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	movslq	%eax, %rcx
	movq	%rcx, 864(%rsp)         # 8-byte Spill
	vmovups	8(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	movslq	%eax, %r13
	vmovups	32(%r15,%r13,4), %xmm0
	vmovaps	%xmm0, 1136(%rsp)       # 16-byte Spill
	vmovups	48(%r15,%r13,4), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	movq	304(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	movslq	%eax, %rdi
	movq	%rdi, 848(%rsp)         # 8-byte Spill
	vmovups	8(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	movq	264(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	movslq	%eax, %rsi
	vmovups	32(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vmovups	48(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	movq	-48(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm6, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	vmovups	40(%r15,%r13,4), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%rdi,4), %xmm9
	vmovaps	%xmm9, 1168(%rsp)       # 16-byte Spill
	vmovups	40(%r15,%rsi,4), %xmm8
	vmovaps	%xmm8, 1184(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rdi,4), %xmm13
	vmovaps	%xmm13, 512(%rsp)       # 16-byte Spill
	vmovups	56(%r15,%rsi,4), %xmm12
	vmovaps	%xmm12, 496(%rsp)       # 16-byte Spill
	vmovups	32(%rbx,%rcx,4), %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovups	56(%r15,%r13,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	idivl	%r9d
	movl	%edx, %r8d
	movq	-16(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %r10d
	movl	%r10d, 896(%rsp)        # 4-byte Spill
	vmovd	%r10d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vmovd	%xmm4, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpaddd	%xmm6, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpmaxsd	%xmm15, %xmm7, %xmm7
	vmovaps	832(%rsp), %xmm0        # 16-byte Reload
	vblendvps	%xmm0, %xmm2, %xmm7, %xmm7
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r12d
	vmovd	%edi, %xmm4
	vpinsrd	$1, %r8d, %xmm4, %xmm4
	vpinsrd	$2, %ecx, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm2
	vpand	%xmm14, %xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	1296(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm0, %xmm4
	vpsubd	%xmm2, %xmm10, %xmm3
	vblendvps	%xmm4, %xmm2, %xmm3, %xmm2
	vmovdqa	48(%rsp), %xmm1         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm3
	vpxor	.LCPI154_9(%rip), %xmm3, %xmm3
	vmovdqa	32(%rsp), %xmm1         # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vmovdqa	1312(%rsp), %xmm0       # 16-byte Reload
	vpmulld	1328(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	movq	-8(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm2, %xmm1, %xmm15
	vmovdqa	1280(%rsp), %xmm1       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm2
	vpextrq	$1, %xmm2, %r12
	movq	%rbp, %rcx
	vmovq	%xmm2, %r9
	vpmulld	%xmm0, %xmm7, %xmm2
	vmovdqa	%xmm0, %xmm14
	vpaddd	%xmm2, %xmm1, %xmm2
	vmovdqa	%xmm1, %xmm0
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %r15
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1200(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[0,2],mem[0,2]
	vmovaps	1440(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	1376(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmovaps	1264(%rsp), %xmm7       # 16-byte Reload
	vmovaps	880(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	1424(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm2, %xmm2
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1104(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm3, %xmm2, %xmm11
	vshufps	$136, %xmm12, %xmm8, %xmm2 # xmm2 = xmm8[0,2],xmm12[0,2]
	vmovaps	%xmm1, %xmm8
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm10, %xmm2
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vshufps	$136, %xmm13, %xmm9, %xmm3 # xmm3 = xmm9[0,2],xmm13[0,2]
	vsubps	%xmm3, %xmm2, %xmm12
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1120(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm10, %xmm2
	vmovaps	960(%rsp), %xmm9        # 16-byte Reload
	vmulps	%xmm9, %xmm8, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1072(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, 1152(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm7[0,2],mem[0,2]
	vsubps	%xmm3, %xmm2, %xmm13
	vmovaps	1392(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1040(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm9, %xmm1, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1056(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vsubps	%xmm3, %xmm2, %xmm2
	vpmulld	%xmm14, %xmm15, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm1
	vpextrq	$1, %xmm1, %r14
	movq	%r14, 832(%rsp)         # 8-byte Spill
	vmovq	%xmm1, %rdi
	movq	%rdi, 800(%rsp)         # 8-byte Spill
	movq	%r9, %r8
	sarq	$32, %r8
	movq	%r12, %rbx
	sarq	$32, %rbx
	movq	%r15, %rbp
	sarq	$32, %rbp
	movq	%r11, %rdx
	sarq	$32, %rdx
	orq	$6, %r13
	orq	$6, %rsi
	movl	%r10d, %eax
	andl	$1, %eax
	movq	%rdi, %r10
	sarq	$32, %r10
	sarq	$32, %r14
	testl	%eax, %eax
	movq	280(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	cltq
	movq	368(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 768(%rsp)         # 4-byte Spill
	movq	320(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 816(%rsp)         # 4-byte Spill
	movq	272(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 744(%rsp)         # 4-byte Spill
	movq	312(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 784(%rsp)         # 4-byte Spill
	movq	288(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	jne	.LBB154_3
# BB#4:                                 # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm2, 416(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 432(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 448(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 464(%rsp)       # 16-byte Spill
	vpxor	%xmm1, %xmm1, %xmm1
	jmp	.LBB154_5
	.align	16, 0x90
.LBB154_3:                              #   in Loop: Header=BB154_2 Depth=1
	vaddps	%xmm2, %xmm12, %xmm1
	vmovaps	%xmm2, 416(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 448(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm13, %xmm1
	vmovaps	%xmm13, 432(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm11, %xmm1
	vmovaps	%xmm11, 464(%rsp)       # 16-byte Spill
	vmulps	-112(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
.LBB154_5:                              # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovdqa	%xmm1, 1328(%rsp)       # 16-byte Spill
	vmovaps	1232(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1136(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1120(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1104(%rsp), %xmm13      # 16-byte Reload
	movslq	%r9d, %rcx
	movq	1024(%rsp), %r9         # 8-byte Reload
	vmovss	(%r9,%rcx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%r12d, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rbx,4), %xmm1, %xmm12 # xmm12 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm12, 1072(%rsp)      # 16-byte Spill
	movslq	%r15d, %rcx
	vmovss	(%r9,%rcx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%r11d, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 656(%rsp)        # 16-byte Spill
	movq	360(%rsp), %r15         # 8-byte Reload
	vmovups	(%r15,%r13,4), %xmm3
	vmovaps	%xmm3, 560(%rsp)        # 16-byte Spill
	vmovups	(%r15,%rsi,4), %xmm4
	vmovaps	%xmm4, 544(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm7, %xmm1 # xmm1 = xmm7[1,3],xmm2[1,3]
	vmovaps	%xmm1, 752(%rsp)        # 16-byte Spill
	vmulps	%xmm9, %xmm0, %xmm1
	vshufps	$221, %xmm6, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm6[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm6
	vmulps	%xmm2, %xmm1, %xmm10
	vshufps	$221, 1344(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm13[1,3],mem[1,3]
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	vmovaps	1264(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm2
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm11
	vmulps	%xmm9, %xmm12, %xmm2
	vshufps	$221, 1392(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm2, %xmm13
	vmulps	%xmm7, %xmm12, %xmm2
	vshufps	$221, 1184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm12
	vmovups	32(%r15,%rax,4), %xmm1
	vmovaps	%xmm1, 1200(%rsp)       # 16-byte Spill
	vmovups	48(%r15,%rax,4), %xmm3
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	movslq	%edi, %r11
	vmovups	32(%r15,%r11,4), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vmulps	%xmm14, %xmm8, %xmm2
	vshufps	$136, %xmm3, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm3[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm15
	vmovups	48(%r15,%r11,4), %xmm1
	vmovaps	%xmm1, 1136(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	1248(%rsp), %xmm8, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm1
	movslq	744(%rsp), %r13         # 4-byte Folded Reload
	vmulps	1088(%rsp), %xmm8, %xmm2 # 16-byte Folded Reload
	vmovups	32(%r15,%r13,4), %xmm0
	vmovaps	%xmm0, 624(%rsp)        # 16-byte Spill
	vmovups	48(%r15,%r13,4), %xmm3
	vmovaps	%xmm3, 608(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm2, %xmm0
	vmovups	40(%r15,%rax,4), %xmm2
	vmovaps	%xmm2, 1344(%rsp)       # 16-byte Spill
	vmovups	56(%r15,%rax,4), %xmm3
	vmovaps	%xmm3, 480(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm3[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	1360(%rsp), %xmm14, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm3
	movq	800(%rsp), %rcx         # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r9,%rcx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	832(%rsp), %rcx         # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 640(%rsp)        # 16-byte Spill
	movq	%r9, %r10
	movslq	768(%rsp), %rcx         # 4-byte Folded Reload
	vmovaps	1424(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm10, %xmm2
	vmaxps	.LCPI154_10(%rip), %xmm2, %xmm10
	vminps	%xmm6, %xmm11, %xmm4
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm4, %xmm2
	vmovaps	%xmm2, 832(%rsp)        # 16-byte Spill
	vminps	%xmm6, %xmm13, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm2
	vmovaps	%xmm2, 768(%rsp)        # 16-byte Spill
	vminps	%xmm6, %xmm12, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm2
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	movslq	816(%rsp), %rdx         # 4-byte Folded Reload
	movq	%rdx, 744(%rsp)         # 8-byte Spill
	movslq	784(%rsp), %r12         # 4-byte Folded Reload
	vminps	%xmm6, %xmm1, %xmm11
	vminps	%xmm6, %xmm0, %xmm7
	vminps	%xmm6, %xmm3, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vminps	%xmm6, %xmm15, %xmm0
	vmovaps	%xmm0, 784(%rsp)        # 16-byte Spill
	movl	332(%rsp), %r9d         # 4-byte Reload
	testl	%r9d, %r9d
	movq	352(%rsp), %rbx         # 8-byte Reload
	movq	864(%rsp), %rsi         # 8-byte Reload
	vmovups	(%rbx,%rsi,4), %xmm1
	vmovaps	%xmm1, 528(%rsp)        # 16-byte Spill
	movq	848(%rsp), %rsi         # 8-byte Reload
	vmovups	(%rbx,%rsi,4), %xmm6
	vmovups	8(%rbx,%rcx,4), %xmm4
	vmovups	24(%rbx,%rcx,4), %xmm15
	vshufps	$221, 1408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vshufps	$221, 1168(%rsp), %xmm6, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm6[1,3],mem[1,3]
	vmovups	16(%rbx,%rcx,4), %xmm14
	vmovups	8(%rbx,%rdx,4), %xmm0
	vmovups	24(%rbx,%rdx,4), %xmm12
	vmovups	8(%rbx,%r12,4), %xmm2
	vmovups	24(%rbx,%r12,4), %xmm5
	vmovups	32(%rbx,%rcx,4), %xmm13
	vmovups	16(%rbx,%r12,4), %xmm3
	vmovaps	%xmm3, 672(%rsp)        # 16-byte Spill
	vmovups	40(%r15,%r11,4), %xmm3
	vmovaps	%xmm3, 688(%rsp)        # 16-byte Spill
	vmovups	16(%rbx,%rdx,4), %xmm3
	vmovaps	%xmm3, 848(%rsp)        # 16-byte Spill
	vmovups	40(%r15,%r13,4), %xmm3
	vmovaps	%xmm3, 864(%rsp)        # 16-byte Spill
	je	.LBB154_7
# BB#6:                                 # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	%xmm3, 1328(%rsp)       # 16-byte Spill
.LBB154_7:                              # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vsubps	752(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
	vshufps	$136, %xmm15, %xmm4, %xmm3 # xmm3 = xmm4[0,2],xmm15[0,2]
	vmovaps	%xmm15, 880(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1104(%rsp)       # 16-byte Spill
	vmovaps	832(%rsp), %xmm4        # 16-byte Reload
	vsubps	1120(%rsp), %xmm4, %xmm15 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm5[0,2]
	vmovaps	%xmm4, 576(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1120(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm11, %xmm4
	vmovaps	%xmm4, 752(%rsp)        # 16-byte Spill
	vmovaps	768(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vshufps	$136, %xmm12, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm12[0,2]
	vmovaps	%xmm1, 768(%rsp)        # 16-byte Spill
	vmaxps	%xmm9, %xmm7, %xmm1
	vmovaps	%xmm1, 592(%rsp)        # 16-byte Spill
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm5
	vshufps	$136, %xmm13, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm13[0,2]
	vmovaps	816(%rsp), %xmm7        # 16-byte Reload
	vmaxps	%xmm9, %xmm7, %xmm8
	vmovaps	784(%rsp), %xmm7        # 16-byte Reload
	vmaxps	%xmm9, %xmm7, %xmm9
	movl	896(%rsp), %esi         # 4-byte Reload
	movl	%esi, %edx
	movq	1032(%rsp), %rdi        # 8-byte Reload
	orl	%edi, %edx
	testb	$1, %dl
	movq	928(%rsp), %rbp         # 8-byte Reload
	movl	924(%rsp), %r14d        # 4-byte Reload
	je	.LBB154_8
# BB#9:                                 # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm12, 800(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 384(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 496(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 512(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 784(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1040(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, 400(%rsp)        # 16-byte Spill
	jmp	.LBB154_10
	.align	16, 0x90
.LBB154_8:                              #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm12, 800(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 384(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 400(%rsp)        # 16-byte Spill
	vxorps	%xmm13, %xmm13, %xmm13
	vmovaps	640(%rsp), %xmm7        # 16-byte Reload
	vmulps	960(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm1, %xmm12
	vmovaps	1392(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1040(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1440(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	1376(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
                                        # xmm11 = xmm1[1,3],mem[1,3]
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vsubps	%xmm11, %xmm0, %xmm11
	vmulps	1264(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm3, %xmm0
	vmovaps	1184(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 496(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm7, %xmm3, %xmm2
	vmovaps	1168(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 512(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vminps	%xmm1, %xmm2, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	%xmm5, %xmm15, %xmm3
	vmovaps	%xmm5, 496(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 784(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm0, %xmm3
	vaddps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm4, 512(%rsp)        # 16-byte Spill
	vaddps	%xmm1, %xmm10, %xmm1
	vmovaps	%xmm10, 1040(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm11, %xmm0
	vmovaps	%xmm12, %xmm1
	vmulps	944(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
.LBB154_10:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	1232(%rsp), %xmm11      # 16-byte Reload
	movl	712(%rsp), %edi         # 4-byte Reload
	vmovaps	%xmm14, %xmm13
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vmovaps	752(%rsp), %xmm2        # 16-byte Reload
	vmovaps	592(%rsp), %xmm4        # 16-byte Reload
	vmovaps	576(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm2, %xmm14
	vsubps	%xmm0, %xmm4, %xmm7
	vsubps	%xmm1, %xmm8, %xmm15
	vsubps	%xmm3, %xmm9, %xmm1
	orq	$6, %rax
	testl	%esi, %r9d
	jne	.LBB154_11
# BB#12:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm1, 576(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 592(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 1360(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, 1056(%rsp)       # 16-byte Spill
	vmovaps	1376(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1088(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1440(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1424(%rsp), %xmm6       # 16-byte Reload
	vxorps	%xmm10, %xmm10, %xmm10
	jmp	.LBB154_13
	.align	16, 0x90
.LBB154_11:                             #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm3, 1056(%rsp)       # 16-byte Spill
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vmulps	1248(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm1, %xmm3
	vmovaps	%xmm3, 576(%rsp)        # 16-byte Spill
	vmovaps	688(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 56(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1440(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm1, %xmm1
	vmovaps	1376(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	672(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 32(%rbx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1424(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm10, %xmm10, %xmm10
	vsubps	%xmm1, %xmm0, %xmm12
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm1
	vmovaps	864(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 56(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm9, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	movq	744(%rsp), %rdx         # 8-byte Reload
	vmovaps	848(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 32(%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm10, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm3, %xmm7, %xmm2
	vmovaps	%xmm7, 752(%rsp)        # 16-byte Spill
	vaddps	%xmm14, %xmm2, %xmm2
	vmovaps	%xmm14, 1360(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm15, %xmm1
	vmovaps	%xmm15, 592(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm8
	vaddps	%xmm1, %xmm12, %xmm0
	vmulps	944(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
.LBB154_13:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	1344(%rsp), %xmm12      # 16-byte Reload
	vmovaps	656(%rsp), %xmm3        # 16-byte Reload
	vmovaps	624(%rsp), %xmm5        # 16-byte Reload
	vmovaps	608(%rsp), %xmm2        # 16-byte Reload
	vmulps	1072(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovups	(%r15,%rax,4), %xmm7
	vshufps	$221, %xmm12, %xmm7, %xmm1 # xmm1 = xmm7[1,3],xmm12[1,3]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovups	(%rbx,%rcx,4), %xmm1
	vmovaps	%xmm1, 768(%rsp)        # 16-byte Spill
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vshufps	$221, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm13[1,3]
	vsubps	%xmm1, %xmm0, %xmm15
	vmovaps	816(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 800(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm2, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm2[1,3]
	vmulps	%xmm8, %xmm3, %xmm2
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm10, %xmm1, %xmm1
	vmovaps	%xmm4, %xmm5
	vsubps	%xmm0, %xmm1, %xmm14
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 880(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm3, %xmm1
	vsubps	%xmm9, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	832(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 1120(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1136(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	1248(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm9, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm8
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm3
	andl	$1, %esi
	je	.LBB154_14
# BB#15:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm13, %xmm1
	vmovaps	%xmm14, 1200(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 896(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1344(%rsp)      # 16-byte Spill
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
	jmp	.LBB154_16
	.align	16, 0x90
.LBB154_14:                             #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm7, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1216(%rsp)       # 16-byte Spill
	vshufps	$221, 384(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm13[1,3],mem[1,3]
	vmovaps	%xmm13, %xmm1
	vmulps	640(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
	vshufps	$221, 480(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm12[1,3],mem[1,3]
	vmovaps	%xmm12, 1344(%rsp)      # 16-byte Spill
	vsubps	%xmm9, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm8, %xmm3, %xmm2
	vaddps	%xmm2, %xmm14, %xmm2
	vmovaps	%xmm14, 1200(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vmovaps	%xmm15, 896(%rsp)       # 16-byte Spill
	vmulps	-128(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB154_16:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovdqa	-96(%rsp), %xmm15       # 16-byte Reload
	movl	716(%rsp), %ecx         # 4-byte Reload
	vmovaps	1040(%rsp), %xmm2       # 16-byte Reload
	vmovaps	784(%rsp), %xmm4        # 16-byte Reload
	testl	%r9d, %r9d
	jne	.LBB154_18
# BB#17:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
.LBB154_18:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	movl	%r14d, %eax
	andl	$1, %eax
	vmovaps	%xmm5, %xmm14
	jne	.LBB154_19
# BB#20:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, %xmm13
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	.LBB154_21
	.align	16, 0x90
.LBB154_19:                             #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, %xmm13
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	vaddps	%xmm4, %xmm2, %xmm2
	vaddps	512(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	496(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	-112(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
.LBB154_21:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovdqa	720(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm3, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1152(%rsp)       # 16-byte Spill
	vxorps	%xmm10, %xmm10, %xmm10
	vmovaps	%xmm6, %xmm11
	vmovaps	%xmm9, 1440(%rsp)       # 16-byte Spill
	testl	%r9d, %r9d
	je	.LBB154_23
# BB#22:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
.LBB154_23:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	movq	-72(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vmovdqa	.LCPI154_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	916(%rsp)               # 4-byte Folded Reload
	movl	%edx, %r8d
	vmovd	%xmm4, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	920(%rsp)               # 4-byte Folded Reload
	vmovd	%edi, %xmm4
	vpinsrd	$1, %r8d, %xmm4, %xmm4
	vpinsrd	$2, %ecx, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	992(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm4
	vmovdqa	128(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm5
	vpxor	.LCPI154_9(%rip), %xmm5, %xmm5
	vmovdqa	112(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovdqa	1296(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm6
	vmovdqa	976(%rsp), %xmm1        # 16-byte Reload
	vpsubd	%xmm4, %xmm1, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vmovdqa	1008(%rsp), %xmm1       # 16-byte Reload
	vpminsd	%xmm1, %xmm4, %xmm4
	vpmaxsd	%xmm15, %xmm4, %xmm4
	movq	-32(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm6
	vmovdqa	%xmm3, %xmm12
	vpminsd	%xmm1, %xmm6, %xmm6
	vpmaxsd	%xmm15, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	vpmulld	1312(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	1280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmovq	%xmm4, %rax
	movslq	%eax, %rcx
	vmovss	(%r10,%rcx,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm4, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0],mem[0],xmm5[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r10,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	sarq	$32, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm4, %xmm5 # xmm5 = xmm4[0,1,2],mem[0]
	vmovups	(%rbx,%r12,4), %xmm6
	movl	%r14d, %eax
	movq	1032(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	je	.LBB154_24
# BB#25:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm11, %xmm7
	jmp	.LBB154_26
	.align	16, 0x90
.LBB154_24:                             #   in Loop: Header=BB154_2 Depth=1
	vmovaps	528(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1408(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	560(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1392(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[0,2],mem[0,2]
	vmulps	960(%rsp), %xmm5, %xmm7 # 16-byte Folded Reload
	vmovaps	1440(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm14, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vminps	%xmm11, %xmm4, %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm2
	vmovaps	400(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1168(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[0,2],mem[0,2]
	vmovaps	544(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1184(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vmulps	1264(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm7, %xmm7
	vmulps	%xmm7, %xmm14, %xmm7
	vmulps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm11, %xmm7
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm0, %xmm1, %xmm1
	vxorps	%xmm10, %xmm10, %xmm10
	vsubps	%xmm4, %xmm1, %xmm1
	vaddps	464(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	448(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	432(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	416(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	944(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
.LBB154_26:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	1360(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	testl	%r14d, %r9d
	je	.LBB154_28
# BB#27:                                #   in Loop: Header=BB154_2 Depth=1
	vshufps	$221, 672(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[1,3],mem[1,3]
	orq	$6, %r11
	vmovups	(%r15,%r11,4), %xmm2
	vshufps	$221, 688(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1072(%rsp), %xmm6       # 16-byte Reload
	vmulps	1248(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vmovaps	1440(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vminps	%xmm7, %xmm2, %xmm2
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	movq	744(%rsp), %rax         # 8-byte Reload
	vmovups	(%rbx,%rax,4), %xmm2
	vshufps	$221, 848(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	orq	$6, %r13
	vmovups	(%r15,%r13,4), %xmm4
	vshufps	$221, 864(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	1088(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm14, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vxorps	%xmm10, %xmm10, %xmm10
	vsubps	%xmm2, %xmm4, %xmm2
	vaddps	896(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	1136(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	1152(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	944(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
.LBB154_28:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	movl	%r14d, %eax
	andl	$1, %eax
	je	.LBB154_29
# BB#30:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm2, %xmm1
	jmp	.LBB154_31
	.align	16, 0x90
.LBB154_29:                             #   in Loop: Header=BB154_2 Depth=1
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1120(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vmulps	%xmm13, %xmm5, %xmm3
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[0,2],mem[0,2]
	vsubps	1440(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm14, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	576(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vaddps	752(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	592(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm1, %xmm1
	vmulps	-128(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
.LBB154_31:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1216(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1328(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm7, 1424(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1232(%rsp)      # 16-byte Spill
	vmovaps	%xmm14, 1376(%rsp)      # 16-byte Spill
	testl	%r9d, %r9d
	jne	.LBB154_33
# BB#32:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vmovaps	%xmm2, %xmm1
.LBB154_33:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB154_2 Depth=1
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	%xmm4, %xmm3, %xmm0
	vmovaps	.LCPI154_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI154_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r14d, %rax
	movq	296(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	336(%rsp), %rcx         # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %ebp
	movl	940(%rsp), %r9d         # 4-byte Reload
	addl	$-1, %r9d
	jne	.LBB154_2
.LBB154_34:                             # %destructor_block
	xorl	%eax, %eax
	addq	$1464, %rsp             # imm = 0x5B8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end154:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11, .Lfunc_end154-par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI155_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI155_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI155_9:
	.zero	16,255
.LCPI155_10:
	.zero	16
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI155_1:
	.long	1199570688              # float 65535
.LCPI155_3:
	.long	1065353216              # float 1
.LCPI155_4:
	.long	1045220557              # float 0.200000003
.LCPI155_5:
	.long	1042983595              # float 0.166666672
.LCPI155_6:
	.long	1048576000              # float 0.25
	.section	.rodata,"a",@progbits
	.align	32
.LCPI155_7:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
.LCPI155_8:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11: # @par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1496, %rsp             # imm = 0x5D8
	movl	%esi, %r15d
	movl	28(%rdx), %eax
	addl	$39, %eax
	sarl	$3, %eax
	movl	%eax, 1012(%rsp)        # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB155_34
# BB#1:                                 # %for f8.s0.v10.v10.preheader
	vmovss	(%rdx), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm9          # xmm9 = mem[0],zero,zero,zero
	movslq	32(%rdx), %rax
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	movl	36(%rdx), %eax
	movq	%rax, 1312(%rsp)        # 8-byte Spill
	movl	40(%rdx), %eax
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	movl	44(%rdx), %eax
	movq	%rax, 1184(%rsp)        # 8-byte Spill
	movl	48(%rdx), %eax
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	movl	52(%rdx), %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	56(%rdx), %r12d
	movl	60(%rdx), %edi
	movq	%rdi, 1232(%rsp)        # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	movl	68(%rdx), %ebx
	movslq	72(%rdx), %rax
	movq	%rax, 1344(%rsp)        # 8-byte Spill
	vmovss	76(%rdx), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	80(%rdx), %rax
	movq	%rax, 360(%rsp)         # 8-byte Spill
	movq	%r15, 1016(%rsp)        # 8-byte Spill
	movl	%r15d, %ecx
	subl	%ebx, %ecx
	movq	%rcx, 1424(%rsp)        # 8-byte Spill
	movq	96(%rdx), %rax
	movq	%rax, 352(%rsp)         # 8-byte Spill
	movq	112(%rdx), %rax
	movq	%rax, 336(%rsp)         # 8-byte Spill
	movq	128(%rdx), %r8
	movq	144(%rdx), %rax
	movq	%rax, 1376(%rsp)        # 8-byte Spill
	leal	1(%rcx), %eax
	leal	(%rdi,%rdi), %esi
	movl	%esi, 1408(%rsp)        # 4-byte Spill
	movq	%rdx, 1392(%rsp)        # 8-byte Spill
	cltd
	idivl	%esi
	movl	%edx, %r11d
	leal	-1(%rcx), %eax
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%edi, %r13d
	sarl	$31, %r13d
	andnl	%esi, %r13d, %ecx
	andl	%eax, %r13d
	orl	%ecx, %r13d
	movl	%r13d, 1200(%rsp)       # 4-byte Spill
	movl	%r11d, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%r11d, %eax
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r13d, %ecx
	addl	%edx, %ecx
	leal	-1(%rdi,%rdi), %r14d
	movl	%r14d, 1248(%rsp)       # 4-byte Spill
	movl	%r14d, %r9d
	subl	%ecx, %r9d
	cmpl	%ecx, %edi
	cmovgl	%ecx, %r9d
	addl	%ebx, %r9d
	leal	-1(%rbx,%rdi), %r10d
	cmpl	%r9d, %r10d
	cmovlel	%r10d, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	movl	%r9d, 1280(%rsp)        # 4-byte Spill
	leal	(%rbx,%rdi), %ebp
	cmpl	%r15d, %ebp
	movl	%ebp, %ecx
	cmovgl	%r15d, %ecx
	addl	$-1, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, %edx
	movl	%r14d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r10d
	cmovlel	%r10d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, 1360(%rsp)        # 4-byte Spill
	leal	1(%r15), %eax
	movq	%rax, 376(%rsp)         # 8-byte Spill
	cmpl	%eax, %r10d
	movl	%r10d, %esi
	cmovgl	%eax, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	cmpl	%r15d, %r10d
	movl	%r10d, %r11d
	cmovgl	%r15d, %r11d
	cmovlel	%ecx, %esi
	movl	%esi, 1328(%rsp)        # 4-byte Spill
	cmpl	%r15d, %ebp
	cmovll	%r9d, %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	movq	1424(%rsp), %rax        # 8-byte Reload
	cltd
	idivl	1408(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	movl	%r14d, %r13d
	subl	%eax, %r13d
	cmpl	%eax, %edi
	cmovgl	%eax, %r13d
	addl	%ebx, %r13d
	cmpl	%r13d, %r10d
	cmovlel	%r10d, %r13d
	cmpl	%ebx, %r13d
	cmovll	%ebx, %r13d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	cmpl	%r15d, %ebp
	cmovlel	%r13d, %r11d
	movl	%r11d, 1168(%rsp)       # 4-byte Spill
	movl	$2, %eax
	movq	1152(%rsp), %r11        # 8-byte Reload
	subl	%r11d, %eax
	leal	(%r12,%r12), %esi
	cltd
	idivl	%esi
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%esi, %eax, %ebp
	negl	%esi
	andl	%eax, %esi
	orl	%ebp, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r12,%r12), %ebp
	subl	%eax, %ebp
	cmpl	%eax, %r12d
	cmovgl	%eax, %ebp
	leal	(%r11,%r12), %eax
	leal	-1(%r11,%r12), %ecx
	addl	%r11d, %ebp
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	cmpl	%r11d, %ebp
	cmovll	%r11d, %ebp
	cmpl	$3, %eax
	movl	$2, %r12d
	cmovgel	%r12d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	cmpl	$3, %eax
	cmovll	%ebp, %ecx
	movl	$2, %eax
	movq	1184(%rsp), %r9         # 8-byte Reload
	subl	%r9d, %eax
	movq	1312(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%rdi), %esi
	cltd
	idivl	%esi
	movl	%edi, %eax
	movq	%rdi, %r14
	sarl	$31, %eax
	andnl	%esi, %eax, %edi
	negl	%esi
	andl	%eax, %esi
	orl	%edi, %esi
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movq	%r14, %rsi
	leal	-1(%rsi,%rsi), %eax
	subl	%edi, %eax
	cmpl	%edi, %esi
	cmovgl	%edi, %eax
	movq	%r9, %rdi
	leal	(%rdi,%rsi), %edx
	leal	-1(%rdi,%rsi), %esi
	addl	%edi, %eax
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %edx
	cmovll	%esi, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	cmpl	$3, %edx
	cmovll	%eax, %r12d
	movq	1440(%rsp), %rsi        # 8-byte Reload
	movq	%rsi, %rdx
	sarq	$63, %rdx
	movq	%rdx, 1312(%rsp)        # 8-byte Spill
	movl	%esi, %edx
	sarl	$31, %edx
	andl	%esi, %edx
	movq	%rdx, 344(%rsp)         # 8-byte Spill
	movl	%r15d, %edx
	andl	$1, %edx
	movl	%edx, 332(%rsp)         # 4-byte Spill
	movl	%r15d, %edx
	andl	$63, %edx
	movq	%rdx, 1296(%rsp)        # 8-byte Spill
	cmpl	$2, %r9d
	cmovgl	%eax, %r12d
	movl	1216(%rsp), %edi        # 4-byte Reload
	movl	%edi, %r15d
	movq	1472(%rsp), %rax        # 8-byte Reload
	imull	%eax, %r15d
	addl	%r9d, %r15d
	cmpl	$2, %r11d
	cmovgl	%ebp, %ecx
	movq	1392(%rsp), %rax        # 8-byte Reload
	vmovss	4(%rax), %xmm11         # xmm11 = mem[0],zero,zero,zero
	movl	20(%rax), %edx
	movq	%rdx, 368(%rsp)         # 8-byte Spill
	movl	16(%rax), %edx
	movl	%edx, 1184(%rsp)        # 4-byte Spill
	movl	12(%rax), %r14d
	movq	1424(%rsp), %rbp        # 8-byte Reload
	leal	-2(%rbp), %eax
	cltd
	movl	1408(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, %esi
	movq	%rbp, %rax
	addl	$2, %eax
	cltd
	idivl	%r9d
	vmovd	%edi, %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	movq	1456(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%rdi), %eax
	vmovd	%eax, %xmm5
	vmovd	%r15d, %xmm0
	vmovd	%r15d, %xmm3
	movq	1472(%rsp), %rbp        # 8-byte Reload
	leal	-1(%rbp,%rdi), %eax
	vmovd	%eax, %xmm2
	leal	-1(%rbp), %eax
	vmovd	%eax, %xmm14
	vmovd	%r12d, %xmm7
	vmovd	%r12d, %xmm6
	vmovaps	%xmm6, 1408(%rsp)       # 16-byte Spill
	leal	1(%rbp,%rdi), %eax
	movq	%rdi, %r9
	vmovd	%eax, %xmm15
	movq	1344(%rsp), %r15        # 8-byte Reload
	movl	%r15d, %eax
	imull	%ebx, %eax
	addl	%r11d, %eax
	leal	1(%rbp), %edi
	vmovd	%edi, %xmm12
	leal	2(%rbp,%r9), %edi
	vmovd	%edi, %xmm13
	movl	%esi, %ebp
	sarl	$31, %ebp
	movl	1200(%rsp), %r12d       # 4-byte Reload
	andl	%r12d, %ebp
	addl	%esi, %ebp
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r12d, %edi
	addl	%edx, %edi
	andl	$-32, %r14d
	movl	%r14d, %edx
	shll	$5, %edx
	movq	%rdx, 1216(%rsp)        # 8-byte Spill
	movq	1016(%rsp), %rsi        # 8-byte Reload
	cmpl	%esi, %ebx
	movl	1168(%rsp), %edx        # 4-byte Reload
	cmovgl	%r13d, %edx
	movl	1264(%rsp), %r12d       # 4-byte Reload
	cmovgel	1280(%rsp), %r12d       # 4-byte Folded Reload
	cltq
	movslq	%ecx, %r9
	subq	%rax, %r9
	movslq	%edx, %rax
	imulq	%r15, %rax
	movq	%rax, 1280(%rsp)        # 8-byte Spill
	movl	1248(%rsp), %r11d       # 4-byte Reload
	movl	%r11d, %edx
	subl	%edi, %edx
	movq	1232(%rsp), %rax        # 8-byte Reload
	cmpl	%edi, %eax
	cmovgl	%edi, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r10d
	cmovlel	%r10d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movq	%rsi, %r13
	leal	2(%r13), %ecx
	cmpl	%ecx, %r10d
	movl	%r10d, %edi
	cmovgl	%ecx, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	leal	-2(%rbx,%rax), %esi
	cmpl	%r13d, %esi
	cmovlel	%edx, %edi
	leal	-2(%rbx), %esi
	cmpl	%r13d, %esi
	cmovgl	%edx, %edi
	vpsubd	%xmm0, %xmm7, %xmm0
	vmovss	.LCPI155_1(%rip), %xmm7 # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm7, %xmm10
	vmulss	%xmm9, %xmm10, %xmm7
	vdivss	%xmm4, %xmm7, %xmm7
	vaddss	%xmm7, %xmm1, %xmm8
	movslq	%edi, %rdx
	imulq	%r15, %rdx
	movq	1472(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi), %esi
	vmovd	%esi, %xmm6
	movl	%r11d, %edi
	subl	%ebp, %edi
	cmpl	%ebp, %eax
	cmovgl	%ebp, %edi
	leal	2(%rbx,%rax), %esi
	addl	%ebx, %edi
	cmpl	%edi, %r10d
	cmovlel	%r10d, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	leal	-2(%r13), %ebp
	cmpl	%ebp, %r10d
	cmovgl	%ebp, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	cmpl	%r13d, %esi
	cmovlel	%edi, %r10d
	leal	2(%rbx), %esi
	cmpl	%r13d, %esi
	cmovgl	%edi, %r10d
	movslq	%r10d, %r11
	imulq	%r15, %r11
	movslq	%r12d, %rdi
	imulq	%r15, %rdi
	addl	$-1, %ebx
	cmpl	%r13d, %ebx
	movl	1328(%rsp), %ebx        # 4-byte Reload
	cmovgl	1360(%rsp), %ebx        # 4-byte Folded Reload
	movslq	%ebx, %rbx
	imulq	%r15, %rbx
	movq	1280(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r9), %r10
	leaq	(%rdx,%r9), %r12
	leaq	(%r11,%r9), %r11
	movq	368(%rsp), %rdx         # 8-byte Reload
	addl	$3, %edx
	movq	376(%rsp), %rax         # 8-byte Reload
	andl	$63, %eax
	imull	%edx, %eax
	movq	%rdx, %rsi
	movq	%rax, 376(%rsp)         # 8-byte Spill
	leaq	(%rdi,%r9), %r15
	leal	9(%r13), %eax
	movl	1184(%rsp), %edi        # 4-byte Reload
	subl	%edi, %eax
	addl	$64, %r14d
	imull	%r14d, %eax
	leal	63(%r13), %edx
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 296(%rsp)         # 8-byte Spill
	andl	$63, %ebp
	imull	%esi, %ebp
	movq	%rbp, 312(%rsp)         # 8-byte Spill
	andl	$63, %ecx
	imull	%esi, %ecx
	movq	%rcx, 320(%rsp)         # 8-byte Spill
	addq	%r9, %rbx
	movq	1296(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %esi
	movq	%rsi, 368(%rsp)         # 8-byte Spill
	movq	1392(%rsp), %rdx        # 8-byte Reload
	movslq	24(%rdx), %rdx
	addq	$32, %rdx
	movq	%rcx, %r9
	leal	7(%r13), %esi
	subl	%edi, %esi
	imull	%r14d, %esi
	leal	6(%r13), %ebp
	subl	%edi, %ebp
	imull	%r14d, %ebp
	imulq	%rdx, %r9
	leal	10(%r13), %edx
	subl	%edi, %edx
	imull	%r14d, %edx
	leal	8(%r13), %ecx
	subl	%edi, %ecx
	imull	%r14d, %ecx
	movq	1312(%rsp), %rdi        # 8-byte Reload
	andq	1440(%rsp), %rdi        # 8-byte Folded Reload
	subq	%rdi, %r9
	movq	%r9, 288(%rsp)          # 8-byte Spill
	movq	1216(%rsp), %rdi        # 8-byte Reload
	leal	(%rdi,%rdi,2), %r9d
	addl	%r9d, %eax
	movq	%rax, 304(%rsp)         # 8-byte Spill
	addl	%r9d, %esi
	movq	%rsi, 280(%rsp)         # 8-byte Spill
	addl	%r9d, %ebp
	movq	%rbp, 272(%rsp)         # 8-byte Spill
	addl	%r9d, %edx
	movq	%rdx, 264(%rsp)         # 8-byte Spill
	addl	%r9d, %ecx
	movq	%rcx, 256(%rsp)         # 8-byte Spill
	vsubss	%xmm9, %xmm11, %xmm9
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovd	%eax, %xmm11
	movq	1472(%rsp), %rbp        # 8-byte Reload
	leal	(%rbp,%rax), %ecx
	leal	-2(%rbp,%rax), %edx
	leal	-3(%rbp,%rax), %r9d
	vmulss	%xmm9, %xmm10, %xmm9
	movq	344(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %esi
	subl	%ebp, %esi
	movq	%rsi, 248(%rsp)         # 8-byte Spill
	vmovd	%ebp, %xmm10
	leal	-2(%rbp), %eax
	leal	-3(%rbp), %ebp
	vpbroadcastd	%xmm5, %xmm1
	vmovdqa	%xmm1, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 208(%rsp)        # 16-byte Spill
	vbroadcastss	1424(%rsp), %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 1264(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm7
	vmovdqa	%xmm7, 992(%rsp)        # 16-byte Spill
	vdivss	%xmm9, %xmm4, %xmm9
	vmovdqa	.LCPI155_0(%rip), %xmm3 # xmm3 = [0,4294967294,4294967292,4294967290]
	vpbroadcastd	%xmm14, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm5
	vmovdqa	%xmm5, 192(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm2
	vmovdqa	%xmm2, 176(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm5
	vmovaps	%xmm5, 976(%rsp)        # 16-byte Spill
	vbroadcastss	1408(%rsp), %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm5, 160(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm5
	vmovdqa	%xmm5, 144(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm5
	vmovdqa	%xmm5, 128(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 1248(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 112(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 96(%rsp)         # 16-byte Spill
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 80(%rsp)         # 16-byte Spill
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 64(%rsp)         # 16-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 48(%rsp)         # 16-byte Spill
	vmovd	%r9d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 32(%rsp)         # 16-byte Spill
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 16(%rsp)         # 16-byte Spill
	vpaddd	%xmm3, %xmm7, %xmm0
	vmovdqa	%xmm0, (%rsp)           # 16-byte Spill
	vpaddd	%xmm3, %xmm2, %xmm0
	vmovdqa	%xmm0, -16(%rsp)        # 16-byte Spill
	vmovdqa	%xmm2, %xmm10
	vbroadcastss	%xmm9, %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	movq	%rdi, %rcx
	leal	3(%rcx), %eax
	movq	%rax, -24(%rsp)         # 8-byte Spill
	leal	2(%rcx), %eax
	movq	%rax, -32(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %eax
	movq	%rax, -40(%rsp)         # 8-byte Spill
	leal	-1(%rcx), %eax
	movq	%rax, -48(%rsp)         # 8-byte Spill
	leal	1(%rcx), %eax
	movq	%rax, -56(%rsp)         # 8-byte Spill
	leal	3(%rsi), %eax
	movq	%rax, -64(%rsp)         # 8-byte Spill
	leal	2(%rsi), %eax
	movq	%rax, -72(%rsp)         # 8-byte Spill
	leal	-2(%rsi), %eax
	movq	%rax, -80(%rsp)         # 8-byte Spill
	leal	-1(%rsi), %eax
	movq	%rax, -88(%rsp)         # 8-byte Spill
	leal	1(%rsi), %eax
	movq	%rax, -96(%rsp)         # 8-byte Spill
	xorl	%r9d, %r9d
	movq	1376(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 576(%rsp)        # 16-byte Spill
	movq	360(%rsp), %r10         # 8-byte Reload
	vbroadcastss	(%rax,%r12,4), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	movq	352(%rsp), %r11         # 8-byte Reload
	vbroadcastss	(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 960(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 944(%rsp)        # 16-byte Spill
	vmovdqa	.LCPI155_2(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vbroadcastss	.LCPI155_3(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI155_4(%rip), %xmm0
	vmovaps	%xmm0, -112(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI155_5(%rip), %xmm0
	vmovaps	%xmm0, 928(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI155_6(%rip), %xmm0
	vmovaps	%xmm0, -128(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB155_2:                              # %for f8.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %r15
	vmovups	8(%r11,%r15,4), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%r15,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	movq	-96(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm13, %xmm15
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	224(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r12d
	movl	%r12d, 780(%rsp)        # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %ebx
	movl	%ebx, 912(%rsp)         # 4-byte Spill
	cltd
	idivl	%ebx
	movq	344(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %esi
	movl	%esi, 924(%rsp)         # 4-byte Spill
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r13d
	movl	%r13d, 916(%rsp)        # 4-byte Spill
	cltd
	idivl	%r13d
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r14d
	movl	%r14d, 920(%rsp)        # 4-byte Spill
	cltd
	idivl	%r14d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	960(%rsp), %xmm9        # 16-byte Reload
	vpand	%xmm9, %xmm1, %xmm1
	vmovdqa	%xmm9, %xmm14
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm12
	vmovdqa	%xmm12, 784(%rsp)       # 16-byte Spill
	vmovdqa	(%rsp), %xmm1           # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm13, %xmm13, %xmm13
	vmovdqa	192(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vmovdqa	976(%rsp), %xmm9        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm3
	vmovdqa	%xmm9, %xmm6
	vmovdqa	944(%rsp), %xmm8        # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	%xmm10, %xmm5
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	992(%rsp), %xmm11       # 16-byte Reload
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm5, %xmm0, %xmm0
	movq	-56(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movl	%eax, 1184(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm15, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm5, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vmovdqa	1264(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm7
	vpsubd	208(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	160(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r8,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r8,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r8,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	movq	-88(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm14, %xmm1, %xmm1
	vmovdqa	%xmm14, %xmm9
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	144(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm1
	vpxor	%xmm13, %xmm1, %xmm1
	vmovdqa	128(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm2, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm0, %xmm6, %xmm3
	vmovdqa	%xmm6, %xmm2
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm5, %xmm0, %xmm0
	movq	-48(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm15, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm5, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r8,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r8,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r8,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	movq	248(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 1440(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 880(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, 864(%rsp)         # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 848(%rsp)         # 4-byte Spill
	movq	-72(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 832(%rsp)         # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 816(%rsp)         # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebp
	movq	-64(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm4
	movq	256(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %rcx
	vmovups	24608(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	movq	320(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	movq	%rax, 1136(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	movq	264(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	movq	%rax, 1144(%rsp)        # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	movq	312(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	movq	%rax, 896(%rsp)         # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movq	272(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	movq	%rax, 904(%rsp)         # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	vmovups	16(%r11,%r15,4), %xmm13
	vmovaps	%xmm13, 1360(%rsp)      # 16-byte Spill
	vmovups	32(%r11,%r15,4), %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vmovups	24616(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmovups	24632(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vmovups	(%r11,%r15,4), %xmm14
	vmovaps	%xmm14, 544(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rcx,4), %xmm10
	vmovaps	%xmm10, 560(%rsp)       # 16-byte Spill
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vmovd	880(%rsp), %xmm6        # 4-byte Folded Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vpinsrd	$1, 1440(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vpinsrd	$2, 864(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vpinsrd	$3, 848(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vpsrad	$31, %xmm6, %xmm7
	vmovdqa	%xmm9, %xmm3
	vpand	%xmm3, %xmm7, %xmm7
	vpaddd	%xmm6, %xmm7, %xmm6
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovdqa	%xmm2, %xmm1
	vpcmpgtd	%xmm6, %xmm1, %xmm7
	vpsubd	%xmm6, %xmm8, %xmm2
	vblendvps	%xmm7, %xmm6, %xmm2, %xmm2
	vmovdqa	80(%rsp), %xmm0         # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm0, %xmm6
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vpxor	%xmm9, %xmm6, %xmm6
	vmovdqa	-16(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm0, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm5, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm5, %xmm2, %xmm2
	vpaddd	%xmm15, %xmm12, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm5, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm2, %xmm7, %xmm2
	vpextrd	$3, %xmm4, %eax
	vmovd	816(%rsp), %xmm4        # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 832(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, %esi, %xmm4, %xmm4
	vpinsrd	$3, %ebp, %xmm4, %xmm4
	cltd
	idivl	%r14d
	vpsrad	$31, %xmm4, %xmm6
	vpand	%xmm3, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm4, %xmm1, %xmm6
	vpsubd	%xmm4, %xmm8, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vmovdqa	64(%rsp), %xmm0         # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm0, %xmm6
	vpxor	%xmm9, %xmm6, %xmm6
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	48(%rsp), %xmm0         # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm0, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm5, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm5, %xmm4, %xmm4
	movq	-32(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm15, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm5, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vmovd	%edi, %xmm6
	vpinsrd	$1, %ecx, %xmm6, %xmm6
	vpinsrd	$2, %ebx, %xmm6, %xmm6
	vpinsrd	$3, %edx, %xmm6, %xmm6
	vpsrad	$31, %xmm6, %xmm7
	vpand	%xmm3, %xmm7, %xmm7
	vpaddd	%xmm6, %xmm7, %xmm6
	vpcmpgtd	%xmm6, %xmm1, %xmm7
	vpsubd	%xmm6, %xmm8, %xmm0
	vblendvps	%xmm7, %xmm6, %xmm0, %xmm0
	vmovdqa	32(%rsp), %xmm6         # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm6, %xmm6
	vpxor	%xmm9, %xmm6, %xmm6
	vmovdqa	16(%rsp), %xmm7         # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm7, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm5, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm5, %xmm0, %xmm0
	movq	-24(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm15, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vmovaps	1376(%rsp), %xmm11      # 16-byte Reload
	vpmaxsd	%xmm5, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm0, %xmm7, %xmm0
	vmovdqa	1264(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm2, %xmm2
	vmovdqa	1248(%rsp), %xmm8       # 16-byte Reload
	vpaddd	%xmm2, %xmm8, %xmm2
	vpextrq	$1, %xmm2, %r13
	vpmulld	%xmm9, %xmm4, %xmm4
	vmovq	%xmm2, %r10
	vpaddd	%xmm4, %xmm8, %xmm2
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %r15
	vshufps	$221, %xmm11, %xmm10, %xmm2 # xmm2 = xmm10[1,3],xmm11[1,3]
	vmovaps	1408(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	1424(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	576(%rsp), %xmm3        # 16-byte Reload
	vmulps	1392(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1472(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm2, %xmm2
	vxorps	%xmm10, %xmm10, %xmm10
	vmaxps	%xmm10, %xmm2, %xmm2
	vshufps	$221, %xmm13, %xmm14, %xmm4 # xmm4 = xmm14[1,3],xmm13[1,3]
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 752(%rsp)        # 16-byte Spill
	vmovaps	1328(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	1456(%rsp), %xmm6       # 16-byte Reload
	vmulps	1216(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vmovaps	1168(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1280(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vsubps	%xmm4, %xmm2, %xmm15
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1072(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm3, %xmm6, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1088(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1104(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	1232(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm1, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vmovaps	1312(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 1296(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm13
	vmovaps	1040(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1120(%rsp), %xmm12      # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm8, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r8,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rax
	vinsertps	$48, (%r8,%rax,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	%r10, %rbp
	sarq	$32, %rbp
	movq	%r13, %rdi
	sarq	$32, %rdi
	movq	%r15, %rbx
	sarq	$32, %rbx
	movq	%r11, %rdx
	sarq	$32, %rdx
	movl	1184(%rsp), %eax        # 4-byte Reload
	andl	$1, %eax
	testl	%eax, %eax
	vminps	%xmm1, %xmm2, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vshufps	$221, %xmm14, %xmm12, %xmm2 # xmm2 = xmm12[1,3],xmm14[1,3]
	vsubps	%xmm2, %xmm0, %xmm6
	movq	296(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %r12d
	movq	280(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movq	376(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r9), %r14d
	movq	304(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r9), %esi
	jne	.LBB155_3
# BB#4:                                 # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm4, 384(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 448(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 464(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 480(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vmovaps	1360(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm1, %xmm6
	vmovaps	%xmm11, %xmm15
	vmovaps	800(%rsp), %xmm13       # 16-byte Reload
	jmp	.LBB155_5
	.align	16, 0x90
.LBB155_3:                              #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	vmulps	%xmm4, %xmm3, %xmm0
	vmovaps	%xmm4, 384(%rsp)        # 16-byte Spill
	vshufps	$221, 1024(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm11[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	800(%rsp), %xmm9        # 16-byte Reload
	vmovaps	1360(%rsp), %xmm8       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm9[1,3]
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vaddps	%xmm13, %xmm6, %xmm2
	vmovaps	%xmm6, 448(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 464(%rsp)       # 16-byte Spill
	vaddps	%xmm15, %xmm2, %xmm2
	vmovaps	%xmm15, 480(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	752(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	-112(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, %xmm6
	vmovaps	%xmm11, %xmm15
	vmovaps	%xmm9, %xmm13
.LBB155_5:                              # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	1104(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1088(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1072(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm8, 1360(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1376(%rsp)      # 16-byte Spill
	movslq	%r10d, %rcx
	vmovss	(%r8,%rcx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r13d, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r8,%rdi,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm1, 688(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm14, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm14[0,2]
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	movslq	%r15d, %rcx
	vmovss	(%r8,%rcx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r11d, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r8,%rdx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, 1040(%rsp)       # 16-byte Spill
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm2[0,2]
	vmulps	%xmm3, %xmm1, %xmm2
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovaps	1312(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 864(%rsp)        # 16-byte Spill
	vmulps	1232(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm11, %xmm2 # xmm2 = xmm11[0,2],xmm9[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vmovaps	1168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 848(%rsp)        # 16-byte Spill
	vmulps	1216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm9
	vmulps	%xmm3, %xmm4, %xmm2
	vshufps	$136, 1024(%rsp), %xmm15, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm15[0,2],mem[0,2]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm11
	movslq	%eax, %rdx
	movq	360(%rsp), %r10         # 8-byte Reload
	vmovups	24608(%r10,%rdx,4), %xmm2
	vmovaps	%xmm2, 672(%rsp)        # 16-byte Spill
	movslq	%esi, %rsi
	vmovups	24608(%r10,%rsi,4), %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rsi,4), %xmm1
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1456(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm3, %xmm15, %xmm12
	vshufps	$221, %xmm1, %xmm0, %xmm4 # xmm4 = xmm0[1,3],xmm1[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm12, %xmm14
	vmovups	24624(%r10,%rdx,4), %xmm4
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm15, %xmm1
	vshufps	$221, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[1,3],xmm4[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm1, %xmm12
	vmovups	24616(%r10,%rsi,4), %xmm1
	vmovaps	%xmm1, 1344(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rsi,4), %xmm2
	vmovaps	%xmm2, 528(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm1, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm1[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	1392(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm3
	vmovups	24616(%r10,%rdx,4), %xmm2
	vmovaps	%xmm2, 1296(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rdx,4), %xmm4
	vmovaps	%xmm4, 512(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm4, %xmm2 # xmm2 = xmm4[1,3],xmm2[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm0, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm4
	vmovaps	1312(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm6, %xmm0, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vminps	%xmm6, %xmm9, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm13, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm13[0,2]
	vmovaps	%xmm0, 832(%rsp)        # 16-byte Spill
	vminps	%xmm6, %xmm11, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	movslq	%r12d, %rax
	movslq	%r14d, %rcx
	vminps	%xmm6, %xmm14, %xmm1
	vminps	%xmm6, %xmm12, %xmm11
	vminps	%xmm6, %xmm3, %xmm5
	vminps	%xmm6, %xmm4, %xmm14
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm6, %xmm0, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm12
	movl	332(%rsp), %r14d        # 4-byte Reload
	testl	%r14d, %r14d
	vmovups	24632(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovups	24632(%r10,%rsi,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	movq	352(%rsp), %r11         # 8-byte Reload
	vmovups	8(%r11,%rcx,4), %xmm3
	vmovups	24(%r11,%rcx,4), %xmm8
	vmovups	16(%r11,%rcx,4), %xmm4
	vmovups	32(%r11,%rcx,4), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovups	(%r11,%rcx,4), %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmovups	8(%r11,%rax,4), %xmm10
	vmovups	24(%r11,%rax,4), %xmm13
	vmovups	16(%r11,%rax,4), %xmm2
	vmovups	32(%r11,%rax,4), %xmm6
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vmovups	(%r11,%rax,4), %xmm7
	movq	1136(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r11,%rax,4), %xmm6
	vmovaps	%xmm6, 704(%rsp)        # 16-byte Spill
	movq	1144(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r10,%rax,4), %xmm6
	vmovaps	%xmm6, 720(%rsp)        # 16-byte Spill
	movq	896(%rsp), %r13         # 8-byte Reload
	vmovups	16(%r11,%r13,4), %xmm6
	vmovaps	%xmm6, 736(%rsp)        # 16-byte Spill
	movq	904(%rsp), %r12         # 8-byte Reload
	vmovups	24616(%r10,%r12,4), %xmm15
	je	.LBB155_7
# BB#6:                                 # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm3, 1168(%rsp)       # 16-byte Spill
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	%xmm3, 1440(%rsp)       # 16-byte Spill
	vmovaps	1168(%rsp), %xmm3       # 16-byte Reload
.LBB155_7:                              # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm3, 1168(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm8[1,3]
	vmovaps	%xmm3, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 880(%rsp)        # 16-byte Spill
	vmaxps	%xmm9, %xmm1, %xmm1
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vsubps	864(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 1280(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm13, %xmm10, %xmm1 # xmm1 = xmm10[1,3],xmm13[1,3]
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 864(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm11, %xmm8
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vsubps	848(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm4, %xmm0, %xmm6 # xmm6 = xmm0[1,3],xmm4[1,3]
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 1328(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm5, %xmm4
	vmovaps	816(%rsp), %xmm0        # 16-byte Reload
	vsubps	832(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	vmovaps	%xmm2, 1312(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 496(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm2[1,3]
	vmaxps	%xmm9, %xmm14, %xmm9
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm12, %xmm12
	movl	1184(%rsp), %edx        # 4-byte Reload
	movl	%edx, %eax
	movq	1016(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	movl	924(%rsp), %r15d        # 4-byte Reload
	je	.LBB155_8
# BB#9:                                 # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm13, 848(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 416(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 432(%rsp)       # 16-byte Spill
	movl	780(%rsp), %ecx         # 4-byte Reload
	vmovaps	1440(%rsp), %xmm11      # 16-byte Reload
	vmovaps	624(%rsp), %xmm0        # 16-byte Reload
	vmovaps	608(%rsp), %xmm5        # 16-byte Reload
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1280(%rsp), %xmm3       # 16-byte Reload
	jmp	.LBB155_10
	.align	16, 0x90
.LBB155_8:                              #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm13, 848(%rsp)       # 16-byte Spill
	movq	1136(%rsp), %rax        # 8-byte Reload
	vmovaps	704(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 32(%r11,%rax,4), %xmm0, %xmm13 # xmm13 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm6, 1440(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, %xmm11
	vmovaps	%xmm11, 416(%rsp)       # 16-byte Spill
	vmovaps	1040(%rsp), %xmm1       # 16-byte Reload
	vmulps	1232(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	movq	1144(%rsp), %rax        # 8-byte Reload
	vmovaps	720(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm4, %xmm14
	vshufps	$136, 24632(%r10,%rax,4), %xmm0, %xmm4 # xmm4 = xmm0[0,2],mem[0,2]
	vmovaps	1408(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmovaps	1424(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	.LCPI155_10(%rip), %xmm3, %xmm3
	vsubps	%xmm13, %xmm3, %xmm13
	vmulps	1216(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vshufps	$136, 24632(%r10,%r12,4), %xmm15, %xmm4 # xmm4 = xmm15[0,2],mem[0,2]
	vmovaps	%xmm15, 432(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	736(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 32(%r11,%r13,4), %xmm0, %xmm4 # xmm4 = xmm0[0,2],mem[0,2]
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	.LCPI155_10(%rip), %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm11, %xmm12, %xmm4
	vmovaps	1440(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1280(%rsp), %xmm1       # 16-byte Reload
	vaddps	%xmm4, %xmm1, %xmm4
	vaddps	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm14, %xmm4
	vaddps	%xmm3, %xmm10, %xmm3
	vaddps	%xmm3, %xmm13, %xmm2
	vmulps	928(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	movl	780(%rsp), %ecx         # 4-byte Reload
	vmovaps	624(%rsp), %xmm0        # 16-byte Reload
	vmovaps	608(%rsp), %xmm5        # 16-byte Reload
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm1, %xmm3
.LBB155_10:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vsubps	%xmm0, %xmm5, %xmm5
	vsubps	%xmm2, %xmm8, %xmm1
	vsubps	%xmm6, %xmm4, %xmm14
	vsubps	%xmm7, %xmm9, %xmm0
	testl	%edx, %r14d
	jne	.LBB155_11
# BB#12:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm5, 1456(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1440(%rsp)      # 16-byte Spill
	vmovaps	%xmm12, 400(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1280(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 832(%rsp)        # 16-byte Spill
	vmovaps	1424(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1408(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1200(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1472(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1344(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1296(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1328(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1312(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm14, 608(%rsp)       # 16-byte Spill
	vmovaps	1056(%rsp), %xmm5       # 16-byte Reload
	vmovaps	688(%rsp), %xmm4        # 16-byte Reload
	vmovaps	672(%rsp), %xmm1        # 16-byte Reload
	vmovaps	656(%rsp), %xmm2        # 16-byte Reload
	vmovaps	640(%rsp), %xmm10       # 16-byte Reload
	vmovaps	864(%rsp), %xmm0        # 16-byte Reload
	vxorps	%xmm8, %xmm8, %xmm8
	jmp	.LBB155_13
	.align	16, 0x90
.LBB155_11:                             #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm12, 400(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1280(%rsp)       # 16-byte Spill
	vmovaps	1200(%rsp), %xmm11      # 16-byte Reload
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1456(%rsp)       # 16-byte Spill
	vmovaps	384(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm5, %xmm11, %xmm0
	vmovaps	1344(%rsp), %xmm13      # 16-byte Reload
	vmovaps	%xmm1, 832(%rsp)        # 16-byte Spill
	vshufps	$221, 1104(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm13[1,3],mem[1,3]
	vmovaps	1408(%rsp), %xmm9       # 16-byte Reload
	vsubps	%xmm9, %xmm1, %xmm1
	vmovaps	1424(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1328(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, 1152(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[1,3],mem[1,3]
	vmovaps	1472(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm3, %xmm0, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm4
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm1
	vmovaps	1296(%rsp), %xmm15      # 16-byte Reload
	vshufps	$221, 1088(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm15[1,3],mem[1,3]
	vsubps	%xmm9, %xmm2, %xmm2
	vmulps	%xmm2, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1312(%rsp), %xmm7       # 16-byte Reload
	vmovaps	640(%rsp), %xmm10       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm7, %xmm2 # xmm2 = xmm7[1,3],xmm10[1,3]
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmovaps	832(%rsp), %xmm2        # 16-byte Reload
	vaddps	816(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm5
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm14, %xmm1
	vaddps	1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm4, %xmm0
	vmulps	928(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 608(%rsp)       # 16-byte Spill
	vmovaps	688(%rsp), %xmm4        # 16-byte Reload
	vmovaps	672(%rsp), %xmm1        # 16-byte Reload
	vmovaps	656(%rsp), %xmm2        # 16-byte Reload
	vmovaps	864(%rsp), %xmm0        # 16-byte Reload
.LBB155_13:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm7, 1312(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1328(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1344(%rsp)      # 16-byte Spill
	vshufps	$136, 848(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmulps	%xmm5, %xmm4, %xmm2
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm14
	vmovaps	%xmm5, %xmm0
	vmovaps	1040(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vshufps	$136, 1088(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, %xmm10, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm10[0,2]
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm7
	vmovaps	1168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1072(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1024(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	%xmm11, %xmm4, %xmm2
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm2
	vmulps	%xmm11, %xmm5, %xmm0
	vshufps	$136, 1104(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm13[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1152(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[0,2],mem[0,2]
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm8
	andl	$1, %edx
	je	.LBB155_14
# BB#15:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1184(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1296(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	vmovaps	1440(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm0, %xmm15
	vmovdqa	784(%rsp), %xmm4        # 16-byte Reload
	jmp	.LBB155_16
	.align	16, 0x90
.LBB155_14:                             #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm15, 1296(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	vaddps	%xmm8, %xmm7, %xmm0
	vmovaps	%xmm7, 1184(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm14, %xmm0
	vmulps	-128(%rsp), %xmm0, %xmm15 # 16-byte Folded Reload
	vmovdqa	784(%rsp), %xmm4        # 16-byte Reload
	vmovaps	1440(%rsp), %xmm0       # 16-byte Reload
.LBB155_16:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	testl	%r14d, %r14d
	jne	.LBB155_18
# BB#17:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm0, %xmm15
.LBB155_18:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	movq	-80(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI155_2(%rip), %xmm2 # xmm2 = [0,2,4,6]
	vpaddd	%xmm2, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	912(%rsp)               # 4-byte Folded Reload
	movl	%edx, %edi
	movl	%r15d, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	916(%rsp)               # 4-byte Folded Reload
	movl	%edx, %ebp
	andl	$1, %ebx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	920(%rsp)               # 4-byte Folded Reload
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	960(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	112(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm1
	vpxor	.LCPI155_9(%rip), %xmm1, %xmm1
	vmovdqa	96(%rsp), %xmm3         # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vmovdqa	976(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vmovdqa	944(%rsp), %xmm4        # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	176(%rsp), %xmm5        # 16-byte Reload
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	992(%rsp), %xmm4        # 16-byte Reload
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm5, %xmm0, %xmm0
	movq	-40(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm2, %xmm13
	vpminsd	%xmm4, %xmm3, %xmm3
	vpmaxsd	%xmm5, %xmm3, %xmm3
	vmovdqa	%xmm5, %xmm10
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	1264(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r8,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r8,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r8,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	testl	%ebx, %ebx
	jne	.LBB155_19
# BB#20:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB155_21
	.align	16, 0x90
.LBB155_19:                             #   in Loop: Header=BB155_2 Depth=1
	vmovaps	544(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	576(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	560(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1376(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm9, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	1472(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmaxps	.LCPI155_10(%rip), %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovaps	400(%rsp), %xmm2        # 16-byte Reload
	vaddps	1280(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vaddps	416(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm3, %xmm1
	vmulps	-112(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
.LBB155_21:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	testl	%r14d, %r14d
	je	.LBB155_23
# BB#22:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
.LBB155_23:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	movl	%r15d, %eax
	movq	1016(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	testb	$1, %al
	je	.LBB155_24
# BB#25:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	movl	1012(%rsp), %ecx        # 4-byte Reload
	vpxor	%xmm5, %xmm5, %xmm5
	jmp	.LBB155_26
	.align	16, 0x90
.LBB155_24:                             #   in Loop: Header=BB155_2 Depth=1
	movq	1136(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm1
	vshufps	$221, 704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	movq	1144(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%r10,%rax,4), %xmm3
	vshufps	$221, 720(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	1392(%rsp), %xmm5       # 16-byte Reload
	vmulps	1232(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm9, %xmm3, %xmm3
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmovups	(%r11,%r13,4), %xmm3
	vshufps	$221, 736(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovups	24600(%r10,%r12,4), %xmm4
	vshufps	$221, 432(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	1216(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm9, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	vminps	%xmm2, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vaddps	752(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm1, %xmm1
	vaddps	480(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	448(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	464(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	928(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	movl	1012(%rsp), %ecx        # 4-byte Reload
.LBB155_26:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	testl	%r15d, %r14d
	je	.LBB155_28
# BB#27:                                #   in Loop: Header=BB155_2 Depth=1
	vmovaps	512(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm9, %xmm1, %xmm1
	vmovaps	528(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[0,2],mem[0,2]
	vmulps	1056(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vsubps	%xmm9, %xmm3, %xmm3
	vmulps	%xmm11, %xmm0, %xmm0
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovaps	496(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1312(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[0,2],mem[0,2]
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm3, %xmm1, %xmm1
	vmovaps	800(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1328(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vaddps	%xmm1, %xmm14, %xmm1
	vsubps	%xmm3, %xmm0, %xmm0
	vaddps	1184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	1168(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm8, %xmm0
	vmulps	928(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
.LBB155_28:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	592(%rsp), %xmm2        # 16-byte Reload
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	movl	%r15d, %eax
	andl	$1, %eax
	je	.LBB155_29
# BB#30:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm11, 1200(%rsp)      # 16-byte Spill
	vmovaps	%xmm1, %xmm0
	jmp	.LBB155_31
	.align	16, 0x90
.LBB155_29:                             #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm11, 1200(%rsp)      # 16-byte Spill
	vmovaps	1456(%rsp), %xmm0       # 16-byte Reload
	vaddps	832(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	608(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	816(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	-128(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB155_31:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm9, 1408(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1424(%rsp)      # 16-byte Spill
	testl	%r14d, %r14d
	jne	.LBB155_33
# BB#32:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vmovaps	%xmm1, %xmm0
.LBB155_33:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB155_2 Depth=1
	vaddps	%xmm0, %xmm3, %xmm0
	vaddps	%xmm15, %xmm2, %xmm1
	vmovaps	.LCPI155_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI155_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r15d, %rax
	movq	288(%rsp), %rdx         # 8-byte Reload
	leaq	(%rax,%rdx), %rax
	movq	336(%rsp), %rdx         # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r9d
	addl	$-1, %ecx
	movl	%ecx, 1012(%rsp)        # 4-byte Spill
	jne	.LBB155_2
.LBB155_34:                             # %destructor_block
	xorl	%eax, %eax
	addq	$1496, %rsp             # imm = 0x5D8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end155:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11, .Lfunc_end155-par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI156_0:
	.long	1199570688              # float 65535
.LCPI156_3:
	.long	1065353216              # float 1
.LCPI156_4:
	.long	1073741824              # float 2
.LCPI156_5:
	.long	1048576000              # float 0.25
.LCPI156_6:
	.long	1056964608              # float 0.5
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI156_1:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI156_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI156_9:
	.zero	16,255
	.section	.rodata,"a",@progbits
	.align	32
.LCPI156_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI156_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172,@function
par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172: # @par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1576, %rsp             # imm = 0x628
	movl	%esi, %r15d
	movq	%r15, 1464(%rsp)        # 8-byte Spill
	vmovss	(%rdx), %xmm12          # xmm12 = mem[0],zero,zero,zero
	vmovss	4(%rdx), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 52(%rsp)         # 4-byte Spill
	vmovss	12(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 96(%rsp)         # 4-byte Spill
	vmovss	16(%rdx), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vmovss	20(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 80(%rsp)         # 4-byte Spill
	vmovss	24(%rdx), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	28(%rdx), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 68(%rsp)         # 4-byte Spill
	movl	36(%rdx), %eax
	movq	%rax, 936(%rsp)         # 8-byte Spill
	movl	40(%rdx), %eax
	movl	%eax, 100(%rsp)         # 4-byte Spill
	movl	44(%rdx), %esi
	movslq	56(%rdx), %rax
	movq	%rax, 128(%rsp)         # 8-byte Spill
	movl	60(%rdx), %eax
	movq	%rax, 144(%rsp)         # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	movl	68(%rdx), %eax
	movq	%rax, 56(%rsp)          # 8-byte Spill
	movl	72(%rdx), %eax
	movq	%rax, 440(%rsp)         # 8-byte Spill
	movslq	76(%rdx), %rax
	movq	%rax, 672(%rsp)         # 8-byte Spill
	movl	80(%rdx), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	movl	84(%rdx), %ecx
	movl	88(%rdx), %eax
	movq	%rax, 72(%rsp)          # 8-byte Spill
	vmovss	100(%rdx), %xmm4        # xmm4 = mem[0],zero,zero,zero
	vmovss	104(%rdx), %xmm1        # xmm1 = mem[0],zero,zero,zero
	vmovss	108(%rdx), %xmm0        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 84(%rsp)         # 4-byte Spill
	cmpl	%r15d, 52(%rdx)
	movl	92(%rdx), %eax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	movslq	96(%rdx), %rax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	movq	112(%rdx), %rbx
	movq	128(%rdx), %rax
	movq	%rax, 1400(%rsp)        # 8-byte Spill
	movq	144(%rdx), %r9
	movq	160(%rdx), %rax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	jle	.LBB156_32
# BB#1:                                 # %true_bb
	movq	%rcx, 1472(%rsp)        # 8-byte Spill
	vmovss	%xmm4, 88(%rsp)         # 4-byte Spill
	vmovss	%xmm2, 92(%rsp)         # 4-byte Spill
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	addl	$43, %esi
	sarl	$3, %esi
	movq	%rsi, 1408(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	jle	.LBB156_148
# BB#2:                                 # %for gH.s0.v10.v10.preheader
	movl	$2, %eax
	movq	72(%rsp), %r9           # 8-byte Reload
	subl	%r9d, %eax
	movq	112(%rsp), %r12         # 8-byte Reload
	leal	(%r12,%r12), %edi
	movl	%edi, 1152(%rsp)        # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edi, %eax
	negl	%eax
	movl	%r12d, %r8d
	sarl	$31, %r8d
	andnl	%edi, %r8d, %esi
	movl	%edi, %r14d
	andl	%eax, %r8d
	orl	%esi, %r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	leal	-1(%r12,%r12), %edx
	movl	%edx, 1504(%rsp)        # 4-byte Spill
	subl	%eax, %edx
	cmpl	%eax, %r12d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	leal	-1(%r9,%r12), %r10d
	cmpl	%edx, %r10d
	cmovlel	%r10d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movq	%rbx, 1560(%rsp)        # 8-byte Spill
	leal	(%r9,%r12), %ecx
	movl	%ecx, 1328(%rsp)        # 4-byte Spill
	cmpl	$3, %ecx
	movl	$2, %eax
	cmovll	%r10d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	$3, %ecx
	cmovll	%edx, %eax
	movl	%eax, 1264(%rsp)        # 4-byte Spill
	movq	144(%rsp), %rbx         # 8-byte Reload
	leal	(%rbx,%rbx), %edx
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%ebx, %esi
	sarl	$31, %esi
	andnl	%edx, %esi, %ecx
	movl	%edx, %ebp
	andl	%eax, %esi
	movl	$2, %eax
	movq	56(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	cltd
	idivl	%ebp
	orl	%ecx, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %r11d
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	addl	%edi, %edx
	leal	-1(%rdi,%rbx), %r13d
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	leal	(%rdi,%rbx), %ebp
	cmpl	$3, %ebp
	movl	$2, %eax
	cmovll	%r13d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %ebp
	cmovll	%edx, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movl	%r9d, %eax
	negl	%eax
	cltd
	idivl	%r14d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	1504(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r12d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %r10d
	cmovlel	%r10d, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	xorl	%r14d, %r14d
	movl	1328(%rsp), %ecx        # 4-byte Reload
	testl	%ecx, %ecx
	movl	$0, %eax
	cmovlel	%r10d, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	testl	%ecx, %ecx
	cmovlel	%edx, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	1344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	addl	%edi, %edx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%edx, 1184(%rsp)        # 4-byte Spill
	testl	%ebp, %ebp
	movl	$0, %eax
	cmovlel	%r13d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%ebp, %ebp
	cmovlel	%edx, %eax
	movl	%eax, 1168(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r9d, %eax
	cltd
	idivl	1152(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	1504(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	cmpl	%eax, %r12d
	cmovgl	%eax, %ecx
	addl	%r9d, %ecx
	cmpl	%ecx, %r10d
	cmovlel	%r10d, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	movl	%ecx, 1504(%rsp)        # 4-byte Spill
	movl	%ecx, %edx
	movl	1328(%rsp), %ecx        # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	cmovgel	%r14d, %r10d
	movzbl	%al, %eax
	orl	%eax, %r10d
	cmpl	%r9d, %r10d
	cmovll	%r9d, %r10d
	cmpl	$2, %ecx
	movq	1560(%rsp), %r12        # 8-byte Reload
	cmovll	%edx, %r10d
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1344(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %r11d
	cmpl	%eax, %ebx
	cmovgl	%eax, %r11d
	addl	%edi, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	cmpl	%edi, %r11d
	cmovll	%edi, %r11d
	cmpl	$1, %ebp
	setg	%al
	cmpl	$2, %ebp
	cmovgel	%r14d, %r13d
	movzbl	%al, %eax
	orl	%eax, %r13d
	cmpl	%edi, %r13d
	cmovll	%edi, %r13d
	cmpl	$2, %ebp
	cmovll	%r11d, %r13d
	movl	%r15d, %eax
	movq	40(%rsp), %rsi          # 8-byte Reload
	subl	%esi, %eax
	movq	1472(%rsp), %rbx        # 8-byte Reload
	leal	(%rbx,%rbx), %ecx
	cltd
	idivl	%ecx
	movl	%ebx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %r8d
	negl	%ecx
	andl	%eax, %ecx
	orl	%r8d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movq	128(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 752(%rsp)         # 8-byte Spill
	movl	%r15d, %ecx
	andl	$1, %ecx
	movl	%ecx, 24(%rsp)          # 4-byte Spill
	leal	-1(%rbx,%rbx), %edx
	subl	%eax, %edx
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	leal	(%rsi,%rbx), %eax
	leal	-1(%rsi,%rbx), %ecx
	addl	%esi, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	cmpl	%r15d, %eax
	cmovlel	%edx, %ecx
	cmpl	%r15d, %esi
	cmovgl	%edx, %ecx
	movq	160(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %eax
	vmovd	%eax, %xmm2
	movq	32(%rsp), %rbp          # 8-byte Reload
	imull	%ebp, %esi
	vmovss	.LCPI156_0(%rip), %xmm11 # xmm11 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm11, %xmm4
	vmulss	%xmm6, %xmm4, %xmm0
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	movq	440(%rsp), %r8          # 8-byte Reload
	leal	2(%r8,%rdx), %eax
	vmovd	%eax, %xmm3
	addl	%r9d, %esi
	movq	936(%rsp), %rbx         # 8-byte Reload
	movl	%ebx, %eax
	sarl	$5, %eax
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	cmpl	$1, %edi
	cmovgl	%r11d, %r13d
	vsubss	%xmm6, %xmm5, %xmm0
	leal	2(%r8), %edx
	vmovd	%edx, %xmm5
	movslq	%ecx, %rdx
	imulq	%rbp, %rdx
	vmulss	%xmm0, %xmm4, %xmm0
	vdivss	%xmm0, %xmm1, %xmm8
	vmovss	52(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm11, %xmm7
	vmovss	68(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm7, %xmm0
	vmovss	84(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm10
	movq	672(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm0
	imull	%r8d, %ecx
	addl	%edi, %ecx
	cmpl	$1, %r9d
	movl	%r10d, %r11d
	cmovgl	1504(%rsp), %r11d       # 4-byte Folded Reload
	movslq	%esi, %rbp
	movq	%rdx, %r10
	subq	%rbp, %r10
	testl	%edi, %edi
	movl	1168(%rsp), %eax        # 4-byte Reload
	cmovgl	1184(%rsp), %eax        # 4-byte Folded Reload
	testl	%r9d, %r9d
	movl	1200(%rsp), %esi        # 4-byte Reload
	cmovgl	1216(%rsp), %esi        # 4-byte Folded Reload
	movslq	%esi, %rsi
	subq	%rbp, %rsi
	addq	%rdx, %rsi
	movq	%rsi, 1504(%rsp)        # 8-byte Spill
	cmpl	$2, %edi
	movl	1232(%rsp), %esi        # 4-byte Reload
	cmovgl	1248(%rsp), %esi        # 4-byte Folded Reload
	cmpl	$2, %r9d
	movl	1264(%rsp), %edx        # 4-byte Reload
	cmovgl	1280(%rsp), %edx        # 4-byte Folded Reload
	vpbroadcastd	%xmm2, %xmm14
	vmovdqa	%xmm14, 736(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm9
	vmovdqa	%xmm9, 288(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vmovss	80(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm2, %xmm3
	vmovdqa	.LCPI156_1(%rip), %xmm2 # xmm2 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 720(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 704(%rsp)        # 16-byte Spill
	vmulss	%xmm3, %xmm7, %xmm0
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm15
	vmovdqa	%xmm15, 304(%rsp)       # 16-byte Spill
	vdivss	%xmm0, %xmm4, %xmm5
	vsubss	%xmm12, %xmm11, %xmm0
	vmovss	92(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm1
	vmovss	88(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm12
	vmovd	%ecx, %xmm7
	vmovss	96(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm1, %xmm4
	vmovd	%r13d, %xmm6
	vpsubd	%xmm7, %xmm6, %xmm6
	vmulss	%xmm4, %xmm0, %xmm0
	vdivss	%xmm0, %xmm3, %xmm0
	vmovd	%eax, %xmm4
	vpsubd	%xmm7, %xmm4, %xmm4
	vmovd	%esi, %xmm1
	movl	%esi, %r9d
	vpsubd	%xmm7, %xmm1, %xmm1
	movq	160(%rsp), %rdi         # 8-byte Reload
	vmovd	%edi, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1232(%rsp)       # 16-byte Spill
	leal	-1(%r8,%rdi), %esi
	vmovd	%esi, %xmm3
	vpbroadcastd	%xmm3, %xmm7
	vmovdqa	%xmm7, 272(%rsp)        # 16-byte Spill
	movslq	%r11d, %rsi
	leal	1(%r8,%rdi), %ebp
	movq	%rdi, %r11
	vmovd	%ebp, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 688(%rsp)        # 16-byte Spill
	leaq	(%rsi,%r10), %rsi
	leal	1(%r8), %ebp
	vmovd	%ebp, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 672(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 912(%rsp)        # 16-byte Spill
	vmovd	%r13d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 656(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm3
	vmovdqa	%xmm3, 1344(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm3
	vmovaps	%xmm3, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	1328(%rsp), %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	leal	3(%r8,%r11), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 640(%rsp)        # 16-byte Spill
	leal	3(%r8), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 624(%rsp)        # 16-byte Spill
	leal	(%r8,%r11), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 608(%rsp)        # 16-byte Spill
	leal	4(%r8,%r11), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 592(%rsp)        # 16-byte Spill
	leal	4(%r8), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 576(%rsp)        # 16-byte Spill
	movslq	%edx, %rdi
	addq	%r10, %rdi
	leal	-1(%r8), %edx
	vmovd	%edx, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 560(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm15, %xmm3
	vmovdqa	%xmm3, 544(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm7, %xmm2
	vmovdqa	%xmm7, %xmm8
	vmovdqa	%xmm2, 528(%rsp)        # 16-byte Spill
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 256(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm2
	vmovdqa	%xmm2, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm2
	vmovaps	%xmm2, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm2
	vmovaps	%xmm2, 208(%rsp)        # 16-byte Spill
	vmovd	%r9d, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 192(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 176(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	movq	1472(%rsp), %rbp        # 8-byte Reload
	movslq	%ebp, %rdx
	shlq	$5, %rdx
	addq	$40, %rdx
	movl	%r15d, %eax
	andl	$63, %eax
	imulq	%rdx, %rax
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	movq	128(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rdx
	sarq	$63, %rdx
	andq	%rcx, %rdx
	subq	%rdx, %rax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	leal	8(%r15), %edx
	subl	100(%rsp), %edx         # 4-byte Folded Reload
	andl	$-32, %ebx
	addl	$64, %ebx
	imull	%edx, %ebx
	movq	%rbx, 936(%rsp)         # 8-byte Spill
	leal	(%rbp,%rbp,2), %eax
	movl	%eax, %edx
	shll	$10, %edx
	leal	(%rdx,%rbx), %ecx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	shll	$9, %eax
	leal	(%rax,%rbx), %eax
	movq	%rax, 480(%rsp)         # 8-byte Spill
	movl	24(%rsp), %r11d         # 4-byte Reload
	movq	752(%rsp), %rdx         # 8-byte Reload
	leal	1(%rdx), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	movl	%edx, %eax
	subl	%r8d, %eax
	movq	%rax, 440(%rsp)         # 8-byte Spill
	leal	-3(%rdx), %ecx
	movq	%rcx, 416(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %ecx
	movq	%rcx, 408(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 400(%rsp)         # 8-byte Spill
	leal	1(%rax), %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	leal	-4(%rax), %ecx
	movq	%rcx, 368(%rsp)         # 8-byte Spill
	leal	-3(%rax), %ecx
	movq	%rcx, 352(%rsp)         # 8-byte Spill
	leal	-1(%rax), %ecx
	movq	%rcx, 336(%rsp)         # 8-byte Spill
	leal	-2(%rax), %eax
	movq	%rax, 320(%rsp)         # 8-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	movq	1504(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 128(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 112(%rsp)        # 16-byte Spill
	vpabsd	%xmm14, %xmm0
	vmovdqa	%xmm0, 1200(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm14, %xmm0
	vmovdqa	%xmm0, 1184(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_3(%rip), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_4(%rip), %xmm0
	vmovaps	%xmm0, 1136(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_5(%rip), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_6(%rip), %xmm0
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB156_3:                              # %for gH.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	320(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	736(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r13d
	movl	%r13d, 1072(%rsp)       # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r10d
	movl	%r10d, 1056(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %edi
	movq	752(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %esi
	movl	%esi, 1088(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %ebx
	movl	%ebx, 1040(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %ebp
	movl	%ebp, 1024(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	vpinsrd	$1, %r8d, %xmm1, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1200(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm7
	vmovdqa	%xmm7, 976(%rsp)        # 16-byte Spill
	vmovdqa	720(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	704(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1232(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	1184(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	movq	336(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm8, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpmulld	%xmm9, %xmm0, %xmm12
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	352(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebp
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	688(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm1
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vmovdqa	672(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm2, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vpcmpgtd	%xmm3, %xmm14, %xmm4
	vpsubd	%xmm3, %xmm13, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpaddd	%xmm15, %xmm3, %xmm3
	vpminsd	%xmm8, %xmm3, %xmm3
	vpmaxsd	%xmm15, %xmm3, %xmm3
	movq	408(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm8, %xmm4, %xmm4
	vpmaxsd	%xmm15, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm3, %xmm4, %xmm1
	vpmulld	%xmm9, %xmm1, %xmm1
	vmovdqa	%xmm1, 1472(%rsp)       # 16-byte Spill
	vpaddd	1344(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1568(%rsp), %r9         # 8-byte Reload
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	movslq	%eax, %r15
	vmovups	12312(%r12,%r15,4), %xmm1
	vmovaps	%xmm1, 1264(%rsp)       # 16-byte Spill
	vmovups	12328(%r12,%r15,4), %xmm5
	vmovdqa	640(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm3
	vpxor	%xmm6, %xmm3, %xmm3
	vmovdqa	624(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm6
	vpor	%xmm3, %xmm6, %xmm6
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm13, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vmovups	12320(%r12,%r15,4), %xmm1
	vmovaps	%xmm1, 1280(%rsp)       # 16-byte Spill
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm8, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vmovdqa	%xmm9, %xmm4
	vblendvps	%xmm6, %xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebp
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm10, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm3
	vmovdqa	912(%rsp), %xmm9        # 16-byte Reload
	vpsubd	%xmm9, %xmm12, %xmm0
	vmovdqa	%xmm0, 1008(%rsp)       # 16-byte Spill
	vpaddd	656(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovdqa	608(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm2
	vpxor	.LCPI156_9(%rip), %xmm2, %xmm2
	vmovdqa	544(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm1, %xmm1
	vpor	%xmm2, %xmm1, %xmm2
	vpcmpgtd	%xmm3, %xmm14, %xmm1
	vpsubd	%xmm3, %xmm13, %xmm6
	vblendvps	%xmm1, %xmm3, %xmm6, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm8, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	vpaddd	%xmm11, %xmm7, %xmm6
	vpminsd	%xmm8, %xmm6, %xmm6
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm12
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %r10d
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebp
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, %ecx, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm3
	vpsubd	%xmm1, %xmm13, %xmm6
	vblendvps	%xmm3, %xmm1, %xmm6, %xmm1
	vmovdqa	528(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm3, %xmm3
	vpxor	.LCPI156_9(%rip), %xmm3, %xmm3
	vmovdqa	560(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm6, %xmm6
	vpor	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm8, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	vmovd	%r10d, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm8, %xmm6, %xmm6
	vpmaxsd	%xmm15, %xmm6, %xmm6
	vblendvps	%xmm3, %xmm1, %xmm6, %xmm8
	vmovaps	1328(%rsp), %xmm2       # 16-byte Reload
	vmulps	1504(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	1264(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm7, %xmm6 # xmm6 = xmm7[1,3],xmm5[1,3]
	vmovaps	1152(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm6, %xmm6
	vmovaps	1216(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vpmulld	1104(%rsp), %xmm4, %xmm11 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm7, %xmm5 # xmm5 = xmm7[0,2],xmm5[0,2]
	vpxor	%xmm13, %xmm13, %xmm13
	vmovaps	1248(%rsp), %xmm7       # 16-byte Reload
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm6
	vmovups	%ymm6, 1504(%rsp)       # 32-byte Spill
	vmulps	%xmm0, %xmm2, %xmm0
	vsubps	%xmm3, %xmm5, %xmm1
	vmovaps	%xmm14, %xmm5
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpmulld	%xmm4, %xmm12, %xmm1
	vmovdqa	%xmm1, 992(%rsp)        # 16-byte Spill
	vpaddd	1344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rbp
	vmovq	%xmm1, %rbx
	movq	%rbx, %rsi
	sarq	$32, %rsi
	movq	%rbp, %rdi
	sarq	$32, %rdi
	movq	936(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovaps	%xmm6, %xmm14
	testl	%r10d, %r11d
	jne	.LBB156_5
# BB#4:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB156_5:                              # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vpmulld	%xmm4, %xmm8, %xmm6
	vpsubd	%xmm9, %xmm11, %xmm4
	vmovdqa	1472(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm9, %xmm1, %xmm15
	movl	%r10d, %r8d
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm1
	vmovups	%ymm1, 1472(%rsp)       # 32-byte Spill
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1280(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 12336(%r12,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm2
	vmovups	(%r12,%rdx,4), %xmm1
	vmovups	32(%r12,%rax,4), %xmm8
	vmovups	48(%r12,%rax,4), %xmm5
	vmovups	(%r12,%rcx,4), %xmm10
	vmovups	40(%r12,%rax,4), %xmm3
	vmulps	1168(%rsp), %xmm2, %xmm9 # 16-byte Folded Reload
	andl	$1, %r8d
	je	.LBB156_6
# BB#7:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	%xmm4, 896(%rsp)        # 16-byte Spill
	vmovdqa	%xmm6, 944(%rsp)        # 16-byte Spill
	vmovdqa	%xmm11, 960(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 784(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 880(%rsp)       # 16-byte Spill
	movq	%r15, 1104(%rsp)        # 8-byte Spill
	movl	%r10d, 1264(%rsp)       # 4-byte Spill
	vmovaps	%xmm7, %xmm11
	vmovaps	%xmm14, %xmm10
	jmp	.LBB156_8
	.align	16, 0x90
.LBB156_6:                              #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	%xmm11, 960(%rsp)       # 16-byte Spill
	movq	%r15, 1104(%rsp)        # 8-byte Spill
	movl	%r10d, 1264(%rsp)       # 4-byte Spill
	vmovdqa	256(%rsp), %xmm0        # 16-byte Reload
	vpaddd	%xmm0, %xmm4, %xmm2
	vmovdqa	%xmm4, 896(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	movq	%rcx, 848(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %rcx
	movq	%rcx, 800(%rsp)         # 8-byte Spill
	sarq	$32, %rdx
	vpaddd	240(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm6, 944(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rsi
	vmovq	%xmm2, %rbp
	movslq	%ebp, %r13
	sarq	$32, %rbp
	movq	%r9, %rcx
	movslq	%esi, %r9
	sarq	$32, %rsi
	vpaddd	%xmm0, %xmm15, %xmm2
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbx
	movq	%r14, %r10
	movl	%r11d, %r14d
	movslq	%ebx, %r11
	sarq	$32, %rbx
	movq	%r12, %r15
	movslq	%edi, %r12
	sarq	$32, %rdi
	movq	848(%rsp), %r8          # 8-byte Reload
	vmovss	(%rcx,%r8,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	800(%rsp), %rax         # 8-byte Reload
	vinsertps	$32, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	128(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm5, %xmm6
	vmovaps	%xmm6, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, %xmm5
	vmovaps	%xmm5, 832(%rsp)        # 16-byte Spill
	vmulps	%xmm2, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm11
	vmovaps	%xmm11, 864(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm5[1,3]
	vmovaps	%xmm1, 784(%rsp)        # 16-byte Spill
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm10, %xmm2
	vmovaps	%xmm2, 880(%rsp)        # 16-byte Spill
	vmovaps	224(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm8, %xmm3, %xmm8
	vmovss	(%rcx,%r13,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rcx,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%rcx, %r9
	vinsertps	$48, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$221, %xmm6, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm6[1,3]
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm3, %xmm5, %xmm3
	vmovss	(%r9,%r11,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	movl	%r14d, %r11d
	movq	%r10, %r14
	vinsertps	$16, (%r9,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%r15, %r12
	vinsertps	$48, (%r9,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm0, %xmm5
	vshufps	$221, %xmm11, %xmm2, %xmm0 # xmm0 = xmm2[1,3],xmm11[1,3]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm5, %xmm0, %xmm0
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vmovaps	1136(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm0
	vminps	%xmm7, %xmm8, %xmm2
	vmovaps	%xmm7, %xmm11
	vmaxps	%xmm13, %xmm2, %xmm2
	vsubps	%xmm2, %xmm0, %xmm10
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm9, %xmm0, %xmm10
.LBB156_8:                              # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	272(%rsp), %xmm13       # 16-byte Reload
	vmovdqa	976(%rsp), %xmm12       # 16-byte Reload
	movq	%r9, %r15
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	testl	%r11d, %r11d
	movl	1264(%rsp), %ecx        # 4-byte Reload
	jne	.LBB156_10
# BB#9:                                 # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm14, %xmm10
.LBB156_10:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	cltq
	vmovups	24592(%r12,%rax,4), %xmm3
	vmovups	24608(%r12,%rax,4), %xmm5
	vmovups	24624(%r12,%rax,4), %xmm2
	vmovups	24600(%r12,%rax,4), %xmm7
	vmovups	24616(%r12,%rax,4), %xmm8
	andl	$1, %ecx
	jne	.LBB156_11
# BB#12:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm3, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 944(%rsp)       # 16-byte Spill
	movq	%r15, %r9
	jmp	.LBB156_13
	.align	16, 0x90
.LBB156_11:                             #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	192(%rsp), %xmm1        # 16-byte Reload
	vpaddd	896(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	%rcx, 896(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %r10
	sarq	$32, %rdx
	vmovdqa	944(%rsp), %xmm0        # 16-byte Reload
	vpaddd	176(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbp
	movq	%r14, 104(%rsp)         # 8-byte Spill
	movl	%r11d, %r14d
	movslq	%ebp, %r11
	sarq	$32, %rbp
	movq	%r12, %r9
	movslq	%esi, %r12
	sarq	$32, %rsi
	vpaddd	%xmm1, %xmm15, %xmm0
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbx
	movslq	%ebx, %r13
	sarq	$32, %rbx
	movslq	%edi, %rcx
	sarq	$32, %rdi
	movq	896(%rsp), %r8          # 8-byte Reload
	vmovss	(%r15,%r8,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	112(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vshufps	$221, %xmm5, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm5[1,3]
	vmovaps	%xmm3, 768(%rsp)        # 16-byte Spill
	vmovaps	144(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm1, %xmm1
	vmovaps	160(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovss	(%r15,%r11,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movl	%r14d, %r11d
	movq	104(%rsp), %r14         # 8-byte Reload
	vinsertps	$16, (%r15,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%r9, %r12
	movq	%r15, %r9
	vinsertps	$48, (%r9,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm6, %xmm1
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm2[1,3]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovss	(%r9,%r13,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vshufps	$221, %xmm8, %xmm7, %xmm6 # xmm6 = xmm7[1,3],xmm8[1,3]
	vmovaps	%xmm7, 848(%rsp)        # 16-byte Spill
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm2, %xmm6, %xmm2
	vminps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1136(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm3, %xmm2
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	%xmm0, %xmm2, %xmm1
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm9, %xmm0, %xmm1
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
.LBB156_13:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	288(%rsp), %xmm7        # 16-byte Reload
	vmovaps	1328(%rsp), %xmm9       # 16-byte Reload
	movl	1088(%rsp), %r15d       # 4-byte Reload
	movl	1072(%rsp), %ecx        # 4-byte Reload
	movl	1056(%rsp), %esi        # 4-byte Reload
	movl	1040(%rsp), %ebx        # 4-byte Reload
	movl	1024(%rsp), %ebp        # 4-byte Reload
	vmovaps	1152(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm8, 896(%rsp)        # 16-byte Spill
	vmovdqa	960(%rsp), %xmm0        # 16-byte Reload
	vpaddd	1344(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r10
	vmovq	%xmm0, %r13
	testl	%r11d, %r11d
	je	.LBB156_15
# BB#14:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm10, 944(%rsp)       # 16-byte Spill
.LBB156_15:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm5, 960(%rsp)        # 16-byte Spill
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	%r13, %rax
	sarq	$32, %rax
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	1200(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	1232(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm2
	vmovdqa	1184(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	movq	448(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	vmovd	%ecx, %xmm2
	movq	%r10, %rcx
	sarq	$32, %rcx
	vmovdqa	592(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm4
	vpxor	.LCPI156_9(%rip), %xmm4, %xmm4
	vmovdqa	576(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vmovdqa	304(%rsp), %xmm15       # 16-byte Reload
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm2
	vmovups	1472(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%xmm0, %xmm8
	testl	%r15d, %r11d
	jne	.LBB156_17
# BB#16:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vxorps	%xmm8, %xmm8, %xmm8
.LBB156_17:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vpmulld	%xmm7, %xmm2, %xmm12
	movl	%r15d, %edx
	movslq	%r13d, %rsi
	movslq	%r10d, %rdi
	vmovss	(%r9,%rsi,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm9, %xmm0
	movq	1104(%rsp), %rax        # 8-byte Reload
	vmovups	12304(%r12,%rax,4), %xmm2
	vshufps	$221, 1280(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	1216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm11, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovups	1504(%rsp), %ymm1       # 32-byte Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1168(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	andl	$1, %edx
	je	.LBB156_18
# BB#19:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm11, 1248(%rsp)      # 16-byte Spill
	vmovaps	%xmm9, 1328(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, %xmm6
	jmp	.LBB156_20
	.align	16, 0x90
.LBB156_18:                             #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm9, 1328(%rsp)       # 16-byte Spill
	vpaddd	240(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	vmovdqa	992(%rsp), %xmm0        # 16-byte Reload
	vpsubd	912(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	256(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm0, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rbp
	vpaddd	1008(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	128(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	832(%rsp), %xmm1        # 16-byte Reload
	vmovaps	784(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, %xmm1, %xmm2, %xmm6 # xmm6 = xmm2[0,2],xmm1[0,2]
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm6, %xmm6
	vmovaps	224(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm0, %xmm6
	movslq	%ebp, %rax
	sarq	$32, %rbp
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, 816(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm1[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm5, %xmm3
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	880(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 864(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm5, %xmm1
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm2, %xmm1
	vminps	%xmm11, %xmm6, %xmm0
	vmovaps	%xmm11, 1248(%rsp)      # 16-byte Spill
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm0, %xmm1, %xmm6
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm0, %xmm6
.LBB156_20:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	movl	1264(%rsp), %r10d       # 4-byte Reload
	testl	%r11d, %r11d
	jne	.LBB156_22
# BB#21:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm8, %xmm6
.LBB156_22:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	movl	%r15d, %eax
	vmovdqa	%xmm7, %xmm9
	andl	$1, %eax
	jne	.LBB156_23
# BB#24:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm6, %xmm0
	jmp	.LBB156_25
	.align	16, 0x90
.LBB156_23:                             #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	176(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm12, %xmm1, %xmm0
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbp
	vpaddd	992(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rdi
	vmovdqa	1008(%rsp), %xmm0       # 16-byte Reload
	vpaddd	192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	112(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	960(%rsp), %xmm3        # 16-byte Reload
	vmovaps	768(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	144(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	160(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	movslq	%edi, %rax
	sarq	$32, %rdi
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, 800(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	848(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 896(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm7, %xmm4
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm2, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm1, %xmm0
.LBB156_25:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	movq	1464(%rsp), %rcx        # 8-byte Reload
	movq	1408(%rsp), %rdx        # 8-byte Reload
	testl	%r11d, %r11d
	je	.LBB156_27
# BB#26:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	%xmm6, %xmm0
.LBB156_27:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	movl	%r15d, %eax
	orl	%ecx, %eax
	testb	$1, %al
	je	.LBB156_29
# BB#28:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
.LBB156_29:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovdqa	%xmm13, %xmm8
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	orl	%ecx, %r10d
	testb	$1, %r10b
	je	.LBB156_31
# BB#30:                                # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	944(%rsp), %xmm0        # 16-byte Reload
	vmovups	%ymm0, 1504(%rsp)       # 32-byte Spill
.LBB156_31:                             # %for gH.s0.v10.v10
                                        #   in Loop: Header=BB156_3 Depth=1
	vmovaps	.LCPI156_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	1504(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovaps	.LCPI156_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	1472(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	512(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1400(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	addl	$-1, %edx
	movq	%rdx, 1408(%rsp)        # 8-byte Spill
	jne	.LBB156_3
	jmp	.LBB156_148
.LBB156_32:                             # %false_bb
	cmpl	%r15d, 48(%rdx)
	jle	.LBB156_39
# BB#33:                                # %true_bb1
	movq	%rbx, 1560(%rsp)        # 8-byte Spill
	vmovss	%xmm3, 4(%rsp)          # 4-byte Spill
	vmovss	%xmm6, 8(%rsp)          # 4-byte Spill
	vmovss	%xmm5, 12(%rsp)         # 4-byte Spill
	vmovss	%xmm1, 16(%rsp)         # 4-byte Spill
	vmovss	%xmm4, 88(%rsp)         # 4-byte Spill
	vmovss	%xmm2, 92(%rsp)         # 4-byte Spill
	vmovss	%xmm12, 20(%rsp)        # 4-byte Spill
	movq	128(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edi
	sarl	$31, %edi
	andl	%eax, %edi
	movq	%rdi, 1328(%rsp)        # 8-byte Spill
	movq	440(%rsp), %rdx         # 8-byte Reload
	movq	%rdx, %r10
	movl	%r10d, %ebp
	subl	%edi, %ebp
	movl	%ebp, %eax
	sarl	$3, %eax
	leal	1(%rax), %r8d
	leal	6(%rbp), %ecx
	sarl	$3, %ecx
	cmpl	%ecx, %eax
	cmovgel	%r8d, %ecx
	movl	%ecx, -20(%rsp)         # 4-byte Spill
	leal	7(%rbp), %eax
	sarl	$3, %eax
	movq	%rax, -8(%rsp)          # 8-byte Spill
	cmpl	%eax, %ecx
	movl	%eax, %edx
	cmovgel	%ecx, %edx
	leal	9(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -24(%rsp)         # 4-byte Spill
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	leal	10(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -28(%rsp)         # 4-byte Spill
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	addl	$11, %ebp
	sarl	$3, %ebp
	movq	%rbp, -16(%rsp)         # 8-byte Spill
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	xorl	%eax, %eax
	testl	%edx, %edx
	cmovsl	%eax, %edx
	leal	43(%rsi), %eax
	sarl	$3, %eax
	movl	%eax, 656(%rsp)         # 4-byte Spill
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	movl	%edx, 640(%rsp)         # 4-byte Spill
	movq	160(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	subl	%edi, %eax
	leal	(%r10,%rax), %ebp
	leal	(%r10,%rcx), %ecx
	movq	%rcx, 24(%rsp)          # 8-byte Spill
	subl	%edi, %ecx
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	sarl	$3, %ebp
	movq	%rbp, -48(%rsp)         # 8-byte Spill
	leal	-7(%r10,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -52(%rsp)         # 4-byte Spill
	leal	-1(%rbp), %ebp
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	leal	-6(%r10,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -56(%rsp)         # 4-byte Spill
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	leal	-5(%r10,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -60(%rsp)         # 4-byte Spill
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	leal	-4(%r10,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -64(%rsp)         # 4-byte Spill
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	leal	-3(%r10,%rax), %eax
	sarl	$3, %eax
	movl	%eax, -68(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-7(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -72(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-6(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -76(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-5(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -80(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-4(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -84(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-3(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -88(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-2(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -92(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -96(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -100(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	2(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -104(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	3(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -108(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	sarl	$3, %ecx
	movq	%rcx, -40(%rsp)         # 8-byte Spill
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	addl	$35, %esi
	sarl	$3, %esi
	movq	%rsi, 1408(%rsp)        # 8-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	addl	$1, %ebp
	cmpl	%ebp, %edx
	cmovgel	%edx, %ebp
	movl	%ebp, -112(%rsp)        # 4-byte Spill
	testl	%edx, %edx
	jle	.LBB156_70
# BB#34:                                # %for gH.s0.v10.v104.preheader
	movq	112(%rsp), %r13         # 8-byte Reload
	leal	(%r13,%r13), %edx
	movl	%edx, 1504(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r13d, %r10d
	sarl	$31, %r10d
	andnl	%edx, %r10d, %ecx
	movl	%edx, %esi
	andl	%eax, %r10d
	orl	%ecx, %r10d
	movq	72(%rsp), %rbx          # 8-byte Reload
	leal	(%rbx,%r13), %ebp
	movl	%ebp, 1264(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%ebx, %eax
	cltd
	idivl	%esi
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	leal	-1(%r13,%r13), %r9d
	leal	-1(%rbx,%r13), %edi
	movl	%edi, 1472(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%r9d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r13d
	cmovgl	%eax, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %edi
	cmovlel	%edi, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, 1232(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	cmpl	$3, %ebp
	movl	$2, %ecx
	cmovll	%edi, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, %edx
	cmpl	$3, %ebp
	movq	144(%rsp), %r11         # 8-byte Reload
	leal	-1(%r11,%r11), %ebp
	movq	56(%rsp), %rcx          # 8-byte Reload
	leal	-1(%rcx,%r11), %r12d
	cmovll	%eax, %edx
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	leal	(%r11,%r11), %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	%eax, %edx
	negl	%edx
	movl	%r11d, %r8d
	sarl	$31, %r8d
	andnl	%eax, %r8d, %r14d
	movl	%eax, %esi
	andl	%edx, %r8d
	orl	%r14d, %r8d
	movl	$2, %eax
	subl	%ecx, %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%ebp, %edx
	subl	%eax, %edx
	cmpl	%eax, %r11d
	cmovgl	%eax, %edx
	leal	(%rcx,%r11), %esi
	movl	%esi, 1168(%rsp)        # 4-byte Spill
	addl	%ecx, %edx
	cmpl	%edx, %r12d
	cmovlel	%r12d, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %eax
	cmovll	%r12d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	$3, %esi
	cmovll	%edx, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movl	%ebx, %eax
	negl	%eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%r9d, %r14d
	subl	%eax, %r14d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r14d
	addl	%ebx, %r14d
	cmpl	%r14d, %edi
	cmovlel	%edi, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	movl	1264(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	testl	%edx, %edx
	cmovlel	%r14d, %eax
	movl	%eax, 1184(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%ebp, %edi
	subl	%eax, %edi
	cmpl	%eax, %r11d
	cmovgl	%eax, %edi
	addl	%ecx, %edi
	cmpl	%edi, %r12d
	cmovlel	%r12d, %edi
	cmpl	%ecx, %edi
	cmovll	%ecx, %edi
	testl	%esi, %esi
	movl	$0, %eax
	cmovlel	%r12d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	testl	%esi, %esi
	cmovlel	%edi, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%ebx, %eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	subl	%eax, %r9d
	cmpl	%eax, %r13d
	movl	$0, %r13d
	cmovgl	%eax, %r9d
	addl	%ebx, %r9d
	movl	1472(%rsp), %edx        # 4-byte Reload
	cmpl	%r9d, %edx
	cmovlel	%edx, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	movl	1264(%rsp), %esi        # 4-byte Reload
	cmpl	$1, %esi
	setg	%r10b
	cmpl	$2, %esi
	cmovgel	%r13d, %edx
	movzbl	%r10b, %eax
	orl	%eax, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	cmpl	$2, %esi
	cmovll	%r9d, %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%ecx, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	cmpl	%eax, %r11d
	cmovgl	%eax, %ebp
	addl	%ecx, %ebp
	cmpl	%ebp, %r12d
	cmovlel	%r12d, %ebp
	cmpl	%ecx, %ebp
	cmovll	%ecx, %ebp
	movl	1168(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movzbl	%al, %eax
	cmovgel	%r13d, %r12d
	orl	%eax, %r12d
	cmpl	%ecx, %r12d
	cmovll	%ecx, %r12d
	cmpl	$2, %edx
	cmovll	%ebp, %r12d
	movq	936(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rdx
	andl	$-32, %eax
	addl	$64, %eax
	leal	8(%r15), %esi
	subl	100(%rsp), %esi         # 4-byte Folded Reload
	imull	%eax, %esi
	movq	%rsi, 624(%rsp)         # 8-byte Spill
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm0
	movq	32(%rsp), %r11          # 8-byte Reload
	movl	%r11d, %esi
	movq	40(%rsp), %rax          # 8-byte Reload
	imull	%eax, %esi
	movl	%r15d, %eax
	andl	$1, %eax
	movl	%eax, 1504(%rsp)        # 4-byte Spill
	addl	%ebx, %esi
	movl	%edx, %eax
	sarl	$5, %eax
	movl	%eax, %edx
	shll	$9, %edx
	movq	%rdx, 1280(%rsp)        # 8-byte Spill
	cmpl	$1, %ecx
	cmovgl	%ebp, %r12d
	movq	24(%rsp), %rbp          # 8-byte Reload
	leal	2(%rbp), %edx
	vmovd	%edx, %xmm8
	movq	440(%rsp), %r10         # 8-byte Reload
	leal	2(%r10), %edx
	vmovd	%edx, %xmm2
	leal	-1(%rbp), %edx
	vmovd	%edx, %xmm9
	movq	672(%rsp), %rdx         # 8-byte Reload
	imull	%r10d, %edx
	addl	%ecx, %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	movslq	%r15d, %r8
	imulq	%r11, %r8
	cmpl	$1, %ebx
	movl	1472(%rsp), %edx        # 4-byte Reload
	cmovgl	%r9d, %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	movq	1568(%rsp), %r9         # 8-byte Reload
	movslq	%esi, %rsi
	subq	%rsi, %r8
	leal	1(%rbp), %esi
	vmovd	%esi, %xmm11
	leal	1(%r10), %esi
	movq	%r10, %rbp
	vmovd	%esi, %xmm12
	testl	%ecx, %ecx
	movl	1248(%rsp), %edx        # 4-byte Reload
	cmovgl	%edi, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	testl	%ebx, %ebx
	movl	1184(%rsp), %r11d       # 4-byte Reload
	cmovgl	%r14d, %r11d
	movl	%eax, %r14d
	shll	$10, %r14d
	cmpl	$2, %ecx
	movl	1344(%rsp), %esi        # 4-byte Reload
	cmovgl	1200(%rsp), %esi        # 4-byte Folded Reload
	movl	%esi, 1344(%rsp)        # 4-byte Spill
	cltq
	vpbroadcastd	%xmm0, %xmm13
	vmovdqa	%xmm13, 608(%rsp)       # 16-byte Spill
	movq	1280(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx,2), %ecx
	movq	%rcx, 592(%rsp)         # 8-byte Spill
	vmovss	.LCPI156_0(%rip), %xmm6 # xmm6 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm1          # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm6, %xmm0
	vmovss	8(%rsp), %xmm4          # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm7
	vmovss	16(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm7, %xmm7
	vaddss	%xmm7, %xmm1, %xmm7
	vmovss	12(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm1, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm3, %xmm3
	movq	672(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm14
	vmovdqa	%xmm14, 1168(%rsp)      # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm1
	vmovdqa	.LCPI156_1(%rip), %xmm0 # xmm0 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 576(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 560(%rsp)        # 16-byte Spill
	vmovd	%ebp, %xmm1
	vpbroadcastd	%xmm1, %xmm10
	vmovdqa	%xmm10, 336(%rsp)       # 16-byte Spill
	movl	1264(%rsp), %edi        # 4-byte Reload
	vmovd	%edi, %xmm1
	vmovd	%r12d, %xmm2
	vpsubd	%xmm1, %xmm2, %xmm4
	vmovd	%edx, %xmm2
	vpsubd	%xmm1, %xmm2, %xmm15
	vmovd	%esi, %xmm5
	vpsubd	%xmm1, %xmm5, %xmm8
	movq	160(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm5
	vbroadcastss	%xmm5, %xmm1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm2
	vmovdqa	%xmm2, 1104(%rsp)       # 16-byte Spill
	movslq	1472(%rsp), %rsi        # 4-byte Folded Reload
	leaq	(%rsi,%r8), %r10
	vpbroadcastd	%xmm11, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovdqa	%xmm1, 544(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovdqa	%xmm1, 528(%rsp)        # 16-byte Spill
	vmovd	%edi, %xmm5
	vbroadcastss	%xmm5, %xmm1
	vmovaps	%xmm1, 1264(%rsp)       # 16-byte Spill
	vmovd	%r12d, %xmm5
	vbroadcastss	%xmm5, %xmm1
	vmovaps	%xmm1, 512(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm1
	vmovdqa	%xmm1, 1280(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm3, %xmm1
	vmovaps	%xmm1, 1136(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm7, %xmm1
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	movq	24(%rsp), %rcx          # 8-byte Reload
	leal	3(%rcx), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovdqa	%xmm1, 496(%rsp)        # 16-byte Spill
	movq	%rbp, %rdx
	leal	3(%rdx), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm1
	vmovdqa	%xmm1, 480(%rsp)        # 16-byte Spill
	vmovss	52(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm6, %xmm3
	vmovss	80(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	68(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm4
	vmulss	%xmm4, %xmm3, %xmm4
	vmulss	%xmm5, %xmm3, %xmm3
	vmovss	84(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vdivss	%xmm4, %xmm1, %xmm4
	movslq	%r11d, %rdi
	vmovd	%ecx, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovdqa	%xmm1, 464(%rsp)        # 16-byte Spill
	leal	4(%rcx), %ebp
	vmovd	%ebp, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovdqa	%xmm1, 448(%rsp)        # 16-byte Spill
	cmpl	$2, %ebx
	movl	1216(%rsp), %ebx        # 4-byte Reload
	cmovgl	1232(%rsp), %ebx        # 4-byte Folded Reload
	leal	4(%rdx), %ebp
	movq	%rdx, %rsi
	vmovd	%ebp, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm1
	vmovdqa	%xmm1, 416(%rsp)        # 16-byte Spill
	leal	(%r14,%r14,2), %ecx
	movl	%ecx, 408(%rsp)         # 4-byte Spill
	vmovss	20(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm6, %xmm5
	vmovss	96(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	92(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm6, %xmm6
	vmulss	%xmm6, %xmm5, %xmm6
	vmulss	%xmm1, %xmm5, %xmm5
	vmovss	88(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vdivss	%xmm6, %xmm1, %xmm6
	movslq	%ebx, %rcx
	leaq	(%rdi,%r8), %rdi
	leaq	(%rcx,%r8), %rcx
	shlq	$5, %rax
	addq	$40, %rax
	movl	%r15d, %ebx
	andl	$63, %ebx
	imulq	%rax, %rbx
	movq	128(%rsp), %rdx         # 8-byte Reload
	movq	%rdx, %rax
	sarq	$63, %rax
	andq	%rdx, %rax
	subq	%rax, %rbx
	movq	%rbx, 400(%rsp)         # 8-byte Spill
	leal	-1(%rsi), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm0, %xmm7, %xmm1
	vmovdqa	%xmm1, 384(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm10, %xmm1
	vmovdqa	%xmm1, 368(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	movl	1248(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	movl	1344(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%r10,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	vpabsd	%xmm13, %xmm0
	vmovdqa	%xmm0, 1088(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm13, %xmm0
	vmovdqa	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI156_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vbroadcastss	.LCPI156_3(%rip), %xmm0
	vmovaps	%xmm0, 1184(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_4(%rip), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_5(%rip), %xmm0
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_6(%rip), %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB156_35:                             # %for gH.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movq	1328(%rsp), %r15        # 8-byte Reload
	leal	(%r15,%r13,8), %esi
	movl	%esi, 976(%rsp)         # 4-byte Spill
	movl	%esi, %r12d
	movq	440(%rsp), %rax         # 8-byte Reload
	subl	%eax, %r12d
	movq	%r12, 864(%rsp)         # 8-byte Spill
	leal	-2(%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	608(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r10d
	movl	%r10d, 960(%rsp)        # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r14d
	movl	%r14d, 944(%rsp)        # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	movl	%r11d, 912(%rsp)        # 4-byte Spill
	cltd
	idivl	%r11d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1088(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vmovdqa	%xmm2, %xmm5
	vpaddd	%xmm0, %xmm1, %xmm0
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13,8), %eax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	movq	%rax, %rbp
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm8
	vmovdqa	%xmm8, 832(%rsp)        # 16-byte Spill
	vmovdqa	576(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vmovdqa	560(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1152(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm11, %xmm2
	vmovdqa	1072(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vmovdqa	1104(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	leal	-2(%r15,%r13,8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	leal	-1(%r12), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpmulld	%xmm14, %xmm0, %xmm0
	vmovdqa	%xmm0, 1008(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rbp,%rax), %ebp
	vmovd	%edi, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	leal	-3(%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm5, %xmm2, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%r10d
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovd	%edx, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r14d
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edx, %xmm2, %xmm2
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm2, %xmm1
	vpsrad	$31, %xmm1, %xmm2
	vpand	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm5, %xmm12
	vpaddd	%xmm1, %xmm2, %xmm6
	vmovdqa	544(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm1, %xmm1
	vpxor	%xmm4, %xmm1, %xmm1
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vmovdqa	528(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm11, %xmm2
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	movq	%r15, %rdi
	leal	-1(%rdi,%r13,8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm10, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm14, %xmm0, %xmm0
	vmovdqa	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpaddd	1280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	movslq	%ebp, %r15
	movq	%r13, %rbp
	movq	1560(%rsp), %r13        # 8-byte Reload
	vmovups	12312(%r13,%r15,4), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	vmovups	12328(%r13,%r15,4), %xmm1
	vmovdqa	496(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm0, %xmm0
	vpxor	%xmm4, %xmm0, %xmm0
	vmovdqa	480(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm2, %xmm7
	vpor	%xmm0, %xmm7, %xmm7
	vpcmpgtd	%xmm6, %xmm11, %xmm0
	vpsubd	%xmm6, %xmm13, %xmm5
	vblendvps	%xmm0, %xmm6, %xmm5, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	leal	-3(%rdi,%rbp,8), %eax
	vmovd	%eax, %xmm5
	vmovups	12320(%r13,%r15,4), %xmm2
	vmovaps	%xmm2, 1216(%rsp)       # 16-byte Spill
	vmovd	%r12d, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vmovd	%xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vpextrd	$2, %xmm4, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vblendvps	%xmm7, %xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 992(%rsp)        # 16-byte Spill
	vmovd	%edi, %xmm0
	vpextrd	$3, %xmm4, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm4
	vmovdqa	%xmm12, %xmm6
	vpand	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	1264(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	1008(%rsp), %xmm3       # 16-byte Reload
	vpsubd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 896(%rsp)        # 16-byte Spill
	vpaddd	512(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm3, 1008(%rsp)       # 16-byte Spill
	vmovdqa	464(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm5
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm5
	vmovdqa	368(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm7
	vpor	%xmm5, %xmm7, %xmm7
	vpcmpgtd	%xmm0, %xmm11, %xmm5
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm5, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	leal	1(%r12), %eax
	movl	%ebx, %r12d
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %r8d
	vpmaxsd	%xmm10, %xmm0, %xmm0
	vpaddd	%xmm9, %xmm8, %xmm5
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vblendvps	%xmm7, %xmm0, %xmm5, %xmm12
	movq	1328(%rsp), %rax        # 8-byte Reload
	leal	1(%rax,%rbp,8), %r14d
	movq	%rbp, %r10
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm3
	vpand	%xmm6, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm0
	vpcmpgtd	%xmm0, %xmm11, %xmm3
	vpsubd	%xmm0, %xmm13, %xmm5
	vblendvps	%xmm3, %xmm0, %xmm5, %xmm0
	vmovdqa	352(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm3, %xmm3
	vpxor	%xmm4, %xmm3, %xmm3
	vmovdqa	384(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm8, %xmm5, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpaddd	%xmm10, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm10, %xmm0, %xmm0
	vmovd	%r14d, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpmaxsd	%xmm10, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm0, %xmm5, %xmm11
	vmovaps	1248(%rsp), %xmm4       # 16-byte Reload
	vmulps	1472(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	1232(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, %xmm1, %xmm7, %xmm5 # xmm5 = xmm7[1,3],xmm1[1,3]
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm5, %xmm5
	vmovaps	1136(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vpmulld	992(%rsp), %xmm14, %xmm13 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm3, %xmm3
	vmovaps	%xmm0, %xmm5
	vshufps	$136, %xmm1, %xmm7, %xmm1 # xmm1 = xmm7[0,2],xmm1[0,2]
	vpxor	%xmm10, %xmm10, %xmm10
	vmovaps	1184(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm8
	vmovups	%ymm8, 1472(%rsp)       # 32-byte Spill
	vmulps	1008(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm3, %xmm1, %xmm1
	vpmulld	%xmm14, %xmm12, %xmm3
	vmovdqa	%xmm3, 848(%rsp)        # 16-byte Spill
	vpaddd	1280(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rbp
	vmovq	%xmm3, %rbx
	movq	%rbx, %rdi
	sarq	$32, %rdi
	movq	%rbp, %rsi
	sarq	$32, %rsi
	movq	1200(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovaps	%xmm8, %xmm9
	testl	1504(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB156_37
# BB#36:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB156_37:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vpmulld	%xmm14, %xmm11, %xmm7
	vpsubd	%xmm2, %xmm13, %xmm15
	vmovdqa	%xmm13, 1008(%rsp)      # 16-byte Spill
	vmovdqa	1344(%rsp), %xmm3       # 16-byte Reload
	vpsubd	%xmm2, %xmm3, %xmm3
	movl	%r14d, %r8d
	vmovaps	%xmm0, %xmm8
	vminps	%xmm8, %xmm1, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm2
	vmovups	%ymm2, 1344(%rsp)       # 32-byte Spill
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 12336(%r13,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vmovups	(%r13,%rdx,4), %xmm6
	vmovups	32(%r13,%rax,4), %xmm4
	vmovups	48(%r13,%rax,4), %xmm5
	vmovups	(%r13,%rcx,4), %xmm13
	vmovups	40(%r13,%rax,4), %xmm11
	vmulps	1056(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
	andl	$1, %r8d
	je	.LBB156_38
# BB#45:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovdqa	%xmm7, 768(%rsp)        # 16-byte Spill
	vmovdqa	%xmm3, %xmm10
	vmovaps	%xmm6, 704(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 736(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, 784(%rsp)       # 16-byte Spill
	movq	%r15, 880(%rsp)         # 8-byte Spill
	movl	%r14d, 1232(%rsp)       # 4-byte Spill
	movl	%r12d, 816(%rsp)        # 4-byte Spill
	vmovaps	%xmm8, %xmm14
	movq	%r10, 992(%rsp)         # 8-byte Spill
	movq	%r13, %r14
	vmovaps	%xmm9, %xmm0
	jmp	.LBB156_46
	.align	16, 0x90
.LBB156_38:                             #   in Loop: Header=BB156_35 Depth=1
	movq	%r15, 880(%rsp)         # 8-byte Spill
	movl	%r14d, 1232(%rsp)       # 4-byte Spill
	movl	%r12d, 816(%rsp)        # 4-byte Spill
	movq	%r10, 992(%rsp)         # 8-byte Spill
	movq	%r13, %r14
	vmovdqa	320(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm15, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rax
	movslq	%eax, %r8
	sarq	$32, %rax
	movslq	%edx, %r10
	sarq	$32, %rdx
	vpaddd	304(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm7, 768(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vpaddd	%xmm1, %xmm3, %xmm0
	vmovdqa	%xmm3, %xmm10
	vpextrq	$1, %xmm0, %r13
	movq	%r9, %rsi
	vmovq	%xmm0, %r9
	movslq	%r9d, %r12
	sarq	$32, %r9
	movslq	%r13d, %r11
	sarq	$32, %r13
	vmovss	(%rsi,%r8,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	192(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm4, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm4[1,3]
	vmovaps	%xmm6, 704(%rsp)        # 16-byte Spill
	vmovaps	272(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	288(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovss	(%rsi,%rbx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm5, 736(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm5, %xmm4, %xmm5 # xmm5 = xmm4[1,3],xmm5[1,3]
	vmovaps	%xmm4, 752(%rsp)        # 16-byte Spill
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm1, %xmm5, %xmm1
	vmovss	(%rsi,%r12,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rsi, %r9
	vinsertps	$32, (%r9,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vshufps	$221, %xmm11, %xmm13, %xmm14 # xmm14 = xmm13[1,3],xmm11[1,3]
	vmovaps	%xmm11, 784(%rsp)       # 16-byte Spill
	vsubps	%xmm7, %xmm14, %xmm7
	vmulps	%xmm7, %xmm6, %xmm7
	vmulps	%xmm5, %xmm7, %xmm5
	vminps	%xmm8, %xmm1, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm1, %xmm1
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm2, %xmm5, %xmm5
	vpxor	%xmm3, %xmm3, %xmm3
	vmovaps	1040(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm2, %xmm5
	vminps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm8, %xmm14
	vmaxps	%xmm3, %xmm0, %xmm0
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm12, %xmm1, %xmm0
.LBB156_46:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movq	1200(%rsp), %rcx        # 8-byte Reload
	vmovaps	%xmm12, %xmm11
	addl	408(%rsp), %ecx         # 4-byte Folded Reload
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	jne	.LBB156_48
# BB#47:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm9, %xmm0
.LBB156_48:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movl	1232(%rsp), %eax        # 4-byte Reload
	movslq	%ecx, %rcx
	vmovups	24592(%r14,%rcx,4), %xmm3
	vmovups	24608(%r14,%rcx,4), %xmm12
	vmovups	24624(%r14,%rcx,4), %xmm4
	vmovups	24600(%r14,%rcx,4), %xmm8
	vmovups	24616(%r14,%rcx,4), %xmm9
	andl	$1, %eax
	jne	.LBB156_49
# BB#50:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm3, 688(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 800(%rsp)       # 16-byte Spill
	movq	%r14, %r13
	vmovaps	%xmm0, %xmm15
	vmovdqa	1264(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1248(%rsp), %xmm3       # 16-byte Reload
	movl	976(%rsp), %r14d        # 4-byte Reload
	movl	816(%rsp), %ecx         # 4-byte Reload
	movl	960(%rsp), %edi         # 4-byte Reload
	movl	944(%rsp), %ebx         # 4-byte Reload
	movl	912(%rsp), %ebp         # 4-byte Reload
	movq	864(%rsp), %rax         # 8-byte Reload
	vmovaps	1216(%rsp), %xmm7       # 16-byte Reload
	vxorps	%xmm13, %xmm13, %xmm13
	jmp	.LBB156_51
	.align	16, 0x90
.LBB156_49:                             #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm13, 800(%rsp)       # 16-byte Spill
	movq	%r14, %r13
	vmovdqa	256(%rsp), %xmm5        # 16-byte Reload
	vpaddd	%xmm5, %xmm15, %xmm1
	vpextrq	$1, %xmm1, %rdx
	vmovq	%xmm1, %rax
	movslq	%eax, %r8
	sarq	$32, %rax
	movslq	%edx, %r10
	sarq	$32, %rdx
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpaddd	240(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rdi
	vmovq	%xmm1, %rbp
	movslq	%ebp, %r11
	sarq	$32, %rbp
	movslq	%edi, %r14
	sarq	$32, %rdi
	vpaddd	%xmm5, %xmm10, %xmm1
	vpextrq	$1, %xmm1, %rsi
	vmovq	%xmm1, %rbx
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%esi, %rcx
	sarq	$32, %rsi
	vmovss	(%r9,%r8,4), %xmm1      # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	176(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm3, 688(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm12, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm12[1,3]
	vmovaps	208(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	224(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	vmovss	(%r9,%r11,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm2, %xmm3
	vmovaps	%xmm4, 720(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm4, %xmm12, %xmm4 # xmm4 = xmm12[1,3],xmm4[1,3]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	vmovss	(%r9,%r12,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vshufps	$221, %xmm9, %xmm8, %xmm5 # xmm5 = xmm8[1,3],xmm9[1,3]
	vmovaps	%xmm8, 768(%rsp)        # 16-byte Spill
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm3, %xmm3
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm3, %xmm3
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vmovaps	1040(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm2, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm15
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm1, %xmm15
	vmovdqa	1264(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1248(%rsp), %xmm3       # 16-byte Reload
	movl	976(%rsp), %r14d        # 4-byte Reload
	movl	816(%rsp), %ecx         # 4-byte Reload
	movl	960(%rsp), %edi         # 4-byte Reload
	movl	944(%rsp), %ebx         # 4-byte Reload
	movl	912(%rsp), %ebp         # 4-byte Reload
	movq	864(%rsp), %rax         # 8-byte Reload
	vmovaps	1216(%rsp), %xmm7       # 16-byte Reload
.LBB156_51:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovdqa	832(%rsp), %xmm5        # 16-byte Reload
	vmovaps	%xmm9, 1216(%rsp)       # 16-byte Spill
	vmovdqa	1008(%rsp), %xmm1       # 16-byte Reload
	vpaddd	1280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %r10
	vmovq	%xmm1, %rsi
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	movq	%r13, %r11
	je	.LBB156_53
# BB#52:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm0, %xmm15
.LBB156_53:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm12, 1200(%rsp)      # 16-byte Spill
	addl	$-4, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	%rsi, %rax
	sarq	$32, %rax
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	1088(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	1152(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm1
	vmovdqa	1072(%rsp), %xmm2       # 16-byte Reload
	vpsubd	%xmm0, %xmm2, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	movq	1328(%rsp), %rcx        # 8-byte Reload
	movq	992(%rsp), %r13         # 8-byte Reload
	leal	-4(%rcx,%r13,8), %ecx
	vmovd	%ecx, %xmm1
	movq	%r10, %rcx
	sarq	$32, %rcx
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm4
	vpxor	.LCPI156_9(%rip), %xmm4, %xmm4
	vmovdqa	416(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm2, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vmovdqa	336(%rsp), %xmm2        # 16-byte Reload
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	1104(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm0
	vpmaxsd	%xmm2, %xmm0, %xmm0
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpminsd	%xmm10, %xmm1, %xmm1
	vpmaxsd	%xmm2, %xmm1, %xmm1
	vmovdqa	%xmm2, %xmm10
	vblendvps	%xmm4, %xmm0, %xmm1, %xmm1
	vmovups	1344(%rsp), %ymm0       # 32-byte Reload
	testl	1504(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB156_55
# BB#54:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB156_55:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vpmulld	1168(%rsp), %xmm1, %xmm12 # 16-byte Folded Reload
	movl	%r14d, %edx
	movslq	%esi, %rsi
	movslq	%r10d, %rdi
	vmovss	(%r9,%rsi,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm3, %xmm4
	movq	880(%rsp), %rax         # 8-byte Reload
	vmovups	12304(%r11,%rax,4), %xmm5
	vshufps	$221, %xmm7, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm7[1,3]
	vsubps	1120(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vmulps	1136(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vmovups	1472(%rsp), %ymm1       # 32-byte Reload
	vaddps	%xmm4, %xmm1, %xmm4
	vmulps	1056(%rsp), %xmm4, %xmm11 # 16-byte Folded Reload
	andl	$1, %edx
	je	.LBB156_56
# BB#57:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm14, 1184(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, 1248(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm4
	jmp	.LBB156_58
	.align	16, 0x90
.LBB156_56:                             #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm3, 1248(%rsp)       # 16-byte Spill
	vpaddd	304(%rsp), %xmm12, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rsi
	vmovq	%xmm4, %rdi
	vmovdqa	848(%rsp), %xmm1        # 16-byte Reload
	vpsubd	%xmm8, %xmm1, %xmm4
	vmovdqa	320(%rsp), %xmm2        # 16-byte Reload
	vpaddd	%xmm2, %xmm4, %xmm4
	vpextrq	$1, %xmm4, %rdx
	vmovq	%xmm4, %rbp
	vpaddd	896(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %r8
	vmovq	%xmm4, %rcx
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r9,%rbx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	192(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmovaps	752(%rsp), %xmm3        # 16-byte Reload
	vmovaps	704(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, %xmm3, %xmm2, %xmm5 # xmm5 = xmm2[0,2],xmm3[0,2]
	vmovaps	272(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	288(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	movslq	%ebp, %rax
	sarq	$32, %rbp
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, 736(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm1, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	800(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 784(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm1, %xmm3
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	%xmm13, %xmm5, %xmm5
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vmovaps	1040(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm1, %xmm3
	vminps	%xmm14, %xmm4, %xmm4
	vmovaps	%xmm14, 1184(%rsp)      # 16-byte Spill
	vmaxps	%xmm13, %xmm4, %xmm4
	vsubps	%xmm4, %xmm3, %xmm4
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm1, %xmm4
.LBB156_58:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	jne	.LBB156_60
# BB#59:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm0, %xmm4
.LBB156_60:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movl	%r14d, %eax
	andl	$1, %eax
	jne	.LBB156_61
# BB#62:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm4, %xmm0
	jmp	.LBB156_63
	.align	16, 0x90
.LBB156_61:                             #   in Loop: Header=BB156_35 Depth=1
	vmovdqa	240(%rsp), %xmm2        # 16-byte Reload
	vpaddd	%xmm12, %xmm2, %xmm0
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	848(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vmovdqa	896(%rsp), %xmm0        # 16-byte Reload
	vpaddd	256(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	176(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vmovaps	688(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	208(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	224(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	movslq	%esi, %rax
	sarq	$32, %rsi
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vshufps	$136, 720(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	768(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1216(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm7, %xmm5
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmovaps	1184(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm13, %xmm1, %xmm1
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm2
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm3, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm1, %xmm0
.LBB156_63:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movq	1464(%rsp), %r15        # 8-byte Reload
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	je	.LBB156_65
# BB#64:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm4, %xmm0
.LBB156_65:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movl	%r14d, %eax
	orl	%r15d, %eax
	testb	$1, %al
	je	.LBB156_67
# BB#66:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovups	%ymm0, 1344(%rsp)       # 32-byte Spill
.LBB156_67:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	movq	%r11, 1560(%rsp)        # 8-byte Spill
	vmovdqa	%xmm8, 1264(%rsp)       # 16-byte Spill
	movl	1232(%rsp), %eax        # 4-byte Reload
	orl	%r15d, %eax
	testb	$1, %al
	je	.LBB156_69
# BB#68:                                # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	%xmm15, %xmm0
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
.LBB156_69:                             # %for gH.s0.v10.v104
                                        #   in Loop: Header=BB156_35 Depth=1
	vmovaps	.LCPI156_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	1472(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovaps	.LCPI156_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	1344(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r14d, %rax
	movq	400(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1400(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r13
	cmpl	640(%rsp), %r13d        # 4-byte Folded Reload
	vmovaps	1168(%rsp), %xmm14      # 16-byte Reload
	jne	.LBB156_35
.LBB156_70:                             # %end for gH.s0.v10.v105
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	movq	%r15, 1464(%rsp)        # 8-byte Spill
	movl	640(%rsp), %eax         # 4-byte Reload
	cmpl	-112(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB156_92
# BB#71:                                # %for gH.s0.v10.v108.preheader
	movq	112(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	movl	%edx, %r12d
	sarl	$31, %r12d
	andnl	%esi, %r12d, %ecx
	movl	%esi, %edi
	andl	%eax, %r12d
	orl	%ecx, %r12d
	movq	72(%rsp), %r10          # 8-byte Reload
	leal	(%r10,%rdx), %esi
	movl	%esi, 1248(%rsp)        # 4-byte Spill
	leal	-1(%rdx,%rdx), %r15d
	leal	-1(%r10,%rdx), %r11d
	movq	%rdx, %rbp
	movl	$2, %eax
	subl	%r10d, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r15d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %ebp
	movq	%rbp, %rbx
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	cmpl	%ecx, %r11d
	cmovlel	%r11d, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1504(%rsp)        # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %eax
	cmovll	%r11d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %esi
	cmovll	%ecx, %eax
	movl	%eax, 1472(%rsp)        # 4-byte Spill
	movl	%r10d, %eax
	negl	%eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r15d, %ebp
	subl	%eax, %ebp
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebp
	addl	%r10d, %ebp
	cmpl	%ebp, %r11d
	cmovlel	%r11d, %ebp
	cmpl	%r10d, %ebp
	cmovll	%r10d, %ebp
	testl	%esi, %esi
	movl	$0, %eax
	cmovlel	%r11d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	testl	%esi, %esi
	cmovlel	%ebp, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movq	144(%rsp), %r8          # 8-byte Reload
	leal	(%r8,%r8), %edx
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r8d, %edi
	sarl	$31, %edi
	andnl	%edx, %edi, %ecx
	movl	%edx, %esi
	andl	%eax, %edi
	orl	%ecx, %edi
	movq	56(%rsp), %rcx          # 8-byte Reload
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	leal	-1(%r8,%r8), %r9d
	movl	%r9d, %r14d
	subl	%eax, %r14d
	cmpl	%eax, %r8d
	cmovgl	%eax, %r14d
	addl	%ecx, %r14d
	leal	-1(%rcx,%r8), %r13d
	cmpl	%r14d, %r13d
	cmovlel	%r13d, %r14d
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	leal	(%rcx,%r8), %ebx
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovlel	%r13d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	testl	%ebx, %ebx
	cmovlel	%r14d, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%ecx, %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%r9d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r8d
	cmovgl	%eax, %esi
	addl	%ecx, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	cmpl	$3, %ebx
	movl	$2, %eax
	cmovll	%r13d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	$3, %ebx
	cmovll	%esi, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	subl	%eax, %r15d
	movq	112(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %r15d
	addl	%r10d, %r15d
	cmpl	%r15d, %r11d
	cmovlel	%r11d, %r15d
	cmpl	%r10d, %r15d
	cmovll	%r10d, %r15d
	movl	1248(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	$0, %r12d
	cmovgel	%r12d, %r11d
	movzbl	%al, %eax
	orl	%eax, %r11d
	cmpl	%r10d, %r11d
	cmovll	%r10d, %r11d
	cmpl	$2, %edx
	cmovll	%r15d, %r11d
	movl	$1, %eax
	subl	%ecx, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	subl	%eax, %r9d
	cmpl	%eax, %r8d
	cmovgl	%eax, %r9d
	addl	%ecx, %r9d
	cmpl	%r9d, %r13d
	cmovlel	%r13d, %r9d
	cmpl	%ecx, %r9d
	cmovll	%ecx, %r9d
	cmpl	$1, %ebx
	setg	%al
	cmpl	$2, %ebx
	cmovgel	%r12d, %r13d
	movzbl	%al, %eax
	orl	%eax, %r13d
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	cmpl	$2, %ebx
	cmovll	%r9d, %r13d
	movq	32(%rsp), %r8           # 8-byte Reload
	movl	%r8d, %edi
	movq	40(%rsp), %rax          # 8-byte Reload
	imull	%eax, %edi
	movq	1464(%rsp), %rbx        # 8-byte Reload
	movl	%ebx, %eax
	andl	$1, %eax
	movl	%eax, 1264(%rsp)        # 4-byte Spill
	addl	%r10d, %edi
	cmpl	$1, %ecx
	cmovgl	%r9d, %r13d
	movq	672(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r12d
	movq	440(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r12d
	movq	936(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r9d
	sarl	$5, %r9d
	movslq	%ebx, %rdx
	imulq	%r8, %rdx
	addl	%ecx, %r12d
	cmpl	$1, %r10d
	cmovgl	%r15d, %r11d
	movslq	%edi, %rdi
	subq	%rdi, %rdx
	movslq	%r11d, %rdi
	leaq	(%rdi,%rdx), %rdi
	movq	104(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	cmpl	$2, %ecx
	movl	1232(%rsp), %r11d       # 4-byte Reload
	cmovgl	%esi, %r11d
	testl	%ecx, %ecx
	movl	1280(%rsp), %r8d        # 4-byte Reload
	cmovgl	%r14d, %r8d
	testl	%r10d, %r10d
	movl	1344(%rsp), %ecx        # 4-byte Reload
	cmovgl	%ebp, %ecx
	movslq	%ecx, %rsi
	cmpl	$2, %r10d
	movl	1472(%rsp), %ecx        # 4-byte Reload
	cmovgl	1504(%rsp), %ecx        # 4-byte Folded Reload
	movslq	%ecx, %rdi
	leaq	(%rsi,%rdx), %rsi
	leaq	(%rdi,%rdx), %rdx
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	movslq	%r9d, %rdx
	shlq	$5, %rdx
	addq	$40, %rdx
	movl	%ebx, %ecx
	andl	$63, %ecx
	imulq	%rdx, %rcx
	movq	128(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rdx
	sarq	$63, %rdx
	andq	%rax, %rdx
	subq	%rdx, %rcx
	movq	%rcx, 992(%rsp)         # 8-byte Spill
	leal	(%r9,%r9,2), %edi
	movl	%edi, %ebp
	shll	$10, %ebp
	leal	8(%rbx), %ecx
	subl	100(%rsp), %ecx         # 4-byte Folded Reload
	movq	936(%rsp), %rbx         # 8-byte Reload
	andl	$-32, %ebx
	addl	$64, %ebx
	imull	%ecx, %ebx
	movl	656(%rsp), %edx         # 4-byte Reload
	notl	%edx
	movq	-8(%rsp), %rax          # 8-byte Reload
	movl	-20(%rsp), %esi         # 4-byte Reload
	cmpl	%eax, %esi
	movl	%eax, %ecx
	cmovgel	%esi, %ecx
	movl	-24(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	-28(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	-16(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	testl	%ecx, %ecx
	movl	$0, %eax
	cmovsl	%eax, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leal	(,%rcx,8), %edx
	subl	%edx, %ebx
	addl	$-8, %ebp
	movq	%rbp, 960(%rsp)         # 8-byte Spill
	shll	$9, %edi
	addl	$-8, %edi
	movq	%rdi, 976(%rsp)         # 8-byte Spill
	movq	1328(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edi
	subl	%edx, %edi
	movq	-48(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	negl	%edx
	movl	-52(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-56(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-60(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-64(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-68(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-72(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-76(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-80(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-84(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-88(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-92(%rsp), %esi         # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-96(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-100(%rsp), %esi        # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movl	-104(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movl	-108(%rsp), %esi        # 4-byte Reload
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	movq	-40(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	movq	1408(%rsp), %rax        # 8-byte Reload
	movl	%eax, %esi
	notl	%esi
	cmpl	%esi, %edx
	cmovgel	%edx, %esi
	negl	%esi
	movl	%ecx, %edx
	notl	%edx
	cmpl	%edx, %esi
	cmovgel	%esi, %edx
	vmovss	.LCPI156_0(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm3          # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	8(%rsp), %xmm5          # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm1, %xmm2
	vmovss	16(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	12(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm1
	movslq	%r12d, %r12
	movq	%r12, 1232(%rsp)        # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, 1216(%rsp)        # 8-byte Spill
	leal	1(%rcx,%rdx), %esi
	movq	672(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %rax
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 1184(%rsp)       # 16-byte Spill
	vmovss	52(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	68(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm1, %xmm2
	vmovss	84(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	80(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm1
	vmovss	20(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm0, %xmm0
	vmovss	92(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm0, %xmm3
	vmovss	88(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vdivss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	96(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm4, %xmm5
	vmulss	%xmm5, %xmm0, %xmm0
	vdivss	%xmm0, %xmm6, %xmm0
	movslq	%r8d, %rcx
	movq	%rcx, 896(%rsp)         # 8-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 880(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	movslq	%r11d, %rcx
	movq	%rcx, 848(%rsp)         # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 832(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI156_3(%rip), %xmm15
	vbroadcastss	.LCPI156_4(%rip), %xmm0
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_5(%rip), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_6(%rip), %xmm13
	vmovaps	%xmm13, 800(%rsp)       # 16-byte Spill
	vxorps	%xmm14, %xmm14, %xmm14
	.align	16, 0x90
.LBB156_72:                             # %for gH.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movl	%esi, 1104(%rsp)        # 4-byte Spill
	movq	%rdi, 1120(%rsp)        # 8-byte Spill
	movl	1264(%rsp), %r14d       # 4-byte Reload
	testl	%r14d, %r14d
	setne	%r8b
	sete	1472(%rsp)              # 1-byte Folded Spill
	leal	-8(%rdi), %r9d
	movl	%r9d, %ecx
	andl	$1, %ecx
	movl	%ecx, 1344(%rsp)        # 4-byte Spill
	sete	1280(%rsp)              # 1-byte Folded Spill
	movslq	%r9d, %r13
	leaq	-1(%r13), %rcx
	movq	672(%rsp), %rdi         # 8-byte Reload
	imulq	%rdi, %rcx
	subq	%r12, %rcx
	movq	%rcx, 1088(%rsp)        # 8-byte Spill
	movq	1216(%rsp), %r11        # 8-byte Reload
	leaq	(%rcx,%r11), %rdx
	movq	1568(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rdx,4), %rsi
	movq	%rbx, %r10
	leaq	(%rsi,%rax,4), %rbx
	movq	%r12, %rbp
	leaq	(%rbx,%rax,4), %r15
	vmovss	(%rcx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	leaq	-3(%r13), %r15
	imulq	%rdi, %r15
	subq	%rbp, %r15
	leaq	(%r15,%r11), %rdx
	leaq	(%rcx,%rdx,4), %rsi
	leaq	(%rsi,%rax,4), %rbx
	vmovss	(%rcx,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	leaq	(%rbx,%rax,4), %rdx
	vinsertps	$16, (%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%r10, %rbx
	vinsertps	$48, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	leaq	-2(%r13), %r12
	imulq	%rdi, %r12
	subq	%rbp, %r12
	leaq	(%r12,%r11), %rdx
	leaq	(%rcx,%rdx,4), %rsi
	vmovss	(%rcx,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movq	%rcx, %r10
	leaq	(%rsi,%rax,4), %rdx
	vinsertps	$16, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rdx,%rax,4), %rsi
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm3, %xmm9 # xmm9 = xmm3[0,1,2],mem[0]
	movq	%r13, %rdx
	imulq	%rdi, %rdx
	movl	%r13d, %esi
	andl	%r14d, %esi
	movq	976(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rbx), %esi
	movslq	%esi, %rsi
	movq	1560(%rsp), %rcx        # 8-byte Reload
	vmovups	12312(%rcx,%rsi,4), %xmm6
	vmovups	12328(%rcx,%rsi,4), %xmm4
	vshufps	$221, %xmm4, %xmm6, %xmm3 # xmm3 = xmm6[1,3],xmm4[1,3]
	vmovaps	1184(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	1200(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmulps	%xmm1, %xmm2, %xmm1
	vmovups	12304(%rcx,%rsi,4), %xmm3
	vmovups	12320(%rcx,%rsi,4), %xmm8
	vshufps	$221, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm8[1,3]
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovups	%ymm0, 1504(%rsp)       # 32-byte Spill
	vaddps	%xmm1, %xmm0, %xmm3
	vmulps	%xmm9, %xmm2, %xmm0
	vshufps	$136, %xmm4, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm4[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm6
	vmovaps	%xmm6, %xmm12
	jne	.LBB156_74
# BB#73:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vxorps	%xmm12, %xmm12, %xmm12
.LBB156_74:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	subq	%rbp, %rdx
	leaq	-4(%r13), %r14
	imulq	%rdi, %r14
	subq	%rbp, %r14
	movq	%rbx, 1136(%rsp)        # 8-byte Spill
	leal	-8(%rbx), %ebx
	movslq	%ebx, %rbx
	vmovups	32(%rcx,%rbx,4), %xmm11
	vmovups	48(%rcx,%rbx,4), %xmm10
	movq	%rbx, %rbp
	vmovups	40(%rcx,%rbx,4), %xmm5
	orq	$4, %rbx
	vmovups	(%rcx,%rbx,4), %xmm7
	orq	$6, %rbp
	vmovups	(%rcx,%rbp,4), %xmm0
	vmulps	%xmm13, %xmm3, %xmm9
	andb	1280(%rsp), %r8b        # 1-byte Folded Reload
	jne	.LBB156_75
# BB#76:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovaps	%xmm7, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1024(%rsp)      # 16-byte Spill
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1072(%rsp)       # 16-byte Spill
	movq	%rdi, %r11
	jmp	.LBB156_77
	.align	16, 0x90
.LBB156_75:                             #   in Loop: Header=BB156_72 Depth=1
	movq	%rdi, %r11
	movb	%r8b, 1280(%rsp)        # 1-byte Spill
	movq	896(%rsp), %r8          # 8-byte Reload
	leaq	(%r14,%r8), %rbp
	leaq	(%r10,%rbp,4), %rbx
	leaq	(%rbx,%rax,4), %rdi
	leaq	(%rdi,%rax,4), %rcx
	vmovss	(%r10,%rbp,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vshufps	$136, %xmm11, %xmm7, %xmm4 # xmm4 = xmm7[0,2],xmm11[0,2]
	vmovaps	%xmm7, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1024(%rsp)      # 16-byte Spill
	vmovaps	864(%rsp), %xmm1        # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vmovaps	880(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm12
	leaq	(%rdx,%r8), %rcx
	leaq	(%r10,%rcx,4), %rdi
	leaq	(%rdi,%rax,4), %rbp
	leaq	(%rbp,%rax,4), %rbx
	vmovss	(%r10,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vmovaps	%xmm0, %xmm3
	vmovaps	%xmm3, 1040(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm10, %xmm11, %xmm0 # xmm0 = xmm11[0,2],xmm10[0,2]
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	leaq	(%r12,%r8), %rcx
	movb	1280(%rsp), %r8b        # 1-byte Reload
	leaq	(%r10,%rcx,4), %rdi
	leaq	(%rdi,%rax,4), %rbp
	leaq	(%rbp,%rax,4), %rbx
	vmovss	(%r10,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vmovaps	%ymm6, %ymm2
	vshufps	$136, %xmm5, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm5[0,2]
	vmovaps	%xmm5, 1056(%rsp)       # 16-byte Spill
	vsubps	%xmm1, %xmm6, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmulps	%xmm6, %xmm4, %xmm4
	vmovaps	%ymm2, %ymm6
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vmovaps	1168(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm1, %xmm4
	vminps	%xmm15, %xmm12, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm12
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm9, %xmm0, %xmm12
	vmovaps	%xmm9, 1072(%rsp)       # 16-byte Spill
.LBB156_77:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	movq	1216(%rsp), %rcx        # 8-byte Reload
	leaq	(%rdx,%rcx), %rbp
	vmovups	1504(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%xmm0, %xmm9
	testb	%r8b, %r8b
	jne	.LBB156_79
# BB#78:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB156_79:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	leaq	1(%r13), %r8
	imulq	%r11, %r8
	subq	1232(%rsp), %r8         # 8-byte Folded Reload
	movl	1264(%rsp), %ecx        # 4-byte Reload
	leaq	(%r10,%rbp,4), %rdi
	leaq	(%rdi,%rax,4), %rbx
	vmovss	(%r10,%rbp,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rbx,%rax,4), %rbp
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	movq	1560(%rsp), %rdi        # 8-byte Reload
	vshufps	$136, 12336(%rdi,%rsi,4), %xmm8, %xmm3 # xmm3 = xmm8[0,2],mem[0,2]
	vsubps	1184(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm0, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vaddps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm13, %xmm0, %xmm7
	andl	%r13d, %ecx
	jne	.LBB156_80
# BB#81:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovups	%ymm6, 1280(%rsp)       # 32-byte Spill
	jmp	.LBB156_82
	.align	16, 0x90
.LBB156_80:                             #   in Loop: Header=BB156_72 Depth=1
	vmovups	%ymm6, 1280(%rsp)       # 32-byte Spill
	movq	896(%rsp), %rcx         # 8-byte Reload
	leaq	(%r15,%rcx), %rsi
	leaq	(%r8,%rcx), %rdi
	movq	1088(%rsp), %rbp        # 8-byte Reload
	leaq	(%rbp,%rcx), %r11
	leaq	(%r10,%rsi,4), %rbp
	leaq	(%rbp,%rax,4), %rbx
	leaq	(%rbx,%rax,4), %rcx
	vmovss	(%r10,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbp,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1008(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm11[1,3]
	vmovaps	864(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	880(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm1
	leaq	(%r10,%rdi,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rbx
	vmovss	(%r10,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 1024(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm11[1,3],mem[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	leaq	(%r10,%r11,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	vmovss	(%r10,%r11,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm2, %xmm3
	vmovaps	1040(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[1,3],mem[1,3]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm4, %xmm5
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vmovaps	1168(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm2, %xmm3
	vsubps	%xmm1, %xmm3, %xmm9
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm0, %xmm9
.LBB156_82:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	movq	1464(%rsp), %r11        # 8-byte Reload
	movq	1136(%rsp), %rdi        # 8-byte Reload
	movq	960(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movslq	%ecx, %rcx
	movq	1560(%rsp), %rsi        # 8-byte Reload
	vmovups	24592(%rsi,%rcx,4), %xmm13
	vmovups	24608(%rsi,%rcx,4), %xmm6
	vmovups	24624(%rsi,%rcx,4), %xmm11
	vmovups	24600(%rsi,%rcx,4), %xmm8
	vmovups	24616(%rsi,%rcx,4), %xmm10
	movb	1472(%rsp), %cl         # 1-byte Reload
	movl	1344(%rsp), %esi        # 4-byte Reload
	andb	%sil, %cl
	jne	.LBB156_83
# BB#84:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovaps	%xmm7, 1056(%rsp)       # 16-byte Spill
	movq	%rdi, %r10
	jmp	.LBB156_85
	.align	16, 0x90
.LBB156_83:                             #   in Loop: Header=BB156_72 Depth=1
	vmovaps	%xmm7, 1056(%rsp)       # 16-byte Spill
	movq	%rdi, %r10
	movq	848(%rsp), %rbp         # 8-byte Reload
	addq	%rbp, %r14
	movq	1568(%rsp), %rbx        # 8-byte Reload
	leaq	(%rbx,%r14,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	vmovss	(%rbx,%r14,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	912(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vshufps	$136, %xmm6, %xmm13, %xmm4 # xmm4 = xmm13[0,2],xmm6[0,2]
	vmovaps	816(%rsp), %xmm1        # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vmovaps	832(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	addq	%rbp, %rdx
	leaq	(%rbx,%rdx,4), %rcx
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%rsi,%rax,4), %rdi
	vmovss	(%rbx,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vshufps	$136, %xmm11, %xmm6, %xmm0 # xmm0 = xmm6[0,2],xmm11[0,2]
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	addq	%rbp, %r12
	leaq	(%rbx,%r12,4), %rcx
	leaq	(%rcx,%rax,4), %rdx
	leaq	(%rdx,%rax,4), %rsi
	vmovss	(%rbx,%r12,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vshufps	$136, %xmm10, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm1, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vmovaps	1168(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm1, %xmm4
	vminps	%xmm15, %xmm3, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm0, %xmm4, %xmm12
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1072(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
.LBB156_85:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	orl	%r11d, %r9d
	andl	$1, %r9d
	movq	%r10, %rbx
	je	.LBB156_87
# BB#86:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovaps	%xmm12, %xmm0
	vmovups	%ymm0, 1280(%rsp)       # 32-byte Spill
.LBB156_87:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	testl	%r9d, %r9d
	movq	1232(%rsp), %r12        # 8-byte Reload
	jne	.LBB156_89
# BB#88:                                #   in Loop: Header=BB156_72 Depth=1
	movq	848(%rsp), %rcx         # 8-byte Reload
	addq	%rcx, %r15
	addq	%rcx, %r8
	movq	1088(%rsp), %rbp        # 8-byte Reload
	addq	%rcx, %rbp
	movq	1568(%rsp), %rdi        # 8-byte Reload
	leaq	(%rdi,%r15,4), %rcx
	leaq	(%rcx,%rax,4), %rdx
	leaq	(%rdx,%rax,4), %rsi
	vmovss	(%rdi,%r15,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	912(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm6, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm6[1,3]
	vmovaps	816(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	832(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm1
	leaq	(%rdi,%r8,4), %rcx
	leaq	(%rcx,%rax,4), %rdx
	leaq	(%rdx,%rax,4), %rsi
	vmovss	(%rdi,%r8,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm11, %xmm6, %xmm3 # xmm3 = xmm6[1,3],xmm11[1,3]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	leaq	(%rdi,%rbp,4), %rcx
	leaq	(%rcx,%rax,4), %rdx
	leaq	(%rdx,%rax,4), %rsi
	vmovss	(%rdi,%rbp,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm2, %xmm3
	vshufps	$221, %xmm10, %xmm8, %xmm4 # xmm4 = xmm8[1,3],xmm10[1,3]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vmovaps	1168(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm2, %xmm3
	vsubps	%xmm1, %xmm3, %xmm9
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1056(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB156_89:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	movq	1120(%rsp), %rdi        # 8-byte Reload
	movl	1104(%rsp), %esi        # 4-byte Reload
	vmovaps	800(%rsp), %xmm13       # 16-byte Reload
	vmovups	1280(%rsp), %ymm2       # 32-byte Reload
	movl	1344(%rsp), %ecx        # 4-byte Reload
	andb	%cl, 1472(%rsp)         # 1-byte Folded Spill
	jne	.LBB156_91
# BB#90:                                # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovaps	%xmm9, %xmm0
	vmovups	%ymm0, 1504(%rsp)       # 32-byte Spill
.LBB156_91:                             # %for gH.s0.v10.v108
                                        #   in Loop: Header=BB156_72 Depth=1
	vmovaps	.LCPI156_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	1504(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovaps	.LCPI156_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	992(%rsp), %rcx         # 8-byte Reload
	leaq	(%r13,%rcx), %rcx
	movq	1400(%rsp), %rdx        # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rcx,4)
	addl	$8, %ebx
	addl	$8, %edi
	addl	$-1, %esi
	jne	.LBB156_72
.LBB156_92:                             # %end for gH.s0.v10.v109
	movl	-112(%rsp), %eax        # 4-byte Reload
	cmpl	656(%rsp), %eax         # 4-byte Folded Reload
	movq	1464(%rsp), %rbx        # 8-byte Reload
	vmovss	20(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	92(%rsp), %xmm15        # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm11        # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmovss	16(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	12(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmovss	8(%rsp), %xmm7          # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm3          # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	jge	.LBB156_148
# BB#93:                                # %for gH.s0.v10.v1012.preheader
	movl	$2, %eax
	movq	72(%rsp), %r8           # 8-byte Reload
	subl	%r8d, %eax
	movq	112(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%rsi), %ebp
	movl	%ebp, 1280(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%esi, %edi
	sarl	$31, %edi
	andnl	%ebp, %edi, %ecx
	movl	%ebp, %r15d
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %r11d
	movl	%r11d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r8d, %ecx
	leal	-1(%r8,%rsi), %r9d
	cmpl	%ecx, %r9d
	cmovlel	%r9d, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	movl	%ecx, 1264(%rsp)        # 4-byte Spill
	leal	(%r8,%rsi), %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	movq	%rsi, %r13
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%r9d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %edx
	cmovll	%ecx, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movq	144(%rsp), %rbp         # 8-byte Reload
	leal	(%rbp,%rbp), %ecx
	movl	%ecx, 1504(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	movl	%ebp, %r10d
	sarl	$31, %r10d
	andnl	%ecx, %r10d, %r14d
	movl	%ecx, %esi
	andl	%eax, %r10d
	movl	$2, %eax
	movq	56(%rsp), %rcx          # 8-byte Reload
	subl	%ecx, %eax
	cltd
	idivl	%esi
	orl	%r14d, %r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movq	%rbp, %rdx
	leal	-1(%rdx,%rdx), %ebp
	movl	%ebp, %r12d
	subl	%eax, %r12d
	cmpl	%eax, %edx
	cmovgl	%eax, %r12d
	addl	%ecx, %r12d
	leal	-1(%rcx,%rdx), %r14d
	cmpl	%r12d, %r14d
	cmovlel	%r14d, %r12d
	cmpl	%ecx, %r12d
	cmovll	%ecx, %r12d
	leal	(%rcx,%rdx), %edx
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	$3, %edx
	cmovll	%r12d, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	negl	%eax
	cltd
	idivl	%r15d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%r11d, %r15d
	subl	%eax, %r15d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r15d
	addl	%r8d, %r15d
	cmpl	%r15d, %r9d
	cmovlel	%r9d, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	xorl	%r13d, %r13d
	movl	1472(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%r9d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	testl	%edx, %edx
	cmovlel	%r15d, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%ebp, %esi
	subl	%eax, %esi
	movq	144(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %esi
	addl	%ecx, %esi
	cmpl	%esi, %r14d
	cmovlel	%r14d, %esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	movl	1344(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%r14d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	testl	%edx, %edx
	cmovlel	%esi, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r8d, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	subl	%eax, %r11d
	movq	112(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %r11d
	addl	%r8d, %r11d
	cmpl	%r11d, %r9d
	cmovlel	%r9d, %r11d
	cmpl	%r8d, %r11d
	cmovll	%r8d, %r11d
	movl	1472(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r13d, %r9d
	movzbl	%al, %eax
	orl	%eax, %r9d
	cmpl	%r8d, %r9d
	cmovll	%r8d, %r9d
	cmpl	$2, %edx
	cmovll	%r11d, %r9d
	movl	$1, %eax
	subl	%ecx, %eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	movq	144(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ebp
	addl	%ecx, %ebp
	cmpl	%ebp, %r14d
	cmovlel	%r14d, %ebp
	cmpl	%ecx, %ebp
	cmovll	%ecx, %ebp
	movl	1344(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r13d, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	cmpl	$2, %edx
	cmovll	%ebp, %r14d
	movl	%r14d, %edi
	movq	40(%rsp), %r10          # 8-byte Reload
	movq	32(%rsp), %rdx          # 8-byte Reload
	imull	%edx, %r10d
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm9
	movl	%ebx, %eax
	andl	$1, %eax
	movl	%eax, 1504(%rsp)        # 4-byte Spill
	addl	%r8d, %r10d
	movq	936(%rsp), %r14         # 8-byte Reload
	movl	%r14d, %eax
	sarl	$5, %eax
	movq	%rax, 1184(%rsp)        # 8-byte Spill
	cmpl	$1, %ecx
	cmovgl	%ebp, %edi
	movl	%edi, 1344(%rsp)        # 4-byte Spill
	vmovss	.LCPI156_0(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm4, %xmm0
	vmulss	%xmm7, %xmm0, %xmm1
	vdivss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	movq	24(%rsp), %rbp          # 8-byte Reload
	leal	2(%rbp), %eax
	vmovd	%eax, %xmm3
	vsubss	%xmm7, %xmm5, %xmm1
	leal	-1(%rbp), %eax
	vmovd	%eax, %xmm5
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	vmovss	52(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm4, %xmm6
	vmovss	68(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm6, %xmm0
	vmovss	84(%rsp), %xmm13        # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vdivss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm12
	movq	672(%rsp), %rdi         # 8-byte Reload
	vmovd	%edi, %xmm1
	movq	440(%rsp), %rax         # 8-byte Reload
	imull	%eax, %edi
	movq	%rdi, %rax
	movslq	%ebx, %rdi
	imulq	%rdx, %rdi
	addl	%ecx, %eax
	movq	%rax, 672(%rsp)         # 8-byte Spill
	cmpl	$1, %r8d
	cmovgl	%r11d, %r9d
	movslq	%r10d, %rax
	subq	%rax, %rdi
	testl	%ecx, %ecx
	movl	1200(%rsp), %r10d       # 4-byte Reload
	cmovgl	%esi, %r10d
	testl	%r8d, %r8d
	movl	1216(%rsp), %edx        # 4-byte Reload
	cmovgl	%r15d, %edx
	vmovss	80(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm0, %xmm7
	vmovd	%ebp, %xmm10
	leal	1(%rbp), %r15d
	leal	3(%rbp), %eax
	movl	%eax, 1168(%rsp)        # 4-byte Spill
	addl	$4, %ebp
	movq	%rbp, 24(%rsp)          # 8-byte Spill
	cmpl	$2, %ecx
	movl	1232(%rsp), %r11d       # 4-byte Reload
	cmovgl	%r12d, %r11d
	cmpl	$2, %r8d
	movl	1248(%rsp), %eax        # 4-byte Reload
	cmovgl	1264(%rsp), %eax        # 4-byte Folded Reload
	movslq	%r9d, %rsi
	leaq	(%rsi,%rdi), %rcx
	movq	%rcx, 1264(%rsp)        # 8-byte Spill
	movslq	%edx, %rsi
	movq	1560(%rsp), %r12        # 8-byte Reload
	movslq	%eax, %rbp
	leaq	(%rsi,%rdi), %r8
	leaq	(%rbp,%rdi), %r9
	movq	1184(%rsp), %rdi        # 8-byte Reload
	movslq	%edi, %rbp
	shlq	$5, %rbp
	addq	$40, %rbp
	movl	%ebx, %ecx
	andl	$63, %ecx
	imulq	%rbp, %rcx
	movq	128(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rbp
	sarq	$63, %rbp
	andq	%rax, %rbp
	subq	%rbp, %rcx
	movq	%rcx, 736(%rsp)         # 8-byte Spill
	movq	-48(%rsp), %rax         # 8-byte Reload
	negl	%eax
	movl	-52(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-56(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-60(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-64(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-108(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movq	-40(%rsp), %rcx         # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	1408(%rsp), %rax        # 8-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	negl	%eax
	movq	-8(%rsp), %rcx          # 8-byte Reload
	movl	-20(%rsp), %edx         # 4-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-24(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	-28(%rsp), %edx         # 4-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movq	-16(%rsp), %rdx         # 8-byte Reload
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	656(%rsp), %esi         # 4-byte Reload
	movl	%esi, %ebp
	notl	%ebp
	testl	%ecx, %ecx
	cmovsl	%r13d, %ecx
	notl	%ecx
	cmpl	%ecx, %ebp
	cmovgel	%ebp, %ecx
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	leal	8(%rbx), %ebp
	subl	100(%rsp), %ebp         # 4-byte Folded Reload
	andl	$-32, %r14d
	addl	$64, %r14d
	imull	%ebp, %r14d
	leal	(%rdi,%rdi,2), %ebp
	movl	%ebp, %edx
	shll	$10, %edx
	leal	(%rdx,%r14), %edx
	leal	(%rdx,%rcx,8), %eax
	movq	%rax, 720(%rsp)         # 8-byte Spill
	shll	$9, %ebp
	leal	(%rbp,%r14), %edx
	leal	(%rdx,%rcx,8), %eax
	movq	%rax, 704(%rsp)         # 8-byte Spill
	leal	(%r14,%rcx,8), %eax
	movq	%rax, 688(%rsp)         # 8-byte Spill
	movq	1328(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movq	%rdx, 640(%rsp)         # 8-byte Spill
	leal	1(%rax,%rcx,8), %ebp
	movq	%rbp, 624(%rsp)         # 8-byte Spill
	leal	-4(%rax,%rcx,8), %ebp
	movq	%rbp, 608(%rsp)         # 8-byte Spill
	leal	-3(%rax,%rcx,8), %ebp
	movq	%rbp, 592(%rsp)         # 8-byte Spill
	leal	-1(%rax,%rcx,8), %ebp
	movq	%rbp, 576(%rsp)         # 8-byte Spill
	leal	-2(%rax,%rcx,8), %eax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	subl	%ecx, %esi
	movl	%esi, 656(%rsp)         # 4-byte Spill
	vpbroadcastd	%xmm9, %xmm14
	vmovdqa	%xmm14, 544(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm8
	vmovdqa	%xmm8, 1136(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm1
	vmulss	%xmm7, %xmm6, %xmm3
	vmovdqa	.LCPI156_1(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm6, %xmm1, %xmm0
	vmovdqa	%xmm0, 528(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, %xmm0
	vpbroadcastd	%xmm5, %xmm1
	vmovdqa	%xmm1, 1200(%rsp)       # 16-byte Spill
	vdivss	%xmm3, %xmm13, %xmm11
	vsubss	%xmm2, %xmm4, %xmm3
	vmulss	%xmm15, %xmm3, %xmm4
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm2, %xmm9
	movq	672(%rsp), %rbx         # 8-byte Reload
	vmovd	%ebx, %xmm4
	vmovss	96(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm15, %xmm2, %xmm7
	movl	1344(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm2
	vpsubd	%xmm4, %xmm2, %xmm2
	vmulss	%xmm7, %xmm3, %xmm3
	vdivss	%xmm3, %xmm0, %xmm3
	movl	%r10d, %ecx
	vmovd	%ecx, %xmm7
	vpsubd	%xmm4, %xmm7, %xmm7
	vmovd	%r11d, %xmm0
	vpsubd	%xmm4, %xmm0, %xmm0
	movq	440(%rsp), %r10         # 8-byte Reload
	vmovd	%r10d, %xmm4
	vpbroadcastd	%xmm4, %xmm15
	vmovdqa	%xmm15, 352(%rsp)       # 16-byte Spill
	movq	160(%rsp), %rbp         # 8-byte Reload
	vmovd	%ebp, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vmovdqa	%xmm5, 1232(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm14, %xmm5
	vmovdqa	%xmm5, 1216(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm5, 512(%rsp)        # 16-byte Spill
	vmovd	%ebx, %xmm5
	vbroadcastss	%xmm5, %xmm5
	vmovaps	%xmm5, 896(%rsp)        # 16-byte Spill
	vmovd	%esi, %xmm5
	vbroadcastss	%xmm5, %xmm5
	vmovaps	%xmm5, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 1344(%rsp)       # 16-byte Spill
	vbroadcastss	1280(%rsp), %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 1328(%rsp)       # 16-byte Spill
	vbroadcastss	1472(%rsp), %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, 1184(%rsp)       # 16-byte Spill
	movl	1168(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vmovdqa	%xmm2, 496(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vmovdqa	%xmm2, 480(%rsp)        # 16-byte Spill
	movq	24(%rsp), %rsi          # 8-byte Reload
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm6, %xmm2, %xmm2
	vmovdqa	%xmm2, 464(%rsp)        # 16-byte Spill
	vpaddd	%xmm6, %xmm1, %xmm2
	vmovdqa	%xmm2, 448(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 336(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm2
	vmovdqa	%xmm2, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm1
	vmovaps	%xmm1, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm1
	vmovaps	%xmm1, 288(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 272(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	movl	$1, %eax
	movq	%r10, %rcx
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	movl	$-4, %eax
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 408(%rsp)         # 8-byte Spill
	movl	%edx, %eax
	subl	%ecx, %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	movl	$-3, %eax
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 384(%rsp)         # 8-byte Spill
	movl	$-2, %eax
	subl	%ecx, %eax
	notl	%ecx
	leal	(%rcx,%rdx), %ecx
	movq	%rcx, 440(%rsp)         # 8-byte Spill
	leal	(%rax,%rdx), %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	movq	1264(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r9,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	vpabsd	%xmm14, %xmm0
	vmovdqa	%xmm0, 1168(%rsp)       # 16-byte Spill
	vmovdqa	%xmm8, %xmm7
	vbroadcastss	.LCPI156_3(%rip), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_4(%rip), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_5(%rip), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_6(%rip), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB156_94:                             # %for gH.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm4 # xmm4 = [0,2,4,6]
	vpaddd	%xmm4, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	544(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %ebp
	movl	%ebp, 1008(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %ebx
	movl	%ebx, 1040(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r15d
	cltd
	idivl	%r15d
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r10d
	cltd
	idivl	%r10d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1168(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 1056(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm12
	vmovdqa	%xmm12, 1024(%rsp)      # 16-byte Spill
	vmovdqa	528(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm1
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	1200(%rsp), %xmm11      # 16-byte Reload
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vmovdqa	1232(%rsp), %xmm8       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm8, %xmm3
	vmovdqa	1216(%rsp), %xmm14      # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm5
	vblendvps	%xmm3, %xmm0, %xmm5, %xmm0
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm4, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vmovd	%xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vpmulld	%xmm7, %xmm0, %xmm13
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpand	%xmm10, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	512(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm1
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm4, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm15, %xmm3, %xmm3
	vpcmpgtd	%xmm2, %xmm8, %xmm5
	vpsubd	%xmm2, %xmm14, %xmm6
	vblendvps	%xmm5, %xmm2, %xmm6, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm1
	vpmulld	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm1, 1408(%rsp)       # 16-byte Spill
	vpaddd	1344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1568(%rsp), %r9         # 8-byte Reload
	vmovss	(%r9,%rdx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r11
	vmovups	12312(%r12,%r11,4), %xmm1
	vmovaps	%xmm1, 1088(%rsp)       # 16-byte Spill
	vmovups	12328(%r12,%r11,4), %xmm5
	vmovdqa	496(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm6
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	vpcmpgtd	%xmm0, %xmm8, %xmm3
	vmovups	12320(%r12,%r11,4), %xmm2
	vmovaps	%xmm2, 1264(%rsp)       # 16-byte Spill
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm4, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vpsubd	%xmm0, %xmm14, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm2, %xmm0
	vmovd	%xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm6, %xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r10d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm2
	vmovdqa	896(%rsp), %xmm9        # 16-byte Reload
	vpsubd	%xmm9, %xmm13, %xmm0
	vmovdqa	%xmm0, 992(%rsp)        # 16-byte Spill
	vpaddd	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovdqa	480(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm1, %xmm1
	vpcmpgtd	%xmm2, %xmm8, %xmm3
	vpsubd	%xmm2, %xmm14, %xmm7
	vblendvps	%xmm3, %xmm2, %xmm7, %xmm2
	vpaddd	%xmm4, %xmm12, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm15, %xmm3, %xmm3
	vpaddd	%xmm15, %xmm2, %xmm2
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm4, %xmm7, %xmm7
	vpextrd	$1, %xmm7, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vmovd	%xmm7, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vblendvps	%xmm1, %xmm3, %xmm2, %xmm13
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %r14d
	vpextrd	$2, %xmm7, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vmovd	%edi, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpextrd	$3, %xmm7, %eax
	cltd
	idivl	%r10d
	vpinsrd	$2, %ecx, %xmm2, %xmm2
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm3
	vpand	%xmm10, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vpcmpgtd	%xmm2, %xmm8, %xmm3
	vpsubd	%xmm2, %xmm14, %xmm7
	vblendvps	%xmm3, %xmm2, %xmm7, %xmm2
	vmovdqa	448(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm12, %xmm3, %xmm3
	vmovd	%r14d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm4, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm15, %xmm7, %xmm7
	vpaddd	%xmm15, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm3, %xmm7, %xmm2, %xmm8
	vmovaps	1280(%rsp), %xmm4       # 16-byte Reload
	vmulps	1472(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm3, %xmm7 # xmm7 = xmm3[1,3],xmm5[1,3]
	vmovaps	1184(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm7, %xmm7
	vmovaps	1328(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm7, %xmm6, %xmm7
	vmulps	%xmm7, %xmm2, %xmm2
	vmovaps	%xmm4, %xmm7
	vmovdqa	1136(%rsp), %xmm4       # 16-byte Reload
	vpmulld	1072(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vpxor	%xmm14, %xmm14, %xmm14
	vmovaps	1248(%rsp), %xmm15      # 16-byte Reload
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm12
	vmovups	%ymm12, 1472(%rsp)      # 32-byte Spill
	vmulps	%xmm0, %xmm7, %xmm0
	vsubps	%xmm11, %xmm5, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm0, %xmm2, %xmm5
	vmovdqa	%xmm4, %xmm2
	vpmulld	%xmm2, %xmm13, %xmm0
	vmovdqa	%xmm0, 976(%rsp)        # 16-byte Spill
	vpaddd	1344(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbp
	vmovq	%xmm0, %rbx
	movq	%rbx, %rdi
	sarq	$32, %rdi
	movq	%rbp, %rsi
	sarq	$32, %rsi
	movq	688(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovaps	%xmm12, %xmm10
	testl	1504(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB156_96
# BB#95:                                # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
.LBB156_96:                             # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vpmulld	%xmm2, %xmm8, %xmm4
	vpsubd	%xmm9, %xmm1, %xmm12
	vmovdqa	%xmm1, 944(%rsp)        # 16-byte Spill
	vmovdqa	1408(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm9, %xmm0, %xmm8
	movl	%r14d, %r8d
	vminps	%xmm15, %xmm5, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm2
	vmovups	%ymm2, 1408(%rsp)       # 32-byte Spill
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vmovss	(%r9,%rbx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	1264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 12336(%r12,%r11,4), %xmm0, %xmm3 # xmm3 = xmm0[0,2],mem[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vmovups	(%r12,%rdx,4), %xmm7
	vmovups	32(%r12,%rax,4), %xmm5
	vmovups	48(%r12,%rax,4), %xmm0
	vmovups	(%r12,%rcx,4), %xmm2
	vmovups	40(%r12,%rax,4), %xmm9
	vmulps	1152(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	andl	$1, %r8d
	je	.LBB156_97
# BB#98:                                # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovdqa	%xmm4, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 880(%rsp)        # 16-byte Spill
	movq	%r11, 1072(%rsp)        # 8-byte Spill
	movl	%r15d, 912(%rsp)        # 4-byte Spill
	movl	%r10d, 936(%rsp)        # 4-byte Spill
	movl	%r14d, 1088(%rsp)       # 4-byte Spill
	vmovaps	%xmm10, %xmm13
	jmp	.LBB156_99
	.align	16, 0x90
.LBB156_97:                             #   in Loop: Header=BB156_94 Depth=1
	movq	%r11, 1072(%rsp)        # 8-byte Spill
	movl	%r15d, 912(%rsp)        # 4-byte Spill
	movl	%r10d, 936(%rsp)        # 4-byte Spill
	movl	%r14d, 1088(%rsp)       # 4-byte Spill
	vmovaps	%xmm9, 864(%rsp)        # 16-byte Spill
	vmovdqa	336(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rdx
	vmovq	%xmm1, %rax
	movslq	%eax, %rcx
	movq	%rcx, 960(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %r12
	sarq	$32, %rdx
	vpaddd	320(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm4, 848(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm1, %rdi
	vmovq	%xmm1, %rbp
	movslq	%ebp, %r11
	sarq	$32, %rbp
	movslq	%edi, %r8
	sarq	$32, %rdi
	vpaddd	%xmm3, %xmm8, %xmm1
	vpextrq	$1, %xmm1, %rsi
	vmovq	%xmm1, %r15
	movslq	%r15d, %r14
	sarq	$32, %r15
	movq	%r13, %rcx
	movslq	%esi, %r10
	sarq	$32, %rsi
	movq	960(%rsp), %rbx         # 8-byte Reload
	vmovss	(%r9,%rbx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1560(%rsp), %r12        # 8-byte Reload
	vinsertps	$48, (%r9,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm5, %xmm3
	vmovaps	%xmm3, 816(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm3, %xmm7, %xmm5 # xmm5 = xmm7[1,3],xmm3[1,3]
	vmovaps	%xmm7, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, %xmm13
	vmovaps	288(%rsp), %xmm11       # 16-byte Reload
	vsubps	%xmm11, %xmm5, %xmm5
	vmovaps	%xmm2, %xmm7
	vmovaps	%xmm7, 880(%rsp)        # 16-byte Spill
	vmovaps	304(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm1, %xmm5, %xmm1
	vmovss	(%r9,%r11,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm4, %xmm5
	vshufps	$221, %xmm0, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm0[1,3]
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm5, %xmm3, %xmm3
	vmovss	(%r9,%r14,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm4, %xmm5
	vshufps	$221, %xmm9, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm9[1,3]
	vsubps	%xmm11, %xmm7, %xmm7
	vmovaps	%xmm13, %xmm11
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm5, %xmm7, %xmm5
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vminps	%xmm15, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vmovaps	1120(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm2, %xmm5
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm1, %xmm5, %xmm13
	vmovaps	1104(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm1, %xmm13
.LBB156_99:                             # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	movq	%r9, %r11
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	movl	1088(%rsp), %r10d       # 4-byte Reload
	jne	.LBB156_101
# BB#100:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm10, %xmm13
.LBB156_101:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	movl	%r10d, %ecx
	cltq
	vmovups	24592(%r12,%rax,4), %xmm6
	vmovups	24608(%r12,%rax,4), %xmm2
	vmovups	24624(%r12,%rax,4), %xmm5
	vmovups	24600(%r12,%rax,4), %xmm9
	vmovups	24616(%r12,%rax,4), %xmm10
	andl	$1, %ecx
	jne	.LBB156_102
# BB#103:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm6, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 784(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 832(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, %xmm1
	movq	%r11, %r9
	vmovdqa	1136(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1328(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1280(%rsp), %xmm9       # 16-byte Reload
	movl	1056(%rsp), %r11d       # 4-byte Reload
	movl	936(%rsp), %ebx         # 4-byte Reload
	movl	912(%rsp), %ecx         # 4-byte Reload
	movl	1040(%rsp), %edi        # 4-byte Reload
	vmovdqa	1024(%rsp), %xmm7       # 16-byte Reload
	movl	1008(%rsp), %ebp        # 4-byte Reload
	vxorps	%xmm10, %xmm10, %xmm10
	jmp	.LBB156_104
	.align	16, 0x90
.LBB156_102:                            #   in Loop: Header=BB156_94 Depth=1
	vmovdqa	272(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm12, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rax
	movslq	%eax, %r8
	sarq	$32, %rax
	movslq	%edx, %r9
	sarq	$32, %rdx
	vmovdqa	848(%rsp), %xmm0        # 16-byte Reload
	vpaddd	256(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	movslq	%ebp, %r14
	sarq	$32, %rbp
	movslq	%edi, %r15
	sarq	$32, %rdi
	vpaddd	%xmm1, %xmm8, %xmm0
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbx
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%esi, %rcx
	sarq	$32, %rsi
	vmovss	(%r11,%r8,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r11,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r11, %r9
	vinsertps	$48, (%r9,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	192(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vshufps	$221, %xmm2, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm2[1,3]
	vmovaps	%xmm6, 752(%rsp)        # 16-byte Spill
	vmovaps	224(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	240(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovss	(%r9,%r14,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm2, 800(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm5, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm5[1,3]
	vmovaps	%xmm5, 784(%rsp)        # 16-byte Spill
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovss	(%r9,%r12,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm3, %xmm2
	vshufps	$221, %xmm10, %xmm9, %xmm3 # xmm3 = xmm9[1,3],xmm10[1,3]
	vmovaps	%xmm10, 832(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 848(%rsp)        # 16-byte Spill
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm15, %xmm1, %xmm1
	vxorps	%xmm10, %xmm10, %xmm10
	vmaxps	%xmm10, %xmm1, %xmm1
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm3, %xmm2
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm0, %xmm2, %xmm1
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm0, %xmm1
	vmovdqa	1136(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1328(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1280(%rsp), %xmm9       # 16-byte Reload
	movl	1056(%rsp), %r11d       # 4-byte Reload
	movl	936(%rsp), %ebx         # 4-byte Reload
	movl	912(%rsp), %ecx         # 4-byte Reload
	movl	1040(%rsp), %edi        # 4-byte Reload
	vmovdqa	1024(%rsp), %xmm7       # 16-byte Reload
	movl	1008(%rsp), %ebp        # 4-byte Reload
.LBB156_104:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovdqa	944(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm15, %xmm6
	vpaddd	1344(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %r12
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	je	.LBB156_106
# BB#105:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm13, %xmm1
.LBB156_106:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm1, %xmm13
	movq	408(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	%r12, %rax
	sarq	$32, %rax
	vpinsrd	$3, %edx, %xmm0, %xmm0
	movq	608(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm1
	movq	%rsi, %rcx
	sarq	$32, %rcx
	vpsrad	$31, %xmm0, %xmm2
	vpand	1168(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	464(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm7, %xmm2, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	1200(%rsp), %xmm4       # 16-byte Reload
	vpminsd	%xmm4, %xmm1, %xmm1
	vmovdqa	352(%rsp), %xmm15       # 16-byte Reload
	vpmaxsd	%xmm15, %xmm1, %xmm1
	vmovdqa	1232(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vmovdqa	1216(%rsp), %xmm5       # 16-byte Reload
	vpsubd	%xmm0, %xmm5, %xmm5
	vblendvps	%xmm3, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm1, %xmm0, %xmm0
	vmovups	1408(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%xmm1, %xmm5
	testl	1504(%rsp), %r11d       # 4-byte Folded Reload
	jne	.LBB156_108
# BB#107:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
.LBB156_108:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vpmulld	%xmm11, %xmm0, %xmm12
	movl	%r11d, %edx
	movslq	%r12d, %rdi
	movslq	%esi, %rsi
	vmovss	(%r9,%rdi,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm9, %xmm0
	movq	1560(%rsp), %r12        # 8-byte Reload
	movq	1072(%rsp), %rax        # 8-byte Reload
	vmovups	12304(%r12,%rax,4), %xmm1
	vshufps	$221, 1264(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	1184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm6, %xmm4
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vmovups	1472(%rsp), %ymm1       # 32-byte Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1152(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	andl	$1, %edx
	vmovaps	960(%rsp), %xmm7        # 16-byte Reload
	je	.LBB156_109
# BB#110:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm9, 1280(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm0
	jmp	.LBB156_111
	.align	16, 0x90
.LBB156_109:                            #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm9, 1280(%rsp)       # 16-byte Spill
	vpaddd	320(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	vmovdqa	976(%rsp), %xmm0        # 16-byte Reload
	vpsubd	896(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	336(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm0, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rbp
	vpaddd	992(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, %xmm9
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	816(%rsp), %xmm6        # 16-byte Reload
	vmovaps	768(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm6, %xmm1, %xmm3 # xmm3 = xmm1[0,2],xmm6[0,2]
	vmovaps	288(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm3, %xmm3
	vmovaps	304(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	movslq	%ebp, %rax
	sarq	$32, %rbp
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, %xmm7, %xmm6, %xmm3 # xmm3 = xmm6[0,2],xmm7[0,2]
	vmovss	(%r9,%rax,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	880(%rsp), %xmm6        # 16-byte Reload
	vshufps	$136, 864(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm7     # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r9,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm4, %xmm7
	vmovaps	%xmm9, %xmm4
	vsubps	%xmm2, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vminps	%xmm4, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm10, %xmm6, %xmm6
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm6
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm0, %xmm6, %xmm0
	vmovaps	1104(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm8, %xmm1, %xmm0
.LBB156_111:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm4, 1248(%rsp)       # 16-byte Spill
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	jne	.LBB156_113
# BB#112:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm5, %xmm0
.LBB156_113:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	movl	%r11d, %eax
	andl	$1, %eax
	jne	.LBB156_114
# BB#115:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm0, %xmm2
	jmp	.LBB156_116
	.align	16, 0x90
.LBB156_114:                            #   in Loop: Header=BB156_94 Depth=1
	vmovdqa	256(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm12, %xmm1, %xmm2
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	vpaddd	976(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rsi
	vmovdqa	992(%rsp), %xmm1        # 16-byte Reload
	vpaddd	272(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %r8
	vmovq	%xmm2, %rcx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovss	(%r9,%rbx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	192(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	800(%rsp), %xmm5        # 16-byte Reload
	vmovaps	752(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm5, %xmm1, %xmm3 # xmm3 = xmm1[0,2],xmm5[0,2]
	vmovaps	224(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	240(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	movslq	%esi, %rax
	sarq	$32, %rsi
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vshufps	$136, 784(%rsp), %xmm5, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm5[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm4, %xmm5
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	848(%rsp), %xmm5        # 16-byte Reload
	vshufps	$136, 832(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	1248(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vminps	%xmm1, %xmm5, %xmm5
	vmaxps	%xmm10, %xmm5, %xmm5
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm5
	vsubps	%xmm2, %xmm5, %xmm2
	vmovaps	1104(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm8, %xmm1, %xmm2
.LBB156_116:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	movq	1464(%rsp), %rcx        # 8-byte Reload
	vmovdqa	%xmm11, %xmm7
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	je	.LBB156_118
# BB#117:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm0, %xmm2
.LBB156_118:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	movl	%r11d, %eax
	orl	%ecx, %eax
	testb	$1, %al
	je	.LBB156_120
# BB#119:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm2, %xmm0
	vmovups	%ymm0, 1408(%rsp)       # 32-byte Spill
.LBB156_120:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm14, 1328(%rsp)      # 16-byte Spill
	orl	%ecx, %r10d
	testb	$1, %r10b
	je	.LBB156_122
# BB#121:                               # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	%xmm13, %xmm0
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
.LBB156_122:                            # %for gH.s0.v10.v1012
                                        #   in Loop: Header=BB156_94 Depth=1
	vmovaps	.LCPI156_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	1472(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovaps	.LCPI156_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	1408(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r11d, %rax
	movq	736(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1400(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, 656(%rsp)          # 4-byte Folded Spill
	jne	.LBB156_94
	jmp	.LBB156_148
.LBB156_39:                             # %false_bb2
	movq	%rcx, 1472(%rsp)        # 8-byte Spill
	vmovss	%xmm4, 88(%rsp)         # 4-byte Spill
	vmovss	%xmm2, 92(%rsp)         # 4-byte Spill
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	addl	$43, %esi
	sarl	$3, %esi
	movq	%rsi, 1408(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	jle	.LBB156_148
# BB#40:                                # %for gH.s0.v10.v1016.preheader
	movl	$2, %eax
	movq	72(%rsp), %r11          # 8-byte Reload
	subl	%r11d, %eax
	movq	112(%rsp), %r10         # 8-byte Reload
	leal	(%r10,%r10), %ecx
	movl	%ecx, 1184(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%r10d, %edi
	sarl	$31, %edi
	andnl	%ecx, %edi, %esi
	movl	%ecx, %r13d
	andl	%eax, %edi
	orl	%esi, %edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movq	%rbx, 1560(%rsp)        # 8-byte Spill
	leal	-1(%r10,%r10), %edx
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	leal	-1(%r11,%r10), %ecx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	leal	(%r11,%r10), %r12d
	cmpl	$3, %r12d
	movl	$2, %eax
	cmovll	%ecx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	cmpl	$3, %r12d
	cmovll	%edx, %eax
	movl	%eax, 1328(%rsp)        # 4-byte Spill
	movq	144(%rsp), %r14         # 8-byte Reload
	leal	(%r14,%r14), %esi
	movl	%esi, 1504(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	movl	%r14d, %r8d
	sarl	$31, %r8d
	andnl	%esi, %r8d, %ebp
	andl	%eax, %r8d
	movl	$2, %eax
	movq	56(%rsp), %r9           # 8-byte Reload
	subl	%r9d, %eax
	cltd
	idivl	%esi
	orl	%ebp, %r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %ebx
	movl	%ebx, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	leal	-1(%r9,%r14), %esi
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	leal	(%r9,%r14), %ebp
	cmpl	$3, %ebp
	movl	$2, %eax
	cmovll	%esi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	$3, %ebp
	cmovll	%edx, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	%r11d, %eax
	negl	%eax
	cltd
	idivl	%r13d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	1344(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, 1232(%rsp)        # 4-byte Spill
	xorl	%r13d, %r13d
	testl	%r12d, %r12d
	movl	$0, %eax
	cmovlel	%ecx, %eax
	cmpl	%r11d, %eax
	cmovll	%r11d, %eax
	testl	%r12d, %r12d
	cmovlel	%edx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	%r9d, %eax
	negl	%eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%ebx, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r9d, %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movl	%edx, 1168(%rsp)        # 4-byte Spill
	testl	%ebp, %ebp
	movl	$0, %eax
	cmovlel	%esi, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	testl	%ebp, %ebp
	cmovlel	%edx, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r11d, %eax
	cltd
	idivl	1184(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	1344(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	cmpl	$1, %r12d
	setg	%al
	cmpl	$2, %r12d
	cmovgel	%r13d, %ecx
	movzbl	%al, %eax
	orl	%eax, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	cmpl	$2, %r12d
	cmovll	%edx, %ecx
	movl	%ecx, %r12d
	movl	$1, %eax
	subl	%r9d, %eax
	cltd
	idivl	1504(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	subl	%eax, %ebx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ebx
	addl	%r9d, %ebx
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	$1, %ebp
	setg	%al
	cmpl	$2, %ebp
	cmovgel	%r13d, %esi
	movzbl	%al, %eax
	orl	%eax, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	$2, %ebp
	cmovll	%ebx, %esi
	movl	%esi, %r14d
	movl	%r15d, %eax
	movq	40(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	movq	1472(%rsp), %r8         # 8-byte Reload
	leal	(%r8,%r8), %ecx
	cltd
	idivl	%ecx
	movl	%r8d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movq	128(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 768(%rsp)         # 8-byte Spill
	movl	%r15d, %ecx
	andl	$1, %ecx
	movl	%ecx, 1504(%rsp)        # 4-byte Spill
	leal	-1(%rdi,%r8), %edx
	cmpl	%r15d, %edx
	movl	%edx, %esi
	cmovgl	%r15d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	leal	-1(%r8,%r8), %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r8d
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	(%rdi,%r8), %eax
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r15d, %eax
	cmovgl	%esi, %ecx
	movq	160(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %eax
	vmovd	%eax, %xmm2
	movq	32(%rsp), %rbp          # 8-byte Reload
	imull	%ebp, %edi
	vmovss	.LCPI156_0(%rip), %xmm15 # xmm15 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm15, %xmm4
	vmulss	%xmm6, %xmm4, %xmm0
	vdivss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm8
	movq	440(%rsp), %r8          # 8-byte Reload
	leal	2(%r8,%rdx), %eax
	vmovd	%eax, %xmm3
	addl	%r11d, %edi
	movq	936(%rsp), %r10         # 8-byte Reload
	movl	%r10d, %eax
	sarl	$5, %eax
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	cmpl	$1, %r9d
	cmovgl	%ebx, %r14d
	movl	%r14d, 1184(%rsp)       # 4-byte Spill
	vsubss	%xmm6, %xmm5, %xmm0
	leal	2(%r8), %edx
	vmovd	%edx, %xmm14
	movslq	%ecx, %rsi
	imulq	%rbp, %rsi
	vmulss	%xmm0, %xmm4, %xmm0
	vdivss	%xmm0, %xmm1, %xmm10
	vmovss	52(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm15, %xmm7
	vmovss	68(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm7, %xmm0
	vmovss	84(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm11
	movq	672(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm0
	imull	%r8d, %ecx
	addl	%r9d, %ecx
	cmpl	$1, %r11d
	cmovgl	1344(%rsp), %r12d       # 4-byte Folded Reload
	movl	%r12d, 1344(%rsp)       # 4-byte Spill
	movq	1560(%rsp), %r12        # 8-byte Reload
	movq	%r15, %rax
	movslq	%edi, %rbx
	movq	%rsi, %r14
	subq	%rbx, %r14
	testl	%r9d, %r9d
	movl	1200(%rsp), %edi        # 4-byte Reload
	cmovgl	1168(%rsp), %edi        # 4-byte Folded Reload
	testl	%r11d, %r11d
	movl	1216(%rsp), %ebp        # 4-byte Reload
	cmovgl	1232(%rsp), %ebp        # 4-byte Folded Reload
	movslq	%ebp, %r15
	subq	%rbx, %r15
	movq	%rax, %rbx
	addq	%rsi, %r15
	cmpl	$2, %r9d
	movl	1280(%rsp), %eax        # 4-byte Reload
	cmovgl	1248(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	cmpl	$2, %r11d
	movl	1328(%rsp), %r9d        # 4-byte Reload
	cmovgl	1264(%rsp), %r9d        # 4-byte Folded Reload
	vpbroadcastd	%xmm2, %xmm9
	vmovdqa	%xmm9, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, %xmm5
	vpbroadcastd	%xmm0, %xmm12
	vmovdqa	%xmm12, 320(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vmovss	80(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm2, %xmm3
	vmovdqa	.LCPI156_1(%rip), %xmm2 # xmm2 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 736(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm0
	vmovdqa	%xmm0, 720(%rsp)        # 16-byte Spill
	vmulss	%xmm3, %xmm7, %xmm0
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %xmm13
	vdivss	%xmm0, %xmm4, %xmm14
	vsubss	%xmm5, %xmm15, %xmm0
	vmovss	92(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm0, %xmm1
	vmovss	88(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm7
	vmovd	%ecx, %xmm15
	vmovss	96(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm1, %xmm4
	movl	1184(%rsp), %edx        # 4-byte Reload
	vmovd	%edx, %xmm6
	vpsubd	%xmm15, %xmm6, %xmm6
	vmulss	%xmm4, %xmm0, %xmm0
	vdivss	%xmm0, %xmm3, %xmm0
	vmovd	%edi, %xmm4
	movl	%edi, %r11d
	vpsubd	%xmm15, %xmm4, %xmm4
	vmovd	%eax, %xmm1
	vpsubd	%xmm15, %xmm1, %xmm1
	movq	160(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1232(%rsp)       # 16-byte Spill
	leal	-1(%r8,%rax), %esi
	vmovd	%esi, %xmm3
	vpbroadcastd	%xmm3, %xmm15
	vmovdqa	%xmm15, 304(%rsp)       # 16-byte Spill
	movslq	1344(%rsp), %rsi        # 4-byte Folded Reload
	leal	1(%r8,%rax), %ebp
	vmovd	%ebp, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 704(%rsp)        # 16-byte Spill
	leaq	(%rsi,%r14), %rsi
	leal	1(%r8), %ebp
	vmovd	%ebp, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 688(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 944(%rsp)        # 16-byte Spill
	vmovd	%edx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm3
	vmovdqa	%xmm3, 1328(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm10, %xmm3
	vmovaps	%xmm3, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm3
	vmovaps	%xmm3, 1200(%rsp)       # 16-byte Spill
	leal	3(%r8,%rax), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 656(%rsp)        # 16-byte Spill
	leal	3(%r8), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 640(%rsp)        # 16-byte Spill
	leal	(%r8,%rax), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 624(%rsp)        # 16-byte Spill
	leal	4(%r8,%rax), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 608(%rsp)        # 16-byte Spill
	leal	4(%r8), %edi
	vmovd	%edi, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 592(%rsp)        # 16-byte Spill
	movslq	%r9d, %rdi
	addq	%r14, %rdi
	leal	-1(%r8), %edx
	vmovd	%edx, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm3
	vmovdqa	%xmm3, 576(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm13, %xmm3
	vmovdqa	%xmm3, 560(%rsp)        # 16-byte Spill
	vpaddd	%xmm2, %xmm15, %xmm2
	vmovdqa	%xmm2, 544(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 288(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm2
	vmovdqa	%xmm2, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm2
	vmovaps	%xmm2, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm2
	vmovaps	%xmm2, 240(%rsp)        # 16-byte Spill
	movl	1280(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 224(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 208(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	movq	1472(%rsp), %rbp        # 8-byte Reload
	movslq	%ebp, %rdx
	shlq	$5, %rdx
	addq	$40, %rdx
	movl	%ebx, %eax
	andl	$63, %eax
	imulq	%rdx, %rax
	vbroadcastss	%xmm7, %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	128(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rdx
	sarq	$63, %rdx
	andq	%rcx, %rdx
	subq	%rdx, %rax
	movq	%rax, 528(%rsp)         # 8-byte Spill
	leal	8(%rbx), %edx
	subl	100(%rsp), %edx         # 4-byte Folded Reload
	andl	$-32, %r10d
	addl	$64, %r10d
	imull	%edx, %r10d
	movq	%r10, 936(%rsp)         # 8-byte Spill
	leal	(%rbp,%rbp,2), %eax
	movl	%eax, %edx
	shll	$10, %edx
	leal	(%rdx,%r10), %ecx
	movq	%rcx, 512(%rsp)         # 8-byte Spill
	shll	$9, %eax
	leal	(%rax,%r10), %eax
	movq	%rax, 496(%rsp)         # 8-byte Spill
	movq	768(%rsp), %rdx         # 8-byte Reload
	leal	1(%rdx), %eax
	movq	%rax, 480(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	movl	%edx, %eax
	subl	%r8d, %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	leal	-3(%rdx), %ecx
	movq	%rcx, 440(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %ecx
	movq	%rcx, 416(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 408(%rsp)         # 8-byte Spill
	leal	1(%rax), %ecx
	movq	%rcx, 400(%rsp)         # 8-byte Spill
	leal	-4(%rax), %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	leal	-3(%rax), %ecx
	movq	%rcx, 368(%rsp)         # 8-byte Spill
	leal	-1(%rax), %ecx
	movq	%rcx, 352(%rsp)         # 8-byte Spill
	leal	-2(%rax), %eax
	movq	%rax, 336(%rsp)         # 8-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rdi,4), %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	vpabsd	%xmm9, %xmm0
	vmovdqa	%xmm0, 1184(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm9, %xmm0
	vmovdqa	%xmm0, 1168(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_3(%rip), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_4(%rip), %xmm0
	vmovaps	%xmm0, 1136(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_5(%rip), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI156_6(%rip), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB156_41:                             # %for gH.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	movq	336(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	752(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r15d
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r14d
	movl	%r14d, 1072(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %edi
	movq	768(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %r11d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %ebp
	movl	%ebp, 1056(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %ebx
	movl	%ebx, 1040(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	vpinsrd	$1, %r8d, %xmm1, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1184(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm11
	vmovdqa	736(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	720(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1232(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	1168(%rsp), %xmm7       # 16-byte Reload
	vpsubd	%xmm0, %xmm7, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	movq	408(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	movq	352(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vpmulld	%xmm12, %xmm0, %xmm10
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%ebx
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	704(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vmovdqa	688(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vpcmpgtd	%xmm3, %xmm14, %xmm4
	vpsubd	%xmm3, %xmm7, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm15, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm3, %xmm4, %xmm1
	vpmulld	%xmm12, %xmm1, %xmm1
	vmovdqa	%xmm1, 1344(%rsp)       # 16-byte Spill
	vpaddd	1328(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1568(%rsp), %r9         # 8-byte Reload
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r10
	vmovups	12312(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1104(%rsp)       # 16-byte Spill
	vmovups	12328(%r12,%r10,4), %xmm5
	vmovdqa	656(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm3
	vpxor	%xmm6, %xmm3, %xmm3
	vmovdqa	640(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm6
	vpor	%xmm3, %xmm6, %xmm6
	vpcmpgtd	%xmm0, %xmm14, %xmm3
	vpsubd	%xmm0, %xmm7, %xmm2
	vblendvps	%xmm3, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	movq	440(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vmovups	12320(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1264(%rsp)       # 16-byte Spill
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vmovdqa	%xmm12, %xmm4
	vblendvps	%xmm6, %xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebx
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm3
	vmovdqa	944(%rsp), %xmm12       # 16-byte Reload
	vpsubd	%xmm12, %xmm10, %xmm0
	vmovdqa	%xmm0, 1024(%rsp)       # 16-byte Spill
	vpaddd	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovdqa	624(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm2
	vpxor	.LCPI156_9(%rip), %xmm2, %xmm2
	vmovdqa	560(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpor	%xmm2, %xmm1, %xmm2
	vpcmpgtd	%xmm3, %xmm14, %xmm1
	vpsubd	%xmm3, %xmm7, %xmm6
	vblendvps	%xmm1, %xmm3, %xmm6, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	vpaddd	%xmm9, %xmm11, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm2, %xmm1, %xmm6, %xmm10
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %r14d
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, %ecx, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm3
	vpsubd	%xmm1, %xmm7, %xmm6
	vblendvps	%xmm3, %xmm1, %xmm6, %xmm1
	vmovdqa	544(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm3, %xmm3
	vpxor	.LCPI156_9(%rip), %xmm3, %xmm3
	vmovdqa	576(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm6, %xmm6
	vpor	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm13, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vmovd	%r14d, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm6
	vpminsd	%xmm15, %xmm6, %xmm6
	vpmaxsd	%xmm13, %xmm6, %xmm6
	vblendvps	%xmm3, %xmm1, %xmm6, %xmm8
	vmovaps	1280(%rsp), %xmm2       # 16-byte Reload
	vmulps	1472(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	1104(%rsp), %xmm7       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm7, %xmm6 # xmm6 = xmm7[1,3],xmm5[1,3]
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm6, %xmm6
	vmovaps	1216(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm6, %xmm14, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vpmulld	1088(%rsp), %xmm4, %xmm9 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm7, %xmm5 # xmm5 = xmm7[0,2],xmm5[0,2]
	vpxor	%xmm15, %xmm15, %xmm15
	vmovaps	1248(%rsp), %xmm7       # 16-byte Reload
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm15, %xmm1, %xmm6
	vmovups	%ymm6, 1472(%rsp)       # 32-byte Spill
	vmulps	%xmm0, %xmm2, %xmm0
	vsubps	%xmm3, %xmm5, %xmm1
	vmovaps	%xmm14, %xmm5
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vpmulld	%xmm4, %xmm10, %xmm1
	vmovdqa	%xmm1, 1104(%rsp)       # 16-byte Spill
	vpaddd	1328(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rbp
	vmovq	%xmm1, %rbx
	movq	%rbx, %rdi
	sarq	$32, %rdi
	movq	%rbp, %rsi
	sarq	$32, %rsi
	movq	936(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rdx
	orq	$4, %rdx
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovaps	%xmm6, %xmm14
	testl	1504(%rsp), %r14d       # 4-byte Folded Reload
	jne	.LBB156_43
# BB#42:                                # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB156_43:                             # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vpmulld	%xmm4, %xmm8, %xmm6
	vpsubd	%xmm12, %xmm9, %xmm4
	vmovdqa	1344(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm12, %xmm1, %xmm8
	movl	%r14d, %r8d
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm1
	vmovups	%ymm1, 1344(%rsp)       # 32-byte Spill
	movslq	%ebx, %rbx
	movslq	%ebp, %rbp
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1264(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 12336(%r12,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm2
	vmovups	(%r12,%rdx,4), %xmm10
	vmovups	32(%r12,%rax,4), %xmm1
	vmovups	48(%r12,%rax,4), %xmm5
	vmovups	(%r12,%rcx,4), %xmm12
	vmovups	40(%r12,%rax,4), %xmm3
	vmulps	1152(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	andl	$1, %r8d
	je	.LBB156_44
# BB#123:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	%xmm4, 784(%rsp)        # 16-byte Spill
	vmovdqa	%xmm6, 800(%rsp)        # 16-byte Spill
	vmovdqa	%xmm8, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vmovdqa	%xmm9, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 816(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 912(%rsp)       # 16-byte Spill
	movl	%r14d, %r8d
	movl	%r15d, 976(%rsp)        # 4-byte Spill
	movl	%r11d, 992(%rsp)        # 4-byte Spill
	vmovaps	%xmm7, %xmm9
	vmovaps	%xmm14, %xmm10
	jmp	.LBB156_124
	.align	16, 0x90
.LBB156_44:                             #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	%xmm9, 1088(%rsp)       # 16-byte Spill
	movl	%r14d, %r8d
	movl	%r15d, 976(%rsp)        # 4-byte Spill
	movl	%r11d, 992(%rsp)        # 4-byte Spill
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vmovdqa	288(%rsp), %xmm0        # 16-byte Reload
	vpaddd	%xmm0, %xmm4, %xmm2
	vmovdqa	%xmm4, 784(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	movq	%rcx, 1008(%rsp)        # 8-byte Spill
	sarq	$32, %rax
	movq	%r13, 832(%rsp)         # 8-byte Spill
	movslq	%edx, %r13
	sarq	$32, %rdx
	vpaddd	272(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm6, 800(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rcx
	sarq	$32, %rdi
	vpaddd	%xmm0, %xmm8, %xmm2
	vmovdqa	%xmm8, 880(%rsp)        # 16-byte Spill
	vpextrq	$1, %xmm2, %rsi
	vmovq	%xmm2, %r14
	movslq	%r14d, %r11
	sarq	$32, %r14
	movslq	%esi, %r15
	sarq	$32, %rsi
	movq	1008(%rsp), %r12        # 8-byte Reload
	vmovss	(%r9,%r12,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	1560(%rsp), %r12        # 8-byte Reload
	movq	832(%rsp), %r13         # 8-byte Reload
	vinsertps	$48, (%r9,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	160(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm5, %xmm9
	vmovaps	%xmm9, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, %xmm5
	vmovaps	%xmm5, 864(%rsp)        # 16-byte Spill
	vmulps	%xmm2, %xmm0, %xmm8
	vmovaps	%xmm3, %xmm2
	vmovaps	%xmm2, 896(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm5, %xmm10, %xmm3 # xmm3 = xmm10[1,3],xmm5[1,3]
	vmovaps	%xmm10, 816(%rsp)       # 16-byte Spill
	vmovaps	240(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	%xmm12, 912(%rsp)       # 16-byte Spill
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm8, %xmm3, %xmm8
	vmovss	(%r9,%rbx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm0, %xmm3
	vshufps	$221, %xmm9, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm9[1,3]
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm3, %xmm5, %xmm3
	vmovss	(%r9,%r11,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm0, %xmm5
	vshufps	$221, %xmm2, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm2[1,3]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm5, %xmm0, %xmm0
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vmovaps	1136(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm0
	vminps	%xmm7, %xmm8, %xmm2
	vmovaps	%xmm7, %xmm9
	vmaxps	%xmm15, %xmm2, %xmm2
	vsubps	%xmm2, %xmm0, %xmm10
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	960(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
.LBB156_124:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	304(%rsp), %xmm15       # 16-byte Reload
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	jne	.LBB156_126
# BB#125:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm14, %xmm10
.LBB156_126:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	movq	%r10, 1008(%rsp)        # 8-byte Spill
	movl	%r8d, %ecx
	cltq
	vmovups	24592(%r12,%rax,4), %xmm3
	vmovups	24608(%r12,%rax,4), %xmm5
	vmovups	24624(%r12,%rax,4), %xmm2
	vmovups	24600(%r12,%rax,4), %xmm7
	vmovups	24616(%r12,%rax,4), %xmm8
	andl	$1, %ecx
	jne	.LBB156_127
# BB#128:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm3, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 960(%rsp)       # 16-byte Spill
	jmp	.LBB156_129
	.align	16, 0x90
.LBB156_127:                            #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	224(%rsp), %xmm1        # 16-byte Reload
	vpaddd	784(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	%rcx, 784(%rsp)         # 8-byte Spill
	sarq	$32, %rax
	movslq	%edx, %r10
	sarq	$32, %rdx
	vmovdqa	800(%rsp), %xmm0        # 16-byte Reload
	vpaddd	208(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	movslq	%ebp, %r14
	sarq	$32, %rbp
	movslq	%edi, %r15
	sarq	$32, %rdi
	vpaddd	880(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbx
	movslq	%ebx, %r12
	sarq	$32, %rbx
	movslq	%esi, %rcx
	sarq	$32, %rsi
	movq	784(%rsp), %r11         # 8-byte Reload
	vmovss	(%r9,%r11,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	144(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vshufps	$221, %xmm5, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm5[1,3]
	vmovaps	%xmm3, 800(%rsp)        # 16-byte Spill
	vmovaps	176(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm1, %xmm1
	vmovaps	192(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovss	(%r9,%r14,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm6, %xmm1
	vmovaps	%xmm2, 832(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm2[1,3]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovss	(%r9,%r12,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vshufps	$221, %xmm8, %xmm7, %xmm6 # xmm6 = xmm7[1,3],xmm8[1,3]
	vmovaps	%xmm7, 880(%rsp)        # 16-byte Spill
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm2, %xmm6, %xmm2
	vminps	%xmm9, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1136(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm3, %xmm2
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	%xmm0, %xmm2, %xmm1
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	960(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 960(%rsp)        # 16-byte Spill
.LBB156_129:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	320(%rsp), %xmm7        # 16-byte Reload
	vmovaps	1280(%rsp), %xmm12      # 16-byte Reload
	movl	992(%rsp), %r11d        # 4-byte Reload
	movl	976(%rsp), %ecx         # 4-byte Reload
	movl	1072(%rsp), %edi        # 4-byte Reload
	movl	1056(%rsp), %ebx        # 4-byte Reload
	movl	1040(%rsp), %ebp        # 4-byte Reload
	movq	1008(%rsp), %r10        # 8-byte Reload
	vmovaps	%xmm8, 1008(%rsp)       # 16-byte Spill
	vmovdqa	1088(%rsp), %xmm0       # 16-byte Reload
	vpaddd	1328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r12
	vmovq	%xmm0, %rsi
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	movl	%r8d, %r14d
	je	.LBB156_131
# BB#130:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm10, 960(%rsp)       # 16-byte Spill
.LBB156_131:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm5, 1088(%rsp)       # 16-byte Spill
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI156_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	movq	%rsi, %rax
	sarq	$32, %rax
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	1184(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	1232(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm2
	vmovdqa	1168(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm2
	movq	%r12, %rcx
	sarq	$32, %rcx
	vmovdqa	608(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm4
	vpxor	.LCPI156_9(%rip), %xmm4, %xmm4
	vmovdqa	592(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm4, %xmm0, %xmm2, %xmm2
	vmovups	1344(%rsp), %ymm0       # 32-byte Reload
	vmovaps	%xmm0, %xmm8
	testl	1504(%rsp), %r11d       # 4-byte Folded Reload
	jne	.LBB156_133
# BB#132:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vxorps	%xmm8, %xmm8, %xmm8
.LBB156_133:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vpmulld	%xmm7, %xmm2, %xmm11
	movl	%r11d, %edx
	movslq	%esi, %rsi
	movslq	%r12d, %rdi
	vmovss	(%r9,%rsi,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm12, %xmm0
	movq	1560(%rsp), %r12        # 8-byte Reload
	vmovups	12304(%r12,%r10,4), %xmm2
	vshufps	$221, 1264(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	1200(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm9, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovups	1472(%rsp), %ymm1       # 32-byte Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1152(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	andl	$1, %edx
	je	.LBB156_134
# BB#135:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm9, 1248(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1280(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, %xmm6
	jmp	.LBB156_136
	.align	16, 0x90
.LBB156_134:                            #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm12, 1280(%rsp)      # 16-byte Spill
	vpaddd	272(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	vmovdqa	1104(%rsp), %xmm0       # 16-byte Reload
	vpsubd	944(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovdqa	288(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm0, %xmm0
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rbp
	vpaddd	1024(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	160(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	864(%rsp), %xmm1        # 16-byte Reload
	vmovaps	816(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, %xmm1, %xmm2, %xmm6 # xmm6 = xmm2[0,2],xmm1[0,2]
	vmovaps	240(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm6, %xmm6
	vmovaps	256(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm0, %xmm6
	movslq	%ebp, %rax
	sarq	$32, %rbp
	movslq	%edx, %rsi
	sarq	$32, %rdx
	vshufps	$136, 848(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm1[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm5, %xmm3
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	912(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 896(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm5, %xmm1
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vminps	%xmm9, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm2, %xmm1
	vminps	%xmm9, %xmm6, %xmm0
	vmovaps	%xmm9, 1248(%rsp)       # 16-byte Spill
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm0, %xmm1, %xmm6
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm0, %xmm6
.LBB156_136:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	jne	.LBB156_138
# BB#137:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm8, %xmm6
.LBB156_138:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	movl	%r11d, %eax
	vmovdqa	%xmm7, %xmm12
	andl	$1, %eax
	jne	.LBB156_139
# BB#140:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm6, %xmm0
	jmp	.LBB156_141
	.align	16, 0x90
.LBB156_139:                            #   in Loop: Header=BB156_41 Depth=1
	vmovdqa	208(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm11, %xmm1, %xmm0
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	1104(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vmovdqa	1024(%rsp), %xmm0       # 16-byte Reload
	vpaddd	224(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %rcx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	144(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1088(%rsp), %xmm3       # 16-byte Reload
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	176(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	192(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	movslq	%esi, %rax
	sarq	$32, %rsi
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vshufps	$136, 832(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r8d, %rdx
	sarq	$32, %r8
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1008(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmovss	(%r9,%rax,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm7, %xmm4
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm2, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm1, %xmm0
.LBB156_141:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	movq	1464(%rsp), %rcx        # 8-byte Reload
	movq	1408(%rsp), %rdx        # 8-byte Reload
	cmpl	$0, 1504(%rsp)          # 4-byte Folded Reload
	je	.LBB156_143
# BB#142:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	%xmm6, %xmm0
.LBB156_143:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	movl	%r11d, %eax
	orl	%ecx, %eax
	testb	$1, %al
	je	.LBB156_145
# BB#144:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovups	%ymm0, 1344(%rsp)       # 32-byte Spill
.LBB156_145:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	movq	%r9, 1568(%rsp)         # 8-byte Spill
	orl	%ecx, %r14d
	testb	$1, %r14b
	je	.LBB156_147
# BB#146:                               # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	960(%rsp), %xmm0        # 16-byte Reload
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
.LBB156_147:                            # %for gH.s0.v10.v1016
                                        #   in Loop: Header=BB156_41 Depth=1
	vmovaps	.LCPI156_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	1472(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovaps	.LCPI156_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	1344(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r11d, %rax
	movq	528(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1400(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, %edx
	movq	%rdx, 1408(%rsp)        # 8-byte Spill
	jne	.LBB156_41
.LBB156_148:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1576, %rsp             # imm = 0x628
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end156:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172, .Lfunc_end156-par_for_par_for___sharpi_f0.s0.v11.v14_gH.s0.v11.172

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI157_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI157_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI157_9:
	.zero	16,255
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI157_1:
	.long	1199570688              # float 65535
.LCPI157_3:
	.long	1065353216              # float 1
.LCPI157_4:
	.long	1073741824              # float 2
.LCPI157_5:
	.long	1048576000              # float 0.25
.LCPI157_6:
	.long	1056964608              # float 0.5
	.section	.rodata,"a",@progbits
	.align	32
.LCPI157_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI157_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173,@function
par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173: # @par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1416, %rsp             # imm = 0x588
	movl	%esi, %r15d
	vmovss	(%rdx), %xmm14          # xmm14 = mem[0],zero,zero,zero
	vmovss	4(%rdx), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	16(%rdx), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	20(%rdx), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vmovss	24(%rdx), %xmm13        # xmm13 = mem[0],zero,zero,zero
	vmovss	28(%rdx), %xmm10        # xmm10 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm6         # xmm6 = mem[0],zero,zero,zero
	movl	36(%rdx), %eax
	movl	%eax, 52(%rsp)          # 4-byte Spill
	movl	40(%rdx), %eax
	movl	%eax, -4(%rsp)          # 4-byte Spill
	movl	44(%rdx), %ebp
	movslq	56(%rdx), %rax
	movq	%rax, 56(%rsp)          # 8-byte Spill
	movl	60(%rdx), %eax
	movq	%rax, 24(%rsp)          # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	movl	68(%rdx), %eax
	movq	%rax, -16(%rsp)         # 8-byte Spill
	movl	72(%rdx), %eax
	movq	%rax, 864(%rsp)         # 8-byte Spill
	movslq	76(%rdx), %rax
	movq	%rax, 896(%rsp)         # 8-byte Spill
	movl	80(%rdx), %eax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	movl	84(%rdx), %ebx
	movl	88(%rdx), %eax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	vmovss	100(%rdx), %xmm15       # xmm15 = mem[0],zero,zero,zero
	vmovss	104(%rdx), %xmm7        # xmm7 = mem[0],zero,zero,zero
	vmovss	108(%rdx), %xmm12       # xmm12 = mem[0],zero,zero,zero
	cmpl	%r15d, 52(%rdx)
	movl	92(%rdx), %eax
	movq	%rax, 8(%rsp)           # 8-byte Spill
	movslq	96(%rdx), %rax
	movq	%rax, (%rsp)            # 8-byte Spill
	movq	112(%rdx), %rax
	movq	128(%rdx), %rcx
	movq	%rcx, 1368(%rsp)        # 8-byte Spill
	movq	144(%rdx), %rcx
	movq	%rcx, 1408(%rsp)        # 8-byte Spill
	movq	160(%rdx), %rcx
	movq	%rcx, 32(%rsp)          # 8-byte Spill
	jle	.LBB157_23
# BB#1:                                 # %true_bb
	vmovss	%xmm0, -8(%rsp)         # 4-byte Spill
	movq	%rax, 1304(%rsp)        # 8-byte Spill
	movq	%r15, %r8
	addl	$43, %ebp
	sarl	$3, %ebp
	movq	%rbp, 1376(%rsp)        # 8-byte Spill
	testl	%ebp, %ebp
	jle	.LBB157_111
# BB#2:                                 # %for gV.s0.v10.v10.preheader
	movq	104(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %eax
	negl	%eax
	movq	16(%rsp), %rbp          # 8-byte Reload
	leal	(%rbp,%rbp), %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%ebp, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %r9d
	movl	%ecx, %r11d
	andl	%eax, %esi
	orl	%r9d, %esi
	movl	%esi, 912(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	movl	%esi, %r10d
	addl	%edx, %eax
	leal	-1(%rbp,%rbp), %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movl	%edx, %r9d
	subl	%eax, %ecx
	cmpl	%eax, %ebp
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	leal	-1(%rdi,%rbp), %edx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 1056(%rsp)        # 4-byte Spill
	leal	(%rdi,%rbp), %esi
	movl	%esi, 880(%rsp)         # 4-byte Spill
	movq	%rbp, %r15
	xorl	%ebp, %ebp
	testl	%esi, %esi
	movl	$0, %eax
	cmovlel	%edx, %eax
	movl	%edx, %r14d
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%esi, %esi
	cmovlel	%ecx, %eax
	movl	%eax, 1024(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%r9d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r15d
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 960(%rsp)         # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %esi
	cmovll	%ecx, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movl	$2, %eax
	movq	-16(%rsp), %r10         # 8-byte Reload
	subl	%r10d, %eax
	movq	24(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %ecx
	movl	%ecx, 832(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%esi, %edi
	sarl	$31, %edi
	andnl	%ecx, %edi, %r9d
	andl	%eax, %edi
	orl	%r9d, %edi
	movl	%edi, 848(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %r12d
	movl	%r12d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	leal	-1(%r10,%rsi), %edx
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 944(%rsp)         # 4-byte Spill
	leal	(%r10,%rsi), %edi
	movl	%edi, 816(%rsp)         # 4-byte Spill
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movq	%r8, %r14
	movq	%r14, 1400(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	movq	8(%rsp), %r15           # 8-byte Reload
	subl	%r15d, %eax
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	movq	%rax, %rcx
	movq	%rbx, %r11
	movq	%r11, 1152(%rsp)        # 8-byte Spill
	leal	(%r11,%r11), %edi
	movl	%edi, 928(%rsp)         # 4-byte Spill
	leal	-1(%rcx), %eax
	movq	%rcx, %r13
	cltd
	idivl	%edi
	movl	%edi, %eax
	negl	%eax
	movl	%r11d, %esi
	sarl	$31, %esi
	andnl	%edi, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 1120(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r11,%r11), %r9d
	movl	%r9d, 1008(%rsp)        # 4-byte Spill
	movl	%r9d, %esi
	subl	%eax, %esi
	cmpl	%eax, %r11d
	cmovgl	%eax, %esi
	addl	%r15d, %esi
	leal	-1(%r15,%r11), %ebx
	cmpl	%esi, %ebx
	cmovlel	%ebx, %esi
	cmpl	%r15d, %esi
	cmovll	%r15d, %esi
	leal	(%r15,%r11), %r8d
	cmpl	%r14d, %r8d
	movl	%r8d, %eax
	cmovgl	%r14d, %eax
	addl	$-1, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	cmpl	%r14d, %r8d
	cmovll	%esi, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	movl	1120(%rsp), %r13d       # 4-byte Reload
	andl	%r13d, %eax
	addl	%edx, %eax
	movl	%r9d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r11d
	cmovgl	%eax, %ecx
	addl	%r15d, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%r15d, %ecx
	cmovll	%r15d, %ecx
	cmpl	%r14d, %ebx
	movl	%ebx, %eax
	cmovgl	%r14d, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	cmpl	%r14d, %r8d
	cmovlel	%ecx, %eax
	movl	%eax, %r8d
	movq	1088(%rsp), %rax        # 8-byte Reload
	leal	1(%rax), %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	subl	%eax, %r9d
	cmpl	%eax, %r11d
	cmovgl	%eax, %r9d
	addl	%r15d, %r9d
	cmpl	%r9d, %ebx
	cmovlel	%ebx, %r9d
	cmpl	%r15d, %r9d
	cmovll	%r15d, %r9d
	movq	%r14, %rax
	leal	1(%rax), %r14d
	cmpl	%r14d, %ebx
	cmovlel	%ebx, %r14d
	cmpl	%r15d, %r14d
	cmovll	%r15d, %r14d
	cmpl	%eax, %r15d
	cmovgl	%ecx, %r8d
	movl	%r8d, 800(%rsp)         # 4-byte Spill
	movl	1280(%rsp), %ecx        # 4-byte Reload
	cmovgel	%esi, %ecx
	movl	%ecx, 1280(%rsp)        # 4-byte Spill
	cmpl	%eax, %ebx
	cmovlel	%r9d, %r14d
	movl	$1, %eax
	movq	104(%rsp), %rdi         # 8-byte Reload
	subl	%edi, %eax
	cltd
	idivl	1248(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	912(%rsp), %eax         # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1264(%rsp), %esi        # 4-byte Reload
	subl	%eax, %esi
	movq	16(%rsp), %rcx          # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %esi
	movq	%rdi, %rcx
	addl	%ecx, %esi
	movl	1200(%rsp), %edx        # 4-byte Reload
	cmpl	%esi, %edx
	cmovlel	%edx, %esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	movl	880(%rsp), %edi         # 4-byte Reload
	cmpl	$1, %edi
	setg	%al
	cmpl	$2, %edi
	cmovgel	%ebp, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	$2, %edi
	cmovll	%esi, %edx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	movl	832(%rsp), %edi         # 4-byte Reload
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	movl	848(%rsp), %r8d         # 4-byte Reload
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%r12d, %r11d
	subl	%eax, %r11d
	movq	24(%rsp), %r13          # 8-byte Reload
	cmpl	%eax, %r13d
	cmovgl	%eax, %r11d
	addl	%r10d, %r11d
	movl	1312(%rsp), %esi        # 4-byte Reload
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	cmpl	%r10d, %r11d
	cmovll	%r10d, %r11d
	movl	816(%rsp), %ecx         # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	movl	$0, %edx
	cmovll	%esi, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	cmpl	$2, %ecx
	cmovll	%r11d, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	movl	%r10d, %eax
	negl	%eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	subl	%eax, %r12d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r12d
	addl	%r10d, %r12d
	cmpl	%r12d, %esi
	cmovlel	%esi, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	testl	%ecx, %ecx
	cmovgl	%ebp, %esi
	cmpl	%r10d, %esi
	cmovll	%r10d, %esi
	testl	%ecx, %ecx
	cmovlel	%r12d, %esi
	movq	56(%rsp), %rdx          # 8-byte Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edx, %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movq	1400(%rsp), %r13        # 8-byte Reload
	movl	%r13d, %r8d
	andl	$1, %r8d
	testl	%r10d, %r10d
	cmovgl	%r12d, %esi
	movl	%esi, 1312(%rsp)        # 4-byte Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm8
	movq	864(%rsp), %r12         # 8-byte Reload
	imull	%r12d, %eax
	addl	%r10d, %eax
	movq	%rax, 896(%rsp)         # 8-byte Spill
	cmpl	$1, %r10d
	movl	1248(%rsp), %eax        # 4-byte Reload
	cmovgl	%r11d, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movq	104(%rsp), %rdx         # 8-byte Reload
	cmpl	$1, %edx
	movl	1200(%rsp), %ecx        # 4-byte Reload
	cmovgl	1264(%rsp), %ecx        # 4-byte Folded Reload
	movq	(%rsp), %rdi            # 8-byte Reload
	movl	%edi, %eax
	imull	%r15d, %eax
	movl	52(%rsp), %esi          # 4-byte Reload
	sarl	$5, %esi
	movl	%esi, 912(%rsp)         # 4-byte Spill
	addl	%edx, %eax
	movq	%rdx, %rsi
	cltq
	movq	%rax, 880(%rsp)         # 8-byte Spill
	movslq	800(%rsp), %rdx         # 4-byte Folded Reload
	movslq	%ecx, %rcx
	imulq	%rdi, %rdx
	movq	%rdx, 1200(%rsp)        # 8-byte Spill
	subq	%rax, %rcx
	leal	-1(%r15), %eax
	cmpl	%r13d, %eax
	cmovgl	%r9d, %r14d
	movslq	1280(%rsp), %rax        # 4-byte Folded Reload
	imulq	%rdi, %rax
	leaq	(%rax,%rcx), %rax
	movq	32(%rsp), %rdx          # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movslq	%r14d, %rax
	imulq	%rdi, %rax
	movq	%rdi, %r13
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	cmpl	$2, %r10d
	movl	1216(%rsp), %r11d       # 4-byte Reload
	cmovgl	944(%rsp), %r11d        # 4-byte Folded Reload
	movl	%r11d, 1216(%rsp)       # 4-byte Spill
	cmpl	$2, %esi
	movq	%rsi, %r9
	movl	1232(%rsp), %eax        # 4-byte Reload
	cmovgl	960(%rsp), %eax         # 4-byte Folded Reload
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movq	1088(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	928(%rsp), %edi         # 4-byte Reload
	idivl	%edi
	movl	%edx, %r10d
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%edi
	vmovss	.LCPI157_1(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm5, %xmm0
	vmulss	%xmm10, %xmm0, %xmm1
	vdivss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm9
	vsubss	%xmm10, %xmm2, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm7, %xmm10
	vsubss	%xmm11, %xmm5, %xmm1
	vmulss	%xmm6, %xmm1, %xmm0
	vdivss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm11
	movq	40(%rsp), %rsi          # 8-byte Reload
	leal	2(%r12,%rsi), %eax
	vmovd	%eax, %xmm0
	vsubss	%xmm6, %xmm4, %xmm4
	leal	2(%r12), %eax
	vmovd	%eax, %xmm7
	vmulss	%xmm4, %xmm1, %xmm1
	leal	1(%r12,%rsi), %eax
	vmovd	%eax, %xmm2
	vdivss	%xmm1, %xmm12, %xmm12
	vsubss	%xmm14, %xmm5, %xmm1
	vmulss	%xmm13, %xmm1, %xmm5
	vdivss	%xmm15, %xmm5, %xmm5
	vaddss	%xmm5, %xmm14, %xmm6
	leal	1(%r12), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm0, %xmm0
	vmovss	-8(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm13, %xmm3, %xmm3
	vmovdqa	.LCPI157_0(%rip), %xmm4 # xmm4 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 816(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 800(%rsp)        # 16-byte Spill
	vmulss	%xmm3, %xmm1, %xmm0
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm1
	vdivss	%xmm0, %xmm15, %xmm7
	vmovd	1312(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 784(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 768(%rsp)        # 16-byte Spill
	vmovd	1248(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpsubd	%xmm1, %xmm0, %xmm0
	vpsubd	%xmm1, %xmm2, %xmm5
	vmovd	%r11d, %xmm2
	vpsubd	%xmm1, %xmm2, %xmm1
	movl	%r10d, %eax
	sarl	$31, %eax
	movl	1120(%rsp), %esi        # 4-byte Reload
	andl	%esi, %eax
	addl	%r10d, %eax
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	testl	%r9d, %r9d
	movl	1024(%rsp), %r10d       # 4-byte Reload
	cmovgl	1056(%rsp), %r10d       # 4-byte Folded Reload
	movq	1200(%rsp), %rdx        # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	%rcx, 1088(%rsp)        # 8-byte Spill
	movq	880(%rsp), %r9          # 8-byte Reload
	subq	%r9, %rdx
	movq	%rdx, 1200(%rsp)        # 8-byte Spill
	movl	1008(%rsp), %r11d       # 4-byte Reload
	movl	%r11d, %ecx
	subl	%eax, %ecx
	movq	1152(%rsp), %r12        # 8-byte Reload
	cmpl	%eax, %r12d
	cmovgl	%eax, %ecx
	addl	%r15d, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%r15d, %ecx
	cmovll	%r15d, %ecx
	movq	1400(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi), %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	leal	-2(%r15,%r12), %edx
	cmpl	%esi, %edx
	cmovlel	%ecx, %eax
	leal	-2(%r15), %edx
	cmpl	%esi, %edx
	cmovgl	%ecx, %eax
	movslq	%r10d, %rcx
	movslq	%eax, %r14
	imulq	%r13, %r14
	movq	%r14, %rax
	movq	%r9, %r10
	subq	%r10, %rax
	addq	%rcx, %rax
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	subl	%edi, %eax
	cmpl	%edi, %r12d
	cmovgl	%edi, %eax
	addl	%r15d, %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	movl	%eax, %edx
	leal	-2(%rsi), %eax
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebx
	leal	2(%r15,%r12), %eax
	cmpl	%r15d, %ebx
	cmovll	%r15d, %ebx
	cmpl	%esi, %eax
	leal	2(%r15), %eax
	cmovlel	%edx, %ebx
	cmpl	%esi, %eax
	movq	%rsi, %r9
	cmovgl	%edx, %ebx
	movslq	%ebx, %rdi
	imulq	%r13, %rdi
	movq	%rdi, %r12
	movq	%r10, %rax
	subq	%rax, %r12
	addq	%rcx, %r12
	movq	1200(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rcx), %r13
	movslq	1232(%rsp), %rcx        # 4-byte Folded Reload
	addq	%rcx, %rdx
	movq	%rdx, %r15
	subq	%rax, %rcx
	movslq	912(%rsp), %rbx         # 4-byte Folded Reload
	movq	%rbx, %rax
	shlq	$5, %rax
	addq	$40, %rax
	movl	%r9d, %edx
	andl	$63, %edx
	imulq	%rax, %rdx
	leaq	(%rdi,%rcx), %rax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	leal	6(%r9), %r11d
	movl	-4(%rsp), %edi          # 4-byte Reload
	subl	%edi, %r11d
	movl	52(%rsp), %eax          # 4-byte Reload
	andl	$-32, %eax
	addl	$64, %eax
	imull	%eax, %r11d
	movq	%r11, 736(%rsp)         # 8-byte Spill
	leaq	(%r14,%rcx), %rcx
	movq	%rcx, 1152(%rsp)        # 8-byte Spill
	movq	56(%rsp), %r14          # 8-byte Reload
	movq	%r14, %rcx
	sarq	$63, %rcx
	leal	10(%r9), %r10d
	subl	%edi, %r10d
	imull	%eax, %r10d
	movq	%r10, 728(%rsp)         # 8-byte Spill
	leal	7(%r9), %esi
	subl	%edi, %esi
	imull	%eax, %esi
	andq	%r14, %rcx
	subq	%rcx, %rdx
	movq	%rdx, 760(%rsp)         # 8-byte Spill
	leal	9(%r9), %edx
	subl	%edi, %edx
	imull	%eax, %edx
	leal	8(%r9), %r9d
	subl	%edi, %r9d
	imull	%eax, %r9d
	movq	%r9, 704(%rsp)          # 8-byte Spill
	movq	40(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vmovdqa	%xmm3, 688(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm2
	vmovaps	%xmm2, 672(%rsp)        # 16-byte Spill
	movq	864(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 656(%rsp)        # 16-byte Spill
	movq	832(%rsp), %r14         # 8-byte Reload
	movl	%r14d, %eax
	subl	%ecx, %eax
	leal	-1(%rcx,%rdi), %ecx
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 608(%rsp)        # 16-byte Spill
	movq	896(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	movl	1312(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 576(%rsp)        # 16-byte Spill
	movl	1248(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 560(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm2
	vmovaps	%xmm2, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm2
	vmovaps	%xmm2, 1232(%rsp)       # 16-byte Spill
	movl	1216(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 192(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 176(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vmovdqa	%xmm0, 544(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vmovdqa	%xmm0, 528(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm7, %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	movl	%ebx, %ecx
	shll	$10, %ecx
	leal	(%rcx,%rcx,2), %ecx
	shll	$9, %ebx
	leal	(%rbx,%rbx,2), %edi
	addl	%edi, %esi
	movq	%rsi, 720(%rsp)         # 8-byte Spill
	addl	%edi, %edx
	movq	%rdx, 712(%rsp)         # 8-byte Spill
	leal	(%r11,%rcx), %edx
	movq	%rdx, 512(%rsp)         # 8-byte Spill
	leal	(%r10,%rcx), %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	leal	(%rcx,%r9), %ecx
	movq	%rcx, 480(%rsp)         # 8-byte Spill
	leal	(%rdi,%r9), %ecx
	movq	%rcx, 464(%rsp)         # 8-byte Spill
	leal	-2(%rax), %ecx
	movq	%rcx, 448(%rsp)         # 8-byte Spill
	addl	$-1, %eax
	movq	%rax, 640(%rsp)         # 8-byte Spill
	leal	-1(%r14), %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	leal	-2(%r14), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	movq	32(%rsp), %rax          # 8-byte Reload
	movq	1088(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 400(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r12,4), %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	movq	1120(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r13,4), %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1152(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vpabsd	%xmm3, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm3, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI157_3(%rip), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_4(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_5(%rip), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_6(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB157_3:                              # %for gV.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movl	%r8d, %r12d
	testl	%r12d, %r12d
	setne	%sil
	sete	1088(%rsp)              # 1-byte Folded Spill
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %r13d
	movl	%r13d, %r15d
	andl	$1, %r15d
	sete	1120(%rsp)              # 1-byte Folded Spill
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI157_2(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	688(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r11d
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %r11d, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	384(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm12
	vpinsrd	$2, %ebx, %xmm1, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r13d, %xmm1
	vpbroadcastd	%xmm1, %xmm2
	vmovdqa	816(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpxor	%xmm3, %xmm1, %xmm1
	vmovdqa	800(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm4, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vmovdqa	624(%rsp), %xmm10       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm10, %xmm4
	vmovdqa	368(%rsp), %xmm7        # 16-byte Reload
	vpsubd	%xmm0, %xmm7, %xmm6
	vblendvps	%xmm4, %xmm0, %xmm6, %xmm0
	vmovdqa	656(%rsp), %xmm5        # 16-byte Reload
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	608(%rsp), %xmm3        # 16-byte Reload
	vpminsd	%xmm3, %xmm0, %xmm0
	vpmaxsd	%xmm5, %xmm0, %xmm0
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm3, %xmm4, %xmm4
	vpmaxsd	%xmm5, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	672(%rsp), %xmm11       # 16-byte Reload
	vpmulld	%xmm11, %xmm0, %xmm0
	vpsubd	592(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovdqa	%xmm4, 928(%rsp)        # 16-byte Spill
	vpaddd	576(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	1408(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	vpaddd	560(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	movq	1304(%rsp), %rbx        # 8-byte Reload
	vmovups	12312(%rbx,%rax,4), %xmm15
	vmovups	12328(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 1008(%rsp)       # 16-byte Spill
	movq	712(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	12312(%rbx,%rax,4), %xmm9
	vmovups	12328(%rbx,%rax,4), %xmm8
	vmovaps	%xmm8, 960(%rsp)        # 16-byte Spill
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	12312(%rbx,%rax,4), %xmm6
	vmovdqa	784(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm4
	vpxor	.LCPI157_9(%rip), %xmm4, %xmm4
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm2
	vpor	%xmm4, %xmm2, %xmm2
	vpcmpgtd	%xmm12, %xmm10, %xmm4
	vpsubd	%xmm12, %xmm7, %xmm1
	vblendvps	%xmm4, %xmm12, %xmm1, %xmm1
	vpaddd	%xmm5, %xmm1, %xmm1
	vpminsd	%xmm3, %xmm1, %xmm1
	vpmaxsd	%xmm5, %xmm1, %xmm1
	movq	432(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rbp), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm3, %xmm4, %xmm4
	vpmaxsd	%xmm5, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	%xmm11, %xmm1, %xmm1
	vmovdqa	%xmm1, 1024(%rsp)       # 16-byte Spill
	vmovups	12328(%rbx,%rax,4), %xmm2
	vpaddd	544(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovq	%xmm1, %rax
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%rdx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	sarq	$32, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	movl	%r12d, %eax
	movl	%r12d, %r8d
	andl	%r13d, %eax
	vmulps	1280(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm0[0,2]
	vmovaps	%xmm15, %xmm10
	vmovaps	1232(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	1248(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	1312(%rsp), %xmm7       # 16-byte Reload
	vminps	%xmm7, %xmm1, %xmm1
	vmulps	1264(%rsp), %xmm13, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm8[0,2]
	vmovaps	%xmm9, %xmm8
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm1, %xmm1
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm9, %xmm4, %xmm4
	vaddps	%xmm4, %xmm1, %xmm12
	vmovaps	400(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm5, %xmm13, %xmm1
	vshufps	$136, %xmm2, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm14, %xmm5, %xmm5
	vshufps	$221, %xmm2, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm2[1,3]
	vsubps	%xmm0, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm7, %xmm13
	vmaxps	%xmm9, %xmm1, %xmm0
	vmovups	%ymm0, 1056(%rsp)       # 32-byte Spill
	vmovaps	%xmm0, %xmm11
	jne	.LBB157_5
# BB#4:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
.LBB157_5:                              # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmulps	%xmm5, %xmm2, %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm3
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm6
	movq	728(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm4
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm15
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm0
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm1
	vmulps	1216(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	andb	1120(%rsp), %sil        # 1-byte Folded Reload
	movq	1400(%rsp), %r9         # 8-byte Reload
	jne	.LBB157_6
# BB#7:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 864(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, %xmm9
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 912(%rsp)        # 16-byte Spill
	movq	%rdx, %rdi
	movb	1088(%rsp), %r10b       # 1-byte Reload
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm13, %xmm5
	vmovaps	960(%rsp), %xmm15       # 16-byte Reload
	vxorps	%xmm3, %xmm3, %xmm3
	jmp	.LBB157_8
	.align	16, 0x90
.LBB157_6:                              #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm15, 864(%rsp)       # 16-byte Spill
	vmovaps	912(%rsp), %xmm2        # 16-byte Reload
	vmulps	288(%rsp), %xmm2, %xmm9 # 16-byte Folded Reload
	vmovaps	%xmm1, %xmm5
	vmovaps	%xmm5, 880(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm3, %xmm6, %xmm1 # xmm1 = xmm6[0,2],xmm3[0,2]
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 848(%rsp)        # 16-byte Spill
	vmovaps	336(%rsp), %xmm3        # 16-byte Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vmovaps	%xmm0, %xmm6
	vmovaps	%xmm6, 896(%rsp)        # 16-byte Spill
	vmovaps	352(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm1
	vmulps	%xmm1, %xmm9, %xmm11
	vmulps	272(%rsp), %xmm2, %xmm12 # 16-byte Folded Reload
	vmovaps	%xmm4, %xmm9
	vshufps	$136, %xmm9, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm9[0,2]
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	256(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm6, %xmm5, %xmm2 # xmm2 = xmm5[0,2],xmm6[0,2]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmovaps	%xmm13, %xmm5
	vminps	%xmm5, %xmm4, %xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm4, %xmm4
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfmsub213ps	%xmm4, %xmm0, %xmm2
	vminps	%xmm5, %xmm11, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm1, %xmm2, %xmm11
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm0, %xmm11
	vmovaps	%xmm7, 912(%rsp)        # 16-byte Spill
	movq	%rdx, %rdi
	movb	1088(%rsp), %r10b       # 1-byte Reload
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vmovaps	960(%rsp), %xmm15       # 16-byte Reload
.LBB157_8:                              # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	944(%rsp), %xmm1        # 16-byte Reload
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm3
	vmovaps	%xmm3, %xmm12
	testb	%sil, %sil
	vxorps	%xmm4, %xmm4, %xmm4
	jne	.LBB157_10
# BB#9:                                 # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vxorps	%xmm12, %xmm12, %xmm12
.LBB157_10:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	movl	%r8d, %eax
	vmulps	1280(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vshufps	$221, %xmm0, %xmm10, %xmm2 # xmm2 = xmm10[1,3],xmm0[1,3]
	vmovaps	1232(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	1248(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vmulps	1264(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vshufps	$221, %xmm15, %xmm8, %xmm0 # xmm0 = xmm8[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm13, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1216(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	andl	%r13d, %eax
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	jne	.LBB157_11
# BB#12:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm5, 1312(%rsp)       # 16-byte Spill
	vmovups	%ymm3, 1088(%rsp)       # 32-byte Spill
	vmovdqa	1024(%rsp), %xmm7       # 16-byte Reload
	jmp	.LBB157_13
	.align	16, 0x90
.LBB157_11:                             #   in Loop: Header=BB157_3 Depth=1
	vmovups	%ymm3, 1088(%rsp)       # 32-byte Spill
	vmovdqa	1024(%rsp), %xmm7       # 16-byte Reload
	vpaddd	176(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vshufps	$221, 848(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	288(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm5, %xmm6
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovaps	336(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	352(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	864(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, %xmm9, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm9[1,3]
	vxorps	%xmm9, %xmm9, %xmm9
	vmulps	272(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	880(%rsp), %xmm4        # 16-byte Reload
	vshufps	$221, 896(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	256(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm1, %xmm0, %xmm12
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm8, %xmm0, %xmm12
.LBB157_13:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vpaddd	528(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm15
	vmovups	24616(%rbx,%rax,4), %xmm0
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm10
	vmovups	24616(%rbx,%rax,4), %xmm14
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm9
	vmovups	24616(%rbx,%rax,4), %xmm13
	andb	%r15b, %r10b
	jne	.LBB157_14
# BB#15:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm8, 1120(%rsp)       # 16-byte Spill
	jmp	.LBB157_16
	.align	16, 0x90
.LBB157_14:                             #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm8, 1120(%rsp)       # 16-byte Spill
	vmovdqa	928(%rsp), %xmm2        # 16-byte Reload
	vpaddd	192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rdi,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	240(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm15, %xmm5 # xmm5 = xmm15[0,2],xmm0[0,2]
	vmovaps	304(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmovaps	320(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vmulps	224(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmulps	208(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm13, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm13[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	1312(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm5, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm5, %xmm5
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm3, %xmm2
	vminps	%xmm6, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vsubps	%xmm4, %xmm2, %xmm11
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vfmadd213ps	912(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
.LBB157_16:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	movq	1376(%rsp), %rdx        # 8-byte Reload
	vmovups	1056(%rsp), %ymm6       # 32-byte Reload
	movl	%r13d, %eax
	orl	%r9d, %eax
	andl	$1, %eax
	je	.LBB157_18
# BB#17:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm11, %xmm6
.LBB157_18:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	testl	%eax, %eax
	jne	.LBB157_20
# BB#19:                                #   in Loop: Header=BB157_3 Depth=1
	vshufps	$221, %xmm0, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm0[1,3]
	vmulps	240(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	304(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	320(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm14, %xmm10, %xmm2 # xmm2 = xmm10[1,3],xmm14[1,3]
	vmulps	224(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm13, %xmm9, %xmm4 # xmm4 = xmm9[1,3],xmm13[1,3]
	vmulps	208(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vmovaps	1312(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm3, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vmovaps	1200(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm1
	vsubps	%xmm0, %xmm1, %xmm12
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1120(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
.LBB157_20:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovups	1088(%rsp), %ymm1       # 32-byte Reload
	movq	%rdi, 1408(%rsp)        # 8-byte Spill
	testb	%r10b, %r10b
	jne	.LBB157_22
# BB#21:                                # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	%xmm12, %xmm1
.LBB157_22:                             # %for gV.s0.v10.v10
                                        #   in Loop: Header=BB157_3 Depth=1
	vmovaps	.LCPI157_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI157_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm6, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	760(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1368(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %ebp
	addl	$-1, %edx
	movq	%rdx, 1376(%rsp)        # 8-byte Spill
	jne	.LBB157_3
	jmp	.LBB157_111
.LBB157_23:                             # %false_bb
	movq	%rax, 1304(%rsp)        # 8-byte Spill
	cmpl	%r15d, 48(%rdx)
	jle	.LBB157_30
# BB#24:                                # %true_bb1
	vmovss	%xmm7, -60(%rsp)        # 4-byte Spill
	vmovss	%xmm3, -56(%rsp)        # 4-byte Spill
	vmovss	%xmm10, -52(%rsp)       # 4-byte Spill
	vmovss	%xmm2, -48(%rsp)        # 4-byte Spill
	vmovss	%xmm11, -44(%rsp)       # 4-byte Spill
	vmovss	%xmm6, -40(%rsp)        # 4-byte Spill
	vmovss	%xmm4, -36(%rsp)        # 4-byte Spill
	vmovss	%xmm12, -32(%rsp)       # 4-byte Spill
	vmovss	%xmm15, -28(%rsp)       # 4-byte Spill
	vmovss	%xmm14, -24(%rsp)       # 4-byte Spill
	vmovss	%xmm13, -20(%rsp)       # 4-byte Spill
	vmovss	%xmm0, -8(%rsp)         # 4-byte Spill
	movq	56(%rsp), %rax          # 8-byte Reload
	movq	%r15, 1400(%rsp)        # 8-byte Spill
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	movq	%rsi, 1280(%rsp)        # 8-byte Spill
	movq	864(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %edi
	subl	%esi, %edi
	movl	%edi, %eax
	sarl	$3, %eax
	leal	1(%rax), %ecx
	leal	6(%rdi), %ebx
	sarl	$3, %ebx
	cmpl	%ebx, %eax
	cmovgel	%ecx, %ebx
	movl	%ebx, -100(%rsp)        # 4-byte Spill
	leal	7(%rdi), %eax
	sarl	$3, %eax
	movq	%rax, -88(%rsp)         # 8-byte Spill
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovgel	%ebx, %ecx
	addl	$9, %edi
	sarl	$3, %edi
	movq	%rdi, -96(%rsp)         # 8-byte Spill
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	xorl	%r15d, %r15d
	testl	%ecx, %ecx
	cmovsl	%r15d, %ecx
	movl	%ecx, -104(%rsp)        # 4-byte Spill
	leal	43(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, 760(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	movl	%eax, %edi
	cmovgl	%ecx, %edi
	movl	%edi, -108(%rsp)        # 4-byte Spill
	movq	40(%rsp), %rcx          # 8-byte Reload
	leal	(%rdx,%rcx), %eax
	movq	%rax, -72(%rsp)         # 8-byte Spill
	subl	%esi, %eax
	subl	%esi, %ecx
	leal	(%rcx,%rdx), %ecx
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	addl	$-14, %ecx
	leal	-13(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-12(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-11(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-6(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-5(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-2(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	-1(%rax), %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	1(%rax), %edx
	cmpl	%ecx, %eax
	cmovgel	%ecx, %edx
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	addl	$35, %ebp
	cmpl	%edx, %ebp
	cmovgl	%edx, %ebp
	sarl	$3, %ebp
	addl	$1, %ebp
	movq	%rbp, 1376(%rsp)        # 8-byte Spill
	cmpl	%ebp, %edi
	movl	%ebp, %eax
	cmovgel	%edi, %eax
	movl	%eax, -76(%rsp)         # 4-byte Spill
	testl	%edi, %edi
	jle	.LBB157_52
# BB#25:                                # %for gV.s0.v10.v104.preheader
	movq	16(%rsp), %r10          # 8-byte Reload
	leal	(%r10,%r10), %edx
	movl	%edx, 1152(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r10d, %edi
	sarl	$31, %edi
	andnl	%edx, %edi, %ecx
	movl	%edx, %esi
	andl	%eax, %edi
	orl	%ecx, %edi
	movq	104(%rsp), %r8          # 8-byte Reload
	movl	%r8d, %eax
	negl	%eax
	cltd
	idivl	%esi
	movl	%esi, %ebp
	leal	(%r8,%r10), %r12d
	leal	-1(%r10,%r10), %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	leal	-1(%r8,%r10), %r14d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%esi, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r10d
	cmovgl	%eax, %ecx
	addl	%r8d, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	movl	$0, %eax
	cmovlel	%r14d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	testl	%r12d, %r12d
	cmovlel	%ecx, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%esi, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r10d
	cmovgl	%eax, %ecx
	addl	%r8d, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	movl	%ecx, 1216(%rsp)        # 4-byte Spill
	cmpl	$3, %r12d
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %r12d
	cmovll	%ecx, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movq	24(%rsp), %r9           # 8-byte Reload
	leal	(%r9,%r9), %ecx
	movl	%ecx, 1120(%rsp)        # 4-byte Spill
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r9d, %esi
	movl	$2, %eax
	movq	-16(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %eax
	cltd
	idivl	%ecx
	sarl	$31, %esi
	andnl	%ecx, %esi, %eax
	andl	%ebp, %esi
	leal	-1(%r9,%r9), %ebp
	orl	%eax, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%ebp, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	leal	-1(%rbx,%r9), %ecx
	addl	%ebx, %r11d
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	cmpl	%ebx, %r11d
	cmovll	%ebx, %r11d
	leal	(%rbx,%r9), %r13d
	cmpl	$3, %r13d
	movl	$2, %eax
	cmovll	%ecx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	$3, %r13d
	cmovll	%r11d, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r8d, %eax
	cltd
	idivl	1152(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	1264(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %r10d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	cmpl	%edx, %r14d
	cmovlel	%r14d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	cmpl	$1, %r12d
	setg	%al
	cmpl	$2, %r12d
	cmovgel	%r15d, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	cmpl	$2, %r12d
	movl	$1, %eax
	cmovll	%edx, %r14d
	subl	%ebx, %eax
	cltd
	movl	1120(%rsp), %r12d       # 4-byte Reload
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%ebp, %r10d
	subl	%eax, %r10d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r10d
	addl	%ebx, %r10d
	cmpl	%r10d, %ecx
	cmovlel	%ecx, %r10d
	cmpl	%ebx, %r10d
	cmovll	%ebx, %r10d
	cmpl	$1, %r13d
	setg	%al
	cmpl	$2, %r13d
	movl	$0, %edi
	cmovll	%ecx, %edi
	movzbl	%al, %eax
	orl	%eax, %edi
	cmpl	%ebx, %edi
	cmovll	%ebx, %edi
	cmpl	$2, %r13d
	cmovll	%r10d, %edi
	movl	%ebx, %eax
	negl	%eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	cmpl	%eax, %r9d
	cmovgl	%eax, %ebp
	addl	%ebx, %ebp
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	cmpl	%ebx, %ebp
	cmovll	%ebx, %ebp
	testl	%r13d, %r13d
	cmovgl	%r15d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	testl	%r13d, %r13d
	cmovlel	%ebp, %ecx
	movq	1400(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %esi
	andl	$1, %esi
	movq	40(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm10
	testl	%ebx, %ebx
	cmovgl	%ebp, %ecx
	movq	-72(%rsp), %r13         # 8-byte Reload
	leal	2(%r13), %eax
	vmovd	%eax, %xmm8
	movq	864(%rsp), %r12         # 8-byte Reload
	leal	2(%r12), %eax
	vmovd	%eax, %xmm9
	movq	896(%rsp), %rax         # 8-byte Reload
	imull	%r12d, %eax
	leal	-1(%r13), %edx
	vmovd	%edx, %xmm11
	addl	%ebx, %eax
	vmovd	%eax, %xmm4
	vmovd	%eax, %xmm13
	vmovd	%ecx, %xmm12
	vmovd	%ecx, %xmm3
	cmpl	$1, %ebx
	cmovgl	%r10d, %edi
	vmovd	%edi, %xmm14
	vmovd	%edi, %xmm5
	cmpl	$1, %r8d
	cmovgl	1264(%rsp), %r14d       # 4-byte Folded Reload
	movl	52(%rsp), %r10d         # 4-byte Reload
	movq	(%rsp), %rdx            # 8-byte Reload
	movl	%edx, %eax
	movq	8(%rsp), %rdi           # 8-byte Reload
	imull	%edi, %eax
	sarl	$5, %r10d
	addl	%r8d, %eax
	movslq	%eax, %rbp
	movq	%r9, %rcx
	movslq	%ecx, %r9
	movslq	%r14d, %rdi
	movq	%rdx, %rax
	imulq	%r9, %rax
	subq	%rbp, %rax
	movq	%rax, %r14
	leaq	1(%r9), %rax
	imulq	%rdx, %rax
	subq	%rbp, %rax
	addq	%rdi, %rax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	leaq	-1(%r9), %rax
	imulq	%rdx, %rax
	subq	%rbp, %rax
	addq	%rdi, %rax
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	leaq	(%r14,%rdi), %rax
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	cmpl	$2, %ebx
	movl	1312(%rsp), %eax        # 4-byte Reload
	cmovgl	%r11d, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	cmpl	$2, %r8d
	movl	1200(%rsp), %edi        # 4-byte Reload
	cmovgl	1216(%rsp), %edi        # 4-byte Folded Reload
	testl	%r8d, %r8d
	movl	1232(%rsp), %eax        # 4-byte Reload
	cmovgl	1248(%rsp), %eax        # 4-byte Folded Reload
	movslq	%eax, %rbx
	leaq	(%r14,%rbx), %rax
	movq	%rax, 1056(%rsp)        # 8-byte Spill
	leaq	2(%r9), %r12
	imulq	%rdx, %r12
	subq	%rbp, %r12
	leaq	(%r12,%rbx), %rax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	addq	$-2, %r9
	imulq	%rdx, %r9
	subq	%rbp, %r9
	movslq	%edi, %rbp
	addq	%rbp, %r14
	movq	%r14, 1216(%rsp)        # 8-byte Spill
	addq	%rbp, %r12
	movq	%rcx, %r11
	leaq	(%rbx,%r9), %rax
	movq	%rax, 1008(%rsp)        # 8-byte Spill
	leaq	(%rbp,%r9), %rax
	movq	%rax, 1024(%rsp)        # 8-byte Spill
	leal	1(%r13), %ebx
	vmovd	%ebx, %xmm2
	movslq	%r10d, %r8
	movq	%r8, %rbx
	shlq	$5, %rbx
	addq	$40, %rbx
	movl	%r11d, %edi
	andl	$63, %edi
	imulq	%rbx, %rdi
	leal	6(%r11), %r10d
	movl	-4(%rsp), %ecx          # 4-byte Reload
	subl	%ecx, %r10d
	movl	52(%rsp), %ebx          # 4-byte Reload
	andl	$-32, %ebx
	addl	$64, %ebx
	imull	%ebx, %r10d
	movq	%r10, 736(%rsp)         # 8-byte Spill
	leal	10(%r11), %r9d
	subl	%ecx, %r9d
	imull	%ebx, %r9d
	movq	%r9, 728(%rsp)          # 8-byte Spill
	movq	864(%rsp), %r14         # 8-byte Reload
	leal	1(%r14), %ebp
	vmovd	%ebp, %xmm7
	movq	56(%rsp), %rdx          # 8-byte Reload
	movq	%rdx, %rbp
	sarq	$63, %rbp
	leal	7(%r11), %eax
	subl	%ecx, %eax
	imull	%ebx, %eax
	leal	9(%r11), %r13d
	subl	%ecx, %r13d
	imull	%ebx, %r13d
	andq	%rdx, %rbp
	subq	%rbp, %rdi
	movq	%rdi, 768(%rsp)         # 8-byte Spill
	leal	8(%r11), %edi
	subl	%ecx, %edi
	imull	%ebx, %edi
	movq	%rdi, 704(%rsp)         # 8-byte Spill
	movl	%r8d, %r11d
	shll	$10, %r11d
	shll	$9, %r8d
	leal	(%r8,%r8,2), %r8d
	addl	%r8d, %eax
	movq	%rax, 720(%rsp)         # 8-byte Spill
	addl	%r8d, %r13d
	movq	%r13, 712(%rsp)         # 8-byte Spill
	movq	1280(%rsp), %r13        # 8-byte Reload
	movl	%r13d, %eax
	subl	%r14d, %eax
	leal	-2(%rax), %edx
	movq	%rdx, 672(%rsp)         # 8-byte Spill
	addl	$-1, %eax
	movq	%rax, 688(%rsp)         # 8-byte Spill
	movl	760(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	movq	-88(%rsp), %rdx         # 8-byte Reload
	movl	-100(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	movl	%edx, %ebp
	cmovgel	%eax, %ebp
	movq	-96(%rsp), %rdx         # 8-byte Reload
	cmpl	%edx, %ebp
	cmovll	%edx, %ebp
	testl	%ebp, %ebp
	cmovsl	%r15d, %ebp
	notl	%ebp
	cmpl	%ebp, %ebx
	cmovgel	%ebx, %ebp
	vpbroadcastd	%xmm10, %xmm1
	vmovdqa	%xmm1, 656(%rsp)        # 16-byte Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vmovdqa	.LCPI157_0(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 624(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 608(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 592(%rsp)        # 16-byte Spill
	movq	40(%rsp), %rax          # 8-byte Reload
	vmovd	%eax, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 576(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 560(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm0
	vmovdqa	%xmm0, 544(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 528(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 512(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 496(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 480(%rsp)        # 16-byte Spill
	movl	1312(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm0
	vpsubd	%xmm4, %xmm12, %xmm9
	vpsubd	%xmm4, %xmm14, %xmm2
	vpsubd	%xmm4, %xmm0, %xmm8
	vmovss	.LCPI157_1(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vmovss	-56(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm3, %xmm4
	vmovss	-48(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	-52(%rsp), %xmm6        # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm0, %xmm5
	vmulss	%xmm5, %xmm4, %xmm5
	vmulss	%xmm6, %xmm4, %xmm4
	vmovss	-60(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vdivss	%xmm5, %xmm0, %xmm5
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 464(%rsp)        # 16-byte Spill
	vmovd	%eax, %xmm4
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	vmovss	-44(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm3, %xmm4
	vmovss	-36(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	-40(%rsp), %xmm6        # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm0, %xmm5
	vmulss	%xmm5, %xmm4, %xmm5
	vmulss	%xmm6, %xmm4, %xmm4
	vmovss	-32(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vdivss	%xmm5, %xmm0, %xmm5
	vmovss	-24(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm3, %xmm3
	vmovss	-8(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	-20(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm6, %xmm6
	vmulss	%xmm6, %xmm3, %xmm6
	vmulss	%xmm7, %xmm3, %xmm3
	vmovss	-28(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vdivss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm0, %xmm3
	vdivss	%xmm6, %xmm7, %xmm6
	vpbroadcastd	%xmm9, %xmm0
	vmovdqa	%xmm0, 64(%rsp)         # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm0
	vmovdqa	%xmm0, 448(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vmovdqa	%xmm0, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	leal	(%r11,%r11,2), %ebx
	leal	(%r10,%rbx), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	(%r9,%rbx), %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	leal	(%rbx,%rdi), %eax
	movq	%rax, 384(%rsp)         # 8-byte Spill
	leal	(%r8,%rdi), %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	leal	-1(%r13), %eax
	movq	%rax, 352(%rsp)         # 8-byte Spill
	leal	-2(%r13), %eax
	movq	%rax, 336(%rsp)         # 8-byte Spill
	movq	32(%rsp), %rcx          # 8-byte Reload
	movq	1120(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	1152(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	movq	1088(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	movq	1008(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	1056(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	movq	1024(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%r12,4), %xmm0
	vmovaps	%xmm0, 128(%rsp)        # 16-byte Spill
	movq	1216(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 112(%rsp)        # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI157_3(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_4(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_5(%rip), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_6(%rip), %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB157_26:                             # %for gV.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movl	%ebp, 1120(%rsp)        # 4-byte Spill
	testl	%esi, %esi
	setne	%r14b
	sete	1088(%rsp)              # 1-byte Folded Spill
	movq	1280(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r15), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 1056(%rsp)        # 4-byte Spill
	sete	%bpl
	movq	688(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI157_2(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	656(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r11d
	movq	672(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r13d
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %r11d, %xmm0, %xmm0
	vmovd	%ecx, %xmm1
	vpinsrd	$1, %ebx, %xmm1, %xmm2
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	304(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm11
	vpinsrd	$2, %edi, %xmm2, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovd	%r12d, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vmovdqa	624(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm2, %xmm2
	vpcmpeqd	%xmm13, %xmm13, %xmm13
	vpxor	%xmm13, %xmm2, %xmm2
	vmovdqa	608(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	576(%rsp), %xmm14       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vmovdqa	288(%rsp), %xmm1        # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	592(%rsp), %xmm7        # 16-byte Reload
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	560(%rsp), %xmm6        # 16-byte Reload
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	336(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm6, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	vmovdqa	640(%rsp), %xmm10       # 16-byte Reload
	vpmulld	%xmm10, %xmm0, %xmm0
	vpsubd	544(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovdqa	%xmm4, 928(%rsp)        # 16-byte Spill
	vpaddd	528(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	1408(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	vpaddd	512(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	1304(%rsp), %rbx        # 8-byte Reload
	vmovups	12312(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vmovups	12328(%rbx,%rax,4), %xmm12
	vmovaps	%xmm12, 944(%rsp)       # 16-byte Spill
	movq	712(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	12312(%rbx,%rax,4), %xmm9
	vmovups	12328(%rbx,%rax,4), %xmm8
	vmovaps	%xmm8, 960(%rsp)        # 16-byte Spill
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	12312(%rbx,%rax,4), %xmm2
	vmovdqa	496(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vmovdqa	480(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm3
	vpor	%xmm5, %xmm3, %xmm3
	vpcmpgtd	%xmm11, %xmm14, %xmm5
	vpsubd	%xmm11, %xmm1, %xmm0
	vblendvps	%xmm5, %xmm11, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	352(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm7, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm0, %xmm1, %xmm0
	vpmulld	%xmm10, %xmm0, %xmm0
	vmovdqa	%xmm0, 1008(%rsp)       # 16-byte Spill
	vmovups	12328(%rbx,%rax,4), %xmm13
	vpaddd	448(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rdx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	movq	%rdx, %r10
	movl	%esi, %eax
	andl	%r12d, %eax
	vmulps	1248(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovaps	1312(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm12, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm12[0,2]
	vmovaps	464(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	1264(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vmulps	1232(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm9, %xmm5 # xmm5 = xmm9[0,2],xmm8[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vpxor	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vminps	%xmm1, %xmm5, %xmm5
	vmaxps	%xmm14, %xmm5, %xmm5
	vaddps	%xmm5, %xmm0, %xmm10
	vmovaps	320(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm0
	vshufps	$136, %xmm13, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm13[0,2]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm11, %xmm5, %xmm4
	vshufps	$221, %xmm13, %xmm2, %xmm3 # xmm3 = xmm2[1,3],xmm13[1,3]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vminps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm8
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovups	%ymm0, 1024(%rsp)       # 32-byte Spill
	vmovaps	%xmm0, %xmm1
	jne	.LBB157_28
# BB#27:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
.LBB157_28:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmulps	%xmm4, %xmm3, %xmm14
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm6
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm2
	movq	728(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm15
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm5
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	40(%rbx,%rax,4), %xmm3
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm4
	vmovaps	272(%rsp), %xmm13       # 16-byte Reload
	vmulps	%xmm13, %xmm10, %xmm10
	andb	%bpl, %r14b
	movl	1120(%rsp), %ebp        # 4-byte Reload
	jne	.LBB157_29
# BB#36:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	%xmm2, 784(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 848(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	movb	1088(%rsp), %r8b        # 1-byte Reload
	movl	1056(%rsp), %r9d        # 4-byte Reload
	vmovaps	1312(%rsp), %xmm12      # 16-byte Reload
	vmovaps	944(%rsp), %xmm15       # 16-byte Reload
	vmovaps	%xmm8, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB157_37
	.align	16, 0x90
.LBB157_29:                             #   in Loop: Header=BB157_26 Depth=1
	vmovaps	912(%rsp), %xmm1        # 16-byte Reload
	vmovaps	%xmm5, 816(%rsp)        # 16-byte Spill
	vmulps	192(%rsp), %xmm1, %xmm12 # 16-byte Folded Reload
	vmovaps	%xmm6, 800(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm2, %xmm6 # xmm6 = xmm2[0,2],xmm6[0,2]
	vmovaps	%xmm2, 784(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, %xmm0
	vmovaps	%xmm0, 848(%rsp)        # 16-byte Spill
	vmovaps	240(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vmovaps	%xmm4, %xmm15
	vmovaps	%xmm15, 832(%rsp)       # 16-byte Spill
	vmovaps	256(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm12, %xmm12
	vmulps	176(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm5, %xmm0 # xmm0 = xmm5[0,2],xmm0[0,2]
	vsubps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	160(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm3, %xmm15, %xmm3 # xmm3 = xmm15[0,2],xmm3[0,2]
	vsubps	%xmm2, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	%xmm8, %xmm6
	vminps	%xmm6, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm6, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	1200(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm2, %xmm3
	vminps	%xmm6, %xmm12, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	%xmm0, %xmm3, %xmm2
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm0, %xmm2
	vmovaps	%xmm2, 912(%rsp)        # 16-byte Spill
	movb	1088(%rsp), %r8b        # 1-byte Reload
	movl	1056(%rsp), %r9d        # 4-byte Reload
	vmovaps	1312(%rsp), %xmm12      # 16-byte Reload
	vmovaps	944(%rsp), %xmm15       # 16-byte Reload
.LBB157_37:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	960(%rsp), %xmm2        # 16-byte Reload
	vminps	%xmm6, %xmm14, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm4
	vmovaps	%xmm4, %xmm14
	testb	%r14b, %r14b
	jne	.LBB157_39
# BB#38:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB157_39:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	movl	%esi, %eax
	vmulps	1248(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm15, %xmm12, %xmm3 # xmm3 = xmm12[1,3],xmm15[1,3]
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	1264(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmulps	1232(%rsp), %xmm11, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm9, %xmm2 # xmm2 = xmm9[1,3],xmm2[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vaddps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm13, %xmm0, %xmm8
	andl	%r12d, %eax
	vmovaps	%xmm6, %xmm9
	jne	.LBB157_40
# BB#41:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	%xmm8, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1216(%rsp)      # 16-byte Spill
	vmovups	%ymm4, 1312(%rsp)       # 32-byte Spill
	vmovdqa	1008(%rsp), %xmm6       # 16-byte Reload
	jmp	.LBB157_42
	.align	16, 0x90
.LBB157_40:                             #   in Loop: Header=BB157_26 Depth=1
	vmovaps	%xmm10, 1216(%rsp)      # 16-byte Spill
	vmovups	%ymm4, 1312(%rsp)       # 32-byte Spill
	vmovdqa	1008(%rsp), %xmm6       # 16-byte Reload
	vpaddd	64(%rsp), %xmm6, %xmm0  # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r10,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	784(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, 800(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	192(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	240(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	256(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	816(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 848(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	176(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	832(%rsp), %xmm4        # 16-byte Reload
	vshufps	$221, 880(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	160(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm9, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	1200(%rsp), %xmm4       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm4, %xmm0
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm0, %xmm14
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm8, %xmm0, %xmm14
	vmovaps	%xmm8, 1088(%rsp)       # 16-byte Spill
.LBB157_42:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	movq	%r10, 1408(%rsp)        # 8-byte Spill
	vpaddd	432(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rdi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r10,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r10,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm15
	vmovups	24616(%rbx,%rax,4), %xmm3
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm10
	vmovups	24616(%rbx,%rax,4), %xmm8
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm13
	vmovups	24616(%rbx,%rax,4), %xmm12
	andb	%r9b, %r8b
	jne	.LBB157_43
# BB#44:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovups	1024(%rsp), %ymm7       # 32-byte Reload
	vmovaps	912(%rsp), %xmm4        # 16-byte Reload
	jmp	.LBB157_45
	.align	16, 0x90
.LBB157_43:                             #   in Loop: Header=BB157_26 Depth=1
	vmovdqa	928(%rsp), %xmm2        # 16-byte Reload
	vpaddd	80(%rsp), %xmm2, %xmm6  # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rdi
	sarq	$32, %rcx
	sarq	$32, %rax
	movq	1408(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	144(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm3, %xmm15, %xmm5 # xmm5 = xmm15[0,2],xmm3[0,2]
	vmovaps	208(%rsp), %xmm11       # 16-byte Reload
	vsubps	%xmm11, %xmm5, %xmm5
	vmovaps	224(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vmulps	128(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm8[0,2]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	112(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm12, %xmm13, %xmm6 # xmm6 = xmm13[0,2],xmm12[0,2]
	vsubps	%xmm11, %xmm6, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm9, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vmovaps	1200(%rsp), %xmm6       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm6, %xmm5
	vminps	%xmm9, %xmm4, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm2, %xmm5, %xmm4
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vfmadd213ps	1216(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmovups	1024(%rsp), %ymm7       # 32-byte Reload
.LBB157_45:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	movl	%r12d, %eax
	movq	1400(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB157_47
# BB#46:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	%xmm4, %xmm7
.LBB157_47:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	testl	%eax, %eax
	jne	.LBB157_49
# BB#48:                                #   in Loop: Header=BB157_26 Depth=1
	vshufps	$221, %xmm3, %xmm15, %xmm2 # xmm2 = xmm15[1,3],xmm3[1,3]
	vmulps	144(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	208(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm2, %xmm2
	vmovaps	224(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vshufps	$221, %xmm8, %xmm10, %xmm3 # xmm3 = xmm10[1,3],xmm8[1,3]
	vmulps	128(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$221, %xmm12, %xmm13, %xmm4 # xmm4 = xmm13[1,3],xmm12[1,3]
	vmulps	112(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm9, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	1200(%rsp), %xmm1       # 16-byte Reload
	vfmsub213ps	%xmm3, %xmm1, %xmm0
	vsubps	%xmm2, %xmm0, %xmm14
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1088(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
.LBB157_49:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovups	1312(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%xmm9, 1216(%rsp)       # 16-byte Spill
	testb	%r8b, %r8b
	jne	.LBB157_51
# BB#50:                                # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	%xmm14, %xmm1
.LBB157_51:                             # %for gV.s0.v10.v104
                                        #   in Loop: Header=BB157_26 Depth=1
	vmovaps	.LCPI157_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI157_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm7, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r12d, %rax
	movq	768(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1368(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	addl	$1, %ebp
	cmpl	$-1, %ebp
	jne	.LBB157_26
.LBB157_52:                             # %end for gV.s0.v10.v105
	movl	-108(%rsp), %eax        # 4-byte Reload
	cmpl	-76(%rsp), %eax         # 4-byte Folded Reload
	jge	.LBB157_73
# BB#53:                                # %for gV.s0.v10.v108.preheader
	movq	16(%rsp), %r13          # 8-byte Reload
	leal	(%r13,%r13), %edx
	movl	%edx, 1264(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r13d, %r9d
	sarl	$31, %r9d
	andnl	%edx, %r9d, %ecx
	movl	%edx, %esi
	andl	%eax, %r9d
	orl	%ecx, %r9d
	movq	104(%rsp), %rbp         # 8-byte Reload
	leal	(%rbp,%r13), %r12d
	movl	%ebp, %eax
	negl	%eax
	cltd
	idivl	%esi
	leal	-1(%r13,%r13), %r11d
	leal	-1(%rbp,%r13), %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	movl	%r11d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r13d
	cmovgl	%eax, %ecx
	addl	%ebp, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	movl	%ecx, 1232(%rsp)        # 4-byte Spill
	xorl	%r14d, %r14d
	testl	%r12d, %r12d
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	testl	%r12d, %r12d
	cmovlel	%ecx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%ebp, %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	movl	%r11d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r13d
	cmovgl	%eax, %ecx
	addl	%ebp, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	movl	%ecx, 1056(%rsp)        # 4-byte Spill
	cmpl	$3, %r12d
	movl	$2, %eax
	cmovll	%ebx, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	$3, %r12d
	cmovll	%ecx, %eax
	movl	%eax, 1024(%rsp)        # 4-byte Spill
	movq	24(%rsp), %r10          # 8-byte Reload
	leal	(%r10,%r10), %ecx
	movl	%ecx, 1312(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	movl	%r10d, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %edx
	movl	%ecx, %edi
	andl	%eax, %esi
	orl	%edx, %esi
	movl	$2, %eax
	movq	-16(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r10,%r10), %edi
	movl	%edi, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r8d
	addl	%ecx, %r8d
	leal	-1(%rcx,%r10), %r15d
	cmpl	%r8d, %r15d
	cmovlel	%r15d, %r8d
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	leal	(%rcx,%r10), %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%r15d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	$3, %edx
	cmovll	%r8d, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%ebp, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	subl	%eax, %r11d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r11d
	addl	%ebp, %r11d
	cmpl	%r11d, %ebx
	cmovlel	%ebx, %r11d
	cmpl	%ebp, %r11d
	cmovll	%ebp, %r11d
	cmpl	$1, %r12d
	setg	%al
	cmpl	$2, %r12d
	cmovgel	%r14d, %ebx
	movzbl	%al, %eax
	orl	%eax, %ebx
	cmpl	%ebp, %ebx
	cmovll	%ebp, %ebx
	cmpl	$2, %r12d
	cmovll	%r11d, %ebx
	movl	$1, %eax
	subl	%ecx, %eax
	cltd
	idivl	1312(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%edi, %r9d
	subl	%eax, %r9d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r9d
	addl	%ecx, %r9d
	cmpl	%r9d, %r15d
	cmovlel	%r15d, %r9d
	cmpl	%ecx, %r9d
	cmovll	%ecx, %r9d
	movl	1248(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%edx, %r12d
	movl	$0, %edx
	cmovll	%r15d, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	cmpl	$2, %r12d
	cmovll	%r9d, %edx
	movl	%edx, %r13d
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	1312(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %edi
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	addl	%ecx, %edi
	cmpl	%edi, %r15d
	cmovlel	%r15d, %edi
	cmpl	%ecx, %edi
	cmovll	%ecx, %edi
	testl	%r12d, %r12d
	cmovgl	%r14d, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	testl	%r12d, %r12d
	cmovlel	%edi, %r15d
	movq	1400(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	andl	$1, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	testl	%ecx, %ecx
	cmovgl	%edi, %r15d
	movl	%r15d, 1152(%rsp)       # 4-byte Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	movq	864(%rsp), %rax         # 8-byte Reload
	imull	%eax, %edx
	addl	%ecx, %edx
	movl	%edx, 1088(%rsp)        # 4-byte Spill
	cmpl	$1, %ecx
	cmovgl	%r9d, %r13d
	movl	%r13d, 1120(%rsp)       # 4-byte Spill
	cmpl	$1, %ebp
	cmovgl	%r11d, %ebx
	movq	(%rsp), %rdi            # 8-byte Reload
	movl	%edi, %eax
	movq	8(%rsp), %rdx           # 8-byte Reload
	imull	%edx, %eax
	addl	%ebp, %eax
	movslq	%eax, %r11
	movslq	%r10d, %rax
	leaq	-1(%rax), %rdx
	imulq	%rdi, %rdx
	movslq	%ebx, %rsi
	subq	%r11, %rdx
	addq	%rsi, %rdx
	movq	32(%rsp), %r9           # 8-byte Reload
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	leaq	1(%rax), %rdx
	imulq	%rdi, %rdx
	subq	%r11, %rdx
	addq	%rsi, %rdx
	vbroadcastss	(%r9,%rdx,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	%rdi, %rbx
	imulq	%rax, %rbx
	movl	52(%rsp), %r13d         # 4-byte Reload
	movl	%r13d, %r12d
	sarl	$5, %r12d
	subq	%r11, %rbx
	cmpl	$2, %ecx
	movl	1200(%rsp), %ecx        # 4-byte Reload
	cmovgl	%r8d, %ecx
	movl	%ecx, 1200(%rsp)        # 4-byte Spill
	cmpl	$2, %ebp
	movl	1024(%rsp), %edx        # 4-byte Reload
	cmovgl	1056(%rsp), %edx        # 4-byte Folded Reload
	testl	%ebp, %ebp
	movl	1216(%rsp), %ecx        # 4-byte Reload
	cmovgl	1232(%rsp), %ecx        # 4-byte Folded Reload
	leaq	(%rbx,%rsi), %rbp
	leaq	2(%rax), %rsi
	imulq	%rdi, %rsi
	vbroadcastss	(%r9,%rbp,4), %xmm0
	vmovaps	%xmm0, 736(%rsp)        # 16-byte Spill
	addq	$-2, %rax
	imulq	%rdi, %rax
	subq	%r11, %rsi
	subq	%r11, %rax
	movslq	%ecx, %rdi
	movslq	%edx, %rbp
	leaq	(%rbx,%rdi), %rcx
	movq	%rcx, 1232(%rsp)        # 8-byte Spill
	addq	%rbp, %rbx
	leaq	(%rsi,%rdi), %r11
	addq	%rbp, %rsi
	leaq	(%rdi,%rax), %r15
	leaq	(%rbp,%rax), %r8
	movslq	%r12d, %r12
	movq	%r12, %rax
	shlq	$5, %rax
	addq	$40, %rax
	movl	%r10d, %ebp
	andl	$63, %ebp
	imulq	%rax, %rbp
	movq	56(%rsp), %rdx          # 8-byte Reload
	movq	%rdx, %rax
	sarq	$63, %rax
	andq	%rdx, %rax
	subq	%rax, %rbp
	movq	%rbp, 728(%rsp)         # 8-byte Spill
	movq	-88(%rsp), %rbp         # 8-byte Reload
	movl	-100(%rsp), %eax        # 4-byte Reload
	cmpl	%ebp, %eax
	cmovgel	%eax, %ebp
	movq	-96(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %ebp
	cmovll	%eax, %ebp
	movl	760(%rsp), %eax         # 4-byte Reload
	notl	%eax
	testl	%ebp, %ebp
	cmovsl	%r14d, %ebp
	notl	%ebp
	cmpl	%ebp, %eax
	cmovgel	%eax, %ebp
	leal	6(%r10), %ecx
	movl	-4(%rsp), %edx          # 4-byte Reload
	subl	%edx, %ecx
	andl	$-32, %r13d
	addl	$64, %r13d
	imull	%r13d, %ecx
	vbroadcastss	(%r9,%r15,4), %xmm0
	vmovaps	%xmm0, 544(%rsp)        # 16-byte Spill
	vbroadcastss	(%r9,%r11,4), %xmm0
	vmovaps	%xmm0, 528(%rsp)        # 16-byte Spill
	leal	10(%r10), %r11d
	subl	%edx, %r11d
	imull	%r13d, %r11d
	leal	7(%r10), %r15d
	subl	%edx, %r15d
	imull	%r13d, %r15d
	movq	1232(%rsp), %rdi        # 8-byte Reload
	vbroadcastss	(%r9,%rdi,4), %xmm0
	vmovaps	%xmm0, 512(%rsp)        # 16-byte Spill
	vbroadcastss	(%r9,%r8,4), %xmm0
	vmovaps	%xmm0, 496(%rsp)        # 16-byte Spill
	leal	9(%r10), %r8d
	subl	%edx, %r8d
	imull	%r13d, %r8d
	leal	8(%r10), %r10d
	subl	%edx, %r10d
	imull	%r13d, %r10d
	vbroadcastss	(%r9,%rsi,4), %xmm0
	vmovaps	%xmm0, 480(%rsp)        # 16-byte Spill
	vbroadcastss	(%r9,%rbx,4), %xmm0
	vmovaps	%xmm0, 464(%rsp)        # 16-byte Spill
	movq	1304(%rsp), %rbx        # 8-byte Reload
	movl	%r12d, %eax
	shll	$10, %eax
	leal	(%rax,%rax,2), %r13d
	leal	(,%rbp,8), %edx
	leal	(%rcx,%r13), %esi
	subl	%edx, %esi
	subl	%edx, %ecx
	leal	(%r11,%r13), %r9d
	subl	%edx, %r9d
	subl	%edx, %r11d
	shll	$9, %r12d
	leal	(%r12,%r12,2), %eax
	addl	%eax, %r15d
	subl	%edx, %r15d
	addl	%eax, %r8d
	subl	%edx, %r8d
	leal	(%r13,%r10), %r13d
	subl	%edx, %r13d
	leal	(%rax,%r10), %edi
	subl	%edx, %edi
	subl	%edx, %r10d
	movq	1280(%rsp), %rax        # 8-byte Reload
	subl	%edx, %eax
	addl	$-8, %esi
	movq	%rsi, 656(%rsp)         # 8-byte Spill
	addl	$-8, %ecx
	movq	%rcx, 720(%rsp)         # 8-byte Spill
	addl	$-8, %r9d
	movq	%r9, 640(%rsp)          # 8-byte Spill
	addl	$-8, %r11d
	movq	%r11, 712(%rsp)         # 8-byte Spill
	addl	$-8, %r15d
	movq	%r15, 704(%rsp)         # 8-byte Spill
	addl	$-8, %r8d
	movq	%r8, 688(%rsp)          # 8-byte Spill
	addl	$-8, %r13d
	movq	%r13, 624(%rsp)         # 8-byte Spill
	addl	$-8, %edi
	movq	%rdi, 608(%rsp)         # 8-byte Spill
	addl	$-8, %r10d
	movq	%r10, 672(%rsp)         # 8-byte Spill
	addl	$-8, %eax
	movq	%rax, 592(%rsp)         # 8-byte Spill
	movl	%ebp, %eax
	notl	%eax
	movq	1376(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	leal	1(%rbp,%rax), %esi
	movslq	1088(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 576(%rsp)         # 8-byte Spill
	movslq	1152(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 880(%rsp)         # 8-byte Spill
	movslq	1120(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 560(%rsp)         # 8-byte Spill
	vmovss	.LCPI157_1(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	-56(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	-52(%rsp), %xmm4        # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm2
	vmovss	-60(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	-48(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm5, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 1232(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 1216(%rsp)       # 16-byte Spill
	movslq	1200(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	vmovss	-44(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	-40(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm1, %xmm2
	vmovss	-32(%rsp), %xmm4        # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	-36(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm1
	vmovss	-24(%rsp), %xmm4        # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm0, %xmm0
	vmovss	-20(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm0, %xmm3
	vmovss	-28(%rsp), %xmm6        # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vdivss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	-8(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm4, %xmm4
	vmulss	%xmm4, %xmm0, %xmm0
	vdivss	%xmm0, %xmm6, %xmm0
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 448(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 400(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI157_3(%rip), %xmm14
	movq	896(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %r13
	vbroadcastss	.LCPI157_4(%rip), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_5(%rip), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_6(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	movq	560(%rsp), %r12         # 8-byte Reload
	.align	16, 0x90
.LBB157_54:                             # %for gV.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movl	%esi, 1008(%rsp)        # 4-byte Spill
	movl	1312(%rsp), %r9d        # 4-byte Reload
	testl	%r9d, %r9d
	setne	%r15b
	sete	960(%rsp)               # 1-byte Folded Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %r8d
	movl	%r8d, %eax
	andl	$1, %eax
	movl	%eax, 944(%rsp)         # 4-byte Spill
	sete	1056(%rsp)              # 1-byte Folded Spill
	movslq	%r8d, %rbp
	movq	%rbp, 1024(%rsp)        # 8-byte Spill
	leaq	-2(%rbp), %rdx
	movq	896(%rsp), %rcx         # 8-byte Reload
	imulq	%rcx, %rdx
	movq	576(%rsp), %r11         # 8-byte Reload
	subq	%r11, %rdx
	movq	880(%rsp), %rsi         # 8-byte Reload
	leaq	(%rdx,%rsi), %rdi
	movq	1408(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdi,4), %rsi
	vmovss	(%rax,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rsi,%r13,4), %rdi
	vinsertps	$16, (%rsi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdi,%r13,4), %rsi
	vinsertps	$32, (%rdi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 848(%rsp)        # 16-byte Spill
	leaq	(%rdx,%r12), %rsi
	leaq	(%rax,%rsi,4), %rdi
	vmovss	(%rax,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdi,%r13,4), %rsi
	vinsertps	$16, (%rdi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rsi,%r13,4), %rdi
	vinsertps	$32, (%rsi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%r13,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	704(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r14), %esi
	movslq	%esi, %rsi
	vmovups	12312(%rbx,%rsi,4), %xmm15
	vmovups	12328(%rbx,%rsi,4), %xmm8
	movq	688(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r14), %esi
	movslq	%esi, %rsi
	vmovups	12312(%rbx,%rsi,4), %xmm11
	vmovups	12328(%rbx,%rsi,4), %xmm2
	vmovaps	%xmm2, 912(%rsp)        # 16-byte Spill
	movq	608(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r14), %esi
	movslq	%esi, %rsi
	vmovups	12312(%rbx,%rsi,4), %xmm1
	vmovups	12328(%rbx,%rsi,4), %xmm6
	leaq	-1(%rbp), %r10
	imulq	%rcx, %r10
	subq	%r11, %r10
	leaq	(%r10,%r12), %rsi
	vmovss	(%rax,%rsi,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	leaq	(%rax,%rsi,4), %rsi
	movq	%rax, %rbp
	vinsertps	$16, (%rsi,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	leaq	(%rsi,%r13,4), %rsi
	vinsertps	$32, (%rsi,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	leaq	(%rsi,%r13,4), %rsi
	vinsertps	$48, (%rsi,%r13,4), %xmm5, %xmm9 # xmm9 = xmm5[0,1,2],mem[0]
	andl	%r8d, %r9d
	vmulps	1264(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm8[0,2]
	vmovaps	1216(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	1232(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmulps	1248(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm2, %xmm11, %xmm2 # xmm2 = xmm11[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm10
	vmovaps	736(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm7, %xmm12, %xmm0
	vshufps	$136, %xmm6, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vmulps	%xmm9, %xmm7, %xmm7
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, %xmm12
	jne	.LBB157_56
# BB#55:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vxorps	%xmm12, %xmm12, %xmm12
.LBB157_56:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmulps	%xmm7, %xmm1, %xmm13
	movq	720(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %esi
	movslq	%esi, %rsi
	vmovups	40(%rbx,%rsi,4), %xmm6
	orq	$6, %rsi
	vmovups	(%rbx,%rsi,4), %xmm4
	movq	712(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %esi
	movslq	%esi, %rsi
	vmovups	40(%rbx,%rsi,4), %xmm1
	orq	$6, %rsi
	vmovups	(%rbx,%rsi,4), %xmm7
	movq	672(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %esi
	movslq	%esi, %rsi
	vmovups	40(%rbx,%rsi,4), %xmm3
	orq	$6, %rsi
	vmovups	(%rbx,%rsi,4), %xmm5
	vmulps	1200(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	andb	1056(%rsp), %r15b       # 1-byte Folded Reload
	jne	.LBB157_57
# BB#58:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovaps	%xmm4, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 784(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 928(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 832(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 848(%rsp)        # 16-byte Spill
	vmovups	%ymm0, 1056(%rsp)       # 32-byte Spill
	movb	960(%rsp), %r9b         # 1-byte Reload
	movl	944(%rsp), %r11d        # 4-byte Reload
	vmovaps	912(%rsp), %xmm5        # 16-byte Reload
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB157_59
	.align	16, 0x90
.LBB157_57:                             #   in Loop: Header=BB157_54 Depth=1
	vmovups	%ymm0, 1056(%rsp)       # 32-byte Spill
	vmovaps	%xmm4, 768(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, %xmm12
	vmovaps	%xmm12, 784(%rsp)       # 16-byte Spill
	vmovaps	848(%rsp), %xmm7        # 16-byte Reload
	vmulps	544(%rsp), %xmm7, %xmm10 # 16-byte Folded Reload
	vmovaps	%xmm6, 928(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm4, %xmm6 # xmm6 = xmm4[0,2],xmm6[0,2]
	vmovaps	%xmm1, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, %xmm1
	vmovaps	%xmm1, 832(%rsp)        # 16-byte Spill
	vmovaps	432(%rsp), %xmm3        # 16-byte Reload
	vsubps	%xmm3, %xmm6, %xmm6
	vmovaps	%xmm5, %xmm4
	vmovaps	%xmm4, 800(%rsp)        # 16-byte Spill
	vmovaps	448(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm6
	vmulps	%xmm6, %xmm10, %xmm10
	vmulps	528(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,2],xmm0[0,2]
	vsubps	%xmm3, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	512(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm1, %xmm4, %xmm7 # xmm7 = xmm4[0,2],xmm1[0,2]
	vsubps	%xmm3, %xmm7, %xmm7
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm14, %xmm6, %xmm6
	vmaxps	%xmm1, %xmm6, %xmm6
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm3, %xmm6
	vminps	%xmm14, %xmm10, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vsubps	%xmm0, %xmm6, %xmm12
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm2, %xmm0, %xmm12
	vmovaps	%xmm2, 848(%rsp)        # 16-byte Spill
	movb	960(%rsp), %r9b         # 1-byte Reload
	movl	944(%rsp), %r11d        # 4-byte Reload
	vmovaps	912(%rsp), %xmm5        # 16-byte Reload
.LBB157_59:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vminps	%xmm14, %xmm13, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vmovaps	%xmm3, %xmm10
	testb	%r15b, %r15b
	vxorps	%xmm4, %xmm4, %xmm4
	jne	.LBB157_61
# BB#60:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
.LBB157_61:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	movl	1312(%rsp), %eax        # 4-byte Reload
	vmulps	1264(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm8[1,3]
	vmovaps	1216(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmovaps	1232(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vmulps	1248(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vshufps	$221, %xmm5, %xmm11, %xmm5 # xmm5 = xmm11[1,3],xmm5[1,3]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm1, %xmm5, %xmm1
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
	andl	%r8d, %eax
	vmovaps	928(%rsp), %xmm1        # 16-byte Reload
	jne	.LBB157_62
# BB#63:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovaps	%xmm7, 928(%rsp)        # 16-byte Spill
	vmovups	%ymm3, 960(%rsp)        # 32-byte Spill
	jmp	.LBB157_64
	.align	16, 0x90
.LBB157_62:                             #   in Loop: Header=BB157_54 Depth=1
	vmovups	%ymm3, 960(%rsp)        # 32-byte Spill
	movq	880(%rsp), %rax         # 8-byte Reload
	leaq	(%r10,%rax), %rax
	leaq	(%rbp,%rax,4), %rsi
	leaq	(%rsi,%r13,4), %rdi
	movq	%rbx, %rcx
	leaq	(%rdi,%r13,4), %rbx
	vmovss	(%rbp,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	%rcx, %rbx
	vmovaps	768(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm1[1,3]
	vmulps	544(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vmovaps	432(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	448(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	784(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, 816(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	528(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	800(%rsp), %xmm5        # 16-byte Reload
	vshufps	$221, 832(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	512(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm1, %xmm0, %xmm10
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm7, %xmm0, %xmm10
	vmovaps	%xmm7, 928(%rsp)        # 16-byte Spill
.LBB157_64:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	movq	1152(%rsp), %rcx        # 8-byte Reload
	movq	%rbp, 1408(%rsp)        # 8-byte Spill
	addq	%rcx, %r10
	leaq	(%rbp,%r10,4), %rax
	leaq	(%rax,%r13,4), %rsi
	leaq	(%rsi,%r13,4), %rdi
	vmovss	(%rbp,%r10,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	656(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm15
	vmovups	24616(%rbx,%rax,4), %xmm5
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm9
	vmovups	24616(%rbx,%rax,4), %xmm13
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	cltq
	vmovups	24600(%rbx,%rax,4), %xmm8
	vmovups	24616(%rbx,%rax,4), %xmm11
	movb	%r9b, %al
	andb	%r11b, %al
	je	.LBB157_66
# BB#65:                                #   in Loop: Header=BB157_54 Depth=1
	addq	%rcx, %rdx
	movq	1408(%rsp), %rbp        # 8-byte Reload
	leaq	(%rbp,%rdx,4), %rax
	leaq	(%rax,%r13,4), %rsi
	leaq	(%rsi,%r13,4), %rdi
	vmovss	(%rbp,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rsi,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdi,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	496(%rsp), %xmm4, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm15, %xmm7 # xmm7 = xmm15[0,2],xmm5[0,2]
	vmovaps	400(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm7, %xmm7
	vmovaps	416(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	480(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm13, %xmm9, %xmm1 # xmm1 = xmm9[0,2],xmm13[0,2]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	464(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm11, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm11[0,2]
	vsubps	%xmm2, %xmm7, %xmm7
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm3, %xmm4, %xmm4
	vmovaps	1120(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm1, %xmm2, %xmm4
	vminps	%xmm14, %xmm6, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm1, %xmm4, %xmm12
	vmovaps	1088(%rsp), %xmm1       # 16-byte Reload
	vfmadd213ps	848(%rsp), %xmm1, %xmm12 # 16-byte Folded Reload
.LBB157_66:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	movq	1024(%rsp), %rdx        # 8-byte Reload
	movl	1008(%rsp), %esi        # 4-byte Reload
	vmovups	1056(%rsp), %ymm6       # 32-byte Reload
	movq	1400(%rsp), %rax        # 8-byte Reload
	orl	%eax, %r8d
	andl	$1, %r8d
	je	.LBB157_68
# BB#67:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovaps	%xmm12, %xmm6
.LBB157_68:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	testl	%r8d, %r8d
	jne	.LBB157_70
# BB#69:                                #   in Loop: Header=BB157_54 Depth=1
	vshufps	$221, %xmm5, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm5[1,3]
	vmulps	496(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	400(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	416(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vshufps	$221, %xmm13, %xmm9, %xmm2 # xmm2 = xmm9[1,3],xmm13[1,3]
	vmulps	480(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm11, %xmm8, %xmm4 # xmm4 = xmm8[1,3],xmm11[1,3]
	vmulps	464(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm14, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vsubps	%xmm1, %xmm0, %xmm10
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	928(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
.LBB157_70:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovups	960(%rsp), %ymm1        # 32-byte Reload
	movq	%rcx, 1152(%rsp)        # 8-byte Spill
	andb	%r11b, %r9b
	jne	.LBB157_72
# BB#71:                                # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovaps	%xmm10, %xmm1
.LBB157_72:                             # %for gV.s0.v10.v108
                                        #   in Loop: Header=BB157_54 Depth=1
	vmovaps	.LCPI157_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI157_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm6, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	728(%rsp), %rax         # 8-byte Reload
	leaq	(%rdx,%rax), %rax
	movq	1368(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	addl	$-1, %esi
	jne	.LBB157_54
.LBB157_73:                             # %end for gV.s0.v10.v109
	movq	1408(%rsp), %r12        # 8-byte Reload
	movl	-76(%rsp), %eax         # 4-byte Reload
	cmpl	760(%rsp), %eax         # 4-byte Folded Reload
	vmovss	-20(%rsp), %xmm15       # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmovss	-24(%rsp), %xmm14       # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmovss	-32(%rsp), %xmm2        # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	-36(%rsp), %xmm13       # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmovss	-40(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	-44(%rsp), %xmm12       # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vmovss	-48(%rsp), %xmm10       # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmovss	-52(%rsp), %xmm6        # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	-56(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmovss	-60(%rsp), %xmm4        # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	jge	.LBB157_111
# BB#74:                                # %for gV.s0.v10.v1012.preheader
	movq	104(%rsp), %r15         # 8-byte Reload
	movl	%r15d, %eax
	negl	%eax
	movq	16(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %ebp
	movl	%ebp, 1152(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%esi, %ebx
	sarl	$31, %ebx
	andnl	%ebp, %ebx, %ecx
	movl	%ebp, %r9d
	andl	%eax, %ebx
	orl	%ecx, %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %r13d
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r15d, %ecx
	leal	-1(%r15,%rsi), %ebp
	cmpl	%ecx, %ebp
	cmovlel	%ebp, %ecx
	cmpl	%r15d, %ecx
	cmovll	%r15d, %ecx
	movl	%ecx, 1312(%rsp)        # 4-byte Spill
	leal	(%r15,%rsi), %r8d
	testl	%r8d, %r8d
	movl	$0, %eax
	cmovlel	%ebp, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	testl	%r8d, %r8d
	cmovlel	%ecx, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r15d, %ecx
	cmpl	%ecx, %ebp
	cmovlel	%ebp, %ecx
	cmpl	%r15d, %ecx
	cmovll	%r15d, %ecx
	movl	%ecx, 1232(%rsp)        # 4-byte Spill
	cmpl	$3, %r8d
	movl	$2, %eax
	cmovll	%ebp, %eax
	cmpl	%r15d, %eax
	cmovll	%r15d, %eax
	cmpl	$3, %r8d
	cmovll	%ecx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movq	24(%rsp), %r14          # 8-byte Reload
	leal	(%r14,%r14), %edi
	movl	%edi, 1120(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	movl	%r14d, %ecx
	sarl	$31, %ecx
	andnl	%edi, %ecx, %edx
	andl	%eax, %ecx
	orl	%edx, %ecx
	movl	$2, %eax
	movq	-16(%rsp), %r10         # 8-byte Reload
	subl	%r10d, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %edi
	movl	%edi, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%r10d, %edx
	leal	-1(%r10,%r14), %r9d
	cmpl	%edx, %r9d
	cmovlel	%r9d, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	leal	(%r10,%r14), %r11d
	cmpl	$3, %r11d
	movl	$2, %eax
	cmovll	%r9d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %r11d
	cmovll	%edx, %eax
	movl	%eax, 1264(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r15d, %eax
	cltd
	idivl	1152(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	addl	%edx, %eax
	subl	%eax, %r13d
	cmpl	%eax, %esi
	cmovgl	%eax, %r13d
	addl	%r15d, %r13d
	cmpl	%r13d, %ebp
	cmovlel	%ebp, %r13d
	cmpl	%r15d, %r13d
	cmovll	%r15d, %r13d
	cmpl	$1, %r8d
	setg	%al
	cmpl	$2, %r8d
	movl	$0, %edx
	cmovgel	%edx, %ebp
	movzbl	%al, %eax
	orl	%eax, %ebp
	cmpl	%r15d, %ebp
	cmovll	%r15d, %ebp
	cmpl	$2, %r8d
	cmovll	%r13d, %ebp
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	movl	1120(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%edi, %esi
	subl	%eax, %esi
	cmpl	%eax, %r14d
	cmovgl	%eax, %esi
	addl	%r10d, %esi
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	cmpl	%r10d, %esi
	cmovll	%r10d, %esi
	cmpl	$1, %r11d
	setg	%al
	cmpl	$2, %r11d
	movl	$0, %edx
	cmovll	%r9d, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	cmpl	$2, %r11d
	cmovll	%esi, %edx
	movl	%edx, %ebx
	movl	%r10d, %eax
	negl	%eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %edi
	cmpl	%eax, %r14d
	cmovgl	%eax, %edi
	addl	%r10d, %edi
	cmpl	%edi, %r9d
	cmovlel	%r9d, %edi
	cmpl	%r10d, %edi
	cmovll	%r10d, %edi
	testl	%r11d, %r11d
	movl	$0, %eax
	cmovlel	%r9d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	movl	%eax, %edx
	testl	%r11d, %r11d
	movq	40(%rsp), %rcx          # 8-byte Reload
	leal	(%rcx,%rcx), %eax
	vmovd	%eax, %xmm8
	cmovlel	%edi, %edx
	movq	1400(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r14d
	movq	%rax, %r8
	andl	$1, %r14d
	movl	%r14d, 416(%rsp)        # 4-byte Spill
	testl	%r10d, %r10d
	cmovgl	%edi, %edx
	movl	%edx, 1152(%rsp)        # 4-byte Spill
	vmovss	.LCPI157_1(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm3, %xmm0
	vmulss	%xmm6, %xmm0, %xmm1
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm9
	movq	-72(%rsp), %rdx         # 8-byte Reload
	leal	2(%rdx), %eax
	vmovd	%eax, %xmm5
	vsubss	%xmm6, %xmm10, %xmm1
	leal	-1(%rdx), %eax
	vmovd	%eax, %xmm11
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm4, %xmm10
	vsubss	%xmm12, %xmm3, %xmm1
	vmulss	%xmm7, %xmm1, %xmm0
	vdivss	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm7, %xmm4
	vmovaps	%xmm13, %xmm7
	vmovaps	%xmm2, %xmm6
	vaddss	%xmm0, %xmm12, %xmm12
	movq	896(%rsp), %r15         # 8-byte Reload
	vmovd	%r15d, %xmm0
	movq	864(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r15d
	addl	%r10d, %r15d
	cmpl	$1, %r10d
	cmovgl	%esi, %ebx
	movl	%ebx, 1120(%rsp)        # 4-byte Spill
	movq	104(%rsp), %rdi         # 8-byte Reload
	cmpl	$1, %edi
	cmovgl	%r13d, %ebp
	movl	%r8d, %edx
	subl	-4(%rsp), %edx          # 4-byte Folded Reload
	movl	52(%rsp), %esi          # 4-byte Reload
	movl	%esi, %ebx
	andl	$-32, %esi
	addl	$64, %esi
	leal	8(%rdx), %eax
	imull	%esi, %eax
	movq	%rax, 768(%rsp)         # 8-byte Spill
	vsubss	%xmm4, %xmm7, %xmm2
	vmovd	%ecx, %xmm13
	sarl	$5, %ebx
	movl	%ebx, 1088(%rsp)        # 4-byte Spill
	movl	%ebx, %eax
	shll	$9, %eax
	leal	(%rax,%rax,2), %r9d
	movq	%r9, 736(%rsp)          # 8-byte Spill
	movq	(%rsp), %rax            # 8-byte Reload
	movq	8(%rsp), %rcx           # 8-byte Reload
	imull	%eax, %ecx
	addl	%edi, %ecx
	movslq	%ecx, %rbx
	movslq	%r8d, %rdi
	movq	%rax, %r8
	imulq	%rdi, %r8
	leal	9(%rdx), %r11d
	imull	%esi, %r11d
	movslq	%ebp, %rbp
	subq	%rbx, %r8
	addl	%r9d, %r11d
	movq	%r11, 728(%rsp)         # 8-byte Spill
	leaq	1(%rdi), %r11
	imulq	%rax, %r11
	subq	%rbx, %r11
	addq	%rbp, %r11
	leal	7(%rdx), %r13d
	imull	%esi, %r13d
	addl	%r9d, %r13d
	movq	%r13, 720(%rsp)         # 8-byte Spill
	leaq	-1(%rdi), %r9
	imulq	%rax, %r9
	subq	%rbx, %r9
	addq	%rbp, %r9
	cmpl	$2, %r10d
	movl	1264(%rsp), %ecx        # 4-byte Reload
	cmovgl	1200(%rsp), %ecx        # 4-byte Folded Reload
	movl	%ecx, 1264(%rsp)        # 4-byte Spill
	movq	104(%rsp), %rcx         # 8-byte Reload
	cmpl	$2, %ecx
	movl	1216(%rsp), %r13d       # 4-byte Reload
	cmovgl	1232(%rsp), %r13d       # 4-byte Folded Reload
	testl	%ecx, %ecx
	movl	1248(%rsp), %r10d       # 4-byte Reload
	cmovgl	1312(%rsp), %r10d       # 4-byte Folded Reload
	leal	10(%rdx), %ecx
	imull	%esi, %ecx
	movq	%rcx, 712(%rsp)         # 8-byte Spill
	addl	$6, %edx
	imull	%esi, %edx
	movq	%rdx, 784(%rsp)         # 8-byte Spill
	leaq	2(%rdi), %rsi
	imulq	%rax, %rsi
	addq	$-2, %rdi
	imulq	%rax, %rdi
	subq	%rbx, %rsi
	subq	%rbx, %rdi
	leaq	(%r8,%rbp), %rax
	movq	%rax, 1216(%rsp)        # 8-byte Spill
	movslq	%r10d, %rbp
	movslq	%r13d, %rbx
	leaq	(%r8,%rbp), %rax
	movq	%rax, 1200(%rsp)        # 8-byte Spill
	addq	%rbx, %r8
	leaq	(%rsi,%rbp), %rax
	movq	%rax, 1232(%rsp)        # 8-byte Spill
	addq	%rbx, %rsi
	leaq	(%rbp,%rdi), %rbp
	leaq	(%rbx,%rdi), %r13
	movl	1088(%rsp), %edx        # 4-byte Reload
	movslq	%edx, %rdi
	shlq	$5, %rdi
	addq	$40, %rdi
	movq	1400(%rsp), %rax        # 8-byte Reload
	andl	$63, %eax
	imulq	%rdi, %rax
	vmulss	%xmm2, %xmm1, %xmm4
	movq	-72(%rsp), %rcx         # 8-byte Reload
	addl	$1, %ecx
	vmovd	%ecx, %xmm2
	movq	56(%rsp), %rbx          # 8-byte Reload
	movq	%rbx, %rdi
	sarq	$63, %rdi
	andq	%rbx, %rdi
	subq	%rdi, %rax
	movq	%rax, 704(%rsp)         # 8-byte Spill
	movl	%edx, %edi
	shll	$10, %edi
	movl	760(%rsp), %ebx         # 4-byte Reload
	movl	-104(%rsp), %eax        # 4-byte Reload
	cmpl	%ebx, %eax
	cmovgl	%ebx, %eax
	movq	1376(%rsp), %rbx        # 8-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	vpbroadcastd	%xmm8, %xmm1
	vmovdqa	%xmm1, 688(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vdivss	%xmm4, %xmm6, %xmm8
	vsubss	%xmm14, %xmm3, %xmm3
	vmulss	%xmm15, %xmm3, %xmm5
	vmovss	-28(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vdivss	%xmm7, %xmm5, %xmm5
	vaddss	%xmm5, %xmm14, %xmm5
	vmovdqa	.LCPI157_0(%rip), %xmm4 # xmm4 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 656(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vmovss	-8(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm15, %xmm0, %xmm0
	vmovd	%r15d, %xmm6
	vmulss	%xmm0, %xmm3, %xmm0
	movl	1152(%rsp), %edx        # 4-byte Reload
	vmovd	%edx, %xmm3
	vdivss	%xmm0, %xmm7, %xmm0
	movl	1120(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 624(%rsp)        # 16-byte Spill
	movl	1264(%rsp), %ebx        # 4-byte Reload
	vmovd	%ebx, %xmm2
	vpsubd	%xmm6, %xmm3, %xmm3
	vpsubd	%xmm6, %xmm7, %xmm4
	vpsubd	%xmm6, %xmm2, %xmm2
	movq	864(%rsp), %r10         # 8-byte Reload
	vmovd	%r10d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 608(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm6
	vmovdqa	%xmm6, 592(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm6
	vmovdqa	%xmm6, 576(%rsp)        # 16-byte Spill
	vmovd	%r15d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 560(%rsp)        # 16-byte Spill
	vmovd	%edx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 544(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 528(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm6
	vmovaps	%xmm6, 1376(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm6
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 240(%rsp)        # 16-byte Spill
	leal	(%rdi,%rdi,2), %edi
	movl	%edi, 512(%rsp)         # 4-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vmovdqa	%xmm3, 224(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm3
	vmovdqa	%xmm3, 496(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 480(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm2
	vmovaps	%xmm2, 400(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm2
	vmovaps	%xmm2, 384(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	movslq	%eax, %r15
	movq	32(%rsp), %rdi          # 8-byte Reload
	vbroadcastss	(%rdi,%r9,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r11,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	1216(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 464(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdi,%rbp,4), %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	movq	1232(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdi,%r13,4), %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 448(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI157_3(%rip), %xmm13
	vbroadcastss	.LCPI157_4(%rip), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_5(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_6(%rip), %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB157_75:                             # %for gV.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	testl	%r14d, %r14d
	setne	1120(%rsp)              # 1-byte Folded Spill
	sete	1008(%rsp)              # 1-byte Folded Spill
	movq	1280(%rsp), %rbp        # 8-byte Reload
	leal	(%rbp,%r15,8), %r11d
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	sete	1152(%rsp)              # 1-byte Folded Spill
	movl	%r11d, %edi
	movq	864(%rsp), %rax         # 8-byte Reload
	subl	%eax, %edi
	leal	-1(%rdi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI157_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	688(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	addl	$-2, %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%esi
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %ecx, %xmm1, %xmm1
	vmovd	%ebx, %xmm2
	vpsrad	$31, %xmm1, %xmm3
	vmovdqa	448(%rsp), %xmm5        # 16-byte Reload
	vpand	%xmm5, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm10
	vpinsrd	$1, %edi, %xmm2, %xmm1
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r13d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm5, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm2
	vmovdqa	%xmm2, 1024(%rsp)       # 16-byte Spill
	vmovdqa	656(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	leal	-2(%rbp,%r15,8), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm9, %xmm3, %xmm3
	vmovdqa	640(%rsp), %xmm14       # 16-byte Reload
	vpminsd	%xmm14, %xmm3, %xmm3
	vmovdqa	608(%rsp), %xmm6        # 16-byte Reload
	vpmaxsd	%xmm6, %xmm3, %xmm3
	vmovdqa	592(%rsp), %xmm11       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm11, %xmm5
	vmovdqa	576(%rsp), %xmm12       # 16-byte Reload
	vpsubd	%xmm0, %xmm12, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm6, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm3, %xmm0, %xmm0
	vmovdqa	672(%rsp), %xmm8        # 16-byte Reload
	vpmulld	%xmm8, %xmm0, %xmm0
	vpsubd	560(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 912(%rsp)        # 16-byte Spill
	vpaddd	544(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r12,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r12,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	vpaddd	528(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r12,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r12,%rax,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	movq	768(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15,8), %r10d
	movq	736(%rsp), %rcx         # 8-byte Reload
	leal	(%r10,%rcx), %ecx
	movq	720(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r15,8), %edx
	movslq	%edx, %rdx
	movq	1304(%rsp), %rdi        # 8-byte Reload
	vmovups	12312(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vmovups	12328(%rdi,%rdx,4), %xmm5
	vmovaps	%xmm5, 944(%rsp)        # 16-byte Spill
	movq	728(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r15,8), %edx
	vmulps	1264(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm5[0,2]
	vmovaps	1312(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm5, %xmm5
	vmovaps	1376(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm7
	vmulps	1248(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	movslq	%edx, %rdx
	vmovups	12312(%rdi,%rdx,4), %xmm1
	vmovaps	%xmm1, 928(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, %xmm0
	vmovups	12328(%rdi,%rdx,4), %xmm13
	vmovaps	%xmm13, 1088(%rsp)      # 16-byte Spill
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vmovaps	%xmm0, %xmm13
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vaddps	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovaps	464(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm5, %xmm3, %xmm2
	movslq	%ecx, %rcx
	vmovups	12312(%rdi,%rcx,4), %xmm3
	vpcmpgtd	%xmm10, %xmm11, %xmm1
	vpsubd	%xmm10, %xmm12, %xmm7
	vpxor	%xmm11, %xmm11, %xmm11
	vblendvps	%xmm1, %xmm10, %xmm7, %xmm1
	vmovups	12328(%rdi,%rcx,4), %xmm0
	vshufps	$136, %xmm0, %xmm3, %xmm7 # xmm7 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm15, %xmm7, %xmm7
	vmulps	%xmm7, %xmm4, %xmm7
	vmulps	%xmm7, %xmm2, %xmm10
	vmovdqa	624(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	1024(%rsp), %xmm7, %xmm2 # 16-byte Folded Reload
	leal	-1(%rbp,%r15,8), %ecx
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm9, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm6, %xmm7, %xmm7
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm14, %xmm1, %xmm1
	vpmaxsd	%xmm6, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm7, %xmm1, %xmm1
	vpmulld	%xmm8, %xmm1, %xmm12
	vpaddd	496(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	vmovss	(%r12,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rdx
	sarq	$32, %rcx
	vinsertps	$16, (%r12,%rcx,4), %xmm2, %xmm1 # xmm1 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%edx, %rcx
	vinsertps	$32, (%r12,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	712(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r15,8), %ecx
	sarq	$32, %rdx
	vinsertps	$48, (%r12,%rdx,4), %xmm1, %xmm8 # xmm8 = xmm1[0,1,2],mem[0]
	movl	416(%rsp), %r14d        # 4-byte Reload
	movl	%r14d, %edx
	andl	%r11d, %edx
	movq	784(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r15,8), %edx
	vmulps	%xmm8, %xmm5, %xmm2
	vshufps	$221, %xmm0, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm3
	vminps	%xmm13, %xmm10, %xmm0
	vmaxps	%xmm11, %xmm0, %xmm0
	vmovaps	%xmm0, %xmm14
	jne	.LBB157_77
# BB#76:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB157_77:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmulps	%xmm2, %xmm3, %xmm9
	movslq	%edx, %rsi
	vmovups	40(%rdi,%rsi,4), %xmm4
	orq	$6, %rsi
	vmovups	(%rdi,%rsi,4), %xmm6
	movslq	%ecx, %rsi
	vmovups	40(%rdi,%rsi,4), %xmm5
	orq	$6, %rsi
	vmovups	(%rdi,%rsi,4), %xmm10
	movslq	%r10d, %rsi
	vmovups	40(%rdi,%rsi,4), %xmm2
	orq	$6, %rsi
	vmovups	(%rdi,%rsi,4), %xmm3
	vmovaps	432(%rsp), %xmm15       # 16-byte Reload
	vmulps	1056(%rsp), %xmm15, %xmm11 # 16-byte Folded Reload
	movb	1120(%rsp), %al         # 1-byte Reload
	andb	1152(%rsp), %al         # 1-byte Folded Reload
	movq	1400(%rsp), %r8         # 8-byte Reload
	jne	.LBB157_78
# BB#79:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovaps	%xmm6, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 832(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1056(%rsp)      # 16-byte Spill
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	movb	1008(%rsp), %r13b       # 1-byte Reload
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	928(%rsp), %xmm11       # 16-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB157_80
	.align	16, 0x90
.LBB157_78:                             #   in Loop: Header=BB157_75 Depth=1
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	vmovaps	896(%rsp), %xmm7        # 16-byte Reload
	vmovaps	%xmm2, 1024(%rsp)       # 16-byte Spill
	vmulps	336(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm11, 1056(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, %xmm1
	vmovaps	%xmm1, 880(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm4, %xmm6, %xmm3 # xmm3 = xmm6[0,2],xmm4[0,2]
	vmovaps	%xmm6, 800(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, %xmm4
	vmovaps	%xmm5, %xmm9
	vmovaps	%xmm9, 848(%rsp)        # 16-byte Spill
	vmovaps	384(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmovaps	%xmm10, 832(%rsp)       # 16-byte Spill
	vmovaps	400(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm0, %xmm14
	vmulps	320(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm10, %xmm0 # xmm0 = xmm10[0,2],xmm9[0,2]
	vmovaps	%xmm4, %xmm9
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	304(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm0, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vmovaps	1232(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm0, %xmm3, %xmm1
	vminps	%xmm13, %xmm14, %xmm0
	vmaxps	%xmm5, %xmm0, %xmm0
	vsubps	%xmm0, %xmm1, %xmm14
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm11, %xmm0, %xmm14
	vxorps	%xmm7, %xmm7, %xmm7
	movb	1008(%rsp), %r13b       # 1-byte Reload
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	928(%rsp), %xmm11       # 16-byte Reload
.LBB157_80:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vminps	%xmm13, %xmm9, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm3
	vmovaps	%xmm3, %xmm9
	testb	%al, %al
	jne	.LBB157_82
# BB#81:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB157_82:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	movl	%r14d, %esi
	vmulps	1264(%rsp), %xmm8, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm2[1,3]
	vmovaps	1312(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	1376(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmulps	1248(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vshufps	$221, 1088(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm11[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm15, %xmm0, %xmm6
	andl	%r11d, %esi
	jne	.LBB157_83
# BB#84:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovups	%ymm3, 1120(%rsp)       # 32-byte Spill
	movq	%rdi, %r9
	jmp	.LBB157_85
	.align	16, 0x90
.LBB157_83:                             #   in Loop: Header=BB157_75 Depth=1
	vmovups	%ymm3, 1120(%rsp)       # 32-byte Spill
	movq	%rdi, %r9
	vpaddd	224(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	movslq	%edi, %rbx
	sarq	$32, %rdi
	movslq	%esi, %rbp
	sarq	$32, %rsi
	vmovss	(%r12,%rbx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r12,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	800(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	336(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	384(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	400(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	832(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, 848(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	320(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	304(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	1232(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm1, %xmm0, %xmm9
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm6, %xmm0, %xmm9
.LBB157_85:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vpaddd	480(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rdi
	movslq	%edi, %rbp
	movslq	%esi, %rbx
	movl	512(%rsp), %eax         # 4-byte Reload
	addl	%eax, %r10d
	addl	%eax, %ecx
	addl	%eax, %edx
	sarq	$32, %rdi
	sarq	$32, %rsi
	vmovss	(%r12,%rbp,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r12,%rsi,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	movslq	%edx, %rdx
	movq	%r9, %rsi
	vmovups	24600(%rsi,%rdx,4), %xmm15
	vmovups	24616(%rsi,%rdx,4), %xmm0
	movslq	%ecx, %rcx
	vmovups	24600(%rsi,%rcx,4), %xmm11
	vmovups	24616(%rsi,%rcx,4), %xmm12
	movslq	%r10d, %rax
	vmovups	24600(%rsi,%rax,4), %xmm8
	vmovups	24616(%rsi,%rax,4), %xmm10
	movb	%r13b, %al
	movl	1200(%rsp), %edi        # 4-byte Reload
	andb	%dil, %al
	jne	.LBB157_86
# BB#87:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	jmp	.LBB157_88
	.align	16, 0x90
.LBB157_86:                             #   in Loop: Header=BB157_75 Depth=1
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	vmovdqa	912(%rsp), %xmm2        # 16-byte Reload
	vpaddd	240(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r12,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r12,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r12,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	288(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm0[0,2]
	vmovaps	352(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	368(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	272(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm12, %xmm11, %xmm5 # xmm5 = xmm11[0,2],xmm12[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vmulps	256(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm10, %xmm8, %xmm5 # xmm5 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vminps	%xmm13, %xmm4, %xmm4
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm4, %xmm4
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vmovaps	1232(%rsp), %xmm5       # 16-byte Reload
	vfmsub213ps	%xmm4, %xmm5, %xmm2
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vsubps	%xmm3, %xmm2, %xmm14
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vfmadd213ps	1056(%rsp), %xmm2, %xmm14 # 16-byte Folded Reload
.LBB157_88:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovups	1152(%rsp), %ymm6       # 32-byte Reload
	movl	%r11d, %eax
	orl	%r8d, %eax
	andl	$1, %eax
	je	.LBB157_90
# BB#89:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovaps	%xmm14, %xmm6
.LBB157_90:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	testl	%eax, %eax
	jne	.LBB157_92
# BB#91:                                #   in Loop: Header=BB157_75 Depth=1
	vshufps	$221, %xmm0, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm0[1,3]
	vmulps	288(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	352(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	368(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm12, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm12[1,3]
	vmulps	272(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vshufps	$221, %xmm10, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm10[1,3]
	vmulps	256(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vmovaps	1232(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm1
	vsubps	%xmm0, %xmm1, %xmm9
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1088(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB157_92:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovups	1120(%rsp), %ymm1       # 32-byte Reload
	andb	%dil, %r13b
	jne	.LBB157_94
# BB#93:                                # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovaps	%xmm9, %xmm1
.LBB157_94:                             # %for gV.s0.v10.v1012
                                        #   in Loop: Header=BB157_75 Depth=1
	vmovaps	.LCPI157_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI157_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm6, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r11d, %rax
	movq	704(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1368(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r15
	cmpl	760(%rsp), %r15d        # 4-byte Folded Reload
	jne	.LBB157_75
	jmp	.LBB157_111
.LBB157_30:                             # %false_bb2
	vmovss	%xmm0, -8(%rsp)         # 4-byte Spill
	addl	$43, %ebp
	sarl	$3, %ebp
	movq	%rbp, 1376(%rsp)        # 8-byte Spill
	testl	%ebp, %ebp
	jle	.LBB157_111
# BB#31:                                # %for gV.s0.v10.v1016.preheader
	movq	104(%rsp), %r9          # 8-byte Reload
	movl	%r9d, %eax
	negl	%eax
	movq	16(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %esi
	movl	%esi, 816(%rsp)         # 4-byte Spill
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%edi, %ebp
	sarl	$31, %ebp
	andnl	%esi, %ebp, %ecx
	movl	%esi, %r10d
	andl	%eax, %ebp
	orl	%ecx, %ebp
	movl	%ebp, 832(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	movl	%ebp, %r14d
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movl	%edx, %r11d
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	movq	%r9, %rdx
	addl	%edx, %ecx
	leal	-1(%rdx,%rdi), %ebp
	cmpl	%ecx, %ebp
	cmovlel	%ebp, %ecx
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movl	%ecx, 1088(%rsp)        # 4-byte Spill
	leal	(%rdx,%rdi), %esi
	movl	%esi, 912(%rsp)         # 4-byte Spill
	testl	%esi, %esi
	movl	$0, %eax
	cmovlel	%ebp, %eax
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	testl	%esi, %esi
	cmovlel	%ecx, %eax
	movl	%eax, 1120(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%edx, %eax
	movq	%rdx, %rcx
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%ecx, %edx
	cmpl	%edx, %ebp
	cmovlel	%ebp, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%edx, 1008(%rsp)        # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %eax
	cmovll	%ebp, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movq	%rcx, %r12
	cmpl	$3, %esi
	cmovll	%edx, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movl	$2, %eax
	movq	-16(%rsp), %r14         # 8-byte Reload
	subl	%r14d, %eax
	movq	24(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %edi
	movl	%edi, 848(%rsp)         # 4-byte Spill
	cltd
	idivl	%edi
	movl	%edi, %eax
	negl	%eax
	movl	%esi, %ecx
	sarl	$31, %ecx
	andnl	%edi, %ecx, %r9d
	andl	%eax, %ecx
	orl	%r9d, %ecx
	movl	%ecx, 880(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %ecx
	movl	%ecx, 800(%rsp)         # 4-byte Spill
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r14d, %ecx
	leal	-1(%r14,%rsi), %edx
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%r14d, %ecx
	cmovll	%r14d, %ecx
	movl	%ecx, 960(%rsp)         # 4-byte Spill
	leal	(%r14,%rsi), %edi
	movl	%edi, 1200(%rsp)        # 4-byte Spill
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	movq	%r15, 1400(%rsp)        # 8-byte Spill
	movl	%r15d, %edx
	movq	8(%rsp), %rsi           # 8-byte Reload
	subl	%esi, %edx
	movq	%rdx, 928(%rsp)         # 8-byte Spill
	movq	%rbx, %rcx
	movq	%rcx, 1152(%rsp)        # 8-byte Spill
	leal	(%rcx,%rcx), %r10d
	movl	%r10d, 944(%rsp)        # 4-byte Spill
	leal	-1(%rdx), %eax
	movq	%rdx, %r9
	cltd
	idivl	%r10d
	movl	%r10d, %eax
	negl	%eax
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andnl	%r10d, %ebx, %r8d
	andl	%eax, %ebx
	orl	%r8d, %ebx
	movl	%ebx, 1056(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	movl	%ebx, %r13d
	addl	%edx, %eax
	leal	(%rsi,%rcx), %r11d
	cmpl	%r15d, %r11d
	movl	%r11d, %edx
	cmovgl	%r15d, %edx
	addl	$-1, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	leal	-1(%rcx,%rcx), %ebx
	movl	%ebx, 1216(%rsp)        # 4-byte Spill
	subl	%eax, %ebx
	cmpl	%eax, %ecx
	cmovgl	%eax, %ebx
	movq	%rsi, %rax
	addl	%eax, %ebx
	leal	-1(%rax,%rcx), %esi
	movl	%esi, 1024(%rsp)        # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	movq	%rax, %rcx
	cmpl	%r15d, %r11d
	cmovgel	%edx, %ebx
	movl	%ebx, 1264(%rsp)        # 4-byte Spill
	movl	%r9d, %eax
	movq	%r9, %r8
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	%r13d, %edi
	andl	%edi, %eax
	addl	%edx, %eax
	cmpl	%r15d, %esi
	movl	%esi, %edx
	cmovgl	%r15d, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	1216(%rsp), %ebx        # 4-byte Reload
	movl	%ebx, %r13d
	subl	%eax, %r13d
	movq	1152(%rsp), %r9         # 8-byte Reload
	cmpl	%eax, %r9d
	cmovgl	%eax, %r13d
	addl	%ecx, %r13d
	cmpl	%r13d, %esi
	cmovlel	%esi, %r13d
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	cmpl	%r15d, %r11d
	cmovgl	%edx, %r13d
	leal	1(%r8), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	leal	1(%r15), %edx
	cmpl	%edx, %esi
	cmovlel	%esi, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%ebx, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %r9d
	cmovgl	%eax, %r11d
	addl	%ecx, %r11d
	cmpl	%r11d, %esi
	cmovlel	%esi, %r11d
	cmpl	%ecx, %r11d
	cmovll	%ecx, %r11d
	cmpl	%r15d, %esi
	cmovgl	%edx, %r11d
	movl	$1, %eax
	movq	%r12, %rcx
	subl	%ecx, %eax
	cltd
	idivl	816(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	832(%rsp), %eax         # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1280(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	16(%rsp), %rsi          # 8-byte Reload
	cmpl	%eax, %esi
	cmovgl	%eax, %edx
	addl	%ecx, %edx
	cmpl	%edx, %ebp
	cmovlel	%ebp, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movl	%edx, %edi
	movl	912(%rsp), %edx         # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%ebp, %r12d
	movl	$0, %esi
	cmovgel	%esi, %r12d
	movzbl	%al, %eax
	orl	%eax, %r12d
	cmpl	%ecx, %r12d
	cmovll	%ecx, %r12d
	cmpl	$2, %edx
	cmovll	%edi, %r12d
	movl	$1, %eax
	subl	%r14d, %eax
	cltd
	movl	848(%rsp), %r8d         # 4-byte Reload
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	880(%rsp), %r9d         # 4-byte Reload
	andl	%r9d, %eax
	addl	%edx, %eax
	movl	800(%rsp), %ebp         # 4-byte Reload
	movl	%ebp, %esi
	subl	%eax, %esi
	movq	24(%rsp), %rbx          # 8-byte Reload
	cmpl	%eax, %ebx
	cmovgl	%eax, %esi
	addl	%r14d, %esi
	movl	1312(%rsp), %edi        # 4-byte Reload
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	cmpl	%r14d, %esi
	cmovll	%r14d, %esi
	movl	1200(%rsp), %ecx        # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	movl	%ecx, %r10d
	movl	$0, %r15d
	cmovll	%edi, %r15d
	movzbl	%al, %eax
	orl	%eax, %r15d
	cmpl	%r14d, %r15d
	cmovll	%r14d, %r15d
	cmpl	$2, %r10d
	cmovll	%esi, %r15d
	movl	%r14d, %eax
	negl	%eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	cmpl	%eax, %ebx
	cmovgl	%eax, %ebp
	addl	%r14d, %ebp
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	cmpl	%r14d, %ebp
	cmovll	%r14d, %ebp
	movl	%ebp, %ecx
	testl	%r10d, %r10d
	movl	$0, %edx
	cmovgl	%edx, %edi
	cmpl	%r14d, %edi
	cmovll	%r14d, %edi
	testl	%r10d, %r10d
	cmovlel	%ecx, %edi
	movq	56(%rsp), %rbp          # 8-byte Reload
	movl	%ebp, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movq	1400(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r10d
	andl	$1, %r10d
	testl	%r14d, %r14d
	cmovgl	%ecx, %edi
	movl	%edi, 1312(%rsp)        # 4-byte Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm8
	movq	864(%rsp), %rbp         # 8-byte Reload
	imull	%ebp, %eax
	addl	%r14d, %eax
	movq	%rax, 896(%rsp)         # 8-byte Spill
	cmpl	$1, %r14d
	cmovgl	%esi, %r15d
	movl	%r15d, 1200(%rsp)       # 4-byte Spill
	movq	104(%rsp), %r9          # 8-byte Reload
	cmpl	$1, %r9d
	cmovgl	1280(%rsp), %r12d       # 4-byte Folded Reload
	movq	(%rsp), %rsi            # 8-byte Reload
	movl	%esi, %eax
	movq	8(%rsp), %rcx           # 8-byte Reload
	imull	%ecx, %eax
	addl	%r9d, %eax
	cltq
	movslq	%r12d, %rcx
	subq	%rax, %rcx
	movq	%rax, %r12
	movslq	1264(%rsp), %rax        # 4-byte Folded Reload
	imulq	%rsi, %rax
	leaq	(%rax,%rcx), %rax
	movq	32(%rsp), %rdx          # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movslq	%r11d, %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	movl	52(%rsp), %eax          # 4-byte Reload
	sarl	$5, %eax
	movl	%eax, 912(%rsp)         # 4-byte Spill
	movslq	%r13d, %r11
	imulq	%rsi, %r11
	cmpl	$2, %r14d
	movl	1232(%rsp), %esi        # 4-byte Reload
	cmovgl	960(%rsp), %esi         # 4-byte Folded Reload
	movl	%esi, 1232(%rsp)        # 4-byte Spill
	cmpl	$2, %r9d
	movl	1248(%rsp), %eax        # 4-byte Reload
	cmovgl	1008(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movq	928(%rsp), %rdi         # 8-byte Reload
	leal	2(%rdi), %eax
	cltd
	movl	944(%rsp), %ebx         # 4-byte Reload
	idivl	%ebx
	movl	%edx, %r8d
	vmovaps	%xmm2, %xmm5
	vmovss	.LCPI157_1(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm2, %xmm0
	vmulss	%xmm10, %xmm0, %xmm1
	vdivss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm9
	movq	%rdi, %rax
	addl	$-2, %eax
	cltd
	idivl	%ebx
	vsubss	%xmm10, %xmm5, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm7, %xmm10
	vsubss	%xmm11, %xmm2, %xmm5
	vmulss	%xmm6, %xmm5, %xmm0
	vdivss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm11, %xmm11
	movq	40(%rsp), %rdi          # 8-byte Reload
	leal	2(%rbp,%rdi), %eax
	vmovd	%eax, %xmm1
	vsubss	%xmm6, %xmm4, %xmm6
	leal	2(%rbp), %eax
	vmovd	%eax, %xmm0
	vmulss	%xmm6, %xmm5, %xmm5
	leal	1(%rbp,%rdi), %eax
	vmovd	%eax, %xmm7
	vdivss	%xmm5, %xmm12, %xmm12
	vsubss	%xmm14, %xmm2, %xmm6
	vmulss	%xmm13, %xmm6, %xmm4
	vdivss	%xmm15, %xmm4, %xmm4
	vaddss	%xmm4, %xmm14, %xmm4
	leal	1(%rbp), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm1, %xmm1
	vmovss	-8(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm13, %xmm3, %xmm3
	vmovdqa	.LCPI157_0(%rip), %xmm5 # xmm5 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm1, 816(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 800(%rsp)        # 16-byte Spill
	vmulss	%xmm3, %xmm6, %xmm0
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm1
	vdivss	%xmm0, %xmm15, %xmm6
	vmovd	1312(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm7, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vmovdqa	%xmm3, 784(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 768(%rsp)        # 16-byte Spill
	vmovd	1200(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpsubd	%xmm1, %xmm0, %xmm7
	vpsubd	%xmm1, %xmm2, %xmm0
	vmovd	%esi, %xmm2
	vpsubd	%xmm1, %xmm2, %xmm1
	movl	%r8d, %eax
	sarl	$31, %eax
	movl	1056(%rsp), %edi        # 4-byte Reload
	andl	%edi, %eax
	addl	%r8d, %eax
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%edi, %esi
	addl	%edx, %esi
	testl	%r9d, %r9d
	movl	1120(%rsp), %r8d        # 4-byte Reload
	cmovgl	1088(%rsp), %r8d        # 4-byte Folded Reload
	leaq	(%rcx,%r11), %rcx
	movq	%rcx, 1120(%rsp)        # 8-byte Spill
	subq	%r12, %r11
	movq	%r11, 1088(%rsp)        # 8-byte Spill
	movq	1400(%rsp), %r14        # 8-byte Reload
	leal	2(%r14), %ecx
	movl	1024(%rsp), %r9d        # 4-byte Reload
	cmpl	%ecx, %r9d
	cmovlel	%r9d, %ecx
	movq	8(%rsp), %rbx           # 8-byte Reload
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	movl	1216(%rsp), %r11d       # 4-byte Reload
	movl	%r11d, %edx
	subl	%eax, %edx
	movq	1152(%rsp), %rdi        # 8-byte Reload
	cmpl	%eax, %edi
	cmovgl	%eax, %edx
	addl	%ebx, %edx
	cmpl	%edx, %r9d
	cmovlel	%r9d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	leal	-2(%rbx,%rdi), %eax
	cmpl	%r14d, %eax
	cmovgl	%ecx, %edx
	movslq	%r8d, %rax
	movslq	%edx, %r13
	movq	(%rsp), %r15            # 8-byte Reload
	imulq	%r15, %r13
	movq	%r13, %rbp
	subq	%r12, %rbp
	addq	%rax, %rbp
	leal	-2(%r14), %edx
	cmpl	%edx, %r9d
	cmovlel	%r9d, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movl	%r11d, %ecx
	subl	%esi, %ecx
	cmpl	%esi, %edi
	cmovgl	%esi, %ecx
	addl	%ebx, %ecx
	cmpl	%ecx, %r9d
	cmovlel	%r9d, %ecx
	leal	2(%rbx,%rdi), %esi
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	%r14d, %esi
	cmovgl	%edx, %ecx
	movslq	%ecx, %rsi
	imulq	%r15, %rsi
	movq	%rsi, %r15
	movq	%r12, %rcx
	subq	%rcx, %r15
	addq	%rax, %r15
	movq	1088(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rax), %rax
	movq	%rax, 1216(%rsp)        # 8-byte Spill
	movslq	1248(%rsp), %rbx        # 4-byte Folded Reload
	addq	%rbx, %rdx
	movq	%rdx, %r12
	subq	%rcx, %rbx
	movslq	912(%rsp), %r9          # 4-byte Folded Reload
	movq	%r9, %rax
	shlq	$5, %rax
	addq	$40, %rax
	movl	%r14d, %edx
	andl	$63, %edx
	imulq	%rax, %rdx
	leaq	(%rsi,%rbx), %rax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	leal	6(%r14), %r11d
	movl	-4(%rsp), %ecx          # 4-byte Reload
	subl	%ecx, %r11d
	movl	52(%rsp), %edi          # 4-byte Reload
	andl	$-32, %edi
	addl	$64, %edi
	imull	%edi, %r11d
	movq	%r11, 736(%rsp)         # 8-byte Spill
	leaq	(%r13,%rbx), %r13
	movq	56(%rsp), %rbx          # 8-byte Reload
	movq	%rbx, %rax
	sarq	$63, %rax
	leal	10(%r14), %r8d
	subl	%ecx, %r8d
	imull	%edi, %r8d
	movq	%r8, 728(%rsp)          # 8-byte Spill
	leal	7(%r14), %esi
	subl	%ecx, %esi
	imull	%edi, %esi
	andq	%rbx, %rax
	subq	%rax, %rdx
	movq	%rdx, 760(%rsp)         # 8-byte Spill
	leal	9(%r14), %edx
	subl	%ecx, %edx
	imull	%edi, %edx
	leal	8(%r14), %ebx
	subl	%ecx, %ebx
	imull	%edi, %ebx
	movq	%rbx, 704(%rsp)         # 8-byte Spill
	movq	40(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vmovdqa	%xmm3, 688(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm2
	vmovaps	%xmm2, 672(%rsp)        # 16-byte Spill
	movq	864(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 656(%rsp)        # 16-byte Spill
	movq	832(%rsp), %r14         # 8-byte Reload
	movl	%r14d, %ecx
	subl	%eax, %ecx
	leal	-1(%rax,%rdi), %eax
	vmovd	%edi, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 608(%rsp)        # 16-byte Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 592(%rsp)        # 16-byte Spill
	movl	1312(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 576(%rsp)        # 16-byte Spill
	movl	1200(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 560(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm2
	vmovaps	%xmm2, 544(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm2
	vmovaps	%xmm2, 1248(%rsp)       # 16-byte Spill
	movl	1232(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm2
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 176(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm2
	vmovdqa	%xmm2, 160(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 528(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vmovdqa	%xmm0, 512(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	movl	%r9d, %eax
	shll	$10, %eax
	leal	(%rax,%rax,2), %eax
	shll	$9, %r9d
	leal	(%r9,%r9,2), %r9d
	addl	%r9d, %esi
	movq	%rsi, 720(%rsp)         # 8-byte Spill
	addl	%r9d, %edx
	movq	%rdx, 712(%rsp)         # 8-byte Spill
	leal	(%r11,%rax), %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	leal	(%r8,%rax), %edx
	movq	%rdx, 480(%rsp)         # 8-byte Spill
	leal	(%rax,%rbx), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	leal	(%r9,%rbx), %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	addl	$-1, %ecx
	movq	%rcx, 640(%rsp)         # 8-byte Spill
	movq	%r14, %rcx
	leal	-1(%rcx), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	movq	32(%rsp), %rax          # 8-byte Reload
	movq	1120(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 384(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r15,4), %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rbp,4), %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	movq	1216(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1152(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r13,4), %xmm0
	movl	$0, %r13d
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r12,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	vpabsd	%xmm3, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm3, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI157_3(%rip), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_4(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_5(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI157_6(%rip), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB157_32:                             # %for gV.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	movl	%r10d, %r15d
	testl	%r15d, %r15d
	setne	%r14b
	sete	1120(%rsp)              # 1-byte Folded Spill
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %ebp
	movl	%ebp, %eax
	andl	$1, %eax
	movl	%eax, 1088(%rsp)        # 4-byte Spill
	sete	1152(%rsp)              # 1-byte Folded Spill
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI157_2(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	688(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	movq	%r13, %rsi
	vpextrd	$3, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r11d
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rsi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %edi
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r13d
	movq	%rsi, %r13
	vmovd	%r9d, %xmm0
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	vpinsrd	$3, %r11d, %xmm0, %xmm0
	vmovd	%ecx, %xmm1
	vpinsrd	$1, %ebx, %xmm1, %xmm1
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	368(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm15
	vpinsrd	$2, %edi, %xmm1, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%ebp, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	816(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm2
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vmovdqa	800(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	624(%rsp), %xmm9        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm4
	vmovdqa	352(%rsp), %xmm6        # 16-byte Reload
	vpsubd	%xmm0, %xmm6, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	656(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	608(%rsp), %xmm7        # 16-byte Reload
	vpminsd	%xmm7, %xmm0, %xmm0
	vpmaxsd	%xmm3, %xmm0, %xmm0
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm14, %xmm4, %xmm4
	vpminsd	%xmm7, %xmm4, %xmm4
	vpmaxsd	%xmm3, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	vmovdqa	672(%rsp), %xmm5        # 16-byte Reload
	vpmulld	%xmm5, %xmm0, %xmm0
	vpsubd	592(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovdqa	%xmm4, 960(%rsp)        # 16-byte Spill
	vpaddd	576(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	1408(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vpaddd	560(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	1304(%rsp), %rdi        # 8-byte Reload
	vmovups	12312(%rdi,%rax,4), %xmm13
	vmovups	12328(%rdi,%rax,4), %xmm10
	vmovaps	%xmm10, 1008(%rsp)      # 16-byte Spill
	movq	712(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	12312(%rdi,%rax,4), %xmm12
	vmovups	12328(%rdi,%rax,4), %xmm8
	vmovaps	%xmm8, 1024(%rsp)       # 16-byte Spill
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	12312(%rdi,%rax,4), %xmm4
	vmovdqa	784(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm2
	vpxor	.LCPI157_9(%rip), %xmm2, %xmm2
	vmovdqa	768(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm1
	vpor	%xmm2, %xmm1, %xmm1
	vpcmpgtd	%xmm15, %xmm9, %xmm2
	vpsubd	%xmm15, %xmm6, %xmm0
	vblendvps	%xmm2, %xmm15, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vpminsd	%xmm7, %xmm0, %xmm0
	vpmaxsd	%xmm3, %xmm0, %xmm0
	movq	416(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm7, %xmm2, %xmm2
	vpmaxsd	%xmm3, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovups	12328(%rdi,%rax,4), %xmm1
	vpaddd	528(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	movl	%r15d, %eax
	movl	%r15d, %r10d
	andl	%ebp, %eax
	vmulps	1280(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm10, %xmm13, %xmm7 # xmm7 = xmm13[0,2],xmm10[0,2]
	vmovaps	%xmm13, %xmm15
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm7, %xmm7
	vmovaps	544(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm0, %xmm0
	vmovaps	1312(%rsp), %xmm10      # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vmulps	1264(%rsp), %xmm11, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm12, %xmm5 # xmm5 = xmm12[0,2],xmm8[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm0, %xmm0
	vminps	%xmm10, %xmm5, %xmm5
	vmaxps	%xmm13, %xmm5, %xmm5
	vaddps	%xmm5, %xmm0, %xmm9
	vmovaps	384(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm0
	vshufps	$136, %xmm1, %xmm4, %xmm5 # xmm5 = xmm4[0,2],xmm1[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm14, %xmm7, %xmm5
	vshufps	$221, %xmm1, %xmm4, %xmm1 # xmm1 = xmm4[1,3],xmm1[1,3]
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, %xmm11
	jne	.LBB157_34
# BB#33:                                # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
.LBB157_34:                             # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmulps	%xmm5, %xmm1, %xmm13
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	40(%rdi,%rax,4), %xmm7
	orq	$6, %rax
	vmovups	(%rdi,%rax,4), %xmm8
	movq	728(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	40(%rdi,%rax,4), %xmm1
	orq	$6, %rax
	vmovups	(%rdi,%rax,4), %xmm5
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	40(%rdi,%rax,4), %xmm6
	orq	$6, %rax
	vmovups	(%rdi,%rax,4), %xmm2
	vmulps	1232(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	andb	1152(%rsp), %r14b       # 1-byte Folded Reload
	movq	1400(%rsp), %r8         # 8-byte Reload
	movq	1376(%rsp), %r9         # 8-byte Reload
	jne	.LBB157_35
# BB#95:                                # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	%xmm8, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 928(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 944(%rsp)        # 16-byte Spill
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	movb	1120(%rsp), %r15b       # 1-byte Reload
	movl	1088(%rsp), %r11d       # 4-byte Reload
	vmovaps	1008(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm10, %xmm6
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB157_96
	.align	16, 0x90
.LBB157_35:                             #   in Loop: Header=BB157_32 Depth=1
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	vmovaps	%xmm5, %xmm11
	vmovaps	%xmm11, 880(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, %xmm5
	vmovaps	%xmm5, 912(%rsp)        # 16-byte Spill
	vmovaps	944(%rsp), %xmm1        # 16-byte Reload
	vmovaps	%xmm8, %xmm0
	vmovaps	%xmm0, 848(%rsp)        # 16-byte Spill
	vmulps	272(%rsp), %xmm1, %xmm8 # 16-byte Folded Reload
	vshufps	$136, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm7[0,2]
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	320(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm2, %xmm9
	vmovaps	%xmm9, 896(%rsp)        # 16-byte Spill
	vmovaps	336(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm0, %xmm8, %xmm8
	vmulps	256(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm11, %xmm5 # xmm5 = xmm11[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	240(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm6, %xmm9, %xmm1 # xmm1 = xmm9[0,2],xmm6[0,2]
	vmovaps	%xmm6, 928(%rsp)        # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm0, %xmm1
	vmovaps	%xmm10, %xmm6
	vminps	%xmm6, %xmm5, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm5, %xmm5
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm2, %xmm1
	vminps	%xmm6, %xmm8, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm0, %xmm1, %xmm11
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm4, %xmm0, %xmm11
	vmovaps	%xmm4, 944(%rsp)        # 16-byte Spill
	movb	1120(%rsp), %r15b       # 1-byte Reload
	movl	1088(%rsp), %r11d       # 4-byte Reload
	vmovaps	1008(%rsp), %xmm8       # 16-byte Reload
.LBB157_96:                             # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	1024(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm6, %xmm13, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm4
	vmovaps	%xmm4, %xmm9
	testb	%r14b, %r14b
	jne	.LBB157_98
# BB#97:                                # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB157_98:                             # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	movl	%r10d, %eax
	vmulps	1280(%rsp), %xmm14, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm8[1,3]
	vmovaps	1248(%rsp), %xmm8       # 16-byte Reload
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmulps	1264(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm12, %xmm2 # xmm2 = xmm12[1,3],xmm2[1,3]
	vsubps	%xmm8, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vaddps	%xmm0, %xmm1, %xmm0
	vxorps	%xmm12, %xmm12, %xmm12
	vmulps	1232(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	andl	%ebp, %eax
	jne	.LBB157_99
# BB#100:                               # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1024(%rsp)      # 16-byte Spill
	vmovups	%ymm4, 1120(%rsp)       # 32-byte Spill
	vmovdqa	1056(%rsp), %xmm8       # 16-byte Reload
	jmp	.LBB157_101
	.align	16, 0x90
.LBB157_99:                             #   in Loop: Header=BB157_32 Depth=1
	vmovups	%ymm4, 1120(%rsp)       # 32-byte Spill
	vmovdqa	1056(%rsp), %xmm8       # 16-byte Reload
	vpaddd	160(%rsp), %xmm8, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	848(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 864(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	272(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm6, %xmm7
	vmovaps	%xmm7, 1312(%rsp)       # 16-byte Spill
	vmovaps	320(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	336(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	880(%rsp), %xmm2        # 16-byte Reload
	vshufps	$221, 912(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	256(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	896(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, 928(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[1,3],mem[1,3]
	vmulps	240(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vmovaps	1216(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm1, %xmm0, %xmm9
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	%xmm10, %xmm0, %xmm9
	vmovaps	%xmm10, 1024(%rsp)      # 16-byte Spill
.LBB157_101:                            # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vpaddd	512(%rsp), %xmm8, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24600(%rdi,%rax,4), %xmm15
	vmovups	24616(%rdi,%rax,4), %xmm1
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24600(%rdi,%rax,4), %xmm8
	vmovups	24616(%rdi,%rax,4), %xmm14
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24600(%rdi,%rax,4), %xmm10
	vmovups	24616(%rdi,%rax,4), %xmm13
	andb	%r11b, %r15b
	jne	.LBB157_102
# BB#103:                               # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovups	1152(%rsp), %ymm6       # 32-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB157_104
	.align	16, 0x90
.LBB157_102:                            #   in Loop: Header=BB157_32 Depth=1
	vmovdqa	960(%rsp), %xmm2        # 16-byte Reload
	vpaddd	176(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	224(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm1, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm1[0,2]
	vmovaps	288(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmovaps	304(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	208(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm8, %xmm6 # xmm6 = xmm8[0,2],xmm14[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmulps	192(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm13, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm13[0,2]
	vsubps	%xmm7, %xmm6, %xmm6
	vmulps	%xmm6, %xmm3, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vmovaps	1312(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm5, %xmm5
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm5, %xmm5
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vmovaps	1216(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm5, %xmm3, %xmm2
	vminps	%xmm6, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vsubps	%xmm4, %xmm2, %xmm11
	vmovaps	1200(%rsp), %xmm2       # 16-byte Reload
	vfmadd213ps	944(%rsp), %xmm2, %xmm11 # 16-byte Folded Reload
	vmovups	1152(%rsp), %ymm6       # 32-byte Reload
.LBB157_104:                            # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	movl	%ebp, %eax
	orl	%r8d, %eax
	andl	$1, %eax
	je	.LBB157_106
# BB#105:                               # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	%xmm11, %xmm6
.LBB157_106:                            # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	testl	%eax, %eax
	jne	.LBB157_108
# BB#107:                               #   in Loop: Header=BB157_32 Depth=1
	vshufps	$221, %xmm1, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm1[1,3]
	vmulps	224(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	288(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	304(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vshufps	$221, %xmm14, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm14[1,3]
	vmulps	208(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm13, %xmm10, %xmm4 # xmm4 = xmm10[1,3],xmm13[1,3]
	vmulps	192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm0, %xmm0
	vmovaps	1312(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	1216(%rsp), %xmm3       # 16-byte Reload
	vfmsub213ps	%xmm2, %xmm3, %xmm0
	vsubps	%xmm1, %xmm0, %xmm9
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfmadd213ps	1024(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB157_108:                            # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovups	1120(%rsp), %ymm1       # 32-byte Reload
	testb	%r15b, %r15b
	jne	.LBB157_110
# BB#109:                               # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	%xmm9, %xmm1
.LBB157_110:                            # %for gV.s0.v10.v1016
                                        #   in Loop: Header=BB157_32 Depth=1
	vmovaps	.LCPI157_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI157_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm6, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%ebp, %rax
	movq	760(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1368(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, %r9d
	movq	%r9, 1376(%rsp)         # 8-byte Spill
	jne	.LBB157_32
.LBB157_111:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1416, %rsp             # imm = 0x588
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end157:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173, .Lfunc_end157-par_for_par_for___sharpi_f0.s0.v11.v14_gV.s0.v11.173

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI158_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI158_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI158_9:
	.zero	16,255
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI158_1:
	.long	1199570688              # float 65535
.LCPI158_3:
	.long	1065353216              # float 1
.LCPI158_4:
	.long	2147483647              # 0x7fffffff
.LCPI158_5:
	.long	1056964608              # float 0.5
.LCPI158_6:
	.long	1073741824              # float 2
	.section	.rodata,"a",@progbits
	.align	32
.LCPI158_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI158_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174,@function
par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174: # @par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1480, %rsp             # imm = 0x5C8
	vmovss	(%rdx), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 52(%rsp)         # 4-byte Spill
	vmovss	4(%rdx), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vmovss	16(%rdx), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	20(%rdx), %xmm13        # xmm13 = mem[0],zero,zero,zero
	vmovss	24(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 56(%rsp)         # 4-byte Spill
	vmovss	28(%rdx), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm6         # xmm6 = mem[0],zero,zero,zero
	movl	36(%rdx), %ecx
	movl	48(%rdx), %eax
	movl	%eax, 108(%rsp)         # 4-byte Spill
	movl	52(%rdx), %eax
	movl	%eax, 44(%rsp)          # 4-byte Spill
	movslq	56(%rdx), %rax
	movq	%rax, 88(%rsp)          # 8-byte Spill
	movl	60(%rdx), %eax
	movq	%rax, 72(%rsp)          # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	movl	68(%rdx), %eax
	movq	%rax, 80(%rsp)          # 8-byte Spill
	movl	72(%rdx), %eax
	movq	%rax, 840(%rsp)         # 8-byte Spill
	movslq	76(%rdx), %rax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	movl	80(%rdx), %eax
	movq	%rax, 96(%rsp)          # 8-byte Spill
	movl	84(%rdx), %r11d
	movl	88(%rdx), %eax
	movq	%rax, 120(%rsp)         # 8-byte Spill
	vmovss	100(%rdx), %xmm0        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 48(%rsp)         # 4-byte Spill
	vmovss	104(%rdx), %xmm7        # xmm7 = mem[0],zero,zero,zero
	vmovss	108(%rdx), %xmm4        # xmm4 = mem[0],zero,zero,zero
	cmpl	%esi, 44(%rdx)
	movl	92(%rdx), %eax
	movq	%rax, 24(%rsp)          # 8-byte Spill
	movslq	96(%rdx), %rax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	movq	112(%rdx), %rax
	movq	%rax, 1416(%rsp)        # 8-byte Spill
	movq	128(%rdx), %r15
	movq	144(%rdx), %rbp
	movq	160(%rdx), %rax
	movq	%rax, 64(%rsp)          # 8-byte Spill
	jle	.LBB158_22
# BB#1:                                 # %true_bb
	movq	%r15, 936(%rsp)         # 8-byte Spill
	vmovss	%xmm1, 60(%rsp)         # 4-byte Spill
	movq	%rbp, 1384(%rsp)        # 8-byte Spill
	movq	%rsi, %r8
	addl	$51, %ecx
	sarl	$3, %ecx
	movq	%rcx, 1424(%rsp)        # 8-byte Spill
	testl	%ecx, %ecx
	jle	.LBB158_110
# BB#2:                                 # %for dV.s0.v10.v10.preheader
	movq	120(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	96(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ebx
	movl	%ebx, 1088(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movl	%ebx, %eax
	negl	%eax
	movl	%edi, %ebp
	sarl	$31, %ebp
	andnl	%ebx, %ebp, %ecx
	movl	%ebx, %r10d
	andl	%eax, %ebp
	orl	%ecx, %ebp
	movl	%ebp, 1072(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %edx
	movl	%edx, 1104(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movl	%edx, %r9d
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	leal	-1(%rsi,%rdi), %edx
	movl	%edx, 1008(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1296(%rsp)        # 4-byte Spill
	leal	(%rsi,%rdi), %ebx
	movl	%ebx, 1024(%rsp)        # 4-byte Spill
	xorl	%r12d, %r12d
	testl	%ebx, %ebx
	movl	$0, %eax
	cmovlel	%edx, %eax
	movl	%edx, %r14d
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%ebx, %ebx
	cmovlel	%ecx, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%esi, %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	movl	%r9d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1264(%rsp)        # 4-byte Spill
	cmpl	$3, %ebx
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %ebx
	cmovll	%ecx, %eax
	movl	%eax, 1248(%rsp)        # 4-byte Spill
	movl	$2, %eax
	movq	80(%rsp), %rsi          # 8-byte Reload
	subl	%esi, %eax
	movq	72(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	movl	%ecx, 912(%rsp)         # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%edi, %ebp
	sarl	$31, %ebp
	andnl	%ecx, %ebp, %ecx
	andl	%eax, %ebp
	orl	%ecx, %ebp
	movl	%ebp, 944(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %r15d
	movl	%r15d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	leal	-1(%rsi,%rdi), %edx
	movl	%edx, 1456(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1200(%rsp)        # 4-byte Spill
	leal	(%rsi,%rdi), %ebp
	movl	%ebp, 1312(%rsp)        # 4-byte Spill
	cmpl	$3, %ebp
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %ebp
	cmovll	%ecx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movq	%r8, %rbx
	movq	%rbx, 1448(%rsp)        # 8-byte Spill
	movl	%ebx, %esi
	movq	24(%rsp), %rbp          # 8-byte Reload
	subl	%ebp, %esi
	movq	%rsi, 976(%rsp)         # 8-byte Spill
	leal	(%r11,%r11), %edi
	movl	%edi, 992(%rsp)         # 4-byte Spill
	leal	-1(%rsi), %eax
	cltd
	idivl	%edi
	movl	%edi, %eax
	negl	%eax
	movl	%r11d, %r14d
	sarl	$31, %r14d
	andnl	%edi, %r14d, %r8d
	andl	%eax, %r14d
	orl	%r8d, %r14d
	movl	%r14d, 1040(%rsp)       # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	addl	%edx, %eax
	leal	-1(%r11,%r11), %r9d
	movl	%r9d, 1056(%rsp)        # 4-byte Spill
	movl	%r9d, %r13d
	subl	%eax, %r13d
	cmpl	%eax, %r11d
	cmovgl	%eax, %r13d
	addl	%ebp, %r13d
	leal	-1(%rbp,%r11), %ecx
	cmpl	%r13d, %ecx
	cmovlel	%ecx, %r13d
	cmpl	%ebp, %r13d
	cmovll	%ebp, %r13d
	movq	%r11, %r10
	movq	%r10, 1328(%rsp)        # 8-byte Spill
	leal	(%rbp,%r10), %r11d
	cmpl	%ebx, %r11d
	movl	%r11d, %eax
	cmovgl	%ebx, %eax
	addl	$-1, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%ebx, %r11d
	cmovll	%r13d, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	addl	%edx, %eax
	movl	%r9d, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r8d
	addl	%ebp, %r8d
	cmpl	%r8d, %ecx
	cmovlel	%ecx, %r8d
	cmpl	%ebp, %r8d
	cmovll	%ebp, %r8d
	cmpl	%ebx, %ecx
	movl	%ecx, %edx
	cmovgl	%ebx, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	cmpl	%ebx, %r11d
	movq	%rbx, %r11
	cmovlel	%r8d, %edx
	movl	%edx, %ebx
	leal	1(%rsi), %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r14d, %eax
	addl	%edx, %eax
	subl	%eax, %r9d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r9d
	addl	%ebp, %r9d
	cmpl	%r9d, %ecx
	cmovlel	%ecx, %r9d
	cmpl	%ebp, %r9d
	cmovll	%ebp, %r9d
	movl	%r9d, 1152(%rsp)        # 4-byte Spill
	movq	%r11, %rsi
	leal	1(%rsi), %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	%esi, %ebp
	cmovgl	%r8d, %ebx
	movl	%ebx, 960(%rsp)         # 4-byte Spill
	movl	1344(%rsp), %ebx        # 4-byte Reload
	cmovgel	%r13d, %ebx
	movl	%ebx, 1344(%rsp)        # 4-byte Spill
	cmpl	%esi, %ecx
	cmovlel	%r9d, %eax
	movl	%eax, 1120(%rsp)        # 4-byte Spill
	movq	80(%rsp), %rbx          # 8-byte Reload
	movl	%ebx, %eax
	negl	%eax
	cltd
	movl	912(%rsp), %edi         # 4-byte Reload
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	movl	944(%rsp), %esi         # 4-byte Reload
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r15d, %r13d
	subl	%eax, %r13d
	movq	72(%rsp), %r10          # 8-byte Reload
	cmpl	%eax, %r10d
	cmovgl	%eax, %r13d
	movq	%rbx, %rdx
	addl	%edx, %r13d
	movl	1456(%rsp), %ebx        # 4-byte Reload
	cmpl	%r13d, %ebx
	cmovlel	%ebx, %r13d
	cmpl	%edx, %r13d
	cmovll	%edx, %r13d
	movl	1312(%rsp), %r8d        # 4-byte Reload
	testl	%r8d, %r8d
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	movq	%rdx, %r14
	testl	%r8d, %r8d
	cmovlel	%r13d, %eax
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	movl	$1, %eax
	movq	120(%rsp), %r8          # 8-byte Reload
	subl	%r8d, %eax
	cltd
	idivl	1088(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1072(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1104(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	movq	96(%rsp), %rbx          # 8-byte Reload
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	movq	%r8, %r11
	addl	%r11d, %edx
	movl	1008(%rsp), %eax        # 4-byte Reload
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	movl	%eax, %ebx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, %r8d
	movl	1024(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%edx, %r9d
	movl	%ebx, %edx
	cmovgel	%r12d, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	cmpl	$2, %r9d
	cmovll	%r8d, %edx
	movl	%edx, %r9d
	movl	$1, %eax
	subl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %r15d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r15d
	movq	%r14, %rdx
	addl	%edx, %r15d
	movl	1456(%rsp), %r14d       # 4-byte Reload
	cmpl	%r15d, %r14d
	cmovlel	%r14d, %r15d
	cmpl	%edx, %r15d
	cmovll	%edx, %r15d
	movl	1312(%rsp), %ebx        # 4-byte Reload
	cmpl	$1, %ebx
	setg	%al
	cmpl	$2, %ebx
	cmovgel	%r12d, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%edx, %r14d
	cmovll	%edx, %r14d
	movq	%rdx, %r10
	cmpl	$2, %ebx
	cmovll	%r15d, %r14d
	movl	%r14d, %edx
	movq	88(%rsp), %rbx          # 8-byte Reload
	movl	%ebx, %eax
	sarl	$31, %eax
	andl	%ebx, %eax
	movq	%rax, 888(%rsp)         # 8-byte Spill
	movq	1448(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r11d
	andl	$1, %r11d
	cmpl	$1, %r10d
	cmovgl	%r15d, %edx
	movl	%edx, 1456(%rsp)        # 4-byte Spill
	movq	848(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm8
	movq	840(%rsp), %rdx         # 8-byte Reload
	imull	%edx, %eax
	addl	%r10d, %eax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rsi         # 8-byte Reload
	cmpl	$1, %esi
	cmovgl	%r8d, %r9d
	movq	32(%rsp), %r14          # 8-byte Reload
	movl	%r14d, %eax
	imull	%ebp, %eax
	movl	108(%rsp), %edx         # 4-byte Reload
	sarl	$5, %edx
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	addl	%esi, %eax
	cltq
	movq	%rax, 1104(%rsp)        # 8-byte Spill
	movslq	960(%rsp), %rdx         # 4-byte Folded Reload
	movslq	%r9d, %r8
	imulq	%r14, %rdx
	movq	%rdx, 1088(%rsp)        # 8-byte Spill
	subq	%rax, %r8
	testl	%r10d, %r10d
	movl	1392(%rsp), %eax        # 4-byte Reload
	cmovgl	%r13d, %eax
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	movq	976(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	992(%rsp), %ebx         # 4-byte Reload
	idivl	%ebx
	movl	%edx, %edi
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%ebx
	movl	%edi, %eax
	sarl	$31, %eax
	movl	1040(%rsp), %esi        # 4-byte Reload
	andl	%esi, %eax
	addl	%edi, %eax
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%esi, %edi
	addl	%edx, %edi
	movl	1056(%rsp), %r9d        # 4-byte Reload
	movl	%r9d, %edx
	subl	%eax, %edx
	movq	1328(%rsp), %rsi        # 8-byte Reload
	cmpl	%eax, %esi
	cmovgl	%eax, %edx
	addl	%ebp, %edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movq	1448(%rsp), %r15        # 8-byte Reload
	leal	2(%r15), %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	leal	-2(%rbp,%rsi), %ebx
	cmpl	%r15d, %ebx
	cmovlel	%edx, %eax
	leal	-2(%rbp), %ebx
	cmpl	%r15d, %ebx
	cmovgl	%edx, %eax
	vmovss	.LCPI158_1(%rip), %xmm10 # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm10, %xmm1
	vmulss	%xmm3, %xmm1, %xmm0
	vdivss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm9
	movslq	%eax, %r13
	imulq	%r14, %r13
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %edx
	movq	%rax, %rbx
	vmovd	%edx, %xmm0
	movl	%r9d, %eax
	subl	%edi, %eax
	cmpl	%edi, %esi
	cmovgl	%edi, %eax
	addl	%ebp, %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	leal	-2(%r15), %edx
	cmpl	%edx, %ecx
	cmovgl	%edx, %ecx
	leal	2(%rbp,%rsi), %edx
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	cmpl	%r15d, %edx
	cmovlel	%eax, %ecx
	leal	2(%rbp), %edx
	cmpl	%r15d, %edx
	cmovgl	%eax, %ecx
	vsubss	%xmm3, %xmm2, %xmm2
	movslq	%ecx, %rsi
	imulq	%r14, %rsi
	movq	840(%rsp), %rcx         # 8-byte Reload
	leal	6(%rcx,%rbx), %edx
	movq	%rbx, %r9
	vmovd	%edx, %xmm5
	addl	$-1, %ebp
	cmpl	%r15d, %ebp
	movl	1120(%rsp), %eax        # 4-byte Reload
	cmovgl	1152(%rsp), %eax        # 4-byte Folded Reload
	movslq	%eax, %rdx
	imulq	%r14, %rdx
	movslq	1344(%rsp), %rdi        # 4-byte Folded Reload
	imulq	%r14, %rdi
	vmulss	%xmm2, %xmm1, %xmm1
	vdivss	%xmm1, %xmm7, %xmm11
	vsubss	%xmm12, %xmm10, %xmm2
	vmulss	%xmm6, %xmm2, %xmm1
	vdivss	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm6, %xmm3
	vaddss	%xmm1, %xmm12, %xmm12
	movq	%rcx, %rbp
	leal	6(%rbp), %ebx
	vmovaps	%xmm4, %xmm1
	vmovd	%ebx, %xmm6
	cmpl	$2, %r10d
	movl	1216(%rsp), %r14d       # 4-byte Reload
	cmovgl	1200(%rsp), %r14d       # 4-byte Folded Reload
	movq	120(%rsp), %r10         # 8-byte Reload
	cmpl	$2, %r10d
	movl	1248(%rsp), %ecx        # 4-byte Reload
	cmovgl	1264(%rsp), %ecx        # 4-byte Folded Reload
	testl	%r10d, %r10d
	movl	1280(%rsp), %eax        # 4-byte Reload
	cmovgl	1296(%rsp), %eax        # 4-byte Folded Reload
	leaq	(%rdx,%r8), %rdx
	movq	%rdx, 1344(%rsp)        # 8-byte Spill
	leaq	(%rdi,%r8), %rdx
	movq	%rdx, 1248(%rsp)        # 8-byte Spill
	movq	1088(%rsp), %rdx        # 8-byte Reload
	leaq	(%r8,%rdx), %rdi
	movq	%rdi, 1296(%rsp)        # 8-byte Spill
	leaq	(%r8,%r13), %rdi
	movq	%rdi, 1264(%rsp)        # 8-byte Spill
	leaq	(%r8,%rsi), %rdi
	movq	%rdi, 1280(%rsp)        # 8-byte Spill
	movq	1104(%rsp), %rdi        # 8-byte Reload
	subq	%rdi, %r13
	subq	%rdi, %rsi
	subq	%rdi, %rdx
	movq	%rdx, %rdi
	movslq	%eax, %rdx
	movslq	%ecx, %rbx
	leaq	(%r13,%rdx), %rax
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	addq	%rbx, %r13
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	leaq	(%rsi,%rdx), %rax
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	addq	%rbx, %rsi
	movq	%rsi, 1200(%rsp)        # 8-byte Spill
	leaq	(%rdx,%rdi), %rax
	movq	%rax, 1104(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rdi), %rax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	vsubss	%xmm3, %xmm13, %xmm4
	leal	5(%rbp,%r9), %esi
	movq	%rbp, %r13
	vmovd	%esi, %xmm13
	movslq	1312(%rsp), %rsi        # 4-byte Folded Reload
	movq	%rsi, 1072(%rsp)        # 8-byte Spill
	shlq	$5, %rsi
	addq	$48, %rsi
	movl	%r15d, %ebp
	andl	$63, %ebp
	imulq	%rsi, %rbp
	vmulss	%xmm4, %xmm2, %xmm2
	leal	5(%r13), %esi
	vmovd	%esi, %xmm14
	leal	7(%r15), %r10d
	movl	44(%rsp), %ebx          # 4-byte Reload
	subl	%ebx, %r10d
	movl	108(%rsp), %eax         # 4-byte Reload
	andl	$-32, %eax
	addl	$64, %eax
	imull	%eax, %r10d
	movl	%eax, %ecx
	movq	88(%rsp), %rax          # 8-byte Reload
	movq	%rax, %rsi
	sarq	$63, %rsi
	andq	%rax, %rsi
	leal	9(%r15), %edx
	subl	%ebx, %edx
	imull	%ecx, %edx
	leal	6(%r15), %edi
	subl	%ebx, %edi
	imull	%ecx, %edi
	movq	%rdi, 816(%rsp)         # 8-byte Spill
	subq	%rsi, %rbp
	movq	%rbp, 864(%rsp)         # 8-byte Spill
	leal	10(%r15), %ebp
	subl	%ebx, %ebp
	leal	8(%r15), %eax
	subl	%ebx, %eax
	imull	%ecx, %ebp
	movq	%rbp, 800(%rsp)         # 8-byte Spill
	imull	%ecx, %eax
	movq	%rax, 792(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm15
	vmovdqa	%xmm15, 768(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vdivss	%xmm2, %xmm1, %xmm2
	vmovss	52(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm10, %xmm5
	vmovss	56(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm5, %xmm3
	vmovss	48(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm7, %xmm3
	vmovdqa	.LCPI158_0(%rip), %xmm7 # xmm7 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 752(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 736(%rsp)        # 16-byte Spill
	vmovss	60(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	movq	848(%rsp), %r8          # 8-byte Reload
	vmovd	%r8d, %xmm6
	vmulss	%xmm0, %xmm5, %xmm0
	movl	1456(%rsp), %r9d        # 4-byte Reload
	vmovd	%r9d, %xmm5
	vdivss	%xmm0, %xmm4, %xmm0
	movl	1392(%rsp), %r15d       # 4-byte Reload
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm13, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 720(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 704(%rsp)        # 16-byte Spill
	movl	%r14d, %ebx
	vmovd	%ebx, %xmm4
	vpsubd	%xmm6, %xmm5, %xmm5
	vpsubd	%xmm6, %xmm1, %xmm1
	vpsubd	%xmm6, %xmm4, %xmm4
	movq	%r13, %rsi
	vmovd	%esi, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 688(%rsp)        # 16-byte Spill
	movq	888(%rsp), %r13         # 8-byte Reload
	movl	%r13d, %r14d
	subl	%esi, %r14d
	movq	112(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rsi,%rcx), %esi
	vmovd	%ecx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovd	%esi, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 656(%rsp)        # 16-byte Spill
	vmovd	%r8d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vmovd	%r9d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 624(%rsp)        # 16-byte Spill
	vmovd	%r15d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 608(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm6
	vmovaps	%xmm6, 592(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm6
	vmovaps	%xmm6, 576(%rsp)        # 16-byte Spill
	vmovd	%ebx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 560(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm5
	vmovdqa	%xmm5, 544(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 528(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm1
	vmovdqa	%xmm1, 512(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm1
	vmovaps	%xmm1, 288(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	movq	1072(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %esi
	shll	$9, %esi
	leal	(%rsi,%rsi,2), %esi
	addl	%esi, %r10d
	movq	%r10, 848(%rsp)         # 8-byte Spill
	addl	%esi, %edx
	movq	%rdx, 840(%rsp)         # 8-byte Spill
	shll	$10, %ecx
	leal	(%rcx,%rcx,2), %ebx
	leal	(%rdi,%rbx), %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	leal	(%rbp,%rbx), %edx
	movq	%rdx, 480(%rsp)         # 8-byte Spill
	leal	(%rbx,%rax), %edx
	movq	%rdx, 464(%rsp)         # 8-byte Spill
	leal	(%rdi,%rsi), %ecx
	movq	%rcx, 448(%rsp)         # 8-byte Spill
	leal	(%rbp,%rsi), %ecx
	movq	%rcx, 432(%rsp)         # 8-byte Spill
	leal	(%rsi,%rax), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	-6(%r14), %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	addl	$-5, %r14d
	movq	%r14, 672(%rsp)         # 8-byte Spill
	leal	-5(%r13), %eax
	movq	%rax, 384(%rsp)         # 8-byte Spill
	leal	-6(%r13), %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	movq	64(%rsp), %rsi          # 8-byte Reload
	movq	1296(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	movq	1280(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movq	1264(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	movq	1248(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	1344(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	movq	1104(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1120(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	movq	1088(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	movq	1152(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	1216(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	vpabsd	%xmm15, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI158_3(%rip), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_4(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_6(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB158_3:                              # %for dV.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	testl	%r11d, %r11d
	setne	1152(%rsp)              # 1-byte Folded Spill
	sete	960(%rsp)               # 1-byte Folded Spill
	movq	888(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %r15d
	movl	%r15d, %eax
	andl	$1, %eax
	movl	%eax, 944(%rsp)         # 4-byte Spill
	sete	1344(%rsp)              # 1-byte Folded Spill
	movq	672(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI158_2(%rip), %xmm9 # xmm9 = [0,2,4,6]
	vpaddd	%xmm9, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm2
	vmovdqa	%xmm9, %xmm5
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r10d
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vmovd	%xmm2, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovdqa	336(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 1104(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%esi
	vmovd	%ecx, %xmm0
	vpinsrd	$1, %r10d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovd	%r15d, %xmm2
	vpbroadcastd	%xmm2, %xmm13
	vmovdqa	752(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	736(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vmovdqa	1312(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm3
	vmovdqa	320(%rsp), %xmm14       # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	688(%rsp), %xmm9        # 16-byte Reload
	vpaddd	%xmm9, %xmm0, %xmm0
	vmovdqa	656(%rsp), %xmm11       # 16-byte Reload
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm0, %xmm0
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm9, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpmulld	1328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpsubd	640(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 992(%rsp)        # 16-byte Spill
	vpaddd	624(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1384(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	vpaddd	608(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %r9
	vmulps	1296(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	movq	936(%rsp), %r14         # 8-byte Reload
	vmovups	12296(%r14,%rdx,4), %xmm1
	vmovaps	%xmm1, 1088(%rsp)       # 16-byte Spill
	vmovups	12312(%r14,%rdx,4), %xmm2
	vmovaps	%xmm2, 1072(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	576(%rsp), %xmm12       # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	592(%rsp), %xmm15       # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vmovaps	1392(%rsp), %xmm8       # 16-byte Reload
	vminps	%xmm8, %xmm0, %xmm0
	vpxor	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm6
	vmulps	1280(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r14,%rdx,4), %xmm1
	vmovaps	%xmm1, 1040(%rsp)       # 16-byte Spill
	vmovups	12312(%r14,%rdx,4), %xmm3
	vmovaps	%xmm3, 1056(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm3[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vmovaps	1216(%rsp), %xmm4       # 16-byte Reload
	vandps	%xmm4, %xmm2, %xmm5
	vmulps	1264(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r14,%rdx,4), %xmm1
	vmovaps	%xmm1, 1008(%rsp)       # 16-byte Spill
	vmovups	12312(%r14,%rdx,4), %xmm0
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm3 # xmm3 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	%xmm3, %xmm2, %xmm1
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm6, %xmm1, %xmm0
	vandps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmulps	1248(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	movq	848(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r14,%rdx,4), %xmm2
	vmovaps	%xmm2, 976(%rsp)        # 16-byte Spill
	vmovdqa	1104(%rsp), %xmm4       # 16-byte Reload
	vmovdqa	1312(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm1
	vpsubd	%xmm4, %xmm14, %xmm3
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm3
	vmovups	12312(%r14,%rdx,4), %xmm1
	vshufps	$136, %xmm1, %xmm2, %xmm5 # xmm5 = xmm2[0,2],xmm1[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	352(%rsp), %xmm14       # 16-byte Reload
	vmulps	%xmm14, %xmm10, %xmm5
	movq	840(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovdqa	720(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm6, %xmm6
	vpxor	.LCPI158_9(%rip), %xmm6, %xmm6
	vmovdqa	704(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm9, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm9, %xmm3, %xmm3
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %esi
	vmovd	%esi, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	.LCPI158_2(%rip), %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vmovups	12296(%r14,%rdx,4), %xmm10
	vmovups	12312(%r14,%rdx,4), %xmm11
	vshufps	$136, %xmm11, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm11[0,2]
	vsubps	%xmm12, %xmm6, %xmm6
	vmulps	%xmm6, %xmm15, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm8, %xmm0, %xmm0
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovdqa	992(%rsp), %xmm0        # 16-byte Reload
	vpaddd	560(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rcx
	vpmulld	1328(%rsp), %xmm3, %xmm9 # 16-byte Folded Reload
	vpaddd	544(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbp
	vmovq	%xmm0, %rax
	movslq	%eax, %r10
	vpaddd	528(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbx
	vmovq	%xmm0, %rsi
	vmovss	(%rdi,%r10,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rax
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%ebp, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rbp
	vinsertps	$48, (%rdi,%rbp,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rax
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1072(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1296(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm3, %xmm0, %xmm5
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %r9
	vinsertps	$16, (%rdi,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r8d, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %r8
	vinsertps	$48, (%rdi,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 992(%rsp)        # 16-byte Spill
	movslq	%ecx, %rax
	vmovaps	1040(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1280(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm3, %xmm0, %xmm4
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%edx, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rdx
	vinsertps	$48, (%rdi,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1264(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%rdi,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movslq	%ebx, %rax
	sarq	$32, %rbx
	vinsertps	$16, (%rdi,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdi,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1088(%rsp)       # 16-byte Spill
	vpaddd	512(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	vmovss	(%rdi,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm2, %xmm3 # xmm3 = xmm2[0,1],mem[0],xmm2[3]
	vminps	%xmm8, %xmm5, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm7
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm6
	vmovaps	976(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmulps	1248(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm4
	vmulps	%xmm13, %xmm14, %xmm5
	vshufps	$221, %xmm11, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm11[1,3]
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	sarq	$32, %rax
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm10 # xmm10 = xmm3[0,1,2],mem[0]
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vmulps	1456(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	movl	%r11d, %eax
	vmovaps	%xmm3, %xmm13
	andl	%r15d, %eax
	jne	.LBB158_5
# BB#4:                                 # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB158_5:                              # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm15
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vandps	1104(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm7, %xmm12
	vsubps	%xmm2, %xmm6, %xmm11
	movq	792(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r14,%rdx,4), %xmm2
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm4
	movq	816(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r14,%rdx,4), %xmm6
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm7
	movq	800(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r14,%rdx,4), %xmm5
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm1
	movb	1152(%rsp), %cl         # 1-byte Reload
	andb	1344(%rsp), %cl         # 1-byte Folded Reload
	movq	1448(%rsp), %rsi        # 8-byte Reload
	vmovaps	%xmm0, %xmm14
	movb	%cl, %dl
	jne	.LBB158_6
# BB#7:                                 # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm9, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1152(%rsp)      # 16-byte Spill
	vmovups	%ymm3, 1344(%rsp)       # 32-byte Spill
	vmovaps	%xmm8, %xmm4
	jmp	.LBB158_8
	.align	16, 0x90
.LBB158_6:                              #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm10, 1152(%rsp)      # 16-byte Spill
	vmovups	%ymm3, 1344(%rsp)       # 32-byte Spill
	vmovaps	992(%rsp), %xmm0        # 16-byte Reload
	vmulps	240(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	vshufps	$136, %xmm4, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm4[0,2]
	vmovaps	%xmm4, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vmovaps	288(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	304(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm10, %xmm10
	vmulps	224(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm6, 1040(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm7[0,2]
	vmovaps	%xmm7, 1024(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	208(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm1, %xmm5, %xmm7 # xmm7 = xmm5[0,2],xmm1[0,2]
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1008(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm7, %xmm7
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmovaps	%xmm8, %xmm4
	vminps	%xmm4, %xmm3, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm3, %xmm3
	vminps	%xmm4, %xmm6, %xmm6
	vmaxps	%xmm2, %xmm6, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vminps	%xmm4, %xmm10, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vmovaps	1200(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm2, %xmm0
	vandps	%xmm14, %xmm0, %xmm0
	vaddps	%xmm0, %xmm9, %xmm0
	vmovaps	%xmm9, 912(%rsp)        # 16-byte Spill
	vmulps	1456(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
.LBB158_8:                              # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	movb	960(%rsp), %bl          # 1-byte Reload
	movl	944(%rsp), %edi         # 4-byte Reload
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm3
	vminps	%xmm4, %xmm15, %xmm8
	vandps	%xmm14, %xmm12, %xmm9
	vandps	%xmm14, %xmm11, %xmm11
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r14,%rcx,4), %xmm1
	vmovups	24600(%r14,%rcx,4), %xmm6
	movq	496(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r14,%rcx,4), %xmm5
	vmovups	24600(%r14,%rcx,4), %xmm0
	movq	480(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r14,%rcx,4), %xmm12
	vmovups	24600(%r14,%rcx,4), %xmm10
	andb	%dil, %bl
	jne	.LBB158_9
# BB#10:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm6, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1392(%rsp)       # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB158_11
	.align	16, 0x90
.LBB158_9:                              #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm4, 1392(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, %xmm15
	vmovaps	1072(%rsp), %xmm12      # 16-byte Reload
	vmulps	192(%rsp), %xmm12, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm6, 896(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm6[0,2]
	vmovaps	%xmm1, 960(%rsp)        # 16-byte Spill
	vmovaps	256(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	%xmm0, %xmm1
	vmovaps	%xmm1, 976(%rsp)        # 16-byte Spill
	vmovaps	272(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm7
	vmulps	%xmm7, %xmm2, %xmm13
	vmulps	176(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm5, 1120(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm1[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	160(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm15, %xmm12
	vshufps	$136, %xmm10, %xmm12, %xmm2 # xmm2 = xmm12[0,2],xmm10[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vminps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm5, %xmm5
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vminps	%xmm4, %xmm13, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm4
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm4
	vandps	%xmm14, %xmm4, %xmm2
	vaddps	912(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm2, %xmm13 # 16-byte Folded Reload
.LBB158_11:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmaxps	%xmm6, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm8, %xmm4
	vaddps	%xmm9, %xmm11, %xmm1
	movl	%r15d, %ecx
	orl	%esi, %ecx
	andl	$1, %ecx
	vxorps	%xmm8, %xmm8, %xmm8
	je	.LBB158_13
# BB#12:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm13, %xmm0
	vmovups	%ymm0, 1344(%rsp)       # 32-byte Spill
.LBB158_13:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	1456(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, %xmm7
	testb	%dl, %dl
	jne	.LBB158_15
# BB#14:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vxorps	%xmm7, %xmm7, %xmm7
.LBB158_15:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vandps	%xmm14, %xmm4, %xmm1
	testl	%eax, %eax
	je	.LBB158_17
# BB#16:                                #   in Loop: Header=BB158_3 Depth=1
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovaps	1088(%rsp), %xmm7       # 16-byte Reload
	vmulps	240(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmovaps	288(%rsp), %xmm0        # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	304(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1040(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	224(%rsp), %xmm7, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	1008(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 992(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	208(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm4, %xmm4
	vmaxps	%xmm8, %xmm4, %xmm4
	vminps	%xmm0, %xmm5, %xmm5
	vmaxps	%xmm8, %xmm5, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vminps	%xmm0, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm4, %xmm0, %xmm2
	vandps	%xmm14, %xmm2, %xmm2
	vaddps	%xmm2, %xmm1, %xmm2
	vmulps	1456(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
.LBB158_17:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	testl	%ecx, %ecx
	jne	.LBB158_19
# BB#18:                                #   in Loop: Header=BB158_3 Depth=1
	vmovaps	960(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 896(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	1152(%rsp), %xmm5       # 16-byte Reload
	vmulps	192(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	256(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	272(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1120(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	176(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm10, %xmm12, %xmm4 # xmm4 = xmm12[1,3],xmm10[1,3]
	vmulps	160(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	1392(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vminps	%xmm5, %xmm4, %xmm4
	vmaxps	%xmm8, %xmm4, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vmovaps	1200(%rsp), %xmm4       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm4, %xmm0
	vandps	%xmm14, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1456(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
.LBB158_19:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovups	1344(%rsp), %ymm2       # 32-byte Reload
	testb	%bl, %bl
	jne	.LBB158_21
# BB#20:                                # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovaps	%xmm7, %xmm3
.LBB158_21:                             # %for dV.s0.v10.v10
                                        #   in Loop: Header=BB158_3 Depth=1
	vmovaps	.LCPI158_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm3, %ymm0, %ymm0
	vmovaps	.LCPI158_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	864(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1416(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r12d
	movq	1424(%rsp), %rax        # 8-byte Reload
	addl	$-1, %eax
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	jne	.LBB158_3
	jmp	.LBB158_110
.LBB158_22:                             # %false_bb
	movq	%rbp, 1384(%rsp)        # 8-byte Spill
	cmpl	%esi, 40(%rdx)
	movq	%rsi, 1448(%rsp)        # 8-byte Spill
	jle	.LBB158_29
# BB#23:                                # %true_bb1
	vmovss	%xmm5, -16(%rsp)        # 4-byte Spill
	vmovss	%xmm3, -12(%rsp)        # 4-byte Spill
	vmovss	%xmm2, -8(%rsp)         # 4-byte Spill
	vmovss	%xmm7, -4(%rsp)         # 4-byte Spill
	vmovss	%xmm12, (%rsp)          # 4-byte Spill
	vmovss	%xmm6, 4(%rsp)          # 4-byte Spill
	vmovss	%xmm13, 8(%rsp)         # 4-byte Spill
	vmovss	%xmm4, 12(%rsp)         # 4-byte Spill
	vmovss	%xmm1, 60(%rsp)         # 4-byte Spill
	movq	88(%rsp), %rax          # 8-byte Reload
	movl	%eax, %edi
	sarl	$31, %edi
	andl	%eax, %edi
	movq	%rdi, 1328(%rsp)        # 8-byte Spill
	movl	$13, %eax
	subl	%edi, %eax
	movq	840(%rsp), %rdx         # 8-byte Reload
	addl	%edx, %eax
	xorl	%r14d, %r14d
	sarl	$3, %eax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	movl	$0, %esi
	cmovnsl	%eax, %esi
	movl	%esi, -96(%rsp)         # 4-byte Spill
	leal	51(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, 888(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	movl	%eax, %r8d
	cmovgl	%esi, %r8d
	movl	%r8d, -116(%rsp)        # 4-byte Spill
	movq	112(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	subl	%edi, %eax
	leal	(%rdx,%rax), %ebx
	leal	(%rdx,%rsi), %ebp
	movq	%rbp, -24(%rsp)         # 8-byte Spill
	subl	%edi, %ebp
	cmpl	%ebx, %ebp
	cmovlel	%ebp, %ebx
	sarl	$3, %ebx
	movq	%rbx, -112(%rsp)        # 8-byte Spill
	leal	-1(%rbx), %edi
	movl	%edi, -100(%rsp)        # 4-byte Spill
	leal	-10(%rdx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -28(%rsp)         # 4-byte Spill
	cmpl	%edi, %esi
	cmovgl	%edi, %esi
	leal	-9(%rdx,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -44(%rsp)         # 4-byte Spill
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	leal	-7(%rdx,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -48(%rsp)         # 4-byte Spill
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	leal	-2(%rdx,%rax), %edi
	sarl	$3, %edi
	movl	%edi, -52(%rsp)         # 4-byte Spill
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	leal	-1(%rdx,%rax), %eax
	sarl	$3, %eax
	movl	%eax, -56(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	-10(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -60(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	-9(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -64(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	-7(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -68(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	-2(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -72(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	-1(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -76(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	2(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -80(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	3(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -84(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	leal	4(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -88(%rsp)         # 4-byte Spill
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	addl	$5, %ebp
	sarl	$3, %ebp
	movq	%rbp, -40(%rsp)         # 8-byte Spill
	cmpl	%esi, %ebp
	cmovlel	%ebp, %esi
	addl	$43, %ecx
	sarl	$3, %ecx
	movq	%rcx, 1424(%rsp)        # 8-byte Spill
	cmpl	%esi, %ecx
	cmovlel	%ecx, %esi
	addl	$1, %esi
	cmpl	%esi, %r8d
	cmovgel	%r8d, %esi
	movl	%esi, -92(%rsp)         # 4-byte Spill
	testl	%r8d, %r8d
	jle	.LBB158_50
# BB#24:                                # %for dV.s0.v10.v104.preheader
	movq	96(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %edi
	movl	%edi, 1264(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	movl	%esi, %edx
	sarl	$31, %edx
	andnl	%edi, %edx, %ecx
	andl	%eax, %edx
	orl	%ecx, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movq	120(%rsp), %r9          # 8-byte Reload
	movl	%r9d, %eax
	negl	%eax
	cltd
	idivl	%edi
	movl	%edi, %r8d
	leal	(%r9,%rsi), %edi
	movl	%edi, 1216(%rsp)        # 4-byte Spill
	leal	-1(%rsi,%rsi), %r13d
	leal	-1(%r9,%rsi), %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%ecx, %ebp
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r9d, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	movl	%ecx, 1344(%rsp)        # 4-byte Spill
	testl	%edi, %edi
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	testl	%edi, %edi
	cmovlel	%ecx, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r9d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r9d, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%r9d, %ecx
	cmovll	%r9d, %ecx
	movl	%ecx, 1296(%rsp)        # 4-byte Spill
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%ebx, %eax
	cmpl	%r9d, %eax
	cmovll	%r9d, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movq	72(%rsp), %r10          # 8-byte Reload
	leal	(%r10,%r10), %edx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r10d, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ebp
	movl	%edx, %esi
	andl	%eax, %ecx
	movl	$2, %eax
	movq	80(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	cltd
	idivl	%esi
	movl	%esi, %r8d
	leal	-1(%r10,%r10), %esi
	orl	%ebp, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%esi, %r12d
	subl	%eax, %r12d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r12d
	leal	-1(%rdi,%r10), %ebp
	addl	%edi, %r12d
	cmpl	%r12d, %ebp
	cmovlel	%ebp, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	leal	(%rdi,%r10), %r11d
	cmpl	$3, %r11d
	movl	$2, %eax
	cmovll	%ebp, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %r11d
	cmovll	%r12d, %eax
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%esi, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r8d
	addl	%edi, %r8d
	cmpl	%r8d, %ebp
	cmovlel	%ebp, %r8d
	cmpl	%edi, %r8d
	cmovll	%edi, %r8d
	testl	%r11d, %r11d
	movl	$0, %eax
	cmovlel	%ebp, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%r11d, %r11d
	cmovlel	%r8d, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r9d, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1248(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	subl	%eax, %r13d
	movq	96(%rsp), %rdx          # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %r13d
	addl	%r9d, %r13d
	cmpl	%r13d, %ebx
	cmovlel	%ebx, %r13d
	cmpl	%r9d, %r13d
	cmovll	%r9d, %r13d
	movl	%r13d, 1264(%rsp)       # 4-byte Spill
	movl	1216(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r14d, %ebx
	movzbl	%al, %eax
	orl	%eax, %ebx
	cmpl	%r9d, %ebx
	cmovll	%r9d, %ebx
	cmpl	$2, %edx
	cmovll	%r13d, %ebx
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1200(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %esi
	cmpl	%eax, %r10d
	cmovgl	%eax, %esi
	addl	%edi, %esi
	cmpl	%esi, %ebp
	cmovlel	%ebp, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	$1, %r11d
	setg	%al
	cmpl	$2, %r11d
	movzbl	%al, %eax
	cmovgel	%r14d, %ebp
	orl	%eax, %ebp
	cmpl	%edi, %ebp
	cmovll	%edi, %ebp
	cmpl	$2, %r11d
	cmovll	%esi, %ebp
	movq	1448(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r13d
	andl	$1, %r13d
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm8
	cmpl	$1, %edi
	cmovgl	%esi, %ebp
	movq	-24(%rsp), %rcx         # 8-byte Reload
	leal	6(%rcx), %eax
	movq	%rcx, %rdx
	vmovd	%eax, %xmm9
	movq	840(%rsp), %rcx         # 8-byte Reload
	leal	6(%rcx), %eax
	vmovd	%eax, %xmm10
	movq	848(%rsp), %rax         # 8-byte Reload
	imull	%ecx, %eax
	leal	-1(%rdx), %edx
	vmovd	%edx, %xmm11
	addl	%edi, %eax
	vmovd	%eax, %xmm4
	vmovd	%eax, %xmm6
	cmpl	$1, %r9d
	cmovgl	1264(%rsp), %ebx        # 4-byte Folded Reload
	movl	108(%rsp), %r10d        # 4-byte Reload
	movq	32(%rsp), %rcx          # 8-byte Reload
	movl	%ecx, %eax
	movq	24(%rsp), %rdx          # 8-byte Reload
	imull	%edx, %eax
	sarl	$5, %r10d
	addl	%r9d, %eax
	movslq	%eax, %r11
	movq	1448(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rsi
	movq	%rcx, %rax
	imulq	%rsi, %rax
	subq	%r11, %rax
	testl	%edi, %edi
	movl	1456(%rsp), %edx        # 4-byte Reload
	cmovgl	%r8d, %edx
	movl	%edx, 1456(%rsp)        # 4-byte Spill
	leaq	2(%rsi), %r8
	imulq	%rcx, %r8
	vmovd	%ebp, %xmm0
	vmovd	%ebp, %xmm1
	leaq	-2(%rsi), %rbp
	imulq	%rcx, %rbp
	subq	%r11, %r8
	leaq	1(%rsi), %rdx
	imulq	%rcx, %rdx
	subq	%r11, %rbp
	addq	$-1, %rsi
	imulq	%rcx, %rsi
	subq	%r11, %rdx
	subq	%r11, %rsi
	movslq	%ebx, %rcx
	addq	%rcx, %rdx
	movq	%rdx, 1216(%rsp)        # 8-byte Spill
	addq	%rcx, %rsi
	movq	%rsi, 1248(%rsp)        # 8-byte Spill
	cmpl	$2, %edi
	movl	1392(%rsp), %edx        # 4-byte Reload
	cmovgl	%r12d, %edx
	movl	%edx, 1392(%rsp)        # 4-byte Spill
	cmpl	$2, %r9d
	movl	1280(%rsp), %esi        # 4-byte Reload
	cmovgl	1296(%rsp), %esi        # 4-byte Folded Reload
	leaq	(%rax,%rcx), %rdx
	movq	%rdx, 1280(%rsp)        # 8-byte Spill
	testl	%r9d, %r9d
	movl	1312(%rsp), %edx        # 4-byte Reload
	cmovgl	1344(%rsp), %edx        # 4-byte Folded Reload
	leaq	(%r8,%rcx), %rdi
	movq	%rdi, 1152(%rsp)        # 8-byte Spill
	leaq	(%rbp,%rcx), %rcx
	movq	%rcx, 1264(%rsp)        # 8-byte Spill
	movslq	%edx, %rcx
	movslq	%esi, %rbx
	leaq	(%r8,%rcx), %rdx
	movq	%rdx, 1088(%rsp)        # 8-byte Spill
	addq	%rbx, %r8
	movq	%r8, 1344(%rsp)         # 8-byte Spill
	leaq	(%rbp,%rcx), %rdx
	movq	%rdx, 1120(%rsp)        # 8-byte Spill
	addq	%rbx, %rbp
	leaq	(%rcx,%rax), %rcx
	movq	%rcx, 1200(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rax), %rax
	movq	%rax, 1104(%rsp)        # 8-byte Spill
	movslq	%r10d, %r11
	movq	%r11, %rax
	shlq	$5, %rax
	addq	$48, %rax
	movq	1448(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %ecx
	andl	$63, %ecx
	imulq	%rax, %rcx
	movq	-24(%rsp), %rax         # 8-byte Reload
	leal	5(%rax), %eax
	vmovd	%eax, %xmm2
	leal	7(%r9), %edx
	movl	44(%rsp), %r12d         # 4-byte Reload
	subl	%r12d, %edx
	movl	108(%rsp), %eax         # 4-byte Reload
	andl	$-32, %eax
	addl	$64, %eax
	imull	%eax, %edx
	movq	840(%rsp), %rsi         # 8-byte Reload
	leal	5(%rsi), %ebx
	vmovd	%ebx, %xmm7
	movq	88(%rsp), %rsi          # 8-byte Reload
	movq	%rsi, %rbx
	sarq	$63, %rbx
	leal	9(%r9), %r10d
	subl	%r12d, %r10d
	imull	%eax, %r10d
	leal	6(%r9), %r8d
	subl	%r12d, %r8d
	imull	%eax, %r8d
	movq	%r8, 792(%rsp)          # 8-byte Spill
	andq	%rsi, %rbx
	subq	%rbx, %rcx
	movq	%rcx, 864(%rsp)         # 8-byte Spill
	leal	10(%r9), %edi
	subl	%r12d, %edi
	imull	%eax, %edi
	movq	%rdi, 768(%rsp)         # 8-byte Spill
	leal	8(%r9), %esi
	subl	%r12d, %esi
	imull	%eax, %esi
	movq	%rsi, 752(%rsp)         # 8-byte Spill
	movl	%r11d, %eax
	shll	$9, %eax
	leal	(%rax,%rax,2), %eax
	addl	%eax, %edx
	movq	%rdx, 816(%rsp)         # 8-byte Spill
	addl	%eax, %r10d
	movq	%r10, 800(%rsp)         # 8-byte Spill
	shll	$10, %r11d
	movq	1328(%rsp), %r12        # 8-byte Reload
	movl	%r12d, %ecx
	movq	840(%rsp), %r10         # 8-byte Reload
	subl	%r10d, %ecx
	leal	-6(%rcx), %edx
	movq	%rdx, 720(%rsp)         # 8-byte Spill
	addl	$-5, %ecx
	movq	%rcx, 736(%rsp)         # 8-byte Spill
	movl	888(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	movq	16(%rsp), %rcx          # 8-byte Reload
	testl	%ecx, %ecx
	movl	$0, %edx
	cmovnsl	%ecx, %edx
	notl	%edx
	cmpl	%edx, %ebx
	cmovgel	%ebx, %edx
	vpbroadcastd	%xmm8, %xmm12
	vmovdqa	%xmm12, 704(%rsp)       # 16-byte Spill
	movq	848(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1312(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm3
	vmovdqa	.LCPI158_0(%rip), %xmm5 # xmm5 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm5, %xmm3, %xmm3
	vmovdqa	%xmm3, 688(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vmovdqa	%xmm3, 672(%rsp)        # 16-byte Spill
	vmovd	%r10d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 656(%rsp)        # 16-byte Spill
	movq	112(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm11, %xmm3
	vmovaps	%xmm3, 640(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm3
	vmovaps	%xmm3, 624(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovss	.LCPI158_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	movl	1456(%rsp), %ebx        # 4-byte Reload
	vmovd	%ebx, %xmm3
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 592(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm2, 576(%rsp)        # 16-byte Spill
	movl	1392(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm2
	vpsubd	%xmm4, %xmm0, %xmm8
	vpsubd	%xmm4, %xmm3, %xmm3
	vpsubd	%xmm4, %xmm2, %xmm2
	vmovss	-16(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm1, %xmm4
	vmovss	-8(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	-12(%rsp), %xmm6        # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm0, %xmm5
	vmulss	%xmm5, %xmm4, %xmm5
	vmulss	%xmm6, %xmm4, %xmm4
	vmovss	-4(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vdivss	%xmm5, %xmm0, %xmm5
	vmovd	%ebx, %xmm6
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 560(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 544(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 528(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm4
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 512(%rsp)        # 16-byte Spill
	vmovss	(%rsp), %xmm7           # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm1, %xmm4
	vmovss	8(%rsp), %xmm0          # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm6          # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm0, %xmm5
	vmulss	%xmm5, %xmm4, %xmm5
	vmulss	%xmm6, %xmm4, %xmm4
	vmovss	12(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm4
	vdivss	%xmm5, %xmm0, %xmm5
	vmovss	52(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm1, %xmm1
	vmovss	60(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	56(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm6, %xmm6
	vmulss	%xmm6, %xmm1, %xmm6
	vmulss	%xmm7, %xmm1, %xmm1
	vmovss	48(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vdivss	%xmm7, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vdivss	%xmm6, %xmm7, %xmm6
	vpbroadcastd	%xmm8, %xmm0
	vmovdqa	%xmm0, 496(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vmovdqa	%xmm0, 480(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm0
	vmovdqa	%xmm0, 464(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	leal	(%r11,%r11,2), %ecx
	leal	(%r8,%rcx), %ebx
	movq	%rbx, 448(%rsp)         # 8-byte Spill
	leal	(%rdi,%rcx), %ebx
	movq	%rbx, 432(%rsp)         # 8-byte Spill
	leal	(%rcx,%rsi), %ecx
	movq	%rcx, 416(%rsp)         # 8-byte Spill
	leal	(%r8,%rax), %ecx
	movq	%rcx, 400(%rsp)         # 8-byte Spill
	leal	(%rdi,%rax), %ecx
	movq	%rcx, 384(%rsp)         # 8-byte Spill
	leal	(%rax,%rsi), %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	notl	%edx
	leal	-5(%r12), %eax
	movq	%rax, 352(%rsp)         # 8-byte Spill
	leal	-6(%r12), %eax
	movq	%rax, 336(%rsp)         # 8-byte Spill
	movq	64(%rsp), %rax          # 8-byte Reload
	movq	1280(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movq	1264(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	movq	1152(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	movq	1248(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	1216(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	movq	1120(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	movq	1088(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	1104(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%rbp,4), %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	movq	1344(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 128(%rsp)        # 16-byte Spill
	vpabsd	%xmm12, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovdqa	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_3(%rip), %xmm15
	vbroadcastss	.LCPI158_4(%rip), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_6(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB158_25:                             # %for dV.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movl	%edx, 1152(%rsp)        # 4-byte Spill
	testl	%r13d, %r13d
	setne	1120(%rsp)              # 1-byte Folded Spill
	sete	912(%rsp)               # 1-byte Folded Spill
	movq	1328(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r14), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 896(%rsp)         # 4-byte Spill
	sete	1344(%rsp)              # 1-byte Folded Spill
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI158_2(%rip), %xmm13 # xmm13 = [0,2,4,6]
	vpaddd	%xmm13, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	704(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ebp
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, %r9d
	vmovd	%edi, %xmm0
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%esi
	movl	%edx, %r10d
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	vpinsrd	$3, %r9d, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ebp
	movl	%edx, %edi
	vmovdqa	288(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 1056(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vmovd	%ecx, %xmm1
	vpinsrd	$1, %r10d, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm2
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%r12d, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vmovdqa	%xmm3, 1008(%rsp)       # 16-byte Spill
	vmovdqa	688(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm2
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpxor	%xmm0, %xmm2, %xmm2
	vmovdqa	672(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	1296(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vmovdqa	1216(%rsp), %xmm0       # 16-byte Reload
	vpsubd	%xmm1, %xmm0, %xmm5
	vblendvps	%xmm4, %xmm1, %xmm5, %xmm1
	vmovdqa	656(%rsp), %xmm8        # 16-byte Reload
	vpaddd	%xmm8, %xmm1, %xmm1
	vmovdqa	640(%rsp), %xmm12       # 16-byte Reload
	vpminsd	%xmm12, %xmm1, %xmm1
	vpmaxsd	%xmm8, %xmm1, %xmm1
	movq	336(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm1, %xmm4, %xmm1
	vpmulld	1312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpsubd	624(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, 960(%rsp)        # 16-byte Spill
	vpaddd	608(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1384(%rsp), %r10        # 8-byte Reload
	vmovss	(%r10,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r10,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r10,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r10,%rax,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	vpaddd	560(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %r8
	vmovq	%xmm1, %r9
	vmulps	1280(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 1072(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[0,2],xmm2[0,2]
	vmovaps	528(%rsp), %xmm14       # 16-byte Reload
	vsubps	%xmm14, %xmm2, %xmm2
	vmovaps	544(%rsp), %xmm10       # 16-byte Reload
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vxorps	%xmm11, %xmm11, %xmm11
	vmaxps	%xmm11, %xmm1, %xmm5
	vmulps	1264(%rsp), %xmm9, %xmm7 # 16-byte Folded Reload
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm11, %xmm1, %xmm1
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm15, %xmm3
	vmovaps	1392(%rsp), %xmm15      # 16-byte Reload
	vandps	%xmm15, %xmm1, %xmm6
	vmovaps	320(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm7, %xmm9, %xmm4
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm14, %xmm2, %xmm2
	vmulps	%xmm2, %xmm10, %xmm2
	vmulps	%xmm2, %xmm4, %xmm0
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm11, %xmm0, %xmm0
	vsubps	%xmm5, %xmm0, %xmm0
	vandps	%xmm15, %xmm0, %xmm0
	vmovaps	%xmm3, %xmm15
	vaddps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmulps	1248(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	movq	816(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 944(%rsp)        # 16-byte Spill
	vmovdqa	1056(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	1296(%rsp), %xmm0       # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm0, %xmm0
	vmovdqa	1216(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm3, %xmm1, %xmm1
	vblendvps	%xmm0, %xmm3, %xmm1, %xmm0
	vmovups	12312(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 1056(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm1[0,2]
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm10, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	304(%rsp), %xmm11       # 16-byte Reload
	vmulps	%xmm11, %xmm9, %xmm2
	movq	800(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	vmovdqa	592(%rsp), %xmm3        # 16-byte Reload
	vmovdqa	1008(%rsp), %xmm5       # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm4
	vpxor	.LCPI158_9(%rip), %xmm4, %xmm4
	vmovdqa	576(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm3, %xmm3
	vpor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm0, %xmm0
	vpminsd	%xmm12, %xmm0, %xmm0
	vpmaxsd	%xmm8, %xmm0, %xmm0
	movq	352(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14), %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm12, %xmm4, %xmm4
	vpxor	%xmm12, %xmm12, %xmm12
	vpmaxsd	%xmm8, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovups	12296(%r15,%rdx,4), %xmm8
	vmovups	12312(%r15,%rdx,4), %xmm3
	vshufps	$136, %xmm3, %xmm8, %xmm4 # xmm4 = xmm8[0,2],xmm3[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm10, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm1, 1008(%rsp)       # 16-byte Spill
	vmovdqa	960(%rsp), %xmm1        # 16-byte Reload
	vpaddd	512(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rdx
	vmovq	%xmm1, %rdi
	vpmulld	1312(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
	vpaddd	496(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbp
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vpaddd	480(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rbx
	vmovss	(%r10,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rax
	vinsertps	$16, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%ebp, %rax
	vinsertps	$32, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rbp
	vinsertps	$48, (%r10,%rbp,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rax
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1072(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1280(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm1, %xmm0, %xmm6
	vmovss	(%r10,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %r9
	vinsertps	$16, (%r10,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r8d, %rax
	vinsertps	$32, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %r8
	vinsertps	$48, (%r10,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	movslq	%edi, %rax
	vmovaps	1040(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1264(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm1, %xmm0, %xmm1
	vmovss	(%r10,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rdi
	vinsertps	$16, (%r10,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%edx, %rax
	vinsertps	$32, (%r10,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rdx
	vinsertps	$48, (%r10,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	movslq	%ebx, %rax
	sarq	$32, %rbx
	vmovaps	992(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm5, %xmm7, %xmm2
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%r10,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	sarq	$32, %rsi
	vinsertps	$16, (%r10,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r10,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r10,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1072(%rsp)       # 16-byte Spill
	movl	%r12d, %esi
	vpaddd	464(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	vmovss	(%r10,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm2, %xmm4 # xmm4 = xmm2[0,1],mem[0],xmm2[3]
	vminps	%xmm15, %xmm6, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm13
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm1
	vmovaps	944(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1248(%rsp), %xmm5, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm10, %xmm6
	vmulps	%xmm5, %xmm11, %xmm0
	vshufps	$221, %xmm3, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm3[1,3]
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm10, %xmm3
	sarq	$32, %rax
	vinsertps	$48, (%r10,%rax,4), %xmm4, %xmm9 # xmm9 = xmm4[0,1,2],mem[0]
	vmovaps	1104(%rsp), %xmm4       # 16-byte Reload
	vmulps	1456(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	movl	%r13d, %eax
	vmovaps	%xmm4, %xmm14
	andl	%esi, %eax
	jne	.LBB158_27
# BB#26:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vxorps	%xmm14, %xmm14, %xmm14
.LBB158_27:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmulps	%xmm7, %xmm6, %xmm5
	vmovaps	%xmm5, 1088(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vandps	1008(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm13, %xmm11
	vsubps	%xmm2, %xmm1, %xmm6
	movq	752(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm2
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm3
	movq	792(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm10
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm5
	movq	768(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm1
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm13
	movb	1120(%rsp), %cl         # 1-byte Reload
	andb	1344(%rsp), %cl         # 1-byte Folded Reload
	movq	1448(%rsp), %rdx        # 8-byte Reload
	vmovaps	%xmm0, %xmm7
	movb	%cl, %r12b
	jne	.LBB158_28
# BB#35:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovaps	%xmm8, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 1120(%rsp)      # 16-byte Spill
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 976(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 992(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1056(%rsp)       # 16-byte Spill
	vmovups	%ymm4, 1344(%rsp)       # 32-byte Spill
	jmp	.LBB158_36
	.align	16, 0x90
.LBB158_28:                             #   in Loop: Header=BB158_25 Depth=1
	vmovaps	%xmm9, 1056(%rsp)       # 16-byte Spill
	vmovups	%ymm4, 1344(%rsp)       # 32-byte Spill
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, %xmm14
	vmovaps	960(%rsp), %xmm6        # 16-byte Reload
	vmulps	208(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm2, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1008(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm3[0,2]
	vmovaps	256(%rsp), %xmm0        # 16-byte Reload
	vsubps	%xmm0, %xmm3, %xmm3
	vmovaps	272(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm4, %xmm9
	vmulps	192(%rsp), %xmm6, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm5[0,2]
	vmovaps	%xmm5, 976(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 992(%rsp)       # 16-byte Spill
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	176(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm14, %xmm6
	vshufps	$136, %xmm13, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm13[0,2]
	vmovaps	%xmm13, 1120(%rsp)      # 16-byte Spill
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vminps	%xmm15, %xmm9, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm0, %xmm1
	vandps	%xmm7, %xmm1, %xmm1
	vaddps	%xmm1, %xmm8, %xmm1
	vmovaps	%xmm8, 960(%rsp)        # 16-byte Spill
	vmulps	1456(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
.LBB158_36:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	movb	912(%rsp), %bl          # 1-byte Reload
	movl	896(%rsp), %edi         # 4-byte Reload
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm11, %xmm1
	vmovaps	1088(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm15, %xmm2, %xmm11
	vminps	%xmm15, %xmm0, %xmm8
	vandps	%xmm7, %xmm1, %xmm10
	vandps	%xmm7, %xmm6, %xmm12
	movq	416(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm2
	vmovups	24600(%r15,%rcx,4), %xmm9
	movq	448(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm5
	vmovups	24600(%r15,%rcx,4), %xmm13
	movq	432(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r14), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm0
	vmovups	24600(%r15,%rcx,4), %xmm1
	andb	%dil, %bl
	jne	.LBB158_37
# BB#38:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovaps	%xmm2, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	.LBB158_39
	.align	16, 0x90
.LBB158_37:                             #   in Loop: Header=BB158_25 Depth=1
	vmovaps	1040(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vmulps	160(%rsp), %xmm6, %xmm14 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm9[0,2]
	vmovaps	%xmm2, 896(%rsp)        # 16-byte Spill
	vmovaps	224(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm4, %xmm4
	vmovaps	%xmm0, %xmm3
	vmovaps	%xmm3, 1104(%rsp)       # 16-byte Spill
	vmovaps	240(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm14, %xmm14
	vmulps	144(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm5, 1088(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm13, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm13[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vmulps	128(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm1, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm1[0,2]
	vsubps	%xmm2, %xmm6, %xmm6
	vmulps	%xmm6, %xmm0, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm15, %xmm4, %xmm4
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm4, %xmm4
	vminps	%xmm15, %xmm5, %xmm5
	vmaxps	%xmm2, %xmm5, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vminps	%xmm15, %xmm14, %xmm1
	vmaxps	%xmm2, %xmm1, %xmm1
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm4, %xmm0, %xmm1
	vandps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm7, %xmm0
	vaddps	960(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm1, %xmm14 # 16-byte Folded Reload
.LBB158_39:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmaxps	%xmm2, %xmm11, %xmm1
	vmaxps	%xmm2, %xmm8, %xmm4
	vaddps	%xmm10, %xmm12, %xmm3
	movl	%esi, %ecx
	orl	%edx, %ecx
	andl	$1, %ecx
	vxorps	%xmm11, %xmm11, %xmm11
	je	.LBB158_41
# BB#40:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovaps	%xmm14, %xmm2
	vmovups	%ymm2, 1344(%rsp)       # 32-byte Spill
.LBB158_41:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vsubps	%xmm1, %xmm4, %xmm4
	vmulps	1456(%rsp), %xmm3, %xmm12 # 16-byte Folded Reload
	vmovaps	%xmm12, %xmm5
	testb	%r12b, %r12b
	movl	1152(%rsp), %edx        # 4-byte Reload
	vmovaps	%xmm0, %xmm3
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	jne	.LBB158_43
# BB#42:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
.LBB158_43:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vandps	%xmm3, %xmm4, %xmm8
	testl	%eax, %eax
	je	.LBB158_45
# BB#44:                                #   in Loop: Header=BB158_25 Depth=1
	vmovaps	1024(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1008(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	1072(%rsp), %xmm2       # 16-byte Reload
	vmulps	208(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmovaps	256(%rsp), %xmm0        # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	272(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	992(%rsp), %xmm5        # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	192(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	944(%rsp), %xmm6        # 16-byte Reload
	vshufps	$221, %xmm1, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm1[1,3]
	vmulps	176(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm6, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vminps	%xmm15, %xmm5, %xmm5
	vmaxps	%xmm11, %xmm5, %xmm5
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm11, %xmm2, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm4
	vandps	%xmm3, %xmm4, %xmm2
	vaddps	%xmm2, %xmm8, %xmm2
	vmulps	1456(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
.LBB158_45:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	testl	%ecx, %ecx
	jne	.LBB158_47
# BB#46:                                #   in Loop: Header=BB158_25 Depth=1
	vmovaps	896(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm9[1,3]
	vmovaps	1056(%rsp), %xmm5       # 16-byte Reload
	vmulps	160(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
	vmovaps	224(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	240(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1088(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm13[1,3]
	vmulps	144(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1104(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 912(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vmulps	128(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm11, %xmm0, %xmm0
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm11, %xmm2, %xmm2
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm11, %xmm4, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vmovaps	1200(%rsp), %xmm4       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm4, %xmm0
	vandps	%xmm3, %xmm0, %xmm0
	vaddps	%xmm0, %xmm8, %xmm0
	vmulps	1456(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
.LBB158_47:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovups	1344(%rsp), %ymm2       # 32-byte Reload
	vmovaps	%xmm3, 1392(%rsp)       # 16-byte Spill
	testb	%bl, %bl
	jne	.LBB158_49
# BB#48:                                # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovaps	%xmm5, %xmm12
.LBB158_49:                             # %for dV.s0.v10.v104
                                        #   in Loop: Header=BB158_25 Depth=1
	vmovaps	.LCPI158_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm12, %ymm0, %ymm0
	vmovaps	.LCPI158_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%esi, %rax
	movq	864(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1416(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r14d
	addl	$-1, %edx
	jne	.LBB158_25
.LBB158_50:                             # %end for dV.s0.v10.v105
	movl	-116(%rsp), %eax        # 4-byte Reload
	cmpl	-92(%rsp), %eax         # 4-byte Folded Reload
	jge	.LBB158_73
# BB#51:                                # %for dV.s0.v10.v108.preheader
	movq	96(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %edi
	movl	%edi, 1392(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	movl	%esi, %edx
	sarl	$31, %edx
	andnl	%edi, %edx, %ecx
	movl	%edi, %ebp
	andl	%eax, %edx
	orl	%ecx, %edx
	movl	%edx, 1344(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movq	120(%rsp), %r10         # 8-byte Reload
	leal	(%r10,%rsi), %edi
	movl	%edi, 1312(%rsp)        # 4-byte Spill
	movl	%r10d, %eax
	negl	%eax
	cltd
	idivl	%ebp
	movl	%ebp, %ebx
	leal	-1(%rsi,%rsi), %ebp
	leal	-1(%r10,%rsi), %r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%ecx, %r8d
	addl	%edx, %eax
	movl	%ebp, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	cmpl	%ecx, %r12d
	cmovlel	%r12d, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	xorl	%r9d, %r9d
	testl	%edi, %edi
	movl	$0, %eax
	cmovlel	%r12d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	testl	%edi, %edi
	cmovlel	%ecx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r10d, %eax
	cltd
	idivl	%ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r8d, %eax
	addl	%edx, %eax
	movl	%ebp, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	cmpl	%ecx, %r12d
	cmovlel	%r12d, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1200(%rsp)        # 4-byte Spill
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%r12d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 1152(%rsp)        # 4-byte Spill
	movq	72(%rsp), %rbx          # 8-byte Reload
	leal	(%rbx,%rbx), %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %r8d
	movl	%edx, %edi
	andl	%eax, %ecx
	movl	$2, %eax
	movq	80(%rsp), %rsi          # 8-byte Reload
	subl	%esi, %eax
	cltd
	idivl	%edi
	orl	%r8d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %r8d
	movl	%r8d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %ebx
	cmovgl	%eax, %r11d
	addl	%esi, %r11d
	leal	-1(%rsi,%rbx), %r14d
	cmpl	%r11d, %r14d
	cmovlel	%r14d, %r11d
	cmpl	%esi, %r11d
	cmovll	%esi, %r11d
	leal	(%rsi,%rbx), %r13d
	cmpl	$3, %r13d
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %r13d
	cmovll	%r11d, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%r8d, %edi
	subl	%eax, %edi
	cmpl	%eax, %ebx
	cmovgl	%eax, %edi
	addl	%esi, %edi
	cmpl	%edi, %r14d
	cmovlel	%r14d, %edi
	cmpl	%esi, %edi
	cmovll	%esi, %edi
	testl	%r13d, %r13d
	movl	$0, %eax
	cmovlel	%r14d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%r13d, %r13d
	cmovlel	%edi, %eax
	movl	%eax, 1296(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	idivl	1392(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1344(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	subl	%eax, %ebp
	movq	96(%rsp), %rdx          # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ebp
	addl	%r10d, %ebp
	cmpl	%ebp, %r12d
	cmovlel	%r12d, %ebp
	cmpl	%r10d, %ebp
	cmovll	%r10d, %ebp
	movl	1312(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r9d, %r12d
	movzbl	%al, %eax
	orl	%eax, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	cmpl	$2, %edx
	cmovll	%ebp, %r12d
	movl	$1, %eax
	subl	%esi, %eax
	cltd
	idivl	1280(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %r8d
	cmpl	%eax, %ebx
	cmovgl	%eax, %r8d
	addl	%esi, %r8d
	cmpl	%r8d, %r14d
	cmovlel	%r14d, %r8d
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	cmpl	$1, %r13d
	setg	%al
	cmpl	$2, %r13d
	cmovgel	%r9d, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%esi, %r14d
	cmovll	%esi, %r14d
	cmpl	$2, %r13d
	cmovll	%r8d, %r14d
	movq	1448(%rsp), %rax        # 8-byte Reload
	andl	$1, %eax
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	cmpl	$1, %esi
	cmovgl	%r8d, %r14d
	movl	%r14d, 1280(%rsp)       # 4-byte Spill
	movq	848(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ecx
	movq	840(%rsp), %rax         # 8-byte Reload
	imull	%eax, %ecx
	addl	%esi, %ecx
	movl	%ecx, 1264(%rsp)        # 4-byte Spill
	cmpl	$1, %r10d
	cmovgl	%ebp, %r12d
	movl	108(%rsp), %r14d        # 4-byte Reload
	sarl	$5, %r14d
	movq	32(%rsp), %rcx          # 8-byte Reload
	movl	%ecx, %edx
	movq	24(%rsp), %rax          # 8-byte Reload
	imull	%eax, %edx
	addl	%r10d, %edx
	movq	1448(%rsp), %rax        # 8-byte Reload
	cltq
	movq	%rcx, %rbp
	imulq	%rax, %rbp
	movslq	%edx, %r13
	subq	%r13, %rbp
	testl	%esi, %esi
	movl	1296(%rsp), %edx        # 4-byte Reload
	cmovgl	%edi, %edx
	movl	%edx, 1296(%rsp)        # 4-byte Spill
	movslq	%r12d, %rdi
	leaq	(%rbp,%rdi), %rdx
	movq	64(%rsp), %r12          # 8-byte Reload
	vbroadcastss	(%r12,%rdx,4), %xmm0
	vmovaps	%xmm0, 864(%rsp)        # 16-byte Spill
	leaq	-2(%rax), %r8
	imulq	%rcx, %r8
	subq	%r13, %r8
	leaq	(%r8,%rdi), %rdx
	vbroadcastss	(%r12,%rdx,4), %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	leaq	2(%rax), %rbx
	imulq	%rcx, %rbx
	subq	%r13, %rbx
	leaq	(%rbx,%rdi), %rdx
	vbroadcastss	(%r12,%rdx,4), %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	leaq	1(%rax), %rdx
	imulq	%rcx, %rdx
	subq	%r13, %rdx
	addq	$-1, %rax
	imulq	%rcx, %rax
	subq	%r13, %rax
	addq	%rdi, %rdx
	addq	%rdi, %rax
	vbroadcastss	(%r12,%rax,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vbroadcastss	(%r12,%rdx,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	cmpl	$2, %esi
	movl	1456(%rsp), %eax        # 4-byte Reload
	cmovgl	%r11d, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	cmpl	$2, %r10d
	movl	1152(%rsp), %ecx        # 4-byte Reload
	cmovgl	1200(%rsp), %ecx        # 4-byte Folded Reload
	testl	%r10d, %r10d
	movl	1216(%rsp), %eax        # 4-byte Reload
	cmovgl	1248(%rsp), %eax        # 4-byte Folded Reload
	movslq	%eax, %rdx
	movslq	%ecx, %rdi
	leaq	(%rbx,%rdx), %r13
	addq	%rdi, %rbx
	movq	%rbx, 1248(%rsp)        # 8-byte Spill
	leaq	(%r8,%rdx), %rax
	addq	%rdi, %r8
	leaq	(%rdx,%rbp), %rdx
	leaq	(%rdi,%rbp), %r10
	vbroadcastss	(%r12,%rdx,4), %xmm0
	vmovaps	%xmm0, 560(%rsp)        # 16-byte Spill
	movslq	%r14d, %r14
	movq	1384(%rsp), %r11        # 8-byte Reload
	movq	%r14, %rdx
	shlq	$5, %rdx
	addq	$48, %rdx
	movq	1448(%rsp), %rbp        # 8-byte Reload
	movl	%ebp, %esi
	andl	$63, %esi
	imulq	%rdx, %rsi
	movq	88(%rsp), %rbp          # 8-byte Reload
	movq	%rbp, %rdx
	sarq	$63, %rdx
	andq	%rbp, %rdx
	subq	%rdx, %rsi
	movq	%rsi, 792(%rsp)         # 8-byte Spill
	movl	888(%rsp), %edx         # 4-byte Reload
	notl	%edx
	movq	16(%rsp), %rcx          # 8-byte Reload
	testl	%ecx, %ecx
	cmovsl	%r9d, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	%rcx, 16(%rsp)          # 8-byte Spill
	movq	%rcx, %rdi
	movq	1448(%rsp), %rdx        # 8-byte Reload
	leal	7(%rdx), %ebx
	movl	44(%rsp), %edx          # 4-byte Reload
	subl	%edx, %ebx
	movl	108(%rsp), %esi         # 4-byte Reload
	andl	$-32, %esi
	addl	$64, %esi
	imull	%esi, %ebx
	vbroadcastss	(%r12,%rax,4), %xmm0
	vmovaps	%xmm0, 544(%rsp)        # 16-byte Spill
	movq	1448(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%r12,%r13,4), %xmm0
	vmovaps	%xmm0, 528(%rsp)        # 16-byte Spill
	leal	9(%rax), %r13d
	subl	%edx, %r13d
	imull	%esi, %r13d
	leal	6(%rax), %ebp
	subl	%edx, %ebp
	imull	%esi, %ebp
	vbroadcastss	(%r12,%r10,4), %xmm0
	vmovaps	%xmm0, 512(%rsp)        # 16-byte Spill
	vbroadcastss	(%r12,%r8,4), %xmm0
	vmovaps	%xmm0, 496(%rsp)        # 16-byte Spill
	leal	10(%rax), %r8d
	subl	%edx, %r8d
	imull	%esi, %r8d
	leal	8(%rax), %ecx
	subl	%edx, %ecx
	imull	%esi, %ecx
	movl	%r14d, %eax
	shll	$9, %eax
	leal	(%rax,%rax,2), %eax
	addl	%eax, %ebx
	leal	(,%rdi,8), %edx
	subl	%edx, %ebx
	movq	%rbx, 896(%rsp)         # 8-byte Spill
	addl	%eax, %r13d
	subl	%edx, %r13d
	shll	$10, %r14d
	leal	(%r14,%r14,2), %edi
	leal	(%rbp,%rdi), %esi
	subl	%edx, %esi
	leal	(%rbp,%rax), %r14d
	subl	%edx, %r14d
	subl	%edx, %ebp
	leal	(%r8,%rdi), %r10d
	subl	%edx, %r10d
	leal	(%r8,%rax), %ebx
	subl	%edx, %ebx
	subl	%edx, %r8d
	leal	(%rdi,%rcx), %r12d
	subl	%edx, %r12d
	leal	(%rax,%rcx), %edi
	subl	%edx, %edi
	subl	%edx, %ecx
	movq	1328(%rsp), %rax        # 8-byte Reload
	subl	%edx, %eax
	movq	896(%rsp), %rdx         # 8-byte Reload
	addl	$-8, %edx
	movq	%rdx, 896(%rsp)         # 8-byte Spill
	addl	$-8, %r13d
	movq	%r13, 768(%rsp)         # 8-byte Spill
	addl	$-8, %esi
	movq	%rsi, 704(%rsp)         # 8-byte Spill
	addl	$-8, %r14d
	movq	%r14, 688(%rsp)         # 8-byte Spill
	addl	$-8, %ebp
	movq	%rbp, 752(%rsp)         # 8-byte Spill
	addl	$-8, %r10d
	movq	%r10, 672(%rsp)         # 8-byte Spill
	addl	$-8, %ebx
	movq	%rbx, 656(%rsp)         # 8-byte Spill
	addl	$-8, %r8d
	movq	%r8, 736(%rsp)          # 8-byte Spill
	addl	$-8, %r12d
	movq	%r12, 640(%rsp)         # 8-byte Spill
	addl	$-8, %edi
	movq	%rdi, 624(%rsp)         # 8-byte Spill
	addl	$-8, %ecx
	movq	%rcx, 720(%rsp)         # 8-byte Spill
	addl	$-8, %eax
	movq	%rax, 608(%rsp)         # 8-byte Spill
	movq	-112(%rsp), %rcx        # 8-byte Reload
	negl	%ecx
	movl	-28(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-44(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-48(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-52(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-56(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-60(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-64(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-68(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-76(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-84(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movq	-40(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %edx
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movq	1424(%rsp), %rax        # 8-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	negl	%eax
	movq	16(%rsp), %rcx          # 8-byte Reload
	movl	%ecx, %edx
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	leal	1(%rdx,%rcx), %r10d
	movslq	1264(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 592(%rsp)         # 8-byte Spill
	movslq	1280(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 576(%rsp)         # 8-byte Spill
	vmovss	.LCPI158_1(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vmovss	-16(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm2, %xmm0
	vmovss	-12(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm0, %xmm1
	vmovss	-4(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	-8(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm3, %xmm3
	vmulss	%xmm3, %xmm0, %xmm0
	vdivss	%xmm0, %xmm4, %xmm0
	movslq	1296(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 976(%rsp)         # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movslq	1456(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1264(%rsp)        # 8-byte Spill
	vmovss	(%rsp), %xmm1           # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm2, %xmm3
	vmovss	4(%rsp), %xmm5          # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmulss	%xmm5, %xmm3, %xmm4
	vmovss	12(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm4
	vmovss	8(%rsp), %xmm1          # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm5
	vmulss	%xmm5, %xmm3, %xmm3
	vdivss	%xmm3, %xmm0, %xmm3
	vmovss	52(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm2, %xmm2
	vmovss	60(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	56(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm5
	vmulss	%xmm5, %xmm2, %xmm5
	vmulss	%xmm1, %xmm2, %xmm2
	vmovss	48(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vdivss	%xmm5, %xmm0, %xmm5
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 480(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 464(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 448(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	movq	848(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %r8
	movq	1248(%rsp), %rax        # 8-byte Reload
	movq	64(%rsp), %rcx          # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI158_3(%rip), %xmm10
	vbroadcastss	.LCPI158_4(%rip), %xmm9
	vbroadcastss	.LCPI158_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_6(%rip), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB158_52:                             # %for dV.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movl	1392(%rsp), %ebx        # 4-byte Reload
	testl	%ebx, %ebx
	setne	%r13b
	sete	1056(%rsp)              # 1-byte Folded Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 1040(%rsp)        # 4-byte Spill
	sete	1216(%rsp)              # 1-byte Folded Spill
	movslq	%r12d, %r14
	movq	%r14, 1072(%rsp)        # 8-byte Spill
	leaq	-6(%r14), %rdx
	movq	848(%rsp), %rax         # 8-byte Reload
	imulq	%rax, %rdx
	movq	592(%rsp), %rbp         # 8-byte Reload
	subq	%rbp, %rdx
	movq	576(%rsp), %rcx         # 8-byte Reload
	leaq	(%rdx,%rcx), %rsi
	leaq	(%r11,%rsi,4), %rdi
	vmovss	(%r11,%rsi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	leaq	(%rdi,%r8,4), %rsi
	vinsertps	$16, (%rdi,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	leaq	(%rsi,%r8,4), %rdi
	vinsertps	$32, (%rsi,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdi,%r8,4), %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],mem[0]
	vmovaps	864(%rsp), %xmm11       # 16-byte Reload
	vmulps	%xmm11, %xmm4, %xmm2
	movq	624(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r9), %esi
	movslq	%esi, %rsi
	vmovups	12296(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rsi,4), %xmm5
	vshufps	$136, %xmm5, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm5[0,2]
	vmovaps	1280(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm3, %xmm3
	vmovaps	1296(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm3, %xmm13, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm10, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm2, %xmm0
	vmovaps	816(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm7, %xmm4, %xmm12
	movq	688(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r9), %esi
	movslq	%esi, %rsi
	vmovups	12296(%r15,%rsi,4), %xmm1
	vmovaps	%xmm1, 1120(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rsi,4), %xmm2
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm1
	vmovaps	%xmm9, %xmm8
	vandps	%xmm8, %xmm1, %xmm12
	vmovaps	%xmm13, %xmm9
	vmovaps	800(%rsp), %xmm14       # 16-byte Reload
	vmulps	%xmm14, %xmm4, %xmm1
	movq	656(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r9), %esi
	movslq	%esi, %rsi
	vmovaps	%xmm10, %xmm13
	vmovups	12296(%r15,%rsi,4), %xmm6
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rsi,4), %xmm2
	vshufps	$136, %xmm2, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vandps	%xmm8, %xmm0, %xmm0
	vaddps	%xmm0, %xmm12, %xmm13
	vmulps	1344(%rsp), %xmm4, %xmm12 # 16-byte Folded Reload
	movq	896(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r9), %esi
	movslq	%esi, %rsi
	vmovups	12296(%r15,%rsi,4), %xmm6
	vmovaps	%xmm6, 1024(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1008(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm0[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	1312(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	movq	768(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r9), %esi
	movslq	%esi, %rsi
	vmovups	12296(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rsi,4), %xmm1
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vminps	%xmm10, %xmm6, %xmm4
	vmaxps	%xmm3, %xmm4, %xmm4
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm6
	leaq	-5(%r14), %rsi
	imulq	%rax, %rsi
	subq	%rbp, %rsi
	leaq	(%rsi,%rcx), %rdi
	vmovss	(%r11,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	leaq	(%r11,%rdi,4), %rdi
	vinsertps	$16, (%rdi,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdi,%r8,4), %rdi
	vinsertps	$32, (%rdi,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdi,%r8,4), %rdi
	vinsertps	$48, (%rdi,%r8,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm5, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm5[1,3]
	vmulps	%xmm12, %xmm11, %xmm5
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1104(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[1,3],mem[1,3]
	vmulps	%xmm12, %xmm7, %xmm5
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm5, %xmm3, %xmm3
	movq	976(%rsp), %rax         # 8-byte Reload
	leaq	(%rdx,%rax), %rdi
	vmovaps	1088(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[1,3],xmm2[1,3]
	vmulps	%xmm12, %xmm14, %xmm5
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm5, %xmm2, %xmm5
	vmovss	(%r11,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	leaq	(%r11,%rdi,4), %rdi
	movq	%r11, %rbp
	vinsertps	$16, (%rdi,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	leaq	(%rdi,%r8,4), %rdi
	vinsertps	$32, (%rdi,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leaq	(%rdi,%r8,4), %rdi
	vinsertps	$48, (%rdi,%r8,4), %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],mem[0]
	andl	%r12d, %ebx
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm10, %xmm3, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vsubps	%xmm0, %xmm2, %xmm2
	vandps	%xmm8, %xmm2, %xmm2
	vminps	%xmm10, %xmm5, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vandps	%xmm8, %xmm0, %xmm3
	vmovaps	1456(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm13, %xmm0
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	jne	.LBB158_54
# BB#53:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
.LBB158_54:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vandps	%xmm8, %xmm6, %xmm15
	vaddps	%xmm2, %xmm3, %xmm9
	movq	720(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %edi
	movslq	%edi, %r11
	movq	%r11, %rdi
	orq	$2, %rdi
	vmovups	(%r15,%rdi,4), %xmm1
	orq	$6, %r11
	vmovups	(%r15,%r11,4), %xmm2
	movq	752(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %edi
	movslq	%edi, %rax
	movq	%rax, %rdi
	orq	$2, %rdi
	vmovups	(%r15,%rdi,4), %xmm3
	orq	$6, %rax
	vmovups	(%r15,%rax,4), %xmm14
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	movq	%rax, %rdi
	orq	$2, %rdi
	vmovups	(%r15,%rdi,4), %xmm11
	orq	$6, %rax
	vmovups	(%r15,%rax,4), %xmm7
	andb	1216(%rsp), %r13b       # 1-byte Folded Reload
	vmovaps	%xmm8, %xmm6
	jne	.LBB158_55
# BB#56:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	%xmm7, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 944(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1104(%rsp)      # 16-byte Spill
	movq	%rbp, %r11
	movb	1056(%rsp), %bpl        # 1-byte Reload
	movl	1040(%rsp), %r14d       # 4-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB158_57
	.align	16, 0x90
.LBB158_55:                             #   in Loop: Header=BB158_52 Depth=1
	vmulps	560(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm1, 1088(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm2, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, %xmm13
	vmovaps	464(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm3, %xmm2
	vmovaps	%xmm2, 944(%rsp)        # 16-byte Spill
	vmovaps	480(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmulps	544(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmulps	528(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm7, %xmm11, %xmm7 # xmm7 = xmm11[0,2],xmm7[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	%xmm13, %xmm6
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm2, %xmm2
	vminps	%xmm10, %xmm1, %xmm1
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm1, %xmm1
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	%xmm6, %xmm0, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vmovaps	%xmm15, 1104(%rsp)      # 16-byte Spill
	vmulps	%xmm5, %xmm0, %xmm0
	movq	%rbp, %r11
	movb	1056(%rsp), %bpl        # 1-byte Reload
	movl	1040(%rsp), %r14d       # 4-byte Reload
.LBB158_57:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1008(%rsp), %xmm2       # 16-byte Reload
	vmovaps	992(%rsp), %xmm4        # 16-byte Reload
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmulps	%xmm5, %xmm9, %xmm5
	vmovaps	%xmm5, %xmm15
	testb	%r13b, %r13b
	jne	.LBB158_59
# BB#58:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vxorps	%xmm15, %xmm15, %xmm15
.LBB158_59:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	movl	1392(%rsp), %eax        # 4-byte Reload
	vmulps	1344(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm2[1,3]
	vmovaps	1280(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vmovaps	1296(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmulps	1312(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vmovaps	1200(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm4[1,3]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vandps	%xmm6, %xmm0, %xmm12
	andl	%r12d, %eax
	jne	.LBB158_60
# BB#61:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	%xmm12, 1200(%rsp)      # 16-byte Spill
	vmovups	%ymm5, 1216(%rsp)       # 32-byte Spill
	vmovaps	%xmm6, %xmm9
	jmp	.LBB158_62
	.align	16, 0x90
.LBB158_60:                             #   in Loop: Header=BB158_52 Depth=1
	vmovups	%ymm5, 1216(%rsp)       # 32-byte Spill
	movq	976(%rsp), %rax         # 8-byte Reload
	leaq	(%rsi,%rax), %rax
	leaq	(%r11,%rax,4), %rdi
	leaq	(%rdi,%r8,4), %rbx
	leaq	(%rbx,%r8,4), %rcx
	vmovss	(%r11,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 960(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmulps	560(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vmovaps	464(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	480(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	944(%rsp), %xmm3        # 16-byte Reload
	vshufps	$221, %xmm14, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm14[1,3]
	vmulps	544(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm0, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$221, 912(%rsp), %xmm11, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm11[1,3],mem[1,3]
	vmulps	528(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vaddps	%xmm1, %xmm3, %xmm1
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vmovaps	1248(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm0, %xmm2
	vandps	%xmm6, %xmm2, %xmm1
	vmovaps	%xmm6, %xmm9
	vaddps	%xmm1, %xmm12, %xmm1
	vmovaps	%xmm12, 1200(%rsp)      # 16-byte Spill
	vmulps	1456(%rsp), %xmm1, %xmm15 # 16-byte Folded Reload
.LBB158_62:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	movq	1264(%rsp), %rbx        # 8-byte Reload
	addq	%rbx, %rsi
	leaq	(%r11,%rsi,4), %rax
	leaq	(%rax,%r8,4), %rcx
	leaq	(%rcx,%r8,4), %rdi
	vmovss	(%r11,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%r8,4), %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],mem[0]
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	vmovups	24584(%r15,%rax,4), %xmm7
	vmovups	24600(%r15,%rax,4), %xmm4
	movq	704(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	vmovups	24584(%r15,%rax,4), %xmm8
	vmovups	24600(%r15,%rax,4), %xmm11
	movq	672(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	cltq
	vmovups	24584(%r15,%rax,4), %xmm12
	vmovups	24600(%r15,%rax,4), %xmm14
	movb	%bpl, %al
	andb	%r14b, %al
	jne	.LBB158_63
# BB#64:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	movq	1072(%rsp), %rcx        # 8-byte Reload
	vmovups	1152(%rsp), %ymm13      # 32-byte Reload
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	jmp	.LBB158_65
	.align	16, 0x90
.LBB158_63:                             #   in Loop: Header=BB158_52 Depth=1
	addq	%rbx, %rdx
	leaq	(%r11,%rdx,4), %rax
	leaq	(%rax,%r8,4), %rcx
	leaq	(%rcx,%r8,4), %rsi
	vmovss	(%r11,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	512(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm4, %xmm7, %xmm5 # xmm5 = xmm7[0,2],xmm4[0,2]
	vmovaps	432(%rsp), %xmm13       # 16-byte Reload
	vsubps	%xmm13, %xmm5, %xmm5
	vmovaps	448(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmulps	496(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vshufps	$136, %xmm11, %xmm8, %xmm6 # xmm6 = xmm8[0,2],xmm11[0,2]
	vsubps	%xmm13, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmulps	416(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm14, %xmm12, %xmm6 # xmm6 = xmm12[0,2],xmm14[0,2]
	vsubps	%xmm13, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm1, %xmm1
	vminps	%xmm10, %xmm5, %xmm5
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm5, %xmm5
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm1, %xmm5, %xmm1
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	%xmm9, %xmm0, %xmm0
	vaddps	1104(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	movq	1072(%rsp), %rcx        # 8-byte Reload
	vmovups	1152(%rsp), %ymm13      # 32-byte Reload
.LBB158_65:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	movq	1448(%rsp), %rax        # 8-byte Reload
	orl	%eax, %r12d
	andl	$1, %r12d
	je	.LBB158_67
# BB#66:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	%xmm0, %xmm13
.LBB158_67:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	testl	%r12d, %r12d
	je	.LBB158_68
# BB#69:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	movq	%rbx, 1264(%rsp)        # 8-byte Spill
	jmp	.LBB158_70
	.align	16, 0x90
.LBB158_68:                             #   in Loop: Header=BB158_52 Depth=1
	movq	%rbx, 1264(%rsp)        # 8-byte Spill
	vshufps	$221, %xmm4, %xmm7, %xmm0 # xmm0 = xmm7[1,3],xmm4[1,3]
	vmulps	512(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	432(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	448(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$221, %xmm11, %xmm8, %xmm1 # xmm1 = xmm8[1,3],xmm11[1,3]
	vmulps	496(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vshufps	$221, %xmm14, %xmm12, %xmm2 # xmm2 = xmm12[1,3],xmm14[1,3]
	vmulps	416(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm0, %xmm0
	vminps	%xmm10, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vmovaps	1248(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	%xmm9, %xmm0, %xmm0
	vaddps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm0, %xmm15 # 16-byte Folded Reload
.LBB158_70:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovups	1216(%rsp), %ymm1       # 32-byte Reload
	andb	%r14b, %bpl
	jne	.LBB158_72
# BB#71:                                # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	%xmm15, %xmm1
.LBB158_72:                             # %for dV.s0.v10.v108
                                        #   in Loop: Header=BB158_52 Depth=1
	vmovaps	.LCPI158_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI158_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm13, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	792(%rsp), %rax         # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	1416(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r9d
	addl	$-1, %r10d
	jne	.LBB158_52
.LBB158_73:                             # %end for dV.s0.v10.v109
	movq	%r15, 936(%rsp)         # 8-byte Spill
	movl	-92(%rsp), %eax         # 4-byte Reload
	cmpl	888(%rsp), %eax         # 4-byte Folded Reload
	vmovss	52(%rsp), %xmm14        # 4-byte Reload
                                        # xmm14 = mem[0],zero,zero,zero
	vmovss	12(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	8(%rsp), %xmm12         # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm7          # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	(%rsp), %xmm4           # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	-8(%rsp), %xmm11        # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmovss	-12(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmovss	-16(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	jge	.LBB158_110
# BB#74:                                # %for dV.s0.v10.v1012.preheader
	movq	120(%rsp), %r10         # 8-byte Reload
	movl	%r10d, %eax
	negl	%eax
	movq	96(%rsp), %r14          # 8-byte Reload
	leal	(%r14,%r14), %esi
	movl	%esi, 1248(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%r14d, %r12d
	sarl	$31, %r12d
	andnl	%esi, %r12d, %ecx
	andl	%eax, %r12d
	orl	%ecx, %r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %r13d
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	leal	-1(%r10,%r14), %r11d
	cmpl	%ecx, %r11d
	cmovlel	%r11d, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1392(%rsp)        # 4-byte Spill
	leal	(%r10,%r14), %ebp
	testl	%ebp, %ebp
	movl	$0, %eax
	cmovlel	%r11d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	testl	%ebp, %ebp
	cmovlel	%ecx, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%r10d, %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r14d
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	cmpl	%ecx, %r11d
	cmovlel	%r11d, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1312(%rsp)        # 4-byte Spill
	cmpl	$3, %ebp
	movl	$2, %eax
	cmovll	%r11d, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %ebp
	cmovll	%ecx, %eax
	movl	%eax, 1296(%rsp)        # 4-byte Spill
	movq	72(%rsp), %r15          # 8-byte Reload
	leal	(%r15,%r15), %edx
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r15d, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %esi
	movl	%edx, %edi
	andl	%eax, %ecx
	movl	$2, %eax
	movq	80(%rsp), %r8           # 8-byte Reload
	subl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%edi, %ebx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r15,%r15), %esi
	movl	%esi, %edx
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	leal	-1(%r8,%r15), %edi
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	leal	(%r8,%r15), %r9d
	cmpl	$3, %r9d
	movl	$2, %eax
	cmovll	%edi, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %r9d
	cmovll	%edx, %eax
	movl	%eax, 1264(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	negl	%eax
	cltd
	idivl	%ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%esi, %ebx
	subl	%eax, %ebx
	cmpl	%eax, %r15d
	cmovgl	%eax, %ebx
	addl	%r8d, %ebx
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	testl	%r9d, %r9d
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	testl	%r9d, %r9d
	cmovlel	%ebx, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	idivl	1248(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r12d, %eax
	addl	%edx, %eax
	subl	%eax, %r13d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r13d
	addl	%r10d, %r13d
	cmpl	%r13d, %r11d
	cmovlel	%r11d, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	cmpl	$1, %ebp
	setg	%al
	cmpl	$2, %ebp
	movl	%ebp, %edx
	movl	$0, %ebp
	cmovgel	%ebp, %r11d
	movzbl	%al, %eax
	orl	%eax, %r11d
	cmpl	%r10d, %r11d
	cmovll	%r10d, %r11d
	cmpl	$2, %edx
	cmovll	%r13d, %r11d
	movl	$1, %eax
	subl	%r8d, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %esi
	cmpl	%eax, %r15d
	cmovgl	%eax, %esi
	addl	%r8d, %esi
	cmpl	%esi, %edi
	cmovlel	%edi, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	cmpl	$1, %r9d
	setg	%al
	cmpl	$2, %r9d
	movl	$0, %ecx
	cmovll	%edi, %ecx
	movzbl	%al, %eax
	orl	%eax, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	$2, %r9d
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm8
	cmovll	%esi, %ecx
	movq	1448(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r12d
	movq	%rax, %rdx
	andl	$1, %r12d
	movl	%r12d, 496(%rsp)        # 4-byte Spill
	cmpl	$1, %r8d
	cmovgl	%esi, %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	vmovss	.LCPI158_1(%rip), %xmm10 # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm10, %xmm0
	vmulss	%xmm5, %xmm0, %xmm1
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm9
	movq	848(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm3
	movq	840(%rsp), %rcx         # 8-byte Reload
	imull	%ecx, %eax
	addl	%r8d, %eax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	cmpl	$1, %r10d
	cmovgl	%r13d, %r11d
	movl	%r11d, 1216(%rsp)       # 4-byte Spill
	movl	%edx, %esi
	subl	44(%rsp), %esi          # 4-byte Folded Reload
	movl	108(%rsp), %ecx         # 4-byte Reload
	movl	%ecx, %edi
	sarl	$5, %edi
	movl	%edi, 1120(%rsp)        # 4-byte Spill
	andl	$-32, %ecx
	addl	$64, %ecx
	leal	8(%rsi), %eax
	imull	%ecx, %eax
	movq	%rax, 816(%rsp)         # 8-byte Spill
	movl	%edi, %r14d
	shll	$9, %r14d
	movq	32(%rsp), %r9           # 8-byte Reload
	movq	24(%rsp), %rax          # 8-byte Reload
	imull	%r9d, %eax
	addl	%r10d, %eax
	movslq	%eax, %rbp
	movslq	%edx, %r11
	movq	%r9, %r13
	imulq	%r11, %r13
	subq	%rbp, %r13
	testl	%r8d, %r8d
	movl	1456(%rsp), %eax        # 4-byte Reload
	cmovgl	%ebx, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	leal	10(%rsi), %eax
	imull	%ecx, %eax
	movq	%rax, 800(%rsp)         # 8-byte Spill
	leal	6(%rsi), %eax
	imull	%ecx, %eax
	movq	%rax, 792(%rsp)         # 8-byte Spill
	leal	9(%rsi), %eax
	imull	%ecx, %eax
	addl	$7, %esi
	imull	%ecx, %esi
	vsubss	%xmm5, %xmm11, %xmm1
	movq	-24(%rsp), %rdi         # 8-byte Reload
	leal	6(%rdi), %edx
	vmovd	%edx, %xmm5
	leaq	2(%r11), %rbx
	imulq	%r9, %rbx
	vmulss	%xmm1, %xmm0, %xmm0
	leaq	-2(%r11), %rdx
	imulq	%r9, %rdx
	vdivss	%xmm0, %xmm2, %xmm11
	leaq	1(%r11), %r15
	imulq	%r9, %r15
	vsubss	%xmm4, %xmm10, %xmm1
	vmulss	%xmm7, %xmm1, %xmm0
	vdivss	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm2
	vaddss	%xmm0, %xmm4, %xmm13
	leal	-1(%rdi), %ecx
	vmovd	%ecx, %xmm0
	addq	$-1, %r11
	imulq	%r9, %r11
	vsubss	%xmm7, %xmm2, %xmm2
	movq	112(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm12
	subq	%rbp, %rbx
	subq	%rbp, %rdx
	subq	%rbp, %r15
	subq	%rbp, %r11
	leal	(%r14,%r14,2), %ebp
	movq	%rbp, 752(%rsp)         # 8-byte Spill
	movslq	1216(%rsp), %rcx        # 4-byte Folded Reload
	addl	%ebp, %eax
	movq	%rax, 768(%rsp)         # 8-byte Spill
	addq	%rcx, %r15
	addl	%ebp, %esi
	movq	%rsi, 864(%rsp)         # 8-byte Spill
	addq	%rcx, %r11
	cmpl	$2, %r8d
	movl	1264(%rsp), %r14d       # 4-byte Reload
	cmovgl	1280(%rsp), %r14d       # 4-byte Folded Reload
	cmpl	$2, %r10d
	movl	1296(%rsp), %esi        # 4-byte Reload
	cmovgl	1312(%rsp), %esi        # 4-byte Folded Reload
	testl	%r10d, %r10d
	movl	1344(%rsp), %eax        # 4-byte Reload
	cmovgl	1392(%rsp), %eax        # 4-byte Folded Reload
	leaq	(%r13,%rcx), %rbp
	movq	%rbp, 1152(%rsp)        # 8-byte Spill
	leaq	(%rbx,%rcx), %rbp
	movq	%rbp, 1200(%rsp)        # 8-byte Spill
	leaq	(%rdx,%rcx), %rcx
	movq	%rcx, 1264(%rsp)        # 8-byte Spill
	movslq	%eax, %rcx
	movslq	%esi, %rsi
	leaq	(%rbx,%rcx), %rax
	movq	%rax, 1216(%rsp)        # 8-byte Spill
	addq	%rsi, %rbx
	leaq	(%rdx,%rcx), %r9
	addq	%rsi, %rdx
	leaq	(%rcx,%r13), %r10
	leaq	(%rsi,%r13), %r13
	vmulss	%xmm2, %xmm1, %xmm1
	addl	$5, %edi
	vmovd	%edi, %xmm15
	movl	1120(%rsp), %edi        # 4-byte Reload
	movslq	%edi, %rcx
	shlq	$5, %rcx
	addq	$48, %rcx
	movq	1448(%rsp), %rax        # 8-byte Reload
	andl	$63, %eax
	imulq	%rcx, %rax
	movq	88(%rsp), %rsi          # 8-byte Reload
	movq	%rsi, %rcx
	sarq	$63, %rcx
	andq	%rsi, %rcx
	subq	%rcx, %rax
	movq	%rax, 736(%rsp)         # 8-byte Spill
	movl	%edi, %ebp
	shll	$10, %ebp
	movl	-28(%rsp), %ecx         # 4-byte Reload
	movl	-100(%rsp), %esi        # 4-byte Reload
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movl	-44(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-48(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-52(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-56(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-60(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-64(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-68(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	-76(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	-84(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	movq	-40(%rsp), %rsi         # 8-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	movq	1424(%rsp), %rsi        # 8-byte Reload
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	addl	$1, %ecx
	movl	888(%rsp), %esi         # 4-byte Reload
	movl	-96(%rsp), %edi         # 4-byte Reload
	cmpl	%esi, %edi
	cmovgl	%esi, %edi
	cmpl	%edi, %ecx
	cmovgel	%ecx, %edi
	vpbroadcastd	%xmm8, %xmm2
	vmovdqa	%xmm2, 720(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1392(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm3
	vdivss	%xmm1, %xmm6, %xmm8
	vsubss	%xmm14, %xmm10, %xmm5
	vmovss	56(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm5, %xmm4
	vmovss	48(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm4, %xmm4
	vaddss	%xmm4, %xmm14, %xmm4
	vmovdqa	.LCPI158_0(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm6, %xmm3, %xmm3
	vmovdqa	%xmm3, 704(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 688(%rsp)        # 16-byte Spill
	vmovss	60(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm0, %xmm0
	movq	848(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm3
	vmulss	%xmm0, %xmm5, %xmm0
	movl	1248(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm5
	vdivss	%xmm0, %xmm1, %xmm0
	movl	1456(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm15, %xmm1
	vpaddd	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm1, 672(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm1
	vpsubd	%xmm3, %xmm5, %xmm5
	vpsubd	%xmm3, %xmm7, %xmm6
	vpsubd	%xmm3, %xmm1, %xmm1
	movq	840(%rsp), %r8          # 8-byte Reload
	vmovd	%r8d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 656(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm3
	vmovdqa	%xmm3, 1344(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpaddd	%xmm3, %xmm2, %xmm3
	vmovdqa	%xmm3, 1312(%rsp)       # 16-byte Spill
	vmovd	%eax, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 848(%rsp)        # 16-byte Spill
	vmovd	%esi, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 640(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 624(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm3
	vmovaps	%xmm3, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm3
	vmovaps	%xmm3, 1280(%rsp)       # 16-byte Spill
	vmovd	%r14d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 320(%rsp)        # 16-byte Spill
	leal	(%rbp,%rbp,2), %eax
	movl	%eax, 608(%rsp)         # 4-byte Spill
	vpbroadcastd	%xmm5, %xmm3
	vmovdqa	%xmm3, 592(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm3
	vmovdqa	%xmm3, 304(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 576(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm1
	vmovaps	%xmm1, 480(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm1
	vmovaps	%xmm1, 464(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 448(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	movslq	%edi, %r14
	movq	64(%rsp), %rcx          # 8-byte Reload
	movq	1152(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 560(%rsp)        # 16-byte Spill
	movq	1264(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 544(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 528(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%r11,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r15,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r10,4), %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%r9,4), %xmm0
	vmovaps	%xmm0, 400(%rsp)        # 16-byte Spill
	movq	1216(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 384(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%r13,4), %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%rdx,4), %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	vbroadcastss	(%rcx,%rbx,4), %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vpabsd	%xmm2, %xmm0
	vmovdqa	%xmm0, 512(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI158_3(%rip), %xmm13
	vbroadcastss	.LCPI158_4(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_5(%rip), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_6(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB158_75:                             # %for dV.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	testl	%r12d, %r12d
	setne	1120(%rsp)              # 1-byte Folded Spill
	sete	960(%rsp)               # 1-byte Folded Spill
	movq	1328(%rsp), %r15        # 8-byte Reload
	leal	(%r15,%r14,8), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	sete	1152(%rsp)              # 1-byte Folded Spill
	movl	%r12d, %ecx
	movq	840(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ecx
	leal	-5(%rcx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI158_2(%rip), %xmm12 # xmm12 = [0,2,4,6]
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	720(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %esi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %ebp
	cltd
	idivl	%ebp
	movl	%edx, %r11d
	addl	$-6, %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	vmovd	%r9d, %xmm1
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vmovd	%xmm0, %eax
	cltd
	idivl	%esi
	movl	%edx, %edi
	vpinsrd	$3, %r11d, %xmm1, %xmm1
	movl	%r12d, %r11d
	vpsrad	$31, %xmm1, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovdqa	512(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovdqa	%xmm1, 1104(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm2
	vmovdqa	%xmm2, 1088(%rsp)       # 16-byte Spill
	vmovdqa	704(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	leal	-6(%r15,%r14,8), %eax
	movq	%r15, %r9
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm12, %xmm2, %xmm2
	vmovdqa	688(%rsp), %xmm8        # 16-byte Reload
	vpminsd	%xmm8, %xmm2, %xmm2
	vmovdqa	656(%rsp), %xmm11       # 16-byte Reload
	vpmaxsd	%xmm11, %xmm2, %xmm2
	vmovdqa	1344(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vmovdqa	1312(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm11, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vpmulld	1392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpsubd	848(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 896(%rsp)        # 16-byte Spill
	vpaddd	640(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1384(%rsp), %r15        # 8-byte Reload
	vmovss	(%r15,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	movq	816(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14,8), %r8d
	movq	752(%rsp), %rbp         # 8-byte Reload
	leal	(%r8,%rbp), %edx
	vpaddd	624(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rcx
	movq	800(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14,8), %r10d
	leal	(%r10,%rbp), %esi
	vmovaps	560(%rsp), %xmm12       # 16-byte Reload
	vmulps	%xmm12, %xmm2, %xmm0
	movslq	%edx, %rdx
	movq	936(%rsp), %rbx         # 8-byte Reload
	vmovups	12296(%rbx,%rdx,4), %xmm1
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vmovups	12312(%rbx,%rdx,4), %xmm3
	vmovaps	%xmm3, 1056(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm3[0,2]
	vmovaps	1280(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm1, %xmm1
	vmovaps	1296(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	movq	792(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14,8), %edx
	leal	(%rdx,%rbp), %ebp
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm10, %xmm10, %xmm10
	vmaxps	%xmm10, %xmm0, %xmm0
	vmovaps	544(%rsp), %xmm9        # 16-byte Reload
	vmulps	%xmm9, %xmm2, %xmm5
	movslq	%ebp, %rbp
	vmovups	12296(%rbx,%rbp,4), %xmm3
	vmovaps	%xmm3, 1008(%rsp)       # 16-byte Spill
	vmovups	12312(%rbx,%rbp,4), %xmm1
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, %xmm14
	vshufps	$136, %xmm1, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm1[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	%xmm10, %xmm5, %xmm5
	vsubps	%xmm0, %xmm5, %xmm5
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vandps	%xmm1, %xmm5, %xmm13
	vmovaps	528(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm5, %xmm2, %xmm3
	movslq	%esi, %rsi
	vmovups	12296(%rbx,%rsi,4), %xmm4
	vmovaps	%xmm4, 992(%rsp)        # 16-byte Spill
	vmovups	12312(%rbx,%rsi,4), %xmm6
	vmovaps	%xmm6, 976(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm3
	vandps	%xmm1, %xmm3, %xmm3
	vaddps	%xmm3, %xmm13, %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	movq	768(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14,8), %ebp
	movq	864(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r14,8), %esi
	vmulps	1264(%rsp), %xmm2, %xmm13 # 16-byte Folded Reload
	movslq	%esi, %rsi
	vmovups	12296(%rbx,%rsi,4), %xmm3
	vmovaps	%xmm3, 944(%rsp)        # 16-byte Spill
	vmovdqa	1104(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	1344(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vmovdqa	1312(%rsp), %xmm6       # 16-byte Reload
	vpsubd	%xmm0, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm0, %xmm6, %xmm4
	vmovups	12312(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm6 # xmm6 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmulps	%xmm6, %xmm13, %xmm13
	vmulps	1248(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	movslq	%ebp, %rsi
	vmovdqa	672(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	1088(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	leal	-5(%r9,%r14,8), %ebp
	vmovd	%ebp, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	.LCPI158_2(%rip), %xmm2, %xmm2
	vpminsd	%xmm8, %xmm2, %xmm2
	vpmaxsd	%xmm11, %xmm2, %xmm2
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm8, %xmm4, %xmm4
	vpmaxsd	%xmm11, %xmm4, %xmm4
	vblendvps	%xmm6, %xmm2, %xmm4, %xmm2
	vmovups	12296(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovups	12312(%rbx,%rsi,4), %xmm8
	vshufps	$136, %xmm8, %xmm0, %xmm4 # xmm4 = xmm0[0,2],xmm8[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vminps	%xmm14, %xmm13, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm6
	vpmulld	1392(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, 1104(%rsp)       # 16-byte Spill
	vpaddd	592(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rsi
	vmovq	%xmm2, %rbp
	movslq	%ebp, %rax
	vmovss	(%r15,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	sarq	$32, %rbp
	vinsertps	$16, (%r15,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%esi, %rax
	vinsertps	$32, (%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	sarq	$32, %rsi
	vinsertps	$48, (%r15,%rsi,4), %xmm2, %xmm11 # xmm11 = xmm2[0,1,2],mem[0]
	vmovaps	1072(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm12, %xmm4
	vmovaps	%xmm1, %xmm3
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm9, %xmm1
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	movslq	%ecx, %rax
	vmovaps	992(%rsp), %xmm1        # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	%xmm11, %xmm5, %xmm4
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm4, %xmm1, %xmm1
	vmovss	(%r15,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%edi, %rax
	vinsertps	$32, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	sarq	$32, %rdi
	vinsertps	$48, (%r15,%rdi,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	movl	496(%rsp), %r12d        # 4-byte Reload
	movl	%r12d, %eax
	andl	%r11d, %eax
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vandps	%xmm3, %xmm0, %xmm0
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm10, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vandps	%xmm3, %xmm1, %xmm1
	vmovaps	1424(%rsp), %xmm3       # 16-byte Reload
	vmulps	1040(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm2, %xmm10
	jne	.LBB158_77
# BB#76:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
.LBB158_77:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vandps	1456(%rsp), %xmm6, %xmm9 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm1, %xmm15
	movslq	%r8d, %rax
	movq	%rax, %rsi
	orq	$2, %rsi
	vmovups	(%rbx,%rsi,4), %xmm1
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm6
	movslq	%edx, %rax
	movq	%rax, %rsi
	orq	$2, %rsi
	vmovups	(%rbx,%rsi,4), %xmm7
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm4
	movslq	%r10d, %rax
	movq	%rax, %rsi
	orq	$2, %rsi
	vmovups	(%rbx,%rsi,4), %xmm0
	orq	$6, %rax
	vmovups	(%rbx,%rax,4), %xmm13
	movb	1120(%rsp), %al         # 1-byte Reload
	andb	1152(%rsp), %al         # 1-byte Folded Reload
	movq	1448(%rsp), %r9         # 8-byte Reload
	jne	.LBB158_78
# BB#79:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	%xmm6, 976(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 1008(%rsp)      # 16-byte Spill
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1072(%rsp)       # 16-byte Spill
	vmovups	%ymm2, 1152(%rsp)       # 32-byte Spill
	vmovaps	%xmm14, %xmm6
	movb	960(%rsp), %r13b        # 1-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB158_80
	.align	16, 0x90
.LBB158_78:                             #   in Loop: Header=BB158_75 Depth=1
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 1008(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, 976(%rsp)        # 16-byte Spill
	vmovups	%ymm2, 1152(%rsp)       # 32-byte Spill
	vmovaps	%xmm7, %xmm2
	vmovaps	%xmm2, 1040(%rsp)       # 16-byte Spill
	vmulps	416(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm1, 1056(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm6[0,2]
	vmovaps	464(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	480(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm7, %xmm10
	vmulps	400(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,2],xmm4[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	384(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm13, %xmm0, %xmm7 # xmm7 = xmm0[0,2],xmm13[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm1, %xmm4
	vmovaps	%xmm14, %xmm6
	vminps	%xmm6, %xmm2, %xmm2
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm6, %xmm4, %xmm4
	vmaxps	%xmm7, %xmm4, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vminps	%xmm6, %xmm10, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	1216(%rsp), %xmm4       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm4, %xmm1
	vandps	1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm9, %xmm1
	vmovaps	%xmm9, 1072(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm1, %xmm10
	movb	960(%rsp), %r13b        # 1-byte Reload
.LBB158_80:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	944(%rsp), %xmm1        # 16-byte Reload
	vmovaps	912(%rsp), %xmm2        # 16-byte Reload
	vmovaps	1088(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm3, %xmm15, %xmm3
	vmovaps	%xmm3, %xmm9
	testb	%al, %al
	jne	.LBB158_82
# BB#81:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vxorps	%xmm9, %xmm9, %xmm9
.LBB158_82:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	movl	%r12d, %eax
	vmulps	1264(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm2[1,3]
	vmovaps	1280(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm1, %xmm1
	vmovaps	1296(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm1, %xmm13, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmulps	1248(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm4, %xmm2 # xmm2 = xmm4[1,3],xmm8[1,3]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm13, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vandps	1456(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
	andl	%r11d, %eax
	jne	.LBB158_83
# BB#84:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	%xmm14, 1088(%rsp)      # 16-byte Spill
	vmovups	%ymm3, 1120(%rsp)       # 32-byte Spill
	vmovaps	%xmm6, %xmm13
	vmovdqa	1104(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB158_85
	.align	16, 0x90
.LBB158_83:                             #   in Loop: Header=BB158_75 Depth=1
	vmovups	%ymm3, 1120(%rsp)       # 32-byte Spill
	vmovdqa	1104(%rsp), %xmm11      # 16-byte Reload
	vpaddd	304(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rsi
	movslq	%esi, %rdi
	sarq	$32, %rsi
	movslq	%eax, %rbp
	sarq	$32, %rax
	vmovss	(%r15,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	1056(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	416(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	464(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	480(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1040(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 992(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	400(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	1024(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1008(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vminps	%xmm6, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vminps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm6, %xmm13
	vmaxps	%xmm7, %xmm1, %xmm1
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm2, %xmm1
	vandps	1456(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm14, %xmm0
	vmovaps	%xmm14, 1088(%rsp)      # 16-byte Spill
	vmulps	1424(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB158_85:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vpaddd	576(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rsi
	movslq	%esi, %rdi
	movslq	%eax, %rbp
	movl	608(%rsp), %ecx         # 4-byte Reload
	addl	%ecx, %r10d
	addl	%ecx, %edx
	addl	%ecx, %r8d
	sarq	$32, %rsi
	sarq	$32, %rax
	vmovss	(%r15,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	movslq	%r8d, %rax
	vmovups	24584(%rbx,%rax,4), %xmm15
	vmovups	24600(%rbx,%rax,4), %xmm0
	movslq	%edx, %rax
	vmovups	24584(%rbx,%rax,4), %xmm14
	vmovups	24600(%rbx,%rax,4), %xmm11
	movslq	%r10d, %rax
	vmovups	24584(%rbx,%rax,4), %xmm8
	vmovups	24600(%rbx,%rax,4), %xmm12
	movb	%r13b, %al
	movl	1200(%rsp), %edi        # 4-byte Reload
	andb	%dil, %al
	je	.LBB158_87
# BB#86:                                #   in Loop: Header=BB158_75 Depth=1
	vmovdqa	896(%rsp), %xmm2        # 16-byte Reload
	vpaddd	320(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r15,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	368(%rsp), %xmm7, %xmm2 # 16-byte Folded Reload
	vshufps	$136, %xmm0, %xmm15, %xmm3 # xmm3 = xmm15[0,2],xmm0[0,2]
	vmovaps	432(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	448(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vmulps	352(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vshufps	$136, %xmm11, %xmm14, %xmm4 # xmm4 = xmm14[0,2],xmm11[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmulps	336(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vshufps	$136, %xmm12, %xmm8, %xmm7 # xmm7 = xmm8[0,2],xmm12[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm5, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm13, %xmm3, %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm3, %xmm3
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vmovaps	1216(%rsp), %xmm4       # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm4, %xmm2
	vandps	1456(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	1072(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1424(%rsp), %xmm2, %xmm10 # 16-byte Folded Reload
.LBB158_87:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovups	1152(%rsp), %ymm7       # 32-byte Reload
	movl	%r11d, %eax
	orl	%r9d, %eax
	andl	$1, %eax
	je	.LBB158_89
# BB#88:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	%xmm10, %xmm7
.LBB158_89:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	testl	%eax, %eax
	jne	.LBB158_91
# BB#90:                                #   in Loop: Header=BB158_75 Depth=1
	vshufps	$221, %xmm0, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm0[1,3]
	vmulps	368(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	432(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	448(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, %xmm11, %xmm14, %xmm2 # xmm2 = xmm14[1,3],xmm11[1,3]
	vmulps	352(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vshufps	$221, %xmm12, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm12[1,3]
	vmulps	336(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm3, %xmm2, %xmm2
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	1456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1088(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1424(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
.LBB158_91:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovups	1120(%rsp), %ymm1       # 32-byte Reload
	andb	%dil, %r13b
	jne	.LBB158_93
# BB#92:                                # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	%xmm9, %xmm1
.LBB158_93:                             # %for dV.s0.v10.v1012
                                        #   in Loop: Header=BB158_75 Depth=1
	vmovaps	.LCPI158_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI158_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm7, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r11d, %rax
	movq	736(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1416(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r14
	cmpl	888(%rsp), %r14d        # 4-byte Folded Reload
	jne	.LBB158_75
	jmp	.LBB158_110
.LBB158_29:                             # %false_bb2
	vmovss	%xmm1, 60(%rsp)         # 4-byte Spill
	addl	$51, %ecx
	sarl	$3, %ecx
	movq	%rcx, 1424(%rsp)        # 8-byte Spill
	testl	%ecx, %ecx
	jle	.LBB158_110
# BB#30:                                # %for dV.s0.v10.v1016.preheader
	movq	120(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	96(%rsp), %rbx          # 8-byte Reload
	leal	(%rbx,%rbx), %ebp
	movl	%ebp, 992(%rsp)         # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%ebx, %edi
	sarl	$31, %edi
	andnl	%ebp, %edi, %ecx
	movl	%ebp, %r8d
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edi, 1024(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	movl	%edi, %r9d
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %edx
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	movl	%edx, %ecx
	movl	%edx, %ebp
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	leal	-1(%rsi,%rbx), %edi
	movl	%edi, 1008(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edi
	cmovlel	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	leal	(%rsi,%rbx), %edx
	movl	%edx, 1040(%rsp)        # 4-byte Spill
	xorl	%r12d, %r12d
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%edx, %edx
	movl	%edx, %r10d
	cmovlel	%ecx, %eax
	movl	%eax, 1264(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%esi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	movl	%ebp, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	cmpl	%ecx, %edi
	cmovlel	%edi, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1216(%rsp)        # 4-byte Spill
	cmpl	$3, %r10d
	movl	$2, %eax
	cmovll	%edi, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %r10d
	cmovll	%ecx, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	movl	$2, %eax
	movq	80(%rsp), %rsi          # 8-byte Reload
	subl	%esi, %eax
	movq	72(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	movl	%ecx, 1296(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%edi, %ebp
	sarl	$31, %ebp
	andnl	%ecx, %ebp, %ecx
	andl	%eax, %ebp
	orl	%ecx, %ebp
	movl	%ebp, 976(%rsp)         # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %r10d
	movl	%r10d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	leal	-1(%rsi,%rdi), %edx
	movl	%edx, 1456(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, 1152(%rsp)        # 4-byte Spill
	leal	(%rsi,%rdi), %edi
	movl	%edi, 1280(%rsp)        # 4-byte Spill
	cmpl	$3, %edi
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %edi
	cmovll	%ecx, %eax
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	movq	1448(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %edi
	movq	24(%rsp), %r8           # 8-byte Reload
	subl	%r8d, %edi
	movq	%rdi, 1056(%rsp)        # 8-byte Spill
	leal	(%r11,%r11), %ebp
	movl	%ebp, 1072(%rsp)        # 4-byte Spill
	leal	-1(%rdi), %eax
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%r11d, %esi
	sarl	$31, %esi
	andnl	%ebp, %esi, %r9d
	andl	%eax, %esi
	orl	%r9d, %esi
	movl	%esi, 1088(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	movl	%esi, %r13d
	addl	%edx, %eax
	leal	(%r8,%r11), %r9d
	movq	%rcx, %rbx
	cmpl	%ebx, %r9d
	movl	%r9d, %edx
	cmovgl	%ebx, %edx
	addl	$-1, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movq	%r11, %rbx
	movq	%rbx, 1328(%rsp)        # 8-byte Spill
	leal	-1(%rbx,%rbx), %r14d
	movl	%r14d, %esi
	subl	%eax, %esi
	cmpl	%eax, %ebx
	cmovgl	%eax, %esi
	addl	%r8d, %esi
	leal	-1(%r8,%rbx), %r11d
	cmpl	%esi, %r11d
	cmovlel	%r11d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	cmpl	%ecx, %r9d
	cmovgel	%edx, %esi
	movl	%esi, 1120(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	cmpl	%ecx, %r11d
	movl	%r11d, %edx
	cmovgl	%ecx, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%r14d, %esi
	subl	%eax, %esi
	cmpl	%eax, %ebx
	cmovgl	%eax, %esi
	addl	%r8d, %esi
	cmpl	%esi, %r11d
	cmovlel	%r11d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	cmpl	%ecx, %r9d
	cmovgl	%edx, %esi
	leal	1(%rdi), %eax
	cltd
	idivl	%ebp
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edx, %eax
	movq	%rcx, %rdi
	leal	1(%rdi), %edx
	cmpl	%edx, %r11d
	cmovlel	%r11d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%r14d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	addl	%r8d, %ecx
	cmpl	%ecx, %r11d
	cmovlel	%r11d, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	%edi, %r11d
	cmovgl	%edx, %ecx
	movl	%ecx, 1104(%rsp)        # 4-byte Spill
	movq	80(%rsp), %rdi          # 8-byte Reload
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	1296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	movl	976(%rsp), %r9d         # 4-byte Reload
	andl	%r9d, %eax
	addl	%edx, %eax
	movl	%r10d, %r13d
	subl	%eax, %r13d
	movq	72(%rsp), %rcx          # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %r13d
	movq	%rdi, %rcx
	addl	%ecx, %r13d
	movl	1456(%rsp), %edi        # 4-byte Reload
	cmpl	%r13d, %edi
	cmovlel	%edi, %r13d
	cmpl	%ecx, %r13d
	cmovll	%ecx, %r13d
	movl	1280(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movq	%rcx, %rdi
	testl	%edx, %edx
	cmovlel	%r13d, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movl	$1, %eax
	movq	120(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %eax
	cltd
	idivl	992(%rsp)               # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1024(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	movl	1312(%rsp), %ebx        # 4-byte Reload
	subl	%eax, %ebx
	movq	96(%rsp), %rdx          # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %ebx
	movq	%rcx, %rdx
	addl	%edx, %ebx
	movl	1008(%rsp), %ebp        # 4-byte Reload
	cmpl	%ebx, %ebp
	cmovlel	%ebp, %ebx
	cmpl	%edx, %ebx
	cmovll	%edx, %ebx
	movl	%ebx, 1312(%rsp)        # 4-byte Spill
	movl	1040(%rsp), %ecx        # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	cmovgel	%r12d, %ebp
	movzbl	%al, %eax
	orl	%eax, %ebp
	cmpl	%edx, %ebp
	cmovll	%edx, %ebp
	cmpl	$2, %ecx
	cmovll	%ebx, %ebp
	movl	%ebp, %ebx
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r9d, %eax
	addl	%edx, %eax
	subl	%eax, %r10d
	movq	72(%rsp), %rcx          # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %r10d
	addl	%edi, %r10d
	movl	1456(%rsp), %ebp        # 4-byte Reload
	cmpl	%r10d, %ebp
	cmovlel	%ebp, %r10d
	cmpl	%edi, %r10d
	cmovll	%edi, %r10d
	movl	1280(%rsp), %ecx        # 4-byte Reload
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	cmovgel	%r12d, %ebp
	movzbl	%al, %eax
	orl	%eax, %ebp
	cmpl	%edi, %ebp
	cmovll	%edi, %ebp
	cmpl	$2, %ecx
	cmovll	%r10d, %ebp
	movl	%ebp, %ecx
	movq	88(%rsp), %rdx          # 8-byte Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edx, %eax
	movq	%rax, 888(%rsp)         # 8-byte Spill
	movq	1448(%rsp), %rax        # 8-byte Reload
	movl	%eax, %ebp
	andl	$1, %ebp
	movl	%ebp, 304(%rsp)         # 4-byte Spill
	cmpl	$1, %edi
	cmovgl	%r10d, %ecx
	movl	%ecx, 1456(%rsp)        # 4-byte Spill
	movq	848(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm8
	movq	840(%rsp), %rcx         # 8-byte Reload
	imull	%ecx, %eax
	addl	%edi, %eax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rcx         # 8-byte Reload
	cmpl	$1, %ecx
	cmovgl	1312(%rsp), %ebx        # 4-byte Folded Reload
	movq	32(%rsp), %r9           # 8-byte Reload
	movl	%r9d, %eax
	imull	%r8d, %eax
	movl	108(%rsp), %edx         # 4-byte Reload
	sarl	$5, %edx
	movl	%edx, 1312(%rsp)        # 4-byte Spill
	addl	%ecx, %eax
	cltq
	movq	%rax, 1040(%rsp)        # 8-byte Spill
	movslq	%esi, %rcx
	movslq	%ebx, %rsi
	imulq	%r9, %rcx
	movq	%rcx, 1024(%rsp)        # 8-byte Spill
	subq	%rax, %rsi
	testl	%edi, %edi
	movl	1344(%rsp), %eax        # 4-byte Reload
	cmovgl	%r13d, %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	movq	1056(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx), %eax
	cltd
	movl	1072(%rsp), %ebx        # 4-byte Reload
	idivl	%ebx
	movl	%edx, %edi
	movq	%rcx, %rax
	addl	$-2, %eax
	cltd
	idivl	%ebx
	vmovss	.LCPI158_1(%rip), %xmm10 # xmm10 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm10, %xmm1
	vmulss	%xmm3, %xmm1, %xmm0
	vdivss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm9
	movq	112(%rsp), %r10         # 8-byte Reload
	leal	(%r10,%r10), %eax
	vmovd	%eax, %xmm0
	movl	%edi, %eax
	sarl	$31, %eax
	movl	1088(%rsp), %ecx        # 4-byte Reload
	andl	%ecx, %eax
	addl	%edi, %eax
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%ecx, %edi
	addl	%edx, %edi
	movq	1448(%rsp), %r13        # 8-byte Reload
	leal	2(%r13), %edx
	cmpl	%edx, %r11d
	cmovlel	%r11d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%r14d, %ebx
	subl	%eax, %ebx
	movq	1328(%rsp), %rcx        # 8-byte Reload
	cmpl	%eax, %ecx
	cmovgl	%eax, %ebx
	addl	%r8d, %ebx
	cmpl	%ebx, %r11d
	cmovlel	%r11d, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	leal	-2(%r8,%rcx), %eax
	cmpl	%r13d, %eax
	movq	%r13, %rax
	cmovgl	%edx, %ebx
	vsubss	%xmm3, %xmm2, %xmm2
	movslq	%ebx, %r13
	imulq	%r9, %r13
	movq	840(%rsp), %rbx         # 8-byte Reload
	leal	6(%rbx,%r10), %edx
	vmovd	%edx, %xmm3
	leal	-2(%rax), %edx
	movq	%rax, %r10
	cmpl	%edx, %r11d
	cmovlel	%r11d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	subl	%edi, %r14d
	cmpl	%edi, %ecx
	cmovgl	%edi, %r14d
	addl	%r8d, %r14d
	cmpl	%r14d, %r11d
	cmovlel	%r11d, %r14d
	leal	2(%r8,%rcx), %edi
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	cmpl	%r10d, %edi
	movq	%r10, %r11
	cmovgl	%edx, %r14d
	movslq	%r14d, %r10
	imulq	%r9, %r10
	movslq	1104(%rsp), %rdx        # 4-byte Folded Reload
	imulq	%r9, %rdx
	movslq	1120(%rsp), %rdi        # 4-byte Folded Reload
	imulq	%r9, %rdi
	vmulss	%xmm2, %xmm1, %xmm1
	vdivss	%xmm1, %xmm7, %xmm11
	vsubss	%xmm12, %xmm10, %xmm2
	vmulss	%xmm6, %xmm2, %xmm1
	vdivss	%xmm4, %xmm1, %xmm1
	vmovaps	%xmm6, %xmm5
	vaddss	%xmm1, %xmm12, %xmm12
	movq	%rbx, %r8
	leal	6(%r8), %ebx
	vmovaps	%xmm4, %xmm1
	vmovd	%ebx, %xmm6
	movq	80(%rsp), %rax          # 8-byte Reload
	cmpl	$2, %eax
	movl	1392(%rsp), %eax        # 4-byte Reload
	cmovgl	1152(%rsp), %eax        # 4-byte Folded Reload
	movl	%eax, 1392(%rsp)        # 4-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	cmpl	$2, %eax
	movl	1200(%rsp), %ecx        # 4-byte Reload
	cmovgl	1216(%rsp), %ecx        # 4-byte Folded Reload
	testl	%eax, %eax
	movl	1264(%rsp), %eax        # 4-byte Reload
	cmovgl	1248(%rsp), %eax        # 4-byte Folded Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 1104(%rsp)        # 8-byte Spill
	leaq	(%rdi,%rsi), %rdx
	movq	%rdx, 1248(%rsp)        # 8-byte Spill
	movq	1024(%rsp), %rdi        # 8-byte Reload
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 1296(%rsp)        # 8-byte Spill
	leaq	(%rsi,%r13), %rdx
	movq	%rdx, 1264(%rsp)        # 8-byte Spill
	leaq	(%rsi,%r10), %rdx
	movq	%rdx, 1280(%rsp)        # 8-byte Spill
	movq	1040(%rsp), %rdx        # 8-byte Reload
	subq	%rdx, %r13
	subq	%rdx, %r10
	subq	%rdx, %rdi
	movq	%rdi, %rsi
	movslq	%eax, %rdx
	movslq	%ecx, %rdi
	leaq	(%r13,%rdx), %rax
	movq	%rax, 1072(%rsp)        # 8-byte Spill
	addq	%rdi, %r13
	movq	%r13, 1216(%rsp)        # 8-byte Spill
	leaq	(%r10,%rdx), %rax
	movq	%rax, 1088(%rsp)        # 8-byte Spill
	addq	%rdi, %r10
	movq	%r10, 1200(%rsp)        # 8-byte Spill
	movq	%rsi, %rcx
	leaq	(%rdx,%rcx), %rax
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	leaq	(%rdi,%rcx), %rax
	movq	%rax, 1152(%rsp)        # 8-byte Spill
	vsubss	%xmm5, %xmm13, %xmm4
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	5(%r8,%rax), %edi
	vmovd	%edi, %xmm13
	movslq	1312(%rsp), %rdi        # 4-byte Folded Reload
	movq	%rdi, 1056(%rsp)        # 8-byte Spill
	shlq	$5, %rdi
	addq	$48, %rdi
	movq	%r11, %rax
	movl	%eax, %ebx
	andl	$63, %ebx
	imulq	%rdi, %rbx
	vmulss	%xmm4, %xmm2, %xmm2
	leal	5(%r8), %edi
	vmovd	%edi, %xmm14
	leal	7(%rax), %r8d
	movl	44(%rsp), %esi          # 4-byte Reload
	subl	%esi, %r8d
	movl	108(%rsp), %r10d        # 4-byte Reload
	andl	$-32, %r10d
	addl	$64, %r10d
	imull	%r10d, %r8d
	movq	88(%rsp), %rcx          # 8-byte Reload
	movq	%rcx, %rdi
	sarq	$63, %rdi
	andq	%rcx, %rdi
	leal	9(%rax), %edx
	subl	%esi, %edx
	imull	%r10d, %edx
	leal	6(%rax), %r9d
	subl	%esi, %r9d
	imull	%r10d, %r9d
	movq	%r9, 816(%rsp)          # 8-byte Spill
	subq	%rdi, %rbx
	movq	%rbx, 864(%rsp)         # 8-byte Spill
	leal	10(%rax), %ebx
	subl	%esi, %ebx
	leal	8(%rax), %eax
	subl	%esi, %eax
	imull	%r10d, %ebx
	movq	%rbx, 800(%rsp)         # 8-byte Spill
	imull	%r10d, %eax
	movq	%rax, 792(%rsp)         # 8-byte Spill
	vmovss	52(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm0, %xmm15
	vmovdqa	%xmm15, 768(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vdivss	%xmm2, %xmm1, %xmm2
	vsubss	%xmm7, %xmm10, %xmm3
	vmovss	56(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm3, %xmm5
	vmovss	48(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm5, %xmm5
	vaddss	%xmm5, %xmm7, %xmm5
	vmovdqa	.LCPI158_0(%rip), %xmm7 # xmm7 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 752(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 736(%rsp)        # 16-byte Spill
	vmovss	60(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm1, %xmm0, %xmm0
	movq	848(%rsp), %r10         # 8-byte Reload
	vmovd	%r10d, %xmm6
	vmulss	%xmm0, %xmm3, %xmm0
	movl	1456(%rsp), %r11d       # 4-byte Reload
	vmovd	%r11d, %xmm3
	vdivss	%xmm0, %xmm4, %xmm0
	movl	1344(%rsp), %r14d       # 4-byte Reload
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm13, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 720(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 704(%rsp)        # 16-byte Spill
	movl	1392(%rsp), %r13d       # 4-byte Reload
	vmovd	%r13d, %xmm4
	vpsubd	%xmm6, %xmm3, %xmm3
	vpsubd	%xmm6, %xmm1, %xmm1
	vpsubd	%xmm6, %xmm4, %xmm4
	movq	840(%rsp), %rdi         # 8-byte Reload
	vmovd	%edi, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 688(%rsp)        # 16-byte Spill
	movq	888(%rsp), %rsi         # 8-byte Reload
	subl	%edi, %esi
	movq	112(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rdi,%rcx), %edi
	vmovd	%ecx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 656(%rsp)        # 16-byte Spill
	vmovd	%r10d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 624(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 608(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm6
	vmovaps	%xmm6, 592(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm6
	vmovaps	%xmm6, 576(%rsp)        # 16-byte Spill
	vmovd	%r13d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 560(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vmovdqa	%xmm3, 544(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	%xmm1, 528(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm1
	vmovdqa	%xmm1, 512(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 288(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm1
	vmovaps	%xmm1, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1056(%rsp), %rcx        # 8-byte Reload
	movl	%ecx, %edi
	shll	$9, %edi
	leal	(%rdi,%rdi,2), %edi
	addl	%edi, %r8d
	movq	%r8, 848(%rsp)          # 8-byte Spill
	addl	%edi, %edx
	movq	%rdx, 840(%rsp)         # 8-byte Spill
	shll	$10, %ecx
	leal	(%rcx,%rcx,2), %ecx
	leal	(%r9,%rcx), %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	leal	(%rbx,%rcx), %edx
	movq	%rdx, 480(%rsp)         # 8-byte Spill
	leal	(%rcx,%rax), %ecx
	movq	%rcx, 464(%rsp)         # 8-byte Spill
	leal	(%r9,%rdi), %ecx
	movq	%rcx, 448(%rsp)         # 8-byte Spill
	leal	(%rbx,%rdi), %ecx
	movq	%rcx, 432(%rsp)         # 8-byte Spill
	leal	(%rdi,%rax), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	-6(%rsi), %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	addl	$-5, %esi
	movq	%rsi, 672(%rsp)         # 8-byte Spill
	movq	888(%rsp), %rcx         # 8-byte Reload
	leal	-5(%rcx), %eax
	movq	%rax, 384(%rsp)         # 8-byte Spill
	leal	-6(%rcx), %eax
	movq	%rax, 368(%rsp)         # 8-byte Spill
	movq	64(%rsp), %rcx          # 8-byte Reload
	movq	1296(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	movq	1280(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movq	1264(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	movq	1248(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	1104(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	movq	1120(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	movq	1088(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	movq	1072(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	movq	1152(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	1200(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	movq	1216(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	vpabsd	%xmm15, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm15, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI158_3(%rip), %xmm8
	vbroadcastss	.LCPI158_4(%rip), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI158_6(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB158_31:                             # %for dV.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	testl	%ebp, %ebp
	setne	1344(%rsp)              # 1-byte Folded Spill
	sete	976(%rsp)               # 1-byte Folded Spill
	movq	888(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %r11d
	movl	%r11d, %eax
	andl	$1, %eax
	movl	%eax, 960(%rsp)         # 4-byte Spill
	sete	1200(%rsp)              # 1-byte Folded Spill
	movq	672(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI158_2(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r14d
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%edi, %xmm0
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm2
	vmovdqa	%xmm11, %xmm6
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpinsrd	$1, %r8d, %xmm0, %xmm0
	vpinsrd	$2, %ebx, %xmm0, %xmm0
	vmovd	%xmm2, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vpinsrd	$3, %ecx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ecx
	vmovdqa	336(%rsp), %xmm3        # 16-byte Reload
	vpand	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 1152(%rsp)       # 16-byte Spill
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r14d
	vmovd	%ebx, %xmm0
	vpinsrd	$1, %edi, %xmm0, %xmm0
	vpinsrd	$2, %ecx, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovd	%r11d, %xmm2
	vpbroadcastd	%xmm2, %xmm13
	vmovdqa	752(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm2
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm2
	vmovdqa	736(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm1, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vmovdqa	1312(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm1, %xmm3
	vmovdqa	320(%rsp), %xmm14       # 16-byte Reload
	vpsubd	%xmm0, %xmm14, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	688(%rsp), %xmm9        # 16-byte Reload
	vpaddd	%xmm9, %xmm0, %xmm0
	vmovdqa	656(%rsp), %xmm11       # 16-byte Reload
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm0, %xmm0
	movq	368(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm9, %xmm3, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpmulld	1328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpsubd	640(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 1072(%rsp)       # 16-byte Spill
	vpaddd	624(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1384(%rsp), %rdi        # 8-byte Reload
	vmovss	(%rdi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	vpaddd	608(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	vmovq	%xmm0, %r9
	vmulps	1296(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	movq	416(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 1104(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 1088(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm2[0,2]
	vmovaps	576(%rsp), %xmm12       # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm2
	vmovaps	592(%rsp), %xmm15       # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm2
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm8, %xmm0, %xmm0
	vpxor	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm6
	vmulps	1280(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 1040(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm3
	vmovaps	%xmm3, 1056(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm3[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vmovaps	1392(%rsp), %xmm4       # 16-byte Reload
	vandps	%xmm4, %xmm2, %xmm5
	vmulps	1264(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm1
	vmovaps	%xmm1, 1008(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm3 # xmm3 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm15, %xmm3
	vmulps	%xmm3, %xmm2, %xmm1
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm6, %xmm1, %xmm0
	vandps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmulps	1248(%rsp), %xmm10, %xmm0 # 16-byte Folded Reload
	movq	848(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovups	12296(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 992(%rsp)        # 16-byte Spill
	vmovdqa	1152(%rsp), %xmm4       # 16-byte Reload
	vmovdqa	1312(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm1
	vpsubd	%xmm4, %xmm14, %xmm3
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm3
	vmovups	12312(%r15,%rdx,4), %xmm1
	vshufps	$136, %xmm1, %xmm2, %xmm5 # xmm5 = xmm2[0,2],xmm1[0,2]
	vsubps	%xmm12, %xmm5, %xmm5
	vmulps	%xmm5, %xmm15, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	352(%rsp), %xmm14       # 16-byte Reload
	vmulps	%xmm14, %xmm10, %xmm5
	movq	840(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %edx
	movslq	%edx, %rdx
	vmovdqa	720(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm6, %xmm6
	vpxor	.LCPI158_9(%rip), %xmm6, %xmm6
	vmovdqa	704(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm13, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm9, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm9, %xmm3, %xmm3
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12), %esi
	vmovd	%esi, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	.LCPI158_2(%rip), %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm3, %xmm7, %xmm3
	vmovups	12296(%r15,%rdx,4), %xmm10
	vmovups	12312(%r15,%rdx,4), %xmm11
	vshufps	$136, %xmm11, %xmm10, %xmm6 # xmm6 = xmm10[0,2],xmm11[0,2]
	vsubps	%xmm12, %xmm6, %xmm6
	vmulps	%xmm6, %xmm15, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm8, %xmm0, %xmm0
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm8, %xmm5, %xmm5
	vmaxps	%xmm4, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vsubps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovdqa	1072(%rsp), %xmm0       # 16-byte Reload
	vpaddd	560(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rcx
	vpmulld	1328(%rsp), %xmm3, %xmm9 # 16-byte Folded Reload
	vpaddd	544(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbp
	vmovq	%xmm0, %rax
	movslq	%eax, %r10
	vpaddd	528(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rbx
	vmovq	%xmm0, %rsi
	vmovss	(%rdi,%r10,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rax
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%ebp, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rbp
	vinsertps	$48, (%rdi,%rbp,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	movslq	%r9d, %rax
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1088(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1296(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm3, %xmm0, %xmm5
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %r9
	vinsertps	$16, (%rdi,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r8d, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %r8
	vinsertps	$48, (%rdi,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	movslq	%ecx, %rax
	vmovaps	1040(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1056(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1280(%rsp), %xmm13, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm3, %xmm0, %xmm4
	vmovss	(%rdi,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%edx, %rax
	vinsertps	$32, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rdx
	vinsertps	$48, (%rdi,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	movslq	%esi, %rax
	sarq	$32, %rsi
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1024(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	1264(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%rdi,%rax,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	movslq	%ebx, %rax
	sarq	$32, %rbx
	vinsertps	$16, (%rdi,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdi,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vpaddd	512(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	vmovss	(%rdi,%rdx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm2, %xmm3 # xmm3 = xmm2[0,1],mem[0],xmm2[3]
	vminps	%xmm8, %xmm5, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm7
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm6
	vmovaps	992(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmulps	1248(%rsp), %xmm13, %xmm1 # 16-byte Folded Reload
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm4
	vmulps	%xmm13, %xmm14, %xmm5
	vshufps	$221, %xmm11, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm11[1,3]
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm15, %xmm0
	sarq	$32, %rax
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm12 # xmm12 = xmm3[0,1,2],mem[0]
	vmovaps	1120(%rsp), %xmm3       # 16-byte Reload
	vmulps	1456(%rsp), %xmm3, %xmm15 # 16-byte Folded Reload
	movl	304(%rsp), %ebp         # 4-byte Reload
	movl	%ebp, %eax
	vmovaps	%xmm15, %xmm13
	andl	%r11d, %eax
	jne	.LBB158_33
# BB#32:                                # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB158_33:                             # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmulps	%xmm1, %xmm4, %xmm4
	vmulps	%xmm5, %xmm0, %xmm11
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vandps	1152(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm7, %xmm1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vsubps	%xmm2, %xmm6, %xmm9
	movq	792(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm2
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm3
	movq	816(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm6
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm7
	movq	800(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	orq	$2, %rdx
	vmovups	(%r15,%rdx,4), %xmm1
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm5
	movb	1344(%rsp), %cl         # 1-byte Reload
	andb	1200(%rsp), %cl         # 1-byte Folded Reload
	movq	1448(%rsp), %rsi        # 8-byte Reload
	movb	%cl, %dl
	jne	.LBB158_34
# BB#94:                                # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm14, 944(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1024(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1120(%rsp)      # 16-byte Spill
	vmovups	%ymm15, 1344(%rsp)      # 32-byte Spill
	vmovaps	%xmm8, %xmm14
	jmp	.LBB158_95
	.align	16, 0x90
.LBB158_34:                             #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm12, 1120(%rsp)      # 16-byte Spill
	vmovups	%ymm15, 1344(%rsp)      # 32-byte Spill
	vmovaps	%xmm0, %xmm12
	vmovaps	1072(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm4, %xmm15
	vmulps	224(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
	vmovaps	%xmm2, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1040(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm2, %xmm3 # xmm3 = xmm2[0,2],xmm3[0,2]
	vmovaps	%xmm14, %xmm13
	vmovaps	%xmm13, 944(%rsp)       # 16-byte Spill
	vmovaps	272(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	288(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm3, %xmm10, %xmm10
	vmulps	208(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm6, 1024(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0,2],xmm7[0,2]
	vmovaps	%xmm7, 1008(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm6, %xmm6
	vmulps	%xmm6, %xmm2, %xmm6
	vmulps	%xmm6, %xmm3, %xmm3
	vmulps	192(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm5[0,2]
	vmovaps	%xmm5, 992(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vsubps	%xmm4, %xmm7, %xmm7
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmovaps	%xmm8, %xmm14
	vminps	%xmm14, %xmm3, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm3, %xmm3
	vminps	%xmm14, %xmm6, %xmm6
	vmaxps	%xmm2, %xmm6, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vminps	%xmm14, %xmm10, %xmm0
	vmovaps	%xmm15, %xmm4
	vmaxps	%xmm2, %xmm0, %xmm0
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm3, %xmm2, %xmm0
	vandps	%xmm12, %xmm0, %xmm0
	vaddps	%xmm0, %xmm13, %xmm0
	vmulps	1456(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
	vmovaps	%xmm12, %xmm0
.LBB158_95:                             # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	movb	976(%rsp), %bl          # 1-byte Reload
	movl	960(%rsp), %edi         # 4-byte Reload
	vmovaps	1152(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm14, %xmm4, %xmm10
	vminps	%xmm14, %xmm11, %xmm15
	vandps	%xmm0, %xmm1, %xmm12
	vandps	%xmm0, %xmm9, %xmm11
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm6
	vmovups	24600(%r15,%rcx,4), %xmm7
	movq	496(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm5
	vmovups	24600(%r15,%rcx,4), %xmm3
	movq	480(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12), %ecx
	movslq	%ecx, %rcx
	vmovups	24584(%r15,%rcx,4), %xmm1
	vmovups	24600(%r15,%rcx,4), %xmm8
	andb	%dil, %bl
	jne	.LBB158_96
# BB#97:                                # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm7, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 976(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1200(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB158_98
	.align	16, 0x90
.LBB158_96:                             #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm8, 976(%rsp)        # 16-byte Spill
	vmovaps	1088(%rsp), %xmm2       # 16-byte Reload
	vmulps	176(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm6, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 896(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm7, %xmm6, %xmm7 # xmm7 = xmm6[0,2],xmm7[0,2]
	vmovaps	240(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm7, %xmm7
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm9
	vmovaps	256(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm7
	vmulps	%xmm7, %xmm4, %xmm13
	vmulps	160(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vmovaps	%xmm5, 1200(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm3[0,2]
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm0, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	144(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm1, %xmm2 # xmm2 = xmm1[0,2],xmm8[0,2]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmovaps	%xmm9, %xmm0
	vmulps	%xmm2, %xmm7, %xmm2
	vminps	%xmm14, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm5, %xmm5
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vminps	%xmm14, %xmm13, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm4
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm1, %xmm4
	vandps	%xmm0, %xmm4, %xmm2
	vaddps	944(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm2, %xmm13 # 16-byte Folded Reload
.LBB158_98:                             # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmaxps	%xmm6, %xmm10, %xmm3
	vmaxps	%xmm6, %xmm15, %xmm4
	vaddps	%xmm12, %xmm11, %xmm1
	movl	%r11d, %ecx
	orl	%esi, %ecx
	andl	$1, %ecx
	je	.LBB158_100
# BB#99:                                # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm13, %xmm2
	vmovups	%ymm2, 1344(%rsp)       # 32-byte Spill
.LBB158_100:                            # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	1456(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm3, %xmm7
	testb	%dl, %dl
	jne	.LBB158_102
# BB#101:                               # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vxorps	%xmm7, %xmm7, %xmm7
.LBB158_102:                            # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vandps	%xmm0, %xmm4, %xmm8
	testl	%eax, %eax
	jne	.LBB158_103
# BB#104:                               # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	jmp	.LBB158_105
	.align	16, 0x90
.LBB158_103:                            #   in Loop: Header=BB158_31 Depth=1
	vmovaps	1056(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1040(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[1,3],mem[1,3]
	vmovaps	1104(%rsp), %xmm7       # 16-byte Reload
	vmulps	224(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm0, %xmm1
	vmovaps	%xmm1, 1392(%rsp)       # 16-byte Spill
	vmovaps	272(%rsp), %xmm0        # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vmovaps	288(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1024(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1008(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	208(%rsp), %xmm7, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	1072(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 992(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	192(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm0, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vminps	%xmm14, %xmm4, %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm4, %xmm4
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	%xmm0, %xmm5, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm4, %xmm0, %xmm2
	vandps	%xmm1, %xmm2, %xmm2
	vaddps	%xmm2, %xmm8, %xmm2
	vmulps	1456(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
.LBB158_105:                            # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovups	1344(%rsp), %ymm15      # 32-byte Reload
	vmovaps	1200(%rsp), %xmm10      # 16-byte Reload
	testl	%ecx, %ecx
	jne	.LBB158_107
# BB#106:                               #   in Loop: Header=BB158_31 Depth=1
	vmovaps	912(%rsp), %xmm0        # 16-byte Reload
	vshufps	$221, 896(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vmulps	176(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	240(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	256(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$221, 960(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm10[1,3],mem[1,3]
	vmulps	160(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1152(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 976(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	144(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm14, %xmm4, %xmm4
	vmaxps	%xmm1, %xmm4, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm1, %xmm0
	vandps	1392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm8, %xmm0
	vmulps	1456(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
.LBB158_107:                            # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm14, %xmm8
	testb	%bl, %bl
	jne	.LBB158_109
# BB#108:                               # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	%xmm7, %xmm3
.LBB158_109:                            # %for dV.s0.v10.v1016
                                        #   in Loop: Header=BB158_31 Depth=1
	vmovaps	.LCPI158_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm3, %ymm0, %ymm0
	vmovaps	.LCPI158_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm15, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r11d, %rax
	movq	864(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1416(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r12d
	movq	1424(%rsp), %rax        # 8-byte Reload
	addl	$-1, %eax
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	jne	.LBB158_31
.LBB158_110:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1480, %rsp             # imm = 0x5C8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end158:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174, .Lfunc_end158-par_for_par_for___sharpi_f0.s0.v11.v14_dV.s0.v11.174

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI159_0:
	.long	1199570688              # float 65535
.LCPI159_3:
	.long	1065353216              # float 1
.LCPI159_4:
	.long	2147483647              # 0x7fffffff
.LCPI159_5:
	.long	1056964608              # float 0.5
.LCPI159_6:
	.long	1073741824              # float 2
	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI159_1:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI159_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.section	.rodata,"a",@progbits
	.align	32
.LCPI159_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI159_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175,@function
par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175: # @par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1576, %rsp             # imm = 0x628
	vmovss	(%rdx), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	4(%rdx), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 76(%rsp)         # 4-byte Spill
	vmovss	16(%rdx), %xmm10        # xmm10 = mem[0],zero,zero,zero
	vmovss	20(%rdx), %xmm14        # xmm14 = mem[0],zero,zero,zero
	vmovss	24(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 72(%rsp)         # 4-byte Spill
	vmovss	28(%rdx), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vmovss	32(%rdx), %xmm7         # xmm7 = mem[0],zero,zero,zero
	movl	36(%rdx), %edi
	movl	48(%rdx), %eax
	movq	%rax, 1016(%rsp)        # 8-byte Spill
	movl	52(%rdx), %eax
	movl	%eax, 116(%rsp)         # 4-byte Spill
	movslq	56(%rdx), %rax
	movq	%rax, 152(%rsp)         # 8-byte Spill
	movl	60(%rdx), %eax
	movq	%rax, 64(%rsp)          # 8-byte Spill
	movl	64(%rdx), %eax
	movq	%rax, 144(%rsp)         # 8-byte Spill
	movl	68(%rdx), %eax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	movl	72(%rdx), %eax
	movq	%rax, 136(%rsp)         # 8-byte Spill
	movslq	76(%rdx), %rax
	movq	%rax, 960(%rsp)         # 8-byte Spill
	movl	80(%rdx), %eax
	movq	%rax, 128(%rsp)         # 8-byte Spill
	movl	84(%rdx), %eax
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	movl	88(%rdx), %eax
	movq	%rax, 96(%rsp)          # 8-byte Spill
	vmovss	100(%rdx), %xmm9        # xmm9 = mem[0],zero,zero,zero
	vmovss	104(%rdx), %xmm2        # xmm2 = mem[0],zero,zero,zero
	vmovss	108(%rdx), %xmm3        # xmm3 = mem[0],zero,zero,zero
	cmpl	%esi, 44(%rdx)
	movl	92(%rdx), %eax
	movq	%rax, 88(%rsp)          # 8-byte Spill
	movslq	96(%rdx), %rax
	movq	%rax, 80(%rsp)          # 8-byte Spill
	movq	112(%rdx), %rax
	movq	128(%rdx), %rcx
	movq	%rcx, 1528(%rsp)        # 8-byte Spill
	movq	144(%rdx), %r9
	movq	160(%rdx), %rcx
	movq	%rcx, 120(%rsp)         # 8-byte Spill
	jle	.LBB159_23
# BB#1:                                 # %true_bb
	movq	%r9, 992(%rsp)          # 8-byte Spill
	movq	%rax, 1568(%rsp)        # 8-byte Spill
	movq	%rsi, 1560(%rsp)        # 8-byte Spill
	addl	$51, %edi
	sarl	$3, %edi
	movq	%rdi, 1536(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB159_109
# BB#2:                                 # %for dh.s0.v10.v10.preheader
	movl	$2, %eax
	movq	96(%rsp), %rbp          # 8-byte Reload
	subl	%ebp, %eax
	movq	128(%rsp), %r14         # 8-byte Reload
	leal	(%r14,%r14), %r12d
	movl	%r12d, 1248(%rsp)       # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%r12d, %eax
	negl	%eax
	movl	%r14d, %ecx
	sarl	$31, %ecx
	andnl	%r12d, %ecx, %esi
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r14,%r14), %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%ebp, %edx
	leal	-1(%rbp,%r14), %ebx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, 1440(%rsp)        # 4-byte Spill
	leal	(%rbp,%r14), %r13d
	movq	%rbp, %rdi
	cmpl	$3, %r13d
	movl	$2, %eax
	cmovll	%ebx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %r13d
	cmovll	%edx, %eax
	movl	%eax, 1424(%rsp)        # 4-byte Spill
	movq	64(%rsp), %r11          # 8-byte Reload
	leal	(%r11,%r11), %r9d
	movl	%r9d, %eax
	negl	%eax
	movl	%r11d, %esi
	sarl	$31, %esi
	andnl	%r9d, %esi, %r10d
	andl	%eax, %esi
	movl	$2, %eax
	movq	104(%rsp), %r8          # 8-byte Reload
	subl	%r8d, %eax
	cltd
	idivl	%r9d
	orl	%r10d, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r11,%r11), %ebp
	movl	%ebp, %edx
	subl	%eax, %edx
	cmpl	%eax, %r11d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	leal	-1(%r8,%r11), %r15d
	cmpl	%edx, %r15d
	cmovlel	%r15d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 1376(%rsp)        # 4-byte Spill
	leal	(%r8,%r11), %r10d
	cmpl	$3, %r10d
	movl	$2, %eax
	cmovll	%r15d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %r10d
	cmovll	%edx, %eax
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	1472(%rsp), %r12d       # 4-byte Reload
	movl	%r12d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r14d
	cmovgl	%eax, %edx
	addl	%edi, %edx
	cmpl	%edx, %ebx
	cmovlel	%ebx, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	movl	%edx, 1352(%rsp)        # 4-byte Spill
	testl	%r13d, %r13d
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%r13d, %r13d
	cmovlel	%edx, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	negl	%eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%ebp, %edx
	subl	%eax, %edx
	cmpl	%eax, %r11d
	cmovgl	%eax, %edx
	addl	%r8d, %edx
	cmpl	%edx, %r15d
	cmovlel	%r15d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	testl	%r10d, %r10d
	movl	$0, %eax
	cmovlel	%r15d, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	testl	%r10d, %r10d
	cmovlel	%edx, %eax
	movl	%eax, 1296(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1248(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	subl	%eax, %r12d
	cmpl	%eax, %r14d
	cmovgl	%eax, %r12d
	addl	%edi, %r12d
	cmpl	%r12d, %ebx
	cmovlel	%ebx, %r12d
	cmpl	%edi, %r12d
	cmovll	%edi, %r12d
	movl	%r12d, 1472(%rsp)       # 4-byte Spill
	cmpl	$1, %r13d
	setg	%al
	cmpl	$2, %r13d
	movl	$0, %ecx
	cmovgel	%ecx, %ebx
	movzbl	%al, %eax
	orl	%eax, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	movq	%rdi, %r14
	cmpl	$2, %r13d
	movl	$0, %r13d
	cmovll	%r12d, %ebx
	movl	$1, %eax
	subl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	cmpl	%eax, %r11d
	cmovgl	%eax, %ebp
	addl	%r8d, %ebp
	cmpl	%ebp, %r15d
	cmovlel	%r15d, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	cmpl	$1, %r10d
	setg	%al
	cmpl	$2, %r10d
	cmovgel	%r13d, %r15d
	movzbl	%al, %eax
	orl	%eax, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	cmpl	$2, %r10d
	cmovll	%ebp, %r15d
	movq	1560(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	movq	88(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	movq	1456(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rsi), %ecx
	cltd
	idivl	%ecx
	movl	%esi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %r11d
	negl	%ecx
	andl	%eax, %ecx
	orl	%r11d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movq	152(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 984(%rsp)         # 8-byte Spill
	movl	%r10d, %r12d
	andl	$1, %r12d
	movl	%r12d, 352(%rsp)        # 4-byte Spill
	leal	-1(%rsi,%rsi), %ecx
	subl	%eax, %ecx
	cmpl	%eax, %esi
	cmovgl	%eax, %ecx
	leal	(%rdi,%rsi), %edx
	leal	-1(%rdi,%rsi), %esi
	addl	%edi, %ecx
	cmpl	%ecx, %esi
	cmovlel	%esi, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	cmpl	%r10d, %esi
	cmovgl	%r10d, %esi
	cmpl	%edi, %esi
	cmovll	%edi, %esi
	cmpl	%r10d, %edx
	cmovlel	%ecx, %esi
	cmpl	%r10d, %edi
	cmovgl	%ecx, %esi
	movq	136(%rsp), %r10         # 8-byte Reload
	movq	144(%rsp), %r11         # 8-byte Reload
	leal	6(%r10,%r11), %ecx
	vmovd	%ecx, %xmm11
	movq	80(%rsp), %r9           # 8-byte Reload
	imull	%r9d, %edi
	movq	%r14, %rdx
	addl	%edx, %edi
	movq	1016(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %eax
	sarl	$5, %eax
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	cmpl	$1, %r8d
	cmovgl	%ebp, %r15d
	vmovss	.LCPI159_0(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm4, %xmm0
	vmulss	%xmm6, %xmm0, %xmm1
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm12
	vsubss	%xmm6, %xmm10, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	960(%rsp), %rbp         # 8-byte Reload
	vmovd	%ebp, %xmm10
	imull	%r10d, %ebp
	addl	%r8d, %ebp
	vmovd	%ebp, %xmm0
	vmovd	%r15d, %xmm5
	vpsubd	%xmm0, %xmm5, %xmm15
	vsubss	%xmm13, %xmm4, %xmm0
	vmulss	%xmm7, %xmm0, %xmm6
	vdivss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm6, %xmm13, %xmm13
	leal	6(%r10), %ecx
	vmovd	%ecx, %xmm6
	vsubss	%xmm7, %xmm14, %xmm1
	leal	4(%r10,%r11), %ecx
	vmovd	%ecx, %xmm2
	vmulss	%xmm1, %xmm0, %xmm0
	leal	4(%r10), %ecx
	vmovd	%ecx, %xmm1
	vdivss	%xmm0, %xmm3, %xmm14
	vsubss	%xmm8, %xmm4, %xmm3
	vmovss	72(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm3, %xmm4
	vdivss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm8
	leal	8(%r10,%r11), %ecx
	vmovd	%ecx, %xmm5
	vmovss	76(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm0, %xmm7
	leal	8(%r10), %ecx
	vmovd	%ecx, %xmm0
	vmulss	%xmm7, %xmm3, %xmm3
	leal	5(%r10,%r11), %ecx
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm11, %xmm4
	vdivss	%xmm3, %xmm9, %xmm9
	vmovdqa	.LCPI159_1(%rip), %xmm3 # xmm3 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	%xmm4, 960(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	%xmm4, 944(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm2, 928(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	%xmm1, 912(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	%xmm1, 896(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 880(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 864(%rsp)        # 16-byte Spill
	leal	5(%r10), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 848(%rsp)        # 16-byte Spill
	leal	7(%r10,%r11), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 832(%rsp)        # 16-byte Spill
	leal	7(%r10), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 816(%rsp)        # 16-byte Spill
	leal	3(%r10,%r11), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 800(%rsp)        # 16-byte Spill
	leal	3(%r10), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 784(%rsp)        # 16-byte Spill
	movslq	%esi, %rsi
	imulq	%r9, %rsi
	movq	%rdx, %r9
	cmpl	$1, %r9d
	cmovgl	1472(%rsp), %ebx        # 4-byte Folded Reload
	movslq	%edi, %rcx
	movslq	%ebx, %rbx
	subq	%rcx, %rbx
	addq	%rsi, %rbx
	testl	%r8d, %r8d
	movl	1296(%rsp), %edx        # 4-byte Reload
	cmovgl	1280(%rsp), %edx        # 4-byte Folded Reload
	testl	%r9d, %r9d
	movl	1312(%rsp), %eax        # 4-byte Reload
	cmovgl	1352(%rsp), %eax        # 4-byte Folded Reload
	subq	%rcx, %rsi
	cmpl	$2, %r8d
	movl	1360(%rsp), %r8d        # 4-byte Reload
	cmovgl	1376(%rsp), %r8d        # 4-byte Folded Reload
	cmpl	$2, %r9d
	movl	1424(%rsp), %edi        # 4-byte Reload
	cmovgl	1440(%rsp), %edi        # 4-byte Folded Reload
	leal	(%r11,%r11), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm1
	vmovdqa	%xmm1, 768(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 752(%rsp)        # 16-byte Spill
	vmovd	%r10d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 736(%rsp)        # 16-byte Spill
	leal	-1(%r10,%r11), %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 720(%rsp)        # 16-byte Spill
	vmovd	%ebp, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmovd	%r15d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 704(%rsp)        # 16-byte Spill
	vbroadcastss	1248(%rsp), %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 688(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm0
	vmovdqa	%xmm0, 1424(%rsp)       # 16-byte Spill
	movslq	%eax, %rcx
	movslq	%edi, %rdi
	leaq	(%rcx,%rsi), %r9
	leaq	(%rdi,%rsi), %rsi
	vmovd	%edx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	vmovd	%r8d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1456(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rdi
	shlq	$5, %rdi
	addq	$48, %rdi
	movq	1560(%rsp), %rbp        # 8-byte Reload
	movl	%ebp, %edx
	andl	$63, %edx
	imulq	%rdi, %rdx
	movq	152(%rsp), %rax         # 8-byte Reload
	movq	%rax, %rdi
	sarq	$63, %rdi
	andq	%rax, %rdi
	leal	8(%rbp), %ebp
	subl	116(%rsp), %ebp         # 4-byte Folded Reload
	andl	$-32, %r14d
	addl	$64, %r14d
	imull	%ebp, %r14d
	movq	%r14, 1016(%rsp)        # 8-byte Spill
	subq	%rdi, %rdx
	movq	%rdx, 656(%rsp)         # 8-byte Spill
	leal	(%rcx,%rcx,2), %edx
	movl	%edx, %edi
	shll	$10, %edi
	leal	(%rdi,%r14), %edi
	movq	%rdi, 640(%rsp)         # 8-byte Spill
	shll	$9, %edx
	leal	(%rdx,%r14), %edx
	movq	%rdx, 624(%rsp)         # 8-byte Spill
	movq	984(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %edx
	subl	%r10d, %edx
	leal	-3(%rdx), %eax
	movq	%rax, 592(%rsp)         # 8-byte Spill
	leal	-7(%rdx), %eax
	movq	%rax, 576(%rsp)         # 8-byte Spill
	leal	-8(%rdx), %eax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %eax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	leal	-6(%rdx), %eax
	movq	%rax, 528(%rsp)         # 8-byte Spill
	addl	$-5, %edx
	movq	%rdx, 608(%rsp)         # 8-byte Spill
	movq	%rdi, %rdx
	leal	-3(%rdx), %eax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	leal	-7(%rdx), %eax
	movq	%rax, 496(%rsp)         # 8-byte Spill
	leal	-5(%rdx), %eax
	movq	%rax, 488(%rsp)         # 8-byte Spill
	leal	-8(%rdx), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	leal	-6(%rdx), %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rdx,%rbx,4), %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdx,%r9,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdx,%rsi,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_3(%rip), %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_4(%rip), %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_5(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_6(%rip), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB159_3:                              # %for dh.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	testl	%r12d, %r12d
	setne	1312(%rsp)              # 1-byte Folded Spill
	sete	1352(%rsp)              # 1-byte Folded Spill
	movq	984(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %ebp
	movl	%ebp, %eax
	andl	$1, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	sete	1296(%rsp)              # 1-byte Folded Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI159_2(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %ecx
	cltd
	idivl	%ecx
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, 1184(%rsp)        # 4-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %esi
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	vmovd	%r15d, %xmm1
	movl	%ebp, %r15d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	vpinsrd	$1, %r14d, %xmm1, %xmm0
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	vpinsrd	$3, %esi, %xmm0, %xmm0
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r12d
	movl	%edx, %esi
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vmovd	%ebx, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	vpinsrd	$1, %r8d, %xmm3, %xmm3
	vpinsrd	$2, %r9d, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%ecx, %r8d
	movl	%edx, %edi
	vpinsrd	$3, %ebp, %xmm3, %xmm3
	vmovd	%ebx, %xmm4
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %esi, %xmm4, %xmm4
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	400(%rsp), %xmm13       # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	960(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	944(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm6, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vmovdqa	736(%rsp), %xmm9        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm7
	vmovdqa	384(%rsp), %xmm15       # 16-byte Reload
	vpsubd	%xmm0, %xmm15, %xmm6
	vblendvps	%xmm7, %xmm0, %xmm6, %xmm0
	vmovdqa	336(%rsp), %xmm12       # 16-byte Reload
	vpaddd	%xmm12, %xmm0, %xmm0
	vmovdqa	720(%rsp), %xmm14       # 16-byte Reload
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm0, %xmm2, %xmm0
	vmovdqa	752(%rsp), %xmm11       # 16-byte Reload
	vpmulld	%xmm11, %xmm0, %xmm2
	vpsrad	$31, %xmm3, %xmm0
	vpand	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm3
	vpinsrd	$2, %edi, %xmm4, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	928(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	912(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm11, %xmm3, %xmm4
	vmovdqa	%xmm4, 1280(%rsp)       # 16-byte Spill
	vpsubd	1376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm2, 1024(%rsp)       # 16-byte Spill
	vpaddd	704(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	movq	992(%rsp), %rbx         # 8-byte Reload
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vmovdqa	1424(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm8
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rax
	vmovss	(%rbx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rax
	sarq	$32, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovdqa	896(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	880(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm9, %xmm4
	vpsubd	%xmm0, %xmm15, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm0, 1152(%rsp)       # 16-byte Spill
	vinsertps	$48, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1],mem[0],xmm0[3]
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	1568(%rsp), %r14        # 8-byte Reload
	vmovups	12296(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovups	12312(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vmovups	12304(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmovups	12320(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	sarq	$32, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovups	12288(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	1216(%rsp), %xmm7       # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovd	%xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	vpinsrd	$1, 1248(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$2, 1200(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$3, 1184(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpsrad	$31, %xmm7, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovd	%edi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %esi, %xmm7, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm7
	vpand	%xmm13, %xmm7, %xmm7
	vpaddd	%xmm3, %xmm7, %xmm3
	vpcmpgtd	%xmm6, %xmm9, %xmm7
	vpsubd	%xmm6, %xmm15, %xmm4
	vblendvps	%xmm7, %xmm6, %xmm4, %xmm4
	vmovdqa	864(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	848(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	488(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vpmulld	%xmm11, %xmm4, %xmm2
	vmovdqa	%xmm2, 1200(%rsp)       # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm4
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	vpcmpgtd	%xmm3, %xmm9, %xmm4
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovdqa	832(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	816(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	movq	496(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r13), %edx
	vmovd	%edx, %xmm6
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovss	(%rbx,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm7 # xmm7 = xmm4[0,1,2],mem[0]
	vpmulld	%xmm11, %xmm3, %xmm2
	vmovdqa	%xmm2, 1184(%rsp)       # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm3
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%rbx,%rdx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%r10d
	movl	%edx, %esi
	vpextrd	$2, %xmm4, %eax
	vpextrd	$3, %xmm4, %edi
	cltd
	idivl	%r8d
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edx, %xmm4, %xmm4
	movl	%edi, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm4, %xmm9, %xmm6
	vpsubd	%xmm4, %xmm15, %xmm9
	vblendvps	%xmm6, %xmm4, %xmm9, %xmm4
	vmovdqa	800(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	784(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm1, %xmm4, %xmm6, %xmm9
	vmovaps	416(%rsp), %xmm14       # 16-byte Reload
	vmulps	1168(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm0, %xmm6 # xmm6 = xmm0[0,2],xmm3[0,2]
	vmovaps	672(%rsp), %xmm13       # 16-byte Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmovaps	688(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm2, %xmm4
	vmulps	1136(%rsp), %xmm14, %xmm5 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1120(%rsp), %xmm8       # 16-byte Reload
	vshufps	$136, %xmm15, %xmm8, %xmm2 # xmm2 = xmm8[0,2],xmm15[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	1072(%rsp), %xmm14, %xmm10 # 16-byte Folded Reload
	vmovaps	1104(%rsp), %xmm12      # 16-byte Reload
	vshufps	$136, %xmm8, %xmm12, %xmm5 # xmm5 = xmm12[0,2],xmm8[0,2]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmovaps	368(%rsp), %xmm10       # 16-byte Reload
	vminps	%xmm10, %xmm4, %xmm6
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm6, %xmm6
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vminps	%xmm10, %xmm5, %xmm5
	vmaxps	%xmm4, %xmm5, %xmm5
	vsubps	%xmm6, %xmm5, %xmm5
	vshufps	$221, %xmm3, %xmm0, %xmm6 # xmm6 = xmm0[1,3],xmm3[1,3]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm13, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	1248(%rsp), %xmm14, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm12, %xmm7 # xmm7 = xmm12[1,3],xmm8[1,3]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm3, %xmm7, %xmm3
	vpmulld	%xmm11, %xmm9, %xmm0
	vmovdqa	%xmm0, 1168(%rsp)       # 16-byte Spill
	vpaddd	1424(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	vshufps	$221, %xmm15, %xmm8, %xmm8 # xmm8 = xmm8[1,3],xmm15[1,3]
	vpxor	%xmm11, %xmm11, %xmm11
	vmovss	(%rbx,%rdx,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	sarq	$32, %rax
	vinsertps	$48, (%rbx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movl	352(%rsp), %r12d        # 4-byte Reload
	movl	%r12d, %eax
	andl	%r15d, %eax
	vmovaps	1440(%rsp), %xmm7       # 16-byte Reload
	vandps	%xmm7, %xmm2, %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vandps	%xmm7, %xmm5, %xmm2
	vaddps	%xmm2, %xmm0, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vminps	%xmm10, %xmm6, %xmm5
	vmaxps	%xmm11, %xmm5, %xmm5
	vmulps	%xmm4, %xmm14, %xmm4
	vsubps	%xmm13, %xmm8, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm6
	vsubps	%xmm3, %xmm5, %xmm8
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm7, %xmm3, %xmm0
	vminps	%xmm10, %xmm6, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm7, %xmm3, %xmm5
	vmulps	1472(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	%ymm1, 1248(%rsp)       # 32-byte Spill
	vmovaps	%xmm1, %xmm13
	jne	.LBB159_5
# BB#4:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB159_5:                              # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	movq	1016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rcx
	vmovups	(%r14,%rax,4), %xmm6
	movq	%rax, %rdx
	vmovups	32(%r14,%rax,4), %xmm12
	orq	$2, %rax
	vmovups	(%r14,%rax,4), %xmm4
	vandps	%xmm7, %xmm8, %xmm2
	vaddps	%xmm5, %xmm0, %xmm9
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm5
	vmovdqa	1376(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	1152(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm3
	orq	$4, %rdx
	vmovups	(%r14,%rdx,4), %xmm15
	vmovdqa	1280(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm8
	movb	1312(%rsp), %al         # 1-byte Reload
	andb	1296(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%xmm7, %xmm14
	movb	%al, %dil
	vmovdqa	%xmm0, %xmm11
	jne	.LBB159_6
# BB#7:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm13, %xmm14
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovdqa	%xmm3, 1312(%rsp)       # 16-byte Spill
	vmovdqa	%xmm8, 1280(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	jmp	.LBB159_8
	.align	16, 0x90
.LBB159_6:                              #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vmovdqa	320(%rsp), %xmm2        # 16-byte Reload
	vmovaps	%xmm15, %xmm13
	vmovaps	%xmm6, %xmm15
	vmovaps	%xmm15, 1120(%rsp)      # 16-byte Spill
	vpaddd	1024(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	224(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm5, %xmm4, %xmm7 # xmm7 = xmm4[0,2],xmm5[0,2]
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1152(%rsp)       # 16-byte Spill
	vmovaps	288(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm7, %xmm7
	vmovdqa	%xmm8, 1280(%rsp)       # 16-byte Spill
	vmovaps	304(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm7
	vpaddd	%xmm2, %xmm3, %xmm6
	vmovdqa	%xmm3, 1312(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm13, %xmm15, %xmm3 # xmm3 = xmm15[0,2],xmm13[0,2]
	vmovaps	%xmm13, %xmm15
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm2, %xmm8, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm12, %xmm15, %xmm2 # xmm2 = xmm15[0,2],xmm12[0,2]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm3, %xmm3
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vaddps	%xmm2, %xmm3, %xmm2
	vminps	%xmm10, %xmm7, %xmm3
	vmaxps	%xmm0, %xmm3, %xmm3
	vmovaps	1360(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm3
	vandps	%xmm14, %xmm3, %xmm2
	vaddps	1136(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm14
.LBB159_8:                              # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovdqa	1200(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	1184(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	1168(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm12, %xmm8
	vmulps	%xmm7, %xmm9, %xmm0
	vmovaps	%xmm0, %xmm13
	testb	%dil, %dil
	jne	.LBB159_10
# BB#9:                                 # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB159_10:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vpsubd	%xmm11, %xmm1, %xmm5
	vpsubd	%xmm11, %xmm2, %xmm1
	vpsubd	%xmm11, %xmm3, %xmm4
	movl	%r12d, %eax
	andl	%r15d, %eax
	jne	.LBB159_11
# BB#12:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovdqa	%xmm5, 1184(%rsp)       # 16-byte Spill
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
	jmp	.LBB159_13
	.align	16, 0x90
.LBB159_11:                             #   in Loop: Header=BB159_3 Depth=1
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
	vmovdqa	320(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm5, %xmm2
	vmovdqa	%xmm5, 1184(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rsi
	vpaddd	%xmm3, %xmm1, %xmm2
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	vpaddd	%xmm3, %xmm4, %xmm2
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %r9
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1104(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovss	(%rbx,%r8,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rbx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	224(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	288(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	304(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vmovss	(%rbx,%r9,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm0, %xmm4 # xmm4 = xmm0[1,3],xmm15[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm8, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm8[1,3]
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm3, %xmm3
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm2
	vandps	1440(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vaddps	1216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm7, %xmm0, %xmm13
.LBB159_13:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm7, %xmm9
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24584(%r14,%rax,4), %xmm1
	vmovups	24600(%r14,%rax,4), %xmm8
	vmovups	24576(%r14,%rax,4), %xmm5
	vmovups	24592(%r14,%rax,4), %xmm15
	vmovups	24608(%r14,%rax,4), %xmm12
	movb	1352(%rsp), %r14b       # 1-byte Reload
	movb	%r14b, %al
	movl	1456(%rsp), %ecx        # 4-byte Reload
	andb	%cl, %al
	jne	.LBB159_14
# BB#15:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1168(%rsp)       # 16-byte Spill
	movq	1560(%rsp), %rcx        # 8-byte Reload
	vmovaps	%xmm9, %xmm7
	jmp	.LBB159_16
	.align	16, 0x90
.LBB159_14:                             #   in Loop: Header=BB159_3 Depth=1
	vmovdqa	272(%rsp), %xmm7        # 16-byte Reload
	vpaddd	1024(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vshufps	$136, %xmm8, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm8[0,2]
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	240(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm11
	vpaddd	1312(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vmovaps	%xmm5, 1168(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm15[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vpaddd	1280(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm6    # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vshufps	$136, %xmm12, %xmm15, %xmm7 # xmm7 = xmm15[0,2],xmm12[0,2]
	vsubps	%xmm2, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm10, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vminps	%xmm10, %xmm6, %xmm6
	vmaxps	%xmm1, %xmm6, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vminps	%xmm10, %xmm11, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm5, %xmm1, %xmm0
	vandps	1440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1136(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm9, %xmm7
	vmulps	%xmm7, %xmm0, %xmm14
	movq	1560(%rsp), %rcx        # 8-byte Reload
.LBB159_16:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovups	1248(%rsp), %ymm9       # 32-byte Reload
	movl	%r15d, %eax
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB159_18
# BB#17:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm14, %xmm9
.LBB159_18:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	testl	%eax, %eax
	jne	.LBB159_20
# BB#19:                                #   in Loop: Header=BB159_3 Depth=1
	vmovdqa	272(%rsp), %xmm5        # 16-byte Reload
	vpaddd	1184(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	1296(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	1200(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %r9
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm8[1,3]
	vmovss	(%rbx,%r8,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	208(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	240(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%rbx,%r9,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	1168(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm15[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm12, %xmm15, %xmm3 # xmm3 = xmm15[1,3],xmm12[1,3]
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	1440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm7, %xmm0, %xmm13
.LBB159_20:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	movq	1536(%rsp), %rdx        # 8-byte Reload
	vmovups	1472(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%xmm7, 1472(%rsp)       # 16-byte Spill
	movl	1456(%rsp), %eax        # 4-byte Reload
	andb	%al, %r14b
	jne	.LBB159_22
# BB#21:                                # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	%xmm13, %xmm1
.LBB159_22:                             # %for dh.s0.v10.v10
                                        #   in Loop: Header=BB159_3 Depth=1
	vmovaps	.LCPI159_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI159_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	656(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1528(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, %edx
	movq	%rdx, 1536(%rsp)        # 8-byte Spill
	jne	.LBB159_3
	jmp	.LBB159_109
.LBB159_23:                             # %false_bb
	movq	%rax, 1568(%rsp)        # 8-byte Spill
	cmpl	%esi, 40(%rdx)
	movq	%rsi, 1560(%rsp)        # 8-byte Spill
	jle	.LBB159_30
# BB#24:                                # %true_bb1
	vmovss	%xmm2, 24(%rsp)         # 4-byte Spill
	vmovss	%xmm6, 28(%rsp)         # 4-byte Spill
	vmovss	%xmm10, 32(%rsp)        # 4-byte Spill
	vmovss	%xmm5, 36(%rsp)         # 4-byte Spill
	vmovss	%xmm3, 40(%rsp)         # 4-byte Spill
	vmovss	%xmm7, 44(%rsp)         # 4-byte Spill
	vmovss	%xmm13, 48(%rsp)        # 4-byte Spill
	vmovss	%xmm14, 52(%rsp)        # 4-byte Spill
	vmovss	%xmm8, 56(%rsp)         # 4-byte Spill
	vmovss	%xmm9, 60(%rsp)         # 4-byte Spill
	movq	152(%rsp), %rax         # 8-byte Reload
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	movq	%rsi, 944(%rsp)         # 8-byte Spill
	movl	$15, %eax
	subl	%esi, %eax
	movq	136(%rsp), %rcx         # 8-byte Reload
	addl	%ecx, %eax
	sarl	$3, %eax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	movl	$0, %r8d
	cmovnsl	%eax, %r8d
	leal	51(%rdi), %eax
	sarl	$3, %eax
	movl	%eax, 984(%rsp)         # 4-byte Spill
	cmpl	%r8d, %eax
	cmovlel	%eax, %r8d
	movl	%r8d, -112(%rsp)        # 4-byte Spill
	movq	144(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %eax
	subl	%esi, %eax
	leal	(%rcx,%rax), %ebp
	leal	(%rcx,%rdx), %edx
	movq	%rdx, 8(%rsp)           # 8-byte Spill
	subl	%esi, %edx
	cmpl	%ebp, %edx
	movl	%edx, %esi
	cmovgl	%ebp, %esi
	sarl	$3, %esi
	movq	%rsi, -16(%rsp)         # 8-byte Spill
	leal	-1(%rsi), %ebx
	sarl	$3, %ebp
	movl	%ebp, -4(%rsp)          # 4-byte Spill
	cmpl	%ebx, %ebp
	cmovlel	%ebp, %ebx
	leal	-10(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -20(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-9(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -24(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-7(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -28(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-4(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -32(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-3(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -36(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-2(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -40(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	-1(%rcx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -44(%rsp)         # 4-byte Spill
	cmpl	%ebx, %esi
	cmovlel	%esi, %ebx
	leal	1(%rcx,%rax), %eax
	sarl	$3, %eax
	movl	%eax, -48(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-10(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -52(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-9(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -56(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-7(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -60(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-4(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -64(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-3(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -68(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-2(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -72(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-1(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -76(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	1(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -80(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	2(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -84(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	3(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -88(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	4(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -92(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	5(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -96(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	6(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -100(%rsp)        # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	7(%rdx), %eax
	sarl	$3, %eax
	movl	%eax, -104(%rsp)        # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	sarl	$3, %edx
	movq	%rdx, (%rsp)            # 8-byte Spill
	cmpl	%ebx, %edx
	cmovlel	%edx, %ebx
	addl	$43, %edi
	sarl	$3, %edi
	movq	%rdi, 1536(%rsp)        # 8-byte Spill
	cmpl	%ebx, %edi
	cmovlel	%edi, %ebx
	addl	$1, %ebx
	cmpl	%ebx, %r8d
	cmovgel	%r8d, %ebx
	movl	%ebx, -108(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	jle	.LBB159_51
# BB#25:                                # %for dh.s0.v10.v104.preheader
	movq	128(%rsp), %r13         # 8-byte Reload
	leal	(%r13,%r13), %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r13d, %r10d
	sarl	$31, %r10d
	andnl	%edx, %r10d, %ecx
	movl	%edx, %esi
	andl	%eax, %r10d
	orl	%ecx, %r10d
	movq	96(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%r13), %ebp
	movl	%ebp, 1440(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%edi, %eax
	cltd
	idivl	%esi
	leal	-1(%r13,%r13), %r14d
	leal	-1(%rdi,%r13), %ebx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%r14d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r13d
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 1376(%rsp)        # 4-byte Spill
	cmpl	$3, %ebp
	movl	$2, %eax
	cmovll	%ebx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %ebp
	movq	64(%rsp), %r12          # 8-byte Reload
	leal	-1(%r12,%r12), %ebp
	movq	104(%rsp), %rsi         # 8-byte Reload
	leal	-1(%rsi,%r12), %r8d
	movl	%r8d, 1424(%rsp)        # 4-byte Spill
	cmovll	%ecx, %eax
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	leal	(%r12,%r12), %ecx
	movl	%ecx, 1456(%rsp)        # 4-byte Spill
	movl	%ecx, %edx
	negl	%edx
	movl	%r12d, %r11d
	sarl	$31, %r11d
	andnl	%ecx, %r11d, %r15d
	andl	%edx, %r11d
	orl	%r15d, %r11d
	movl	$2, %eax
	subl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movl	%ebp, %edx
	subl	%eax, %edx
	cmpl	%eax, %r12d
	cmovgl	%eax, %edx
	leal	(%rsi,%r12), %ecx
	addl	%esi, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	movl	%edx, 1352(%rsp)        # 4-byte Spill
	cmpl	$3, %ecx
	movl	$2, %eax
	cmovll	%r8d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %ecx
	cmovll	%edx, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	1472(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%r14d, %r15d
	subl	%eax, %r15d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r15d
	addl	%edi, %r15d
	cmpl	%r15d, %ebx
	cmovlel	%ebx, %r15d
	cmpl	%edi, %r15d
	cmovll	%edi, %r15d
	movl	1440(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%edx, %edx
	cmovlel	%r15d, %eax
	movl	%eax, 1296(%rsp)        # 4-byte Spill
	movl	%esi, %eax
	negl	%eax
	cltd
	idivl	1456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movl	%ebp, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r12d
	cmovgl	%eax, %r8d
	addl	%esi, %r8d
	movl	1424(%rsp), %edx        # 4-byte Reload
	cmpl	%r8d, %edx
	cmovlel	%edx, %r8d
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	testl	%ecx, %ecx
	movl	$0, %eax
	cmovlel	%edx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%ecx, %ecx
	cmovlel	%r8d, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1472(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	subl	%eax, %r14d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r14d
	addl	%edi, %r14d
	cmpl	%r14d, %ebx
	cmovlel	%ebx, %r14d
	cmpl	%edi, %r14d
	cmovll	%edi, %r14d
	movl	1440(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%edx, %r10d
	movl	$0, %edx
	cmovgel	%edx, %ebx
	movzbl	%al, %eax
	orl	%eax, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	$2, %r10d
	cmovll	%r14d, %ebx
	movl	$1, %eax
	subl	%esi, %eax
	cltd
	idivl	1456(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	subl	%eax, %ebp
	cmpl	%eax, %r12d
	cmovgl	%eax, %ebp
	addl	%esi, %ebp
	movl	1424(%rsp), %edx        # 4-byte Reload
	cmpl	%ebp, %edx
	cmovlel	%edx, %ebp
	cmpl	%esi, %ebp
	cmovll	%esi, %ebp
	cmpl	$1, %ecx
	setg	%al
	cmpl	$2, %ecx
	movl	%ecx, %r10d
	movzbl	%al, %eax
	movl	$0, %ecx
	cmovgel	%ecx, %edx
	orl	%eax, %edx
	cmpl	%esi, %edx
	cmovll	%esi, %edx
	cmpl	$2, %r10d
	cmovll	%ebp, %edx
	movl	%edx, %r13d
	movq	1560(%rsp), %rax        # 8-byte Reload
	movl	%eax, %r12d
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm0
	movq	80(%rsp), %r11          # 8-byte Reload
	movl	%r11d, %ecx
	movq	88(%rsp), %rax          # 8-byte Reload
	imull	%eax, %ecx
	andl	$1, %r12d
	movl	%r12d, 304(%rsp)        # 4-byte Spill
	addl	%edi, %ecx
	movq	1016(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	sarl	$5, %eax
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	cmpl	$1, %esi
	cmovgl	%ebp, %r13d
	movq	960(%rsp), %rax         # 8-byte Reload
	movq	136(%rsp), %r10         # 8-byte Reload
	imull	%r10d, %eax
	movq	8(%rsp), %rdx           # 8-byte Reload
	leal	6(%rdx), %ebp
	vmovd	%ebp, %xmm6
	leal	6(%r10), %ebp
	vmovd	%ebp, %xmm9
	addl	%esi, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movq	1560(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rbp
	imulq	%r11, %rbp
	cmpl	$1, %edi
	cmovgl	%r14d, %ebx
	movslq	%ecx, %rcx
	subq	%rcx, %rbp
	leal	4(%rdx), %ecx
	vmovd	%ecx, %xmm11
	leal	4(%r10), %ecx
	vmovd	%ecx, %xmm12
	leal	8(%rdx), %ecx
	vmovd	%ecx, %xmm13
	leal	8(%r10), %ecx
	vmovd	%ecx, %xmm14
	leal	5(%rdx), %ecx
	vmovd	%ecx, %xmm15
	leal	5(%r10), %ecx
	vmovd	%ecx, %xmm8
	leal	7(%rdx), %ecx
	vmovd	%ecx, %xmm1
	leal	7(%r10), %ecx
	vmovd	%ecx, %xmm2
	movslq	%ebx, %rcx
	leaq	(%rcx,%rbp), %rax
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	movl	1280(%rsp), %r14d       # 4-byte Reload
	cmovgl	%r8d, %r14d
	testl	%edi, %edi
	movl	1296(%rsp), %eax        # 4-byte Reload
	cmovgl	%r15d, %eax
	movslq	%eax, %rcx
	cmpl	$2, %esi
	movl	1312(%rsp), %r11d       # 4-byte Reload
	cmovgl	1352(%rsp), %r11d       # 4-byte Folded Reload
	cmpl	$2, %edi
	movl	1360(%rsp), %eax        # 4-byte Reload
	cmovgl	1376(%rsp), %eax        # 4-byte Folded Reload
	movslq	%eax, %rdi
	leaq	(%rcx,%rbp), %rax
	movq	%rax, 1360(%rsp)        # 8-byte Spill
	leaq	(%rdi,%rbp), %rax
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	leal	3(%rdx), %ebp
	vmovd	%ebp, %xmm3
	leal	3(%r10), %ebp
	vmovd	%ebp, %xmm4
	movq	1424(%rsp), %rsi        # 8-byte Reload
	movslq	%esi, %rbp
	shlq	$5, %rbp
	addq	$48, %rbp
	movq	1560(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	andl	$63, %edx
	imulq	%rbp, %rdx
	movq	152(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rbp
	sarq	$63, %rbp
	leal	8(%rax), %ebx
	xorl	%r15d, %r15d
	subl	116(%rsp), %ebx         # 4-byte Folded Reload
	movq	1016(%rsp), %rdi        # 8-byte Reload
	andl	$-32, %edi
	addl	$64, %edi
	imull	%ebx, %edi
	movq	%rdi, 912(%rsp)         # 8-byte Spill
	andq	%rcx, %rbp
	subq	%rbp, %rdx
	movq	%rdx, 928(%rsp)         # 8-byte Spill
	leal	(%rsi,%rsi,2), %ebp
	movl	%ebp, %r8d
	shll	$10, %r8d
	shll	$9, %ebp
	movq	944(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %ecx
	subl	%r10d, %ecx
	leal	-3(%rcx), %ebx
	movq	%rbx, 880(%rsp)         # 8-byte Spill
	leal	-7(%rcx), %ebx
	movq	%rbx, 864(%rsp)         # 8-byte Spill
	leal	-8(%rcx), %ebx
	movq	%rbx, 848(%rsp)         # 8-byte Spill
	leal	-4(%rcx), %ebx
	movq	%rbx, 832(%rsp)         # 8-byte Spill
	leal	-6(%rcx), %ebx
	movq	%rbx, 816(%rsp)         # 8-byte Spill
	addl	$-5, %ecx
	movq	%rcx, 896(%rsp)         # 8-byte Spill
	movl	984(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	movq	16(%rsp), %rcx          # 8-byte Reload
	testl	%ecx, %ecx
	movl	$0, %eax
	cmovnsl	%ecx, %eax
	notl	%eax
	cmpl	%eax, %ebx
	cmovgel	%ebx, %eax
	vpbroadcastd	%xmm0, %xmm10
	vmovdqa	%xmm10, 800(%rsp)       # 16-byte Spill
	vmovss	.LCPI159_0(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm6, %xmm6
	movl	1456(%rsp), %edx        # 4-byte Reload
	vmovd	%edx, %xmm7
	vmovd	%r13d, %xmm0
	vpsubd	%xmm7, %xmm0, %xmm0
	vmovdqa	.LCPI159_1(%rip), %xmm7 # xmm7 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 784(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 768(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm11, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 752(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 736(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 720(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 704(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 688(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqa	%xmm6, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm1, 656(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm1, 640(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm1, 624(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovss	36(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm5, %xmm1
	vmovss	32(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm2, %xmm2
	vmulss	%xmm2, %xmm1, %xmm2
	vmulss	%xmm4, %xmm1, %xmm1
	vmovss	24(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vdivss	%xmm2, %xmm4, %xmm2
	movq	960(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 592(%rsp)        # 16-byte Spill
	vmovd	%r10d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 288(%rsp)        # 16-byte Spill
	movq	144(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 560(%rsp)        # 16-byte Spill
	movq	8(%rsp), %rcx           # 8-byte Reload
	leal	-1(%rcx), %ebx
	vmovd	%ebx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 576(%rsp)        # 16-byte Spill
	vmovd	%edx, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 1376(%rsp)       # 16-byte Spill
	vmovd	%r13d, %xmm3
	vbroadcastss	%xmm3, %xmm3
	vmovaps	%xmm3, 544(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm2
	vmovaps	%xmm2, 528(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 512(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 1424(%rsp)       # 16-byte Spill
	vmovss	48(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm5, %xmm0
	vmovss	52(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	44(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm3, %xmm0, %xmm0
	vmovss	40(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vdivss	%xmm1, %xmm3, %xmm1
	vmovss	56(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm5, %xmm2
	vmovss	76(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmovss	72(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm3, %xmm2, %xmm3
	vmulss	%xmm4, %xmm2, %xmm2
	vmovss	60(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vdivss	%xmm3, %xmm4, %xmm3
	vmovd	%r14d, %xmm5
	vpbroadcastd	%xmm5, %xmm4
	vmovdqa	%xmm4, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	leal	(%r8,%rdi), %ecx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	leal	(%rbp,%rdi), %ecx
	movq	%rcx, 488(%rsp)         # 8-byte Spill
	movl	%eax, %edx
	notl	%edx
	leal	-3(%rsi), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	leal	-7(%rsi), %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	leal	-5(%rsi), %eax
	movq	%rax, 432(%rsp)         # 8-byte Spill
	leal	-8(%rsi), %eax
	movq	%rax, 416(%rsp)         # 8-byte Spill
	leal	-4(%rsi), %eax
	movq	%rax, 400(%rsp)         # 8-byte Spill
	leal	-6(%rsi), %eax
	movq	%rax, 384(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	movq	1472(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	movq	1360(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	movq	1440(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	vpabsd	%xmm10, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm10, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_3(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_4(%rip), %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_6(%rip), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB159_26:                             # %for dh.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movl	%edx, 1024(%rsp)        # 4-byte Spill
	testl	%r12d, %r12d
	setne	1312(%rsp)              # 1-byte Folded Spill
	sete	1352(%rsp)              # 1-byte Folded Spill
	movq	944(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movl	%eax, 1040(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 1440(%rsp)        # 4-byte Spill
	sete	1296(%rsp)              # 1-byte Folded Spill
	movq	896(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI159_2(%rip), %xmm12 # xmm12 = [0,2,4,6]
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	800(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %r10d
	cltd
	idivl	%r10d
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, 1184(%rsp)        # 4-byte Spill
	movq	816(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movq	%r15, %rdi
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebx
	vmovd	%r15d, %xmm1
	movq	%rdi, %r15
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpinsrd	$1, %r14d, %xmm1, %xmm0
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	vpinsrd	$3, %ebp, %xmm0, %xmm0
	movq	848(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm12, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	movq	384(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vmovd	%esi, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %ebx, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vpinsrd	$3, %edi, %xmm3, %xmm3
	vmovd	%esi, %xmm5
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ebp, %xmm5, %xmm5
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	352(%rsp), %xmm10       # 16-byte Reload
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movl	1040(%rsp), %ebx        # 4-byte Reload
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	784(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm6
	vpcmpeqd	%xmm4, %xmm4, %xmm4
	vpxor	%xmm4, %xmm6, %xmm6
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	768(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vmovdqa	560(%rsp), %xmm14       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm7
	vmovdqa	336(%rsp), %xmm8        # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm4
	vblendvps	%xmm7, %xmm0, %xmm4, %xmm0
	vmovdqa	288(%rsp), %xmm13       # 16-byte Reload
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	576(%rsp), %xmm11       # 16-byte Reload
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm12, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm6, %xmm0, %xmm2, %xmm0
	vmovdqa	592(%rsp), %xmm15       # 16-byte Reload
	vpmulld	%xmm15, %xmm0, %xmm2
	vpsrad	$31, %xmm3, %xmm0
	vpand	%xmm10, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm3
	vpinsrd	$2, %ecx, %xmm5, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	752(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	736(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm14, %xmm5
	vpsubd	%xmm3, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	movq	400(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm15, %xmm3, %xmm4
	vmovdqa	%xmm4, 1280(%rsp)       # 16-byte Spill
	vpsubd	1376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm2, 992(%rsp)        # 16-byte Spill
	vpaddd	544(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r9,%rcx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vmovdqa	1424(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm9
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rax
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rax
	sarq	$32, %rcx
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovdqa	720(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	704(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm14, %xmm4
	vpsubd	%xmm0, %xmm8, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	movq	416(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r15), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm15, %xmm0, %xmm0
	vmovdqa	%xmm0, 1152(%rsp)       # 16-byte Spill
	vinsertps	$48, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm9, %xmm0
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%r9,%rcx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	488(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	1568(%rsp), %r14        # 8-byte Reload
	vmovups	12296(%r14,%rax,4), %xmm6
	vmovups	12312(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 1056(%rsp)       # 16-byte Spill
	vmovups	12304(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 1088(%rsp)       # 16-byte Spill
	vmovups	12320(%r14,%rax,4), %xmm2
	vmovaps	%xmm2, 1072(%rsp)       # 16-byte Spill
	sarq	$32, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovups	12288(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	movq	864(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm12, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	1216(%rsp), %xmm7       # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovd	%xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$1, 1248(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$2, 1200(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %edi
	vpinsrd	$3, 1184(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpsrad	$31, %xmm7, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovd	%esi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm7, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm7
	vpand	%xmm10, %xmm7, %xmm7
	vpaddd	%xmm3, %xmm7, %xmm3
	vpcmpgtd	%xmm4, %xmm14, %xmm7
	vpsubd	%xmm4, %xmm8, %xmm2
	vblendvps	%xmm7, %xmm4, %xmm2, %xmm2
	vmovdqa	688(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	672(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm12, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm4, %xmm2, %xmm7, %xmm2
	vpmulld	%xmm15, %xmm2, %xmm0
	vmovdqa	%xmm0, 1200(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm9, %xmm2
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	vpcmpgtd	%xmm3, %xmm14, %xmm2
	vpsubd	%xmm3, %xmm8, %xmm4
	vblendvps	%xmm2, %xmm3, %xmm4, %xmm2
	vmovdqa	656(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm3
	vpxor	%xmm5, %xmm3, %xmm3
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	640(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	movq	448(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r15), %edx
	vmovd	%edx, %xmm4
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm2, %xmm4, %xmm2
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vpmulld	%xmm15, %xmm2, %xmm0
	vmovdqa	%xmm0, 1184(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm9, %xmm2
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r9,%rdx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	movq	880(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm12, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm2, %eax
	vpextrd	$3, %xmm2, %edi
	cltd
	idivl	%r13d
	movl	%ebx, %r13d
	vmovd	%esi, %xmm2
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edx, %xmm2, %xmm2
	movl	%edi, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm2, %xmm4, %xmm2
	vpcmpgtd	%xmm2, %xmm14, %xmm4
	vpsubd	%xmm2, %xmm8, %xmm14
	vblendvps	%xmm4, %xmm2, %xmm14, %xmm2
	vmovdqa	624(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	608(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm0, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpaddd	%xmm13, %xmm2, %xmm2
	vpminsd	%xmm11, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm2, %xmm4, %xmm9
	vmovaps	368(%rsp), %xmm10       # 16-byte Reload
	vmulps	1168(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
	vmovaps	1056(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm3[0,2]
	vmovaps	512(%rsp), %xmm14       # 16-byte Reload
	vsubps	%xmm14, %xmm4, %xmm4
	vmovaps	528(%rsp), %xmm11       # 16-byte Reload
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm4, %xmm2, %xmm13
	vmulps	1136(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
	vmovaps	1072(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1088(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm12, %xmm7, %xmm0 # xmm0 = xmm7[0,2],xmm12[0,2]
	vsubps	%xmm14, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	1104(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm7, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm7[0,2]
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm4, %xmm4
	vmovaps	1472(%rsp), %xmm8       # 16-byte Reload
	vminps	%xmm8, %xmm13, %xmm2
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm2, %xmm2
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm3, %xmm6, %xmm4 # xmm4 = xmm6[1,3],xmm3[1,3]
	vmulps	1248(%rsp), %xmm10, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	1216(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
	vshufps	$221, %xmm7, %xmm1, %xmm5 # xmm5 = xmm1[1,3],xmm7[1,3]
	vsubps	%xmm14, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vpmulld	%xmm15, %xmm9, %xmm1
	vmovdqa	%xmm1, 1168(%rsp)       # 16-byte Spill
	vpaddd	1424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	vshufps	$221, %xmm12, %xmm7, %xmm1 # xmm1 = xmm7[1,3],xmm12[1,3]
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	sarq	$32, %rax
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movl	304(%rsp), %r12d        # 4-byte Reload
	movl	%r12d, %eax
	andl	%r13d, %eax
	vmovaps	320(%rsp), %xmm12       # 16-byte Reload
	vandps	%xmm12, %xmm0, %xmm7
	vmovaps	%xmm7, 1248(%rsp)       # 16-byte Spill
	vandps	%xmm12, %xmm2, %xmm0
	vaddps	%xmm0, %xmm7, %xmm0
	vminps	%xmm8, %xmm4, %xmm2
	vmaxps	%xmm13, %xmm2, %xmm2
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm6, %xmm10, %xmm4
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm4, %xmm1, %xmm4
	vsubps	%xmm2, %xmm3, %xmm1
	vsubps	%xmm3, %xmm2, %xmm2
	vandps	%xmm12, %xmm2, %xmm2
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vandps	%xmm12, %xmm3, %xmm3
	vmulps	1456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovups	%ymm0, 1216(%rsp)       # 32-byte Spill
	vmovaps	%xmm0, %xmm15
	jne	.LBB159_28
# BB#27:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vxorps	%xmm15, %xmm15, %xmm15
.LBB159_28:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	movq	912(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	movq	%rax, %rcx
	vmovups	(%r14,%rax,4), %xmm4
	movq	%rax, %rdx
	vmovups	32(%r14,%rax,4), %xmm5
	orq	$2, %rax
	vmovups	(%r14,%rax,4), %xmm8
	vandps	%xmm12, %xmm1, %xmm13
	vaddps	%xmm3, %xmm2, %xmm3
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm9
	vmovdqa	1376(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	1152(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm2
	orq	$4, %rdx
	vmovups	(%r14,%rdx,4), %xmm10
	vmovdqa	1280(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm1
	movb	1312(%rsp), %al         # 1-byte Reload
	andb	1296(%rsp), %al         # 1-byte Folded Reload
	movb	%al, %bl
	vmovdqa	%xmm0, %xmm14
	jne	.LBB159_29
# BB#36:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovaps	%xmm10, %xmm11
	vmovaps	%xmm4, 1136(%rsp)       # 16-byte Spill
	vmovdqa	%xmm2, 1152(%rsp)       # 16-byte Spill
	vmovdqa	%xmm1, 1280(%rsp)       # 16-byte Spill
	vmovdqa	1200(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	1184(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1456(%rsp), %xmm7       # 16-byte Reload
	jmp	.LBB159_37
	.align	16, 0x90
.LBB159_29:                             #   in Loop: Header=BB159_26 Depth=1
	vmovdqa	272(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm3, 1312(%rsp)       # 16-byte Spill
	vpaddd	992(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovdqa	%xmm2, %xmm7
	vmovdqa	%xmm7, 1152(%rsp)       # 16-byte Spill
	vmovaps	176(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm3
	vmovaps	%xmm4, %xmm11
	vmovaps	%xmm11, 1136(%rsp)      # 16-byte Spill
	vshufps	$136, %xmm9, %xmm8, %xmm4 # xmm4 = xmm8[0,2],xmm9[0,2]
	vmovaps	240(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm1, %xmm15
	vmovdqa	%xmm15, 1280(%rsp)      # 16-byte Spill
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm0, %xmm7, %xmm4
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm2, %xmm4
	vshufps	$136, %xmm10, %xmm11, %xmm7 # xmm7 = xmm11[0,2],xmm10[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm15, %xmm7
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm7     # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm2, %xmm7
	vshufps	$136, %xmm5, %xmm10, %xmm0 # xmm0 = xmm10[0,2],xmm5[0,2]
	vmovaps	%xmm10, %xmm11
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1472(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm4, %xmm4
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm4, %xmm4
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm2, %xmm0, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm2, %xmm3, %xmm3
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	%xmm12, %xmm3, %xmm0
	vmovaps	1312(%rsp), %xmm3       # 16-byte Reload
	vaddps	%xmm0, %xmm13, %xmm0
	vmovaps	1456(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm0, %xmm15
	vmovdqa	1200(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	1184(%rsp), %xmm2       # 16-byte Reload
.LBB159_37:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovdqa	1168(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, %xmm10
	testb	%bl, %bl
	jne	.LBB159_39
# BB#38:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
.LBB159_39:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vpsubd	%xmm14, %xmm1, %xmm4
	vpsubd	%xmm14, %xmm2, %xmm3
	vpsubd	%xmm14, %xmm6, %xmm1
	movl	%r12d, %eax
	andl	%r13d, %eax
	jne	.LBB159_40
# BB#41:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovdqa	%xmm4, 1184(%rsp)       # 16-byte Spill
	vmovdqa	%xmm3, 1200(%rsp)       # 16-byte Spill
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1168(%rsp)      # 16-byte Spill
	vmovups	%ymm0, 1312(%rsp)       # 32-byte Spill
	jmp	.LBB159_42
	.align	16, 0x90
.LBB159_40:                             #   in Loop: Header=BB159_26 Depth=1
	vmovaps	%xmm13, 1168(%rsp)      # 16-byte Spill
	vmovups	%ymm0, 1312(%rsp)       # 32-byte Spill
	vmovdqa	272(%rsp), %xmm2        # 16-byte Reload
	vpaddd	%xmm2, %xmm4, %xmm0
	vmovdqa	%xmm4, 1184(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	%xmm2, %xmm3, %xmm0
	vmovdqa	%xmm3, 1200(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	%xmm2, %xmm1, %xmm0
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vshufps	$221, %xmm9, %xmm8, %xmm0 # xmm0 = xmm8[1,3],xmm9[1,3]
	vmovss	(%r9,%r8,4), %xmm1      # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	176(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmovaps	240(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm0, %xmm0
	vmovaps	256(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	vmovss	(%r9,%rbx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm6, %xmm1
	vmovaps	1136(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm11[1,3]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm1, %xmm3, %xmm1
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm5, %xmm11, %xmm3 # xmm3 = xmm11[1,3],xmm5[1,3]
	vmovss	(%r9,%rax,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r9,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm6, %xmm5
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	vmulps	%xmm5, %xmm3, %xmm3
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	%xmm12, %xmm0, %xmm0
	vaddps	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm7, %xmm0, %xmm10
.LBB159_42:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	cltq
	vmovups	24584(%r14,%rax,4), %xmm8
	vmovups	24600(%r14,%rax,4), %xmm9
	vmovups	24576(%r14,%rax,4), %xmm14
	vmovups	24592(%r14,%rax,4), %xmm6
	vmovups	24608(%r14,%rax,4), %xmm13
	movb	1352(%rsp), %r14b       # 1-byte Reload
	movb	%r14b, %al
	movl	1440(%rsp), %ecx        # 4-byte Reload
	andb	%cl, %al
	je	.LBB159_44
# BB#43:                                #   in Loop: Header=BB159_26 Depth=1
	vmovdqa	224(%rsp), %xmm2        # 16-byte Reload
	vpaddd	992(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	160(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm3, %xmm1, %xmm3
	vshufps	$136, %xmm9, %xmm8, %xmm4 # xmm4 = xmm8[0,2],xmm9[0,2]
	vmovaps	192(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm4, %xmm4
	vmovaps	208(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vpaddd	1152(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm1, %xmm4
	vshufps	$136, %xmm6, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm7, %xmm7
	vmulps	%xmm7, %xmm0, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vpaddd	1280(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm7     # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm1, %xmm7
	vshufps	$136, %xmm13, %xmm6, %xmm2 # xmm2 = xmm6[0,2],xmm13[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm0, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	1472(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm4, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm4, %xmm4
	vminps	%xmm0, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vaddps	%xmm2, %xmm4, %xmm2
	vminps	%xmm0, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vmovaps	1360(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm3
	vandps	%xmm12, %xmm3, %xmm2
	vaddps	1168(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm2, %xmm15 # 16-byte Folded Reload
.LBB159_44:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	movq	1560(%rsp), %rcx        # 8-byte Reload
	vmovups	1216(%rsp), %ymm11      # 32-byte Reload
	movl	%r13d, %eax
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB159_46
# BB#45:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovaps	%xmm15, %xmm11
.LBB159_46:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	testl	%eax, %eax
	jne	.LBB159_48
# BB#47:                                #   in Loop: Header=BB159_26 Depth=1
	vmovdqa	224(%rsp), %xmm3        # 16-byte Reload
	vpaddd	1184(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rsi
	vpaddd	1200(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	vpaddd	1296(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vshufps	$221, %xmm9, %xmm8, %xmm1 # xmm1 = xmm8[1,3],xmm9[1,3]
	vmovss	(%r9,%r8,4), %xmm2      # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	160(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm2
	vmovaps	192(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm2, %xmm1, %xmm1
	vmovss	(%r9,%rbx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm7, %xmm2
	vshufps	$221, %xmm6, %xmm14, %xmm0 # xmm0 = xmm14[1,3],xmm6[1,3]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm13, %xmm6, %xmm2 # xmm2 = xmm6[1,3],xmm13[1,3]
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	1472(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm3, %xmm1, %xmm1
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm3, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm2, %xmm1
	vandps	%xmm12, %xmm1, %xmm0
	vaddps	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
.LBB159_48:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	movl	1024(%rsp), %edx        # 4-byte Reload
	vmovups	1312(%rsp), %ymm1       # 32-byte Reload
	movl	1440(%rsp), %eax        # 4-byte Reload
	andb	%al, %r14b
	jne	.LBB159_50
# BB#49:                                # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovaps	%xmm10, %xmm1
.LBB159_50:                             # %for dh.s0.v10.v104
                                        #   in Loop: Header=BB159_26 Depth=1
	vmovaps	.LCPI159_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI159_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm11, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	928(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1528(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	addl	$-1, %edx
	jne	.LBB159_26
.LBB159_51:                             # %end for dh.s0.v10.v105
	movl	-112(%rsp), %eax        # 4-byte Reload
	cmpl	-108(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB159_72
# BB#52:                                # %for dh.s0.v10.v108.preheader
	movq	128(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%rdx), %edi
	movl	%edi, 1376(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	movl	%edx, %esi
	sarl	$31, %esi
	andnl	%edi, %esi, %ecx
	andl	%eax, %esi
	orl	%ecx, %esi
	movl	%esi, 1440(%rsp)        # 4-byte Spill
	movl	%esi, %ecx
	movq	96(%rsp), %r10          # 8-byte Reload
	leal	(%r10,%rdx), %esi
	movl	%esi, 1424(%rsp)        # 4-byte Spill
	leal	-1(%rdx,%rdx), %r13d
	leal	-1(%r10,%rdx), %ebx
	movq	%rdx, %rbp
	movl	$2, %eax
	subl	%r10d, %eax
	cltd
	idivl	%edi
	movl	%edi, %r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%ecx, %edi
	addl	%edx, %eax
	movl	%r13d, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %ebp
	cmovgl	%eax, %ecx
	addl	%r10d, %ecx
	cmpl	%ecx, %ebx
	cmovlel	%ebx, %ecx
	cmpl	%r10d, %ecx
	cmovll	%r10d, %ecx
	movl	%ecx, 1472(%rsp)        # 4-byte Spill
	cmpl	$3, %esi
	movl	$2, %eax
	cmovll	%ebx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	cmpl	$3, %esi
	cmovll	%ecx, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movl	%r10d, %eax
	negl	%eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%r13d, %r12d
	subl	%eax, %r12d
	cmpl	%eax, %ebp
	cmovgl	%eax, %r12d
	addl	%r10d, %r12d
	cmpl	%r12d, %ebx
	cmovlel	%ebx, %r12d
	cmpl	%r10d, %r12d
	cmovll	%r10d, %r12d
	testl	%esi, %esi
	movl	$0, %eax
	cmovlel	%ebx, %eax
	cmpl	%r10d, %eax
	cmovll	%r10d, %eax
	testl	%esi, %esi
	cmovlel	%r12d, %eax
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	movq	64(%rsp), %r11          # 8-byte Reload
	leal	(%r11,%r11), %edx
	movl	%edx, 1296(%rsp)        # 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	movl	%r11d, %edi
	sarl	$31, %edi
	andnl	%edx, %edi, %ecx
	movl	%edx, %ebp
	andl	%eax, %edi
	orl	%ecx, %edi
	movq	104(%rsp), %rsi         # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	cltd
	idivl	%ebp
	movl	%ebp, %r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	leal	-1(%r11,%r11), %ecx
	movl	%ecx, %ebp
	subl	%eax, %ebp
	cmpl	%eax, %r11d
	cmovgl	%eax, %ebp
	addl	%esi, %ebp
	leal	-1(%rsi,%r11), %r14d
	cmpl	%ebp, %r14d
	cmovlel	%r14d, %ebp
	cmpl	%esi, %ebp
	cmovll	%esi, %ebp
	leal	(%rsi,%r11), %r15d
	movl	%r15d, 1280(%rsp)       # 4-byte Spill
	testl	%r15d, %r15d
	movl	$0, %eax
	cmovlel	%r14d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%r15d, %r15d
	cmovlel	%ebp, %eax
	movl	%eax, 1352(%rsp)        # 4-byte Spill
	movl	$2, %eax
	subl	%esi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	movl	%ecx, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r11d
	cmovgl	%eax, %r8d
	addl	%esi, %r8d
	cmpl	%r8d, %r14d
	cmovlel	%r14d, %r8d
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	cmpl	$3, %r15d
	movl	$2, %eax
	cmovll	%r14d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	cmpl	$3, %r15d
	cmovll	%r8d, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r10d, %eax
	cltd
	idivl	1376(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	1440(%rsp), %eax        # 4-byte Folded Reload
	addl	%edx, %eax
	subl	%eax, %r13d
	movq	128(%rsp), %rdx         # 8-byte Reload
	cmpl	%eax, %edx
	cmovgl	%eax, %r13d
	addl	%r10d, %r13d
	cmpl	%r13d, %ebx
	cmovlel	%ebx, %r13d
	cmpl	%r10d, %r13d
	cmovll	%r10d, %r13d
	movl	1424(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	$0, %r15d
	cmovgel	%r15d, %ebx
	movzbl	%al, %eax
	orl	%eax, %ebx
	cmpl	%r10d, %ebx
	cmovll	%r10d, %ebx
	cmpl	$2, %edx
	cmovll	%r13d, %ebx
	movl	$1, %eax
	subl	%esi, %eax
	cltd
	idivl	1296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%edx, %eax
	subl	%eax, %ecx
	cmpl	%eax, %r11d
	cmovgl	%eax, %ecx
	addl	%esi, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	movl	1280(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	movl	%edx, %edi
	movl	$0, %edx
	cmovgel	%edx, %r14d
	movzbl	%al, %eax
	orl	%eax, %r14d
	cmpl	%esi, %r14d
	cmovll	%esi, %r14d
	cmpl	$2, %edi
	cmovll	%ecx, %r14d
	movl	%r14d, %r11d
	movq	80(%rsp), %r15          # 8-byte Reload
	movl	%r15d, %edi
	movq	88(%rsp), %rax          # 8-byte Reload
	imull	%eax, %edi
	movq	1560(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %edx
	andl	$1, %edx
	movl	%edx, 1440(%rsp)        # 4-byte Spill
	addl	%r10d, %edi
	cmpl	$1, %esi
	cmovgl	%ecx, %r11d
	movl	%r11d, 1424(%rsp)       # 4-byte Spill
	movq	960(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	movq	136(%rsp), %rcx         # 8-byte Reload
	imull	%ecx, %eax
	movq	1016(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %ecx
	sarl	$5, %ecx
	movslq	%r14d, %rdx
	imulq	%r15, %rdx
	addl	%esi, %eax
	movl	%eax, 1376(%rsp)        # 4-byte Spill
	cmpl	$1, %r10d
	cmovgl	%r13d, %ebx
	movslq	%edi, %rdi
	subq	%rdi, %rdx
	movslq	%ebx, %rdi
	leaq	(%rdi,%rdx), %rdi
	movq	120(%rsp), %rbx         # 8-byte Reload
	vbroadcastss	(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	cmpl	$2, %esi
	movl	1312(%rsp), %r15d       # 4-byte Reload
	cmovgl	%r8d, %r15d
	testl	%esi, %esi
	movl	1352(%rsp), %r8d        # 4-byte Reload
	cmovgl	%ebp, %r8d
	testl	%r10d, %r10d
	movl	1360(%rsp), %eax        # 4-byte Reload
	cmovgl	%r12d, %eax
	movslq	%eax, %rsi
	cmpl	$2, %r10d
	movl	1456(%rsp), %edi        # 4-byte Reload
	cmovgl	1472(%rsp), %edi        # 4-byte Folded Reload
	movslq	%edi, %rdi
	leaq	(%rsi,%rdx), %rsi
	leaq	(%rdi,%rdx), %rdx
	vbroadcastss	(%rbx,%rsi,4), %xmm0
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	vbroadcastss	(%rbx,%rdx,4), %xmm0
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	movslq	%ecx, %rdx
	shlq	$5, %rdx
	addq	$48, %rdx
	movl	%r14d, %edi
	andl	$63, %edi
	imulq	%rdx, %rdi
	movq	152(%rsp), %rsi         # 8-byte Reload
	movq	%rsi, %rdx
	sarq	$63, %rdx
	andq	%rsi, %rdx
	subq	%rdx, %rdi
	movq	%rdi, 1088(%rsp)        # 8-byte Spill
	leal	(%rcx,%rcx,2), %edx
	movl	%edx, %esi
	shll	$10, %esi
	leal	8(%r14), %ecx
	subl	116(%rsp), %ecx         # 4-byte Folded Reload
	movl	%r11d, %r10d
	andl	$-32, %r10d
	addl	$64, %r10d
	imull	%ecx, %r10d
	movl	984(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	movq	16(%rsp), %rax          # 8-byte Reload
	testl	%eax, %eax
	movl	$0, %edi
	cmovnsl	%eax, %edi
	notl	%edi
	cmpl	%edi, %ecx
	cmovgel	%ecx, %edi
	leal	(,%rdi,8), %ecx
	subl	%ecx, %r10d
	addl	$-8, %esi
	movq	%rsi, 1056(%rsp)        # 8-byte Spill
	shll	$9, %edx
	addl	$-8, %edx
	movq	%rdx, 1072(%rsp)        # 8-byte Spill
	movq	944(%rsp), %rax         # 8-byte Reload
	movl	%eax, %esi
	subl	%ecx, %esi
	movq	-16(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ecx
	negl	%ecx
	movl	-4(%rsp), %edx          # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-20(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-24(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-28(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-32(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-36(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-40(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-44(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-48(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-52(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-56(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-60(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-64(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-72(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-80(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-88(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-96(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movl	-104(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	movq	(%rsp), %rax            # 8-byte Reload
	movl	%eax, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	movq	1536(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	notl	%edx
	cmpl	%edx, %ecx
	cmovgel	%ecx, %edx
	negl	%edx
	movl	%edi, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	vmovss	.LCPI159_0(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	36(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	28(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm2
	vmovss	24(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	32(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm5, %xmm1
	movslq	1376(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1040(%rsp)        # 8-byte Spill
	movslq	1424(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1024(%rsp)        # 8-byte Spill
	leal	1(%rcx,%rdi), %edi
	movq	960(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %rcx
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 928(%rsp)        # 16-byte Spill
	vmovss	48(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm1
	vmovss	44(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmulss	%xmm4, %xmm1, %xmm2
	vmovss	40(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovss	52(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm5, %xmm1
	vmovss	56(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm0, %xmm0
	vmovss	72(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm0, %xmm3
	vmovss	60(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	76(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm4, %xmm4
	vmulss	%xmm4, %xmm0, %xmm0
	vdivss	%xmm0, %xmm5, %xmm0
	movslq	%r8d, %rax
	movq	%rax, 880(%rsp)         # 8-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_3(%rip), %xmm13
	vbroadcastss	.LCPI159_4(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_5(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_6(%rip), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	movq	1024(%rsp), %r15        # 8-byte Reload
	.align	16, 0x90
.LBB159_53:                             # %for dh.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movq	%r10, 1352(%rsp)        # 8-byte Spill
	movl	%edi, 1280(%rsp)        # 4-byte Spill
	movq	%rsi, 1296(%rsp)        # 8-byte Spill
	movl	1440(%rsp), %r14d       # 4-byte Reload
	testl	%r14d, %r14d
	setne	1360(%rsp)              # 1-byte Folded Spill
	sete	1216(%rsp)              # 1-byte Folded Spill
	leal	-8(%rsi), %r12d
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 1200(%rsp)        # 4-byte Spill
	sete	1376(%rsp)              # 1-byte Folded Spill
	movslq	%r12d, %r13
	movq	%r13, 1248(%rsp)        # 8-byte Spill
	leaq	-6(%r13), %rax
	movq	960(%rsp), %r11         # 8-byte Reload
	imulq	%r11, %rax
	movq	1040(%rsp), %r8         # 8-byte Reload
	subq	%r8, %rax
	movq	%rax, 1136(%rsp)        # 8-byte Spill
	leaq	(%rax,%r15), %rbx
	leaq	(%r9,%rbx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rdi
	vmovss	(%r9,%rbx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	movq	1072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %edx
	movslq	%edx, %rbx
	movq	1568(%rsp), %rax        # 8-byte Reload
	vmovups	12296(%rax,%rbx,4), %xmm15
	leaq	-4(%r13), %rdx
	imulq	%r11, %rdx
	vmovups	12312(%rax,%rbx,4), %xmm11
	subq	%r8, %rdx
	movq	%rdx, 1184(%rsp)        # 8-byte Spill
	leaq	(%rdx,%r15), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rbp
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm8 # xmm8 = xmm0[0,1,2],mem[0]
	vmovups	12304(%rax,%rbx,4), %xmm0
	vmovups	12320(%rax,%rbx,4), %xmm3
	leaq	-8(%r13), %r10
	imulq	%r11, %r10
	subq	%r8, %r10
	leaq	(%r10,%r15), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	vmovss	(%r9,%rdx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	leaq	(%rdi,%rcx,4), %rdx
	vinsertps	$16, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm14 # xmm14 = xmm1[0,1,2],mem[0]
	vmovups	12288(%rax,%rbx,4), %xmm1
	leaq	-5(%r13), %rdx
	imulq	%r11, %rdx
	subq	%r8, %rdx
	movq	%rdx, 1168(%rsp)        # 8-byte Spill
	leaq	(%rdx,%r15), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	leaq	(%rdi,%rcx,4), %rdx
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm4, 1312(%rsp)       # 16-byte Spill
	leaq	-7(%r13), %rdx
	imulq	%r11, %rdx
	subq	%r8, %rdx
	movq	%rdx, 1152(%rsp)        # 8-byte Spill
	leaq	(%rdx,%r15), %rdx
	leaq	(%r9,%rdx,4), %rsi
	vmovss	(%r9,%rdx,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	leaq	(%rsi,%rcx,4), %rdx
	vinsertps	$16, (%rsi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	leaq	(%rdx,%rcx,4), %rsi
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	leaq	-3(%r13), %r13
	imulq	%r11, %r13
	subq	%r8, %r13
	leaq	(%r13,%r15), %rdx
	vmovss	(%r9,%rdx,4), %xmm7     # xmm7 = mem[0],zero,zero,zero
	leaq	(%r9,%rdx,4), %rdx
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$32, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rdx,%rcx,4), %rdx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	andl	%r12d, %r14d
	vmovaps	1104(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm2, %xmm9, %xmm2
	vshufps	$136, %xmm11, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm11[0,2]
	vmovaps	928(%rsp), %xmm10       # 16-byte Reload
	vsubps	%xmm10, %xmm6, %xmm6
	vmovaps	992(%rsp), %xmm12       # 16-byte Reload
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	%xmm6, %xmm2, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm8, %xmm9, %xmm6
	vmovaps	%xmm3, %xmm8
	vshufps	$136, %xmm8, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm8[0,2]
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm14, %xmm9, %xmm6
	vshufps	$136, %xmm0, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm0[0,2]
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm2, %xmm2
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm3
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm4
	vshufps	$221, %xmm11, %xmm15, %xmm11 # xmm11 = xmm15[1,3],xmm11[1,3]
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm2, %xmm3, %xmm6
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vandps	%xmm2, %xmm4, %xmm3
	vaddps	%xmm3, %xmm6, %xmm3
	vmulps	1312(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm11, %xmm6
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm5, %xmm9, %xmm5
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm5, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm5
	vminps	%xmm13, %xmm4, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm4
	vsubps	%xmm5, %xmm4, %xmm1
	vsubps	%xmm4, %xmm5, %xmm5
	vandps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm7, %xmm9, %xmm6
	vshufps	$221, %xmm8, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm8[1,3]
	vxorps	%xmm7, %xmm7, %xmm7
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm12, %xmm0
	vmulps	%xmm6, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm0
	vandps	%xmm2, %xmm0, %xmm6
	vmovaps	1456(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm3, %xmm0
	vmovaps	%xmm0, %xmm3
	jne	.LBB159_55
# BB#54:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
.LBB159_55:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	movq	1352(%rsp), %rbp        # 8-byte Reload
	leal	-8(%rbp), %edx
	movslq	%edx, %rdx
	movq	%rdx, %rsi
	vmovups	(%rax,%rdx,4), %xmm9
	movq	%rdx, %rdi
	vmovups	32(%rax,%rdx,4), %xmm4
	orq	$2, %rdx
	vmovups	(%rax,%rdx,4), %xmm14
	vandps	%xmm2, %xmm1, %xmm15
	vaddps	%xmm6, %xmm5, %xmm8
	orq	$6, %rsi
	vmovups	(%rax,%rsi,4), %xmm11
	orq	$4, %rdi
	vmovups	(%rax,%rdi,4), %xmm10
	movb	1360(%rsp), %al         # 1-byte Reload
	andb	1376(%rsp), %al         # 1-byte Folded Reload
	movq	%rbp, %r11
	jne	.LBB159_56
# BB#57:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovaps	%xmm4, 1312(%rsp)       # 16-byte Spill
	vmovups	%ymm0, 1376(%rsp)       # 32-byte Spill
	jmp	.LBB159_58
	.align	16, 0x90
.LBB159_56:                             #   in Loop: Header=BB159_53 Depth=1
	vmovups	%ymm0, 1376(%rsp)       # 32-byte Spill
	movq	880(%rsp), %rbp         # 8-byte Reload
	movq	1136(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rbp), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm4, %xmm6
	vmovaps	%xmm6, 1312(%rsp)       # 16-byte Spill
	vmovaps	912(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm2, %xmm1
	vshufps	$136, %xmm11, %xmm14, %xmm2 # xmm2 = xmm14[0,2],xmm11[0,2]
	vmovaps	848(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	864(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	leaq	(%r10,%rbp), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$136, %xmm10, %xmm9, %xmm0 # xmm0 = xmm9[0,2],xmm10[0,2]
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	movq	1184(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rbp), %rdx
	leaq	(%r9,%rdx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rdi,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm4, %xmm3
	vshufps	$136, %xmm6, %xmm10, %xmm4 # xmm4 = xmm10[0,2],xmm6[0,2]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1424(%rsp), %xmm3       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm3, %xmm2
	vandps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm1, %xmm2
	vaddps	%xmm0, %xmm15, %xmm0
	vmulps	%xmm12, %xmm0, %xmm3
.LBB159_58:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	movb	1216(%rsp), %r8b        # 1-byte Reload
	vmovaps	%xmm2, 1472(%rsp)       # 16-byte Spill
	vmulps	%xmm12, %xmm8, %xmm12
	vmovaps	%xmm12, %xmm6
	testb	%al, %al
	jne	.LBB159_60
# BB#59:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vxorps	%xmm6, %xmm6, %xmm6
.LBB159_60:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	movl	1440(%rsp), %edx        # 4-byte Reload
	andl	%r12d, %edx
	jne	.LBB159_61
# BB#62:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovaps	%xmm15, 1360(%rsp)      # 16-byte Spill
	jmp	.LBB159_63
	.align	16, 0x90
.LBB159_61:                             #   in Loop: Header=BB159_53 Depth=1
	vmovaps	%xmm15, 1360(%rsp)      # 16-byte Spill
	movq	880(%rsp), %rax         # 8-byte Reload
	movq	1168(%rsp), %rdx        # 8-byte Reload
	leaq	(%rdx,%rax), %rdx
	movq	1152(%rsp), %rsi        # 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	leaq	(%r13,%rax), %rbx
	leaq	(%r9,%rdx,4), %rdi
	leaq	(%rdi,%rcx,4), %rbp
	leaq	(%rbp,%rcx,4), %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	912(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vshufps	$221, %xmm11, %xmm14, %xmm2 # xmm2 = xmm14[1,3],xmm11[1,3]
	vmovaps	848(%rsp), %xmm7        # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmovaps	864(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm5
	leaq	(%r9,%rsi,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rdi
	vmovss	(%r9,%rsi,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	%xmm0, %xmm4, %xmm0
	vshufps	$221, %xmm10, %xmm9, %xmm2 # xmm2 = xmm9[1,3],xmm10[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	leaq	(%r9,%rbx,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%rbx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, 1312(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm10[1,3],mem[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm5
	vandps	1472(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vaddps	1120(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
.LBB159_63:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	movq	%r11, %rsi
	movl	1280(%rsp), %edi        # 4-byte Reload
	movl	1200(%rsp), %r11d       # 4-byte Reload
	movq	1056(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rsi), %eax
	cltq
	movq	1568(%rsp), %rdx        # 8-byte Reload
	vmovups	24584(%rdx,%rax,4), %xmm14
	vmovups	24600(%rdx,%rax,4), %xmm9
	vmovups	24576(%rdx,%rax,4), %xmm11
	vmovups	24592(%rdx,%rax,4), %xmm0
	vmovups	24608(%rdx,%rax,4), %xmm10
	movb	%r8b, %al
	movq	%rsi, %r14
	andb	%r11b, %al
	je	.LBB159_65
# BB#64:                                #   in Loop: Header=BB159_53 Depth=1
	movq	832(%rsp), %rbx         # 8-byte Reload
	movq	1136(%rsp), %rbp        # 8-byte Reload
	addq	%rbx, %rbp
	leaq	(%r9,%rbp,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%rbp,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vshufps	$136, %xmm9, %xmm14, %xmm3 # xmm3 = xmm14[0,2],xmm9[0,2]
	vmovaps	800(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm3, %xmm3
	vmovaps	816(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm3
	addq	%rbx, %r10
	leaq	(%r9,%r10,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%r10,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm1, %xmm2
	vshufps	$136, %xmm0, %xmm11, %xmm5 # xmm5 = xmm11[0,2],xmm0[0,2]
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	movq	1184(%rsp), %rbp        # 8-byte Reload
	addq	%rbx, %rbp
	leaq	(%r9,%rbp,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%rbp,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm1, %xmm5
	vshufps	$136, %xmm10, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm10[0,2]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vminps	%xmm13, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm2, %xmm2
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vminps	%xmm13, %xmm3, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vmovaps	1424(%rsp), %xmm3       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm3, %xmm2
	vandps	1472(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vaddps	1360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
.LBB159_65:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovups	1376(%rsp), %ymm7       # 32-byte Reload
	movq	1560(%rsp), %rax        # 8-byte Reload
	orl	%eax, %r12d
	andl	$1, %r12d
	movq	%r14, %r10
	je	.LBB159_67
# BB#66:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovaps	%xmm3, %xmm7
.LBB159_67:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	testl	%r12d, %r12d
	jne	.LBB159_69
# BB#68:                                #   in Loop: Header=BB159_53 Depth=1
	movq	832(%rsp), %rax         # 8-byte Reload
	movq	1168(%rsp), %rbx        # 8-byte Reload
	addq	%rax, %rbx
	movq	1152(%rsp), %rbp        # 8-byte Reload
	addq	%rax, %rbp
	addq	%rax, %r13
	leaq	(%r9,%rbx,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%rbx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	896(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vshufps	$221, %xmm9, %xmm14, %xmm2 # xmm2 = xmm14[1,3],xmm9[1,3]
	vmovaps	800(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm2, %xmm2
	vmovaps	816(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm1, %xmm3
	leaq	(%r9,%rbp,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%rbp,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm5, %xmm1
	vshufps	$221, %xmm0, %xmm11, %xmm2 # xmm2 = xmm11[1,3],xmm0[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	leaq	(%r9,%r13,4), %rax
	leaq	(%rax,%rcx,4), %rdx
	leaq	(%rdx,%rcx,4), %rsi
	vmovss	(%r9,%r13,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdx,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rsi,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm5, %xmm2
	vshufps	$221, %xmm10, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm10[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm3
	vandps	1472(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vaddps	1120(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1456(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
.LBB159_69:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	movq	1296(%rsp), %rsi        # 8-byte Reload
	movq	1248(%rsp), %rdx        # 8-byte Reload
	andb	%r11b, %r8b
	jne	.LBB159_71
# BB#70:                                # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovaps	%xmm6, %xmm12
.LBB159_71:                             # %for dh.s0.v10.v108
                                        #   in Loop: Header=BB159_53 Depth=1
	vmovaps	.LCPI159_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm12, %ymm0, %ymm0
	vmovaps	.LCPI159_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm7, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movq	1088(%rsp), %rax        # 8-byte Reload
	leaq	(%rdx,%rax), %rax
	movq	1528(%rsp), %rdx        # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	addl	$8, %r10d
	addl	$8, %esi
	addl	$-1, %edi
	jne	.LBB159_53
.LBB159_72:                             # %end for dh.s0.v10.v109
	movq	%r9, 992(%rsp)          # 8-byte Spill
	movl	-108(%rsp), %eax        # 4-byte Reload
	cmpl	984(%rsp), %eax         # 4-byte Folded Reload
	vmovss	60(%rsp), %xmm8         # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmovss	56(%rsp), %xmm10        # 4-byte Reload
                                        # xmm10 = mem[0],zero,zero,zero
	vmovss	52(%rsp), %xmm15        # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmovss	48(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	44(%rsp), %xmm11        # 4-byte Reload
                                        # xmm11 = mem[0],zero,zero,zero
	vmovss	40(%rsp), %xmm12        # 4-byte Reload
                                        # xmm12 = mem[0],zero,zero,zero
	vmovss	36(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	32(%rsp), %xmm13        # 4-byte Reload
                                        # xmm13 = mem[0],zero,zero,zero
	vmovss	28(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vmovss	24(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	jge	.LBB159_109
# BB#73:                                # %for dh.s0.v10.v1012.preheader
	movl	$2, %eax
	movq	96(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	movq	128(%rsp), %r12         # 8-byte Reload
	leal	(%r12,%r12), %ecx
	movl	%ecx, 1424(%rsp)        # 4-byte Spill
	cltd
	idivl	%ecx
	movl	%ecx, %eax
	negl	%eax
	movl	%r12d, %r10d
	sarl	$31, %r10d
	andnl	%ecx, %r10d, %ecx
	andl	%eax, %r10d
	orl	%ecx, %r10d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	leal	-1(%r12,%r12), %ebx
	movl	%ebx, %ecx
	subl	%eax, %ecx
	cmpl	%eax, %r12d
	cmovgl	%eax, %ecx
	addl	%edi, %ecx
	leal	-1(%rdi,%r12), %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	cmpl	%edi, %ecx
	cmovll	%edi, %ecx
	movl	%ecx, 1352(%rsp)        # 4-byte Spill
	leal	(%rdi,%r12), %r9d
	cmpl	$3, %r9d
	movl	$2, %eax
	cmovll	%edx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %r9d
	cmovll	%ecx, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movq	64(%rsp), %r13          # 8-byte Reload
	leal	(%r13,%r13), %r8d
	movl	%r8d, 1360(%rsp)        # 4-byte Spill
	movl	%r8d, %eax
	negl	%eax
	movl	%r13d, %r11d
	sarl	$31, %r11d
	andnl	%r8d, %r11d, %esi
	andl	%eax, %r11d
	movl	$2, %eax
	movq	104(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %eax
	cltd
	idivl	%r8d
	orl	%esi, %r11d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	leal	-1(%r13,%r13), %esi
	movl	%esi, %r14d
	subl	%eax, %r14d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r14d
	addl	%ecx, %r14d
	leal	-1(%rcx,%r13), %r15d
	cmpl	%r14d, %r15d
	cmovlel	%r15d, %r14d
	cmpl	%ecx, %r14d
	cmovll	%ecx, %r14d
	leal	(%rcx,%r13), %edx
	movl	%edx, 1376(%rsp)        # 4-byte Spill
	cmpl	$3, %edx
	movl	$2, %eax
	cmovll	%r15d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	$3, %edx
	cmovll	%r14d, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cltd
	idivl	1424(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	movl	%ebx, %ebp
	subl	%eax, %ebp
	cmpl	%eax, %r12d
	cmovgl	%eax, %ebp
	addl	%edi, %ebp
	movl	1472(%rsp), %eax        # 4-byte Reload
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	movl	%eax, %edx
	cmpl	%edi, %ebp
	cmovll	%edi, %ebp
	testl	%r9d, %r9d
	movl	$0, %eax
	cmovlel	%edx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	testl	%r9d, %r9d
	cmovlel	%ebp, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	%ecx, %eax
	negl	%eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	movl	%esi, %r8d
	subl	%eax, %r8d
	cmpl	%eax, %r13d
	cmovgl	%eax, %r8d
	addl	%ecx, %r8d
	cmpl	%r8d, %r15d
	cmovlel	%r15d, %r8d
	cmpl	%ecx, %r8d
	cmovll	%ecx, %r8d
	movl	1376(%rsp), %edx        # 4-byte Reload
	testl	%edx, %edx
	movl	$0, %eax
	cmovlel	%r15d, %eax
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	testl	%edx, %edx
	cmovlel	%r8d, %eax
	movl	%eax, 1440(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%edi, %eax
	cltd
	idivl	1424(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	subl	%eax, %ebx
	cmpl	%eax, %r12d
	cmovgl	%eax, %ebx
	addl	%edi, %ebx
	movl	1472(%rsp), %edx        # 4-byte Reload
	cmpl	%ebx, %edx
	cmovlel	%edx, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	$1, %r9d
	setg	%al
	cmpl	$2, %r9d
	movl	$0, %r10d
	cmovgel	%r10d, %edx
	movzbl	%al, %eax
	orl	%eax, %edx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	cmpl	$2, %r9d
	cmovll	%ebx, %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%ecx, %eax
	cltd
	idivl	1360(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r11d, %eax
	addl	%edx, %eax
	subl	%eax, %esi
	cmpl	%eax, %r13d
	cmovgl	%eax, %esi
	addl	%ecx, %esi
	cmpl	%esi, %r15d
	cmovlel	%r15d, %esi
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	movl	1376(%rsp), %edx        # 4-byte Reload
	cmpl	$1, %edx
	setg	%al
	cmpl	$2, %edx
	cmovgel	%r10d, %r15d
	movzbl	%al, %eax
	orl	%eax, %r15d
	cmpl	%ecx, %r15d
	cmovll	%ecx, %r15d
	cmpl	$2, %edx
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	cmovll	%esi, %r15d
	movq	88(%rsp), %r13          # 8-byte Reload
	movq	80(%rsp), %r11          # 8-byte Reload
	imull	%r11d, %r13d
	movq	1560(%rsp), %r9         # 8-byte Reload
	movl	%r9d, %r12d
	andl	$1, %r12d
	movl	%r12d, 488(%rsp)        # 4-byte Spill
	addl	%edi, %r13d
	movq	1016(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %eax
	sarl	$5, %eax
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	cmpl	$1, %ecx
	cmovgl	%esi, %r15d
	movl	%r15d, 1296(%rsp)       # 4-byte Spill
	movl	%r15d, %eax
	vmovss	.LCPI159_0(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm2, %xmm1
	vmulss	%xmm5, %xmm1, %xmm3
	vdivss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vsubss	%xmm5, %xmm13, %xmm3
	vmulss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm13
	movq	960(%rsp), %rdx         # 8-byte Reload
	vmovaps	%xmm11, %xmm0
	vmovaps	%xmm12, %xmm1
	vmovd	%edx, %xmm12
	movq	136(%rsp), %r15         # 8-byte Reload
	imull	%r15d, %edx
	addl	%ecx, %edx
	movq	%rdx, 960(%rsp)         # 8-byte Spill
	vmovd	%edx, %xmm3
	vmovd	%eax, %xmm4
	vpsubd	%xmm3, %xmm4, %xmm9
	vsubss	%xmm7, %xmm2, %xmm3
	vmulss	%xmm0, %xmm3, %xmm4
	vdivss	%xmm1, %xmm4, %xmm4
	vaddss	%xmm4, %xmm7, %xmm11
	movq	8(%rsp), %rax           # 8-byte Reload
	leal	6(%rax), %edx
	vmovd	%edx, %xmm4
	movslq	%r9d, %rdx
	imulq	%r11, %rdx
	cmpl	$1, %edi
	movl	1472(%rsp), %esi        # 4-byte Reload
	cmovgl	%ebx, %esi
	movl	%esi, 1472(%rsp)        # 4-byte Spill
	movslq	%r13d, %rsi
	xorl	%r13d, %r13d
	subq	%rsi, %rdx
	vmovaps	%xmm10, %xmm5
	vsubss	%xmm0, %xmm15, %xmm7
	leal	4(%rax), %esi
	vmovd	%esi, %xmm15
	vmulss	%xmm7, %xmm3, %xmm3
	leal	8(%rax), %esi
	vmovd	%esi, %xmm6
	vdivss	%xmm3, %xmm1, %xmm14
	vsubss	%xmm5, %xmm2, %xmm2
	vmovss	72(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm2, %xmm3
	vdivss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm7
	leal	5(%rax), %esi
	vmovd	%esi, %xmm3
	vmovss	76(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm1, %xmm5
	leal	7(%rax), %esi
	vmovd	%esi, %xmm1
	testl	%ecx, %ecx
	movl	1440(%rsp), %esi        # 4-byte Reload
	cmovgl	%r8d, %esi
	movl	%esi, 1440(%rsp)        # 4-byte Spill
	testl	%edi, %edi
	movl	1280(%rsp), %r11d       # 4-byte Reload
	cmovgl	%ebp, %r11d
	cmpl	$2, %ecx
	movl	1456(%rsp), %ecx        # 4-byte Reload
	cmovgl	%r14d, %ecx
	movl	%ecx, 1456(%rsp)        # 4-byte Spill
	cmpl	$2, %edi
	movl	1312(%rsp), %edi        # 4-byte Reload
	cmovgl	1352(%rsp), %edi        # 4-byte Folded Reload
	vmulss	%xmm5, %xmm2, %xmm2
	leal	-1(%rax), %ecx
	addl	$3, %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm4, %xmm4
	vdivss	%xmm2, %xmm8, %xmm2
	vmovdqa	.LCPI159_1(%rip), %xmm0 # xmm0 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm0, %xmm4, %xmm4
	vmovdqa	%xmm4, 1040(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm4
	vmovdqa	%xmm4, 1024(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm4
	vmovdqa	%xmm4, 928(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm0, %xmm3, %xmm3
	vmovdqa	%xmm3, 912(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	%xmm1, 896(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 880(%rsp)        # 16-byte Spill
	vmovd	%ecx, %xmm3
	movq	1424(%rsp), %rax        # 8-byte Reload
	movslq	%eax, %rcx
	shlq	$5, %rcx
	addq	$48, %rcx
	movl	%r9d, %eax
	andl	$63, %eax
	imulq	%rcx, %rax
	movq	152(%rsp), %rsi         # 8-byte Reload
	movq	%rsi, %rcx
	sarq	$63, %rcx
	andq	%rsi, %rcx
	subq	%rcx, %rax
	movq	%rax, 864(%rsp)         # 8-byte Spill
	movq	-16(%rsp), %rcx         # 8-byte Reload
	negl	%ecx
	movl	-4(%rsp), %eax          # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-20(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-24(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-28(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-32(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-36(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-40(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-44(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-48(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-52(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-56(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-60(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-64(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-104(%rsp), %esi        # 4-byte Reload
	notl	%esi
	cmpl	%esi, %ecx
	cmovgel	%ecx, %esi
	movq	(%rsp), %rax            # 8-byte Reload
	notl	%eax
	cmpl	%eax, %esi
	cmovgel	%esi, %eax
	movq	1536(%rsp), %rsi        # 8-byte Reload
	notl	%esi
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	negl	%esi
	movl	984(%rsp), %ebx         # 4-byte Reload
	movl	%ebx, %ecx
	notl	%ecx
	movq	16(%rsp), %rbp          # 8-byte Reload
	testl	%ebp, %ebp
	cmovsl	%r13d, %ebp
	notl	%ebp
	cmpl	%ebp, %ecx
	cmovgel	%ecx, %ebp
	notl	%ebp
	cmpl	%ebp, %esi
	cmovgel	%esi, %ebp
	leal	8(%r9), %ecx
	subl	116(%rsp), %ecx         # 4-byte Folded Reload
	andl	$-32, %r10d
	addl	$64, %r10d
	imull	%ecx, %r10d
	movslq	1472(%rsp), %rcx        # 4-byte Folded Reload
	leaq	(%rcx,%rdx), %r8
	movslq	%r11d, %rsi
	movslq	%edi, %rdi
	leaq	(%rsi,%rdx), %r11
	leaq	(%rdi,%rdx), %r14
	movq	1424(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movl	%eax, %edi
	shll	$10, %edi
	leal	(%rdi,%r10), %edi
	leal	(%rdi,%rbp,8), %ecx
	movq	%rcx, 1016(%rsp)        # 8-byte Spill
	shll	$9, %eax
	leal	(%rax,%r10), %eax
	leal	(%rax,%rbp,8), %eax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	leal	(%r10,%rbp,8), %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movq	944(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rbp,8), %r10d
	movq	%r10, 816(%rsp)         # 8-byte Spill
	leal	-3(%rax,%rbp,8), %ecx
	movq	%rcx, 800(%rsp)         # 8-byte Spill
	leal	-7(%rax,%rbp,8), %ecx
	movq	%rcx, 784(%rsp)         # 8-byte Spill
	leal	-5(%rax,%rbp,8), %ecx
	movq	%rcx, 768(%rsp)         # 8-byte Spill
	leal	-8(%rax,%rbp,8), %ecx
	movq	%rcx, 752(%rsp)         # 8-byte Spill
	leal	-4(%rax,%rbp,8), %ecx
	movq	%rcx, 736(%rsp)         # 8-byte Spill
	leal	-6(%rax,%rbp,8), %eax
	movq	%rax, 944(%rsp)         # 8-byte Spill
	subl	%ebp, %ebx
	movl	%ebx, 984(%rsp)         # 4-byte Spill
	vpbroadcastd	1376(%rsp), %xmm1 # 16-byte Folded Reload
	vmovdqa	%xmm1, 720(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 704(%rsp)        # 16-byte Spill
	movl	$-3, %eax
	subl	%r15d, %eax
	movl	$-7, %edi
	subl	%r15d, %edi
	movl	$-8, %ebp
	subl	%r15d, %ebp
	movl	$-4, %ebx
	subl	%r15d, %ebx
	movl	$-6, %ecx
	subl	%r15d, %ecx
	movl	$-5, %edx
	subl	%r15d, %edx
	vmovd	%r15d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 448(%rsp)        # 16-byte Spill
	movq	144(%rsp), %rsi         # 8-byte Reload
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 688(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 672(%rsp)        # 16-byte Spill
	movq	960(%rsp), %rsi         # 8-byte Reload
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm10
	vmovdqa	%xmm10, 464(%rsp)       # 16-byte Spill
	movl	1296(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 656(%rsp)        # 16-byte Spill
	vbroadcastss	1360(%rsp), %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm0
	vmovdqa	%xmm0, 624(%rsp)        # 16-byte Spill
	movl	1440(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 400(%rsp)        # 16-byte Spill
	movl	1456(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 384(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm7, %xmm0
	vmovaps	%xmm0, 352(%rsp)        # 16-byte Spill
	leal	(%rax,%r10), %eax
	movq	%rax, 608(%rsp)         # 8-byte Spill
	leal	(%rdi,%r10), %eax
	movq	%rax, 592(%rsp)         # 8-byte Spill
	leal	(%rbp,%r10), %eax
	movq	%rax, 576(%rsp)         # 8-byte Spill
	leal	(%rbx,%r10), %eax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	leal	(%rcx,%r10), %eax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	leal	(%rdx,%r10), %eax
	movq	%rax, 528(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 512(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r11,4), %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vbroadcastss	(%rax,%r14,4), %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 496(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_3(%rip), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_4(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_5(%rip), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_6(%rip), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB159_74:                             # %for dh.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	testl	%r12d, %r12d
	setne	1424(%rsp)              # 1-byte Folded Spill
	sete	1352(%rsp)              # 1-byte Folded Spill
	movq	816(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %ecx
	movl	%ecx, %eax
	andl	$1, %eax
	movl	%eax, 1440(%rsp)        # 4-byte Spill
	sete	1312(%rsp)              # 1-byte Folded Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI159_2(%rip), %xmm15 # xmm15 = [0,2,4,6]
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	720(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %ebx
	cltd
	idivl	%ebx
	movl	%edx, 1280(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %ebp
	cltd
	idivl	%ebp
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r9d
	cltd
	idivl	%r9d
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 1296(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r12d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r10d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r11d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %esi
	vmovd	%r12d, %xmm0
	vpinsrd	$1, 1296(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r10d, %xmm0, %xmm0
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm15, %xmm2, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%ebx
	movl	%ebx, %r10d
	movl	%edx, %ebx
	vpinsrd	$3, %r15d, %xmm0, %xmm0
	movl	%ecx, %r15d
	movq	944(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm4
	vmovd	%xmm3, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	vmovd	%r11d, %xmm2
	vpinsrd	$1, %r8d, %xmm2, %xmm2
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%edi
	movl	%edi, %r8d
	vpinsrd	$2, %r14d, %xmm2, %xmm2
	vpinsrd	$3, %esi, %xmm2, %xmm5
	vpsrad	$31, %xmm0, %xmm2
	vmovdqa	496(%rsp), %xmm8        # 16-byte Reload
	vpand	%xmm8, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovd	%r15d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	1040(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm6
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vmovdqa	704(%rsp), %xmm11       # 16-byte Reload
	vpminsd	%xmm11, %xmm4, %xmm4
	vmovdqa	448(%rsp), %xmm12       # 16-byte Reload
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vmovdqa	688(%rsp), %xmm13       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm13, %xmm7
	vmovdqa	672(%rsp), %xmm9        # 16-byte Reload
	vpsubd	%xmm0, %xmm9, %xmm1
	vblendvps	%xmm7, %xmm0, %xmm1, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vblendvps	%xmm6, %xmm4, %xmm0, %xmm0
	vmovdqa	1376(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm14
	vmovdqa	%xmm1, %xmm0
	vmovd	%ecx, %xmm1
	vpsrad	$31, %xmm5, %xmm4
	vpand	%xmm8, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm4, %xmm4
	vpinsrd	$1, %ebx, %xmm1, %xmm1
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm3
	vpand	%xmm8, %xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm3
	vmovdqa	1024(%rsp), %xmm1       # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	movq	736(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpminsd	%xmm11, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpcmpgtd	%xmm4, %xmm13, %xmm6
	vpsubd	%xmm4, %xmm9, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm5, %xmm4, %xmm1
	vpmulld	%xmm0, %xmm1, %xmm4
	vmovdqa	%xmm0, %xmm7
	vmovdqa	%xmm4, 1296(%rsp)       # 16-byte Spill
	vpsubd	%xmm10, %xmm14, %xmm0
	vmovdqa	%xmm0, 1056(%rsp)       # 16-byte Spill
	vpaddd	960(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	movq	992(%rsp), %rbx         # 8-byte Reload
	vmovss	(%rbx,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
	vmovdqa	624(%rsp), %xmm14       # 16-byte Reload
	vpaddd	%xmm4, %xmm14, %xmm0
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rax
	vmovss	(%rbx,%rax,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rax
	sarq	$32, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm0 # xmm0 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovdqa	928(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm1, %xmm1
	movq	752(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm15, %xmm4, %xmm4
	vpminsd	%xmm11, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpcmpgtd	%xmm3, %xmm13, %xmm5
	vpsubd	%xmm3, %xmm9, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm11, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm1
	vpmulld	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm7, %xmm5
	vmovdqa	%xmm1, 1184(%rsp)       # 16-byte Spill
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm14, %xmm0
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%rbx,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rax,4), %xmm3, %xmm0 # xmm0 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	848(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	1568(%rsp), %r14        # 8-byte Reload
	vmovups	12296(%r14,%rax,4), %xmm1
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vmovups	12312(%r14,%rax,4), %xmm4
	vmovups	12304(%r14,%rax,4), %xmm1
	vmovaps	%xmm1, 1136(%rsp)       # 16-byte Spill
	vmovups	12320(%r14,%rax,4), %xmm1
	vmovaps	%xmm1, 1088(%rsp)       # 16-byte Spill
	sarq	$32, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmovups	12288(%r14,%rax,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %esi
	vmovd	1200(%rsp), %xmm6       # 4-byte Folded Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpinsrd	$1, 1280(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vpinsrd	$2, 1248(%rsp), %xmm6, %xmm6 # 4-byte Folded Reload
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$3, 1216(%rsp), %xmm6, %xmm0 # 4-byte Folded Reload
	vmovd	%esi, %xmm6
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vpinsrd	$1, %ecx, %xmm6, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm6
	vpand	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm0, %xmm6, %xmm0
	vpcmpgtd	%xmm1, %xmm13, %xmm6
	vpsubd	%xmm1, %xmm9, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vmovdqa	912(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	movq	768(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm15, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm6, %xmm7, %xmm1, %xmm1
	vpmulld	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm1, 1248(%rsp)       # 16-byte Spill
	vpaddd	%xmm1, %xmm14, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movq	784(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r13), %edx
	vpcmpgtd	%xmm0, %xmm13, %xmm1
	vpsubd	%xmm0, %xmm9, %xmm6
	vblendvps	%xmm1, %xmm0, %xmm6, %xmm0
	vmovd	%edx, %xmm1
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovdqa	896(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm6
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm11, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vblendvps	%xmm6, %xmm1, %xmm0, %xmm0
	vmovss	(%rbx,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],mem[0]
	vpmulld	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 1200(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm14, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1280(%rsp)       # 16-byte Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	%ebp
	movl	%edx, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vmovd	%esi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm7
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm1, %xmm7, %xmm1
	vpcmpgtd	%xmm1, %xmm13, %xmm7
	vpsubd	%xmm1, %xmm9, %xmm13
	vblendvps	%xmm7, %xmm1, %xmm13, %xmm1
	vmovdqa	880(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm2, %xmm3, %xmm2
	movq	800(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm15, %xmm7, %xmm7
	vpminsd	%xmm11, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vpaddd	%xmm12, %xmm1, %xmm1
	vpminsd	%xmm11, %xmm1, %xmm1
	vpmaxsd	%xmm12, %xmm1, %xmm1
	vblendvps	%xmm2, %xmm7, %xmm1, %xmm10
	vmovaps	512(%rsp), %xmm15       # 16-byte Reload
	vmulps	1168(%rsp), %xmm15, %xmm2 # 16-byte Folded Reload
	vmovaps	%xmm4, %xmm0
	vmovaps	1072(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm0, %xmm1, %xmm7 # xmm7 = xmm1[0,2],xmm0[0,2]
	vmovaps	640(%rsp), %xmm8        # 16-byte Reload
	vsubps	%xmm8, %xmm7, %xmm7
	vmovaps	656(%rsp), %xmm9        # 16-byte Reload
	vmulps	%xmm7, %xmm9, %xmm7
	vmulps	%xmm7, %xmm2, %xmm13
	vmulps	1152(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1136(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm12, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm12[0,2]
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	1120(%rsp), %xmm15, %xmm11 # 16-byte Folded Reload
	vmovaps	1104(%rsp), %xmm7       # 16-byte Reload
	vshufps	$136, %xmm5, %xmm7, %xmm4 # xmm4 = xmm7[0,2],xmm5[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmovaps	1456(%rsp), %xmm11      # 16-byte Reload
	vminps	%xmm11, %xmm13, %xmm2
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm2, %xmm2
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm2, %xmm3, %xmm3
	vminps	%xmm11, %xmm4, %xmm4
	vmaxps	%xmm13, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm0, %xmm1, %xmm4 # xmm4 = xmm1[1,3],xmm0[1,3]
	vmulps	%xmm6, %xmm15, %xmm6
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm6, %xmm4, %xmm4
	vmulps	1280(%rsp), %xmm15, %xmm0 # 16-byte Folded Reload
	vshufps	$221, %xmm5, %xmm7, %xmm6 # xmm6 = xmm7[1,3],xmm5[1,3]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vpmulld	1376(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
	vpaddd	%xmm10, %xmm14, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	vshufps	$221, %xmm12, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm12[1,3]
	vmovss	(%rbx,%rdx,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	sarq	$32, %rax
	vinsertps	$48, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movl	488(%rsp), %r12d        # 4-byte Reload
	movl	%r12d, %eax
	andl	%r15d, %eax
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	vandps	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm3, 1280(%rsp)       # 16-byte Spill
	vandps	%xmm7, %xmm2, %xmm2
	vaddps	%xmm2, %xmm3, %xmm2
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm3
	vminps	%xmm11, %xmm4, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm4
	vmulps	%xmm5, %xmm15, %xmm0
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm0, %xmm1, %xmm1
	vsubps	%xmm3, %xmm4, %xmm0
	vsubps	%xmm4, %xmm3, %xmm3
	vandps	%xmm7, %xmm3, %xmm3
	vminps	%xmm11, %xmm1, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm4, %xmm1, %xmm1
	vandps	%xmm7, %xmm1, %xmm5
	vmulps	1536(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	%ymm1, 1216(%rsp)       # 32-byte Spill
	vmovaps	%xmm1, %xmm12
	jne	.LBB159_76
# BB#75:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vxorps	%xmm12, %xmm12, %xmm12
.LBB159_76:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, %rcx
	vmovups	(%r14,%rax,4), %xmm6
	movq	%rax, %rdx
	vmovups	32(%r14,%rax,4), %xmm9
	orq	$2, %rax
	vmovups	(%r14,%rax,4), %xmm8
	vandps	%xmm7, %xmm0, %xmm2
	vaddps	%xmm5, %xmm3, %xmm14
	orq	$6, %rcx
	vmovups	(%r14,%rcx,4), %xmm3
	vmovdqa	464(%rsp), %xmm15       # 16-byte Reload
	vmovdqa	1184(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm15, %xmm1, %xmm11
	orq	$4, %rdx
	vmovups	(%r14,%rdx,4), %xmm13
	vmovdqa	1296(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm15, %xmm1, %xmm1
	movb	1424(%rsp), %al         # 1-byte Reload
	andb	1312(%rsp), %al         # 1-byte Folded Reload
	movb	%al, %dil
	jne	.LBB159_77
# BB#78:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovaps	%xmm3, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1152(%rsp)       # 16-byte Spill
	vmovdqa	%xmm11, 1184(%rsp)      # 16-byte Spill
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	jmp	.LBB159_79
	.align	16, 0x90
.LBB159_77:                             #   in Loop: Header=BB159_74 Depth=1
	vmovaps	%xmm9, %xmm12
	vmovdqa	%xmm1, %xmm9
	vmovdqa	%xmm9, 1296(%rsp)       # 16-byte Spill
	vmovdqa	432(%rsp), %xmm1        # 16-byte Reload
	vpaddd	1056(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	336(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm6, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, %xmm4
	vmovaps	%xmm4, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, %xmm3
	vmovaps	%xmm3, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, %xmm8
	vshufps	$136, %xmm4, %xmm3, %xmm7 # xmm7 = xmm3[0,2],xmm4[0,2]
	vmovaps	400(%rsp), %xmm3        # 16-byte Reload
	vsubps	%xmm3, %xmm7, %xmm7
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovaps	416(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm7
	vmulps	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm1, %xmm11, %xmm7
	vmovdqa	%xmm11, 1184(%rsp)      # 16-byte Spill
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vshufps	$136, %xmm13, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm13[0,2]
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm2, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vpaddd	%xmm1, %xmm9, %xmm7
	vmovaps	%xmm12, %xmm9
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmulps	%xmm7, %xmm5, %xmm7
	vshufps	$136, %xmm9, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm9[0,2]
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	%xmm8, %xmm7
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm4, %xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm4, %xmm4
	vminps	%xmm2, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vaddps	%xmm1, %xmm4, %xmm1
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vmovaps	1424(%rsp), %xmm2       # 16-byte Reload
	vandps	%xmm7, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vmovaps	1536(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm0, %xmm12
	vmovaps	%xmm1, %xmm0
.LBB159_79:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovdqa	1248(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	1200(%rsp), %xmm3       # 16-byte Reload
	vmovdqa	%xmm15, %xmm11
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmulps	%xmm0, %xmm14, %xmm0
	vmovaps	%xmm0, %xmm8
	testb	%dil, %dil
	jne	.LBB159_81
# BB#80:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vxorps	%xmm8, %xmm8, %xmm8
.LBB159_81:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vpsubd	%xmm11, %xmm1, %xmm5
	vpsubd	%xmm11, %xmm3, %xmm4
	vpsubd	%xmm11, %xmm10, %xmm3
	movl	%r12d, %eax
	andl	%r15d, %eax
	jne	.LBB159_82
# BB#83:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovdqa	%xmm5, 1168(%rsp)       # 16-byte Spill
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vmovdqa	%xmm3, 1248(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovups	%ymm0, 1312(%rsp)       # 32-byte Spill
	vmovaps	%xmm7, 1472(%rsp)       # 16-byte Spill
	jmp	.LBB159_84
	.align	16, 0x90
.LBB159_82:                             #   in Loop: Header=BB159_74 Depth=1
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovups	%ymm0, 1312(%rsp)       # 32-byte Spill
	vmovdqa	432(%rsp), %xmm1        # 16-byte Reload
	vpaddd	%xmm1, %xmm5, %xmm0
	vmovdqa	%xmm5, 1168(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	%xmm1, %xmm4, %xmm0
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	%xmm1, %xmm3, %xmm0
	vmovdqa	%xmm3, 1248(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %r9
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	1136(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1120(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovss	(%rbx,%r8,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	336(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	400(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	416(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%rbx,%r9,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	1152(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm13[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm9, %xmm13, %xmm3 # xmm3 = xmm13[1,3],xmm9[1,3]
	vmovss	(%rbx,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm5, %xmm4
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm2, %xmm2
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm1, %xmm0
	vandps	%xmm7, %xmm0, %xmm0
	vmovaps	%xmm7, 1472(%rsp)       # 16-byte Spill
	vaddps	1280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1536(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
.LBB159_84:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovups	1216(%rsp), %ymm2       # 32-byte Reload
	movq	1016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	vmovups	24584(%r14,%rax,4), %xmm9
	vmovups	24600(%r14,%rax,4), %xmm15
	vmovups	24576(%r14,%rax,4), %xmm13
	vmovups	24592(%r14,%rax,4), %xmm14
	vmovups	24608(%r14,%rax,4), %xmm10
	movb	1352(%rsp), %r14b       # 1-byte Reload
	movb	%r14b, %al
	movl	1440(%rsp), %ecx        # 4-byte Reload
	andb	%cl, %al
	je	.LBB159_86
# BB#85:                                #   in Loop: Header=BB159_74 Depth=1
	vmovdqa	384(%rsp), %xmm7        # 16-byte Reload
	vpaddd	1056(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%ymm2, %ymm12
	vmovaps	320(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vshufps	$136, %xmm15, %xmm9, %xmm1 # xmm1 = xmm9[0,2],xmm15[0,2]
	vmovaps	352(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	368(%rsp), %xmm3        # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vpaddd	1184(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm2, %xmm1
	vshufps	$136, %xmm14, %xmm13, %xmm5 # xmm5 = xmm13[0,2],xmm14[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmulps	%xmm5, %xmm1, %xmm1
	vpaddd	1296(%rsp), %xmm7, %xmm5 # 16-byte Folded Reload
	vpextrq	$1, %xmm5, %rax
	vmovq	%xmm5, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%xmm5, %xmm2, %xmm5
	vshufps	$136, %xmm10, %xmm14, %xmm7 # xmm7 = xmm14[0,2],xmm10[0,2]
	vsubps	%xmm6, %xmm7, %xmm7
	vmulps	%xmm7, %xmm3, %xmm7
	vmulps	%xmm7, %xmm5, %xmm5
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm1, %xmm1
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vminps	%xmm2, %xmm5, %xmm5
	vmaxps	%xmm3, %xmm5, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vmovaps	%ymm12, %ymm2
	vandps	1472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1536(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
.LBB159_86:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	movq	1560(%rsp), %rcx        # 8-byte Reload
	movl	%r15d, %eax
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB159_88
# BB#87:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovaps	%xmm12, %xmm2
.LBB159_88:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	testl	%eax, %eax
	jne	.LBB159_90
# BB#89:                                #   in Loop: Header=BB159_74 Depth=1
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpaddd	1168(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	1200(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	1248(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r10
	sarq	$32, %rdx
	movslq	%ebp, %r9
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vshufps	$221, %xmm15, %xmm9, %xmm0 # xmm0 = xmm9[1,3],xmm15[1,3]
	vmovss	(%rbx,%r8,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	320(%rsp), %xmm7        # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	352(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	368(%rsp), %xmm5        # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm1, %xmm0, %xmm0
	vmovss	(%rbx,%r9,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	%ymm2, %ymm8
	vshufps	$221, %xmm14, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm14[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r11d, %rdx
	sarq	$32, %r11
	vshufps	$221, %xmm10, %xmm14, %xmm2 # xmm2 = xmm14[1,3],xmm10[1,3]
	vmovss	(%rbx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rbx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rbx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm7, %xmm3
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vmovaps	1456(%rsp), %xmm3       # 16-byte Reload
	vminps	%xmm3, %xmm0, %xmm0
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm0, %xmm0
	vminps	%xmm3, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vminps	%xmm3, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vmovaps	%ymm8, %ymm2
	vandps	1472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1536(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
.LBB159_90:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovups	1312(%rsp), %ymm1       # 32-byte Reload
	vmovdqa	%xmm11, %xmm10
	movl	1440(%rsp), %eax        # 4-byte Reload
	andb	%al, %r14b
	jne	.LBB159_92
# BB#91:                                # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovaps	%xmm8, %xmm1
.LBB159_92:                             # %for dh.s0.v10.v1012
                                        #   in Loop: Header=BB159_74 Depth=1
	vmovaps	.LCPI159_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI159_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm2, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r15d, %rax
	movq	864(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1528(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, 984(%rsp)          # 4-byte Folded Spill
	jne	.LBB159_74
	jmp	.LBB159_109
.LBB159_30:                             # %false_bb2
	addl	$51, %edi
	sarl	$3, %edi
	movq	%rdi, 1536(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB159_109
# BB#31:                                # %for dh.s0.v10.v1016.preheader
	movl	$2, %eax
	movq	96(%rsp), %r14          # 8-byte Reload
	subl	%r14d, %eax
	movq	128(%rsp), %rbx         # 8-byte Reload
	leal	(%rbx,%rbx), %esi
	movl	%esi, 1424(%rsp)        # 4-byte Spill
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andnl	%esi, %ecx, %esi
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rbx,%rbx), %edx
	movl	%edx, 1472(%rsp)        # 4-byte Spill
	subl	%eax, %edx
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	addl	%r14d, %edx
	leal	-1(%r14,%rbx), %r8d
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%r14d, %edx
	cmovll	%r14d, %edx
	movl	%edx, 1376(%rsp)        # 4-byte Spill
	leal	(%r14,%rbx), %r12d
	cmpl	$3, %r12d
	movl	$2, %eax
	cmovll	%r8d, %eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	cmpl	$3, %r12d
	cmovll	%edx, %eax
	movl	%eax, 1360(%rsp)        # 4-byte Spill
	movq	64(%rsp), %r15          # 8-byte Reload
	leal	(%r15,%r15), %edi
	movl	%edi, 1440(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	movl	%r15d, %esi
	sarl	$31, %esi
	andnl	%edi, %esi, %r10d
	andl	%eax, %esi
	movl	$2, %eax
	movq	104(%rsp), %rbp         # 8-byte Reload
	subl	%ebp, %eax
	cltd
	idivl	%edi
	orl	%r10d, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r15,%r15), %r11d
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%ebp, %edx
	leal	-1(%rbp,%r15), %edi
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, 1352(%rsp)        # 4-byte Spill
	leal	(%rbp,%r15), %r13d
	cmpl	$3, %r13d
	movl	$2, %eax
	cmovll	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	cmpl	$3, %r13d
	cmovll	%edx, %eax
	movl	%eax, 1312(%rsp)        # 4-byte Spill
	movl	%r14d, %eax
	negl	%eax
	cltd
	idivl	1424(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	1472(%rsp), %edx        # 4-byte Reload
	subl	%eax, %edx
	cmpl	%eax, %ebx
	cmovgl	%eax, %edx
	addl	%r14d, %edx
	cmpl	%edx, %r8d
	cmovlel	%r8d, %edx
	cmpl	%r14d, %edx
	cmovll	%r14d, %edx
	movl	%edx, 1296(%rsp)        # 4-byte Spill
	xorl	%r10d, %r10d
	testl	%r12d, %r12d
	movl	$0, %eax
	cmovlel	%r8d, %eax
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	testl	%r12d, %r12d
	cmovlel	%edx, %eax
	movl	%eax, 1280(%rsp)        # 4-byte Spill
	movl	%ebp, %eax
	negl	%eax
	cltd
	idivl	1440(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	%eax, %r15d
	cmovgl	%eax, %edx
	addl	%ebp, %edx
	cmpl	%edx, %edi
	cmovlel	%edi, %edx
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	testl	%r13d, %r13d
	movl	$0, %eax
	cmovlel	%edi, %eax
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	testl	%r13d, %r13d
	cmovlel	%edx, %eax
	movl	%eax, 1216(%rsp)        # 4-byte Spill
	movl	$1, %eax
	subl	%r14d, %eax
	cltd
	idivl	1424(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	1472(%rsp), %ecx        # 4-byte Reload
	subl	%eax, %ecx
	cmpl	%eax, %ebx
	cmovgl	%eax, %ecx
	addl	%r14d, %ecx
	cmpl	%ecx, %r8d
	cmovlel	%r8d, %ecx
	cmpl	%r14d, %ecx
	cmovll	%r14d, %ecx
	movl	%ecx, 1472(%rsp)        # 4-byte Spill
	cmpl	$1, %r12d
	setg	%al
	cmpl	$2, %r12d
	cmovgel	%r10d, %r8d
	movzbl	%al, %eax
	orl	%eax, %r8d
	cmpl	%r14d, %r8d
	cmovll	%r14d, %r8d
	cmpl	$2, %r12d
	cmovll	%ecx, %r8d
	movl	$1, %eax
	subl	%ebp, %eax
	cltd
	idivl	1440(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	subl	%eax, %r11d
	cmpl	%eax, %r15d
	cmovgl	%eax, %r11d
	addl	%ebp, %r11d
	cmpl	%r11d, %edi
	cmovlel	%edi, %r11d
	cmpl	%ebp, %r11d
	cmovll	%ebp, %r11d
	cmpl	$1, %r13d
	setg	%al
	cmpl	$2, %r13d
	cmovgel	%r10d, %edi
	movzbl	%al, %eax
	orl	%eax, %edi
	cmpl	%ebp, %edi
	cmovll	%ebp, %edi
	cmpl	$2, %r13d
	cmovll	%r11d, %edi
	movl	%edi, 1440(%rsp)        # 4-byte Spill
	movq	1560(%rsp), %rdi        # 8-byte Reload
	movl	%edi, %eax
	movq	88(%rsp), %rbx          # 8-byte Reload
	subl	%ebx, %eax
	movq	1456(%rsp), %rsi        # 8-byte Reload
	leal	(%rsi,%rsi), %ecx
	cltd
	idivl	%ecx
	movl	%esi, %eax
	movq	%rsi, %r13
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%ecx, %esi
	addl	%edx, %esi
	movq	152(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movq	%rax, 984(%rsp)         # 8-byte Spill
	movl	%edi, %r12d
	andl	$1, %r12d
	movl	%r12d, 352(%rsp)        # 4-byte Spill
	leal	-1(%rbx,%r13), %ecx
	cmpl	%edi, %ecx
	movl	%ecx, %edx
	cmovgl	%edi, %edx
	movq	%rdi, %r15
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movq	%r13, %rdi
	leal	-1(%rdi,%rdi), %eax
	subl	%esi, %eax
	cmpl	%esi, %edi
	cmovgl	%esi, %eax
	addl	%ebx, %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	leal	(%rbx,%rdi), %ecx
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	%r15d, %ecx
	cmovgl	%edx, %eax
	movq	136(%rsp), %r13         # 8-byte Reload
	movq	144(%rsp), %rdi         # 8-byte Reload
	leal	6(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm11
	movq	80(%rsp), %rdx          # 8-byte Reload
	imull	%edx, %ebx
	addl	%r14d, %ebx
	movq	1016(%rsp), %r15        # 8-byte Reload
	movl	%r15d, %ecx
	sarl	$5, %ecx
	movq	%rcx, 1456(%rsp)        # 8-byte Spill
	cmpl	$1, %ebp
	movl	1440(%rsp), %ecx        # 4-byte Reload
	cmovgl	%r11d, %ecx
	movl	%ecx, 1440(%rsp)        # 4-byte Spill
	vmovss	.LCPI159_0(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm4, %xmm0
	vmulss	%xmm6, %xmm0, %xmm1
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm12
	vsubss	%xmm6, %xmm10, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	movq	960(%rsp), %r11         # 8-byte Reload
	vmovd	%r11d, %xmm10
	imull	%r13d, %r11d
	addl	%ebp, %r11d
	vmovd	%r11d, %xmm0
	vmovd	%ecx, %xmm5
	vpsubd	%xmm0, %xmm5, %xmm15
	vsubss	%xmm13, %xmm4, %xmm0
	vmulss	%xmm7, %xmm0, %xmm6
	vdivss	%xmm3, %xmm6, %xmm6
	vaddss	%xmm6, %xmm13, %xmm13
	leal	6(%r13), %ecx
	vmovd	%ecx, %xmm6
	vsubss	%xmm7, %xmm14, %xmm1
	leal	4(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm2
	vmulss	%xmm1, %xmm0, %xmm0
	leal	4(%r13), %ecx
	vmovd	%ecx, %xmm1
	vdivss	%xmm0, %xmm3, %xmm14
	vsubss	%xmm8, %xmm4, %xmm3
	vmovss	72(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm3, %xmm4
	vdivss	%xmm9, %xmm4, %xmm4
	vaddss	%xmm4, %xmm8, %xmm8
	leal	8(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm5
	vmovss	76(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm0, %xmm7
	leal	8(%r13), %ecx
	vmovd	%ecx, %xmm0
	vmulss	%xmm7, %xmm3, %xmm3
	leal	5(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm11, %xmm4
	vdivss	%xmm3, %xmm9, %xmm9
	vmovdqa	.LCPI159_1(%rip), %xmm3 # xmm3 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	%xmm4, 960(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm4
	vmovdqa	%xmm4, 944(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm3, %xmm2, %xmm2
	vmovdqa	%xmm2, 928(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	%xmm1, 912(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vmovdqa	%xmm1, 896(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 880(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm7, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 864(%rsp)        # 16-byte Spill
	leal	5(%r13), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 848(%rsp)        # 16-byte Spill
	leal	7(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 832(%rsp)        # 16-byte Spill
	leal	7(%r13), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 816(%rsp)        # 16-byte Spill
	leal	3(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 800(%rsp)        # 16-byte Spill
	leal	3(%r13), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vmovdqa	%xmm0, 784(%rsp)        # 16-byte Spill
	movslq	%eax, %rsi
	imulq	%rdx, %rsi
	cmpl	$1, %r14d
	cmovgl	1472(%rsp), %r8d        # 4-byte Folded Reload
	movslq	%ebx, %rcx
	movslq	%r8d, %r8
	subq	%rcx, %r8
	addq	%rsi, %r8
	testl	%ebp, %ebp
	movl	1216(%rsp), %edx        # 4-byte Reload
	cmovgl	1248(%rsp), %edx        # 4-byte Folded Reload
	testl	%r14d, %r14d
	movl	1280(%rsp), %eax        # 4-byte Reload
	cmovgl	1296(%rsp), %eax        # 4-byte Folded Reload
	subq	%rcx, %rsi
	cmpl	$2, %ebp
	movl	1312(%rsp), %ebx        # 4-byte Reload
	cmovgl	1352(%rsp), %ebx        # 4-byte Folded Reload
	cmpl	$2, %r14d
	movl	1360(%rsp), %ebp        # 4-byte Reload
	cmovgl	1376(%rsp), %ebp        # 4-byte Folded Reload
	leal	(%rdi,%rdi), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm1
	vmovdqa	%xmm1, 768(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm0
	vmovaps	%xmm0, 752(%rsp)        # 16-byte Spill
	vmovd	%r13d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 336(%rsp)        # 16-byte Spill
	vmovd	%edi, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 736(%rsp)        # 16-byte Spill
	leal	-1(%r13,%rdi), %ecx
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 720(%rsp)        # 16-byte Spill
	vmovd	%r11d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	movl	1440(%rsp), %ecx        # 4-byte Reload
	vmovd	%ecx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 704(%rsp)        # 16-byte Spill
	vbroadcastss	1424(%rsp), %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 688(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm12, %xmm0
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm0
	vmovdqa	%xmm0, 1424(%rsp)       # 16-byte Spill
	movslq	%eax, %rcx
	movslq	%ebp, %rdi
	leaq	(%rcx,%rsi), %rcx
	leaq	(%rdi,%rsi), %rsi
	vmovd	%edx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 320(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm14, %xmm0
	vmovaps	%xmm0, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 288(%rsp)        # 16-byte Spill
	vmovd	%ebx, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 272(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm9, %xmm0
	vmovaps	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm8, %xmm0
	vmovaps	%xmm0, 240(%rsp)        # 16-byte Spill
	movq	1456(%rsp), %rbx        # 8-byte Reload
	movslq	%ebx, %rdi
	shlq	$5, %rdi
	addq	$48, %rdi
	movq	1560(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edx
	andl	$63, %edx
	imulq	%rdi, %rdx
	movq	152(%rsp), %rbp         # 8-byte Reload
	movq	%rbp, %rdi
	sarq	$63, %rdi
	andq	%rbp, %rdi
	leal	8(%rax), %ebp
	subl	116(%rsp), %ebp         # 4-byte Folded Reload
	andl	$-32, %r15d
	addl	$64, %r15d
	imull	%ebp, %r15d
	movq	%r15, 1016(%rsp)        # 8-byte Spill
	subq	%rdi, %rdx
	movq	%rdx, 656(%rsp)         # 8-byte Spill
	leal	(%rbx,%rbx,2), %edx
	movl	%edx, %edi
	shll	$10, %edi
	leal	(%rdi,%r15), %edi
	movq	%rdi, 640(%rsp)         # 8-byte Spill
	shll	$9, %edx
	leal	(%rdx,%r15), %edx
	movq	%rdx, 624(%rsp)         # 8-byte Spill
	movq	984(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	subl	%r13d, %edx
	leal	-3(%rdx), %edi
	movq	%rdi, 592(%rsp)         # 8-byte Spill
	leal	-7(%rdx), %edi
	movq	%rdi, 576(%rsp)         # 8-byte Spill
	leal	-8(%rdx), %edi
	movq	%rdi, 560(%rsp)         # 8-byte Spill
	leal	-4(%rdx), %edi
	movq	%rdi, 544(%rsp)         # 8-byte Spill
	leal	-6(%rdx), %edi
	movq	%rdi, 528(%rsp)         # 8-byte Spill
	addl	$-5, %edx
	movq	%rdx, 608(%rsp)         # 8-byte Spill
	leal	-3(%rax), %edx
	movq	%rdx, 512(%rsp)         # 8-byte Spill
	leal	-7(%rax), %edx
	movq	%rdx, 496(%rsp)         # 8-byte Spill
	leal	-5(%rax), %edx
	movq	%rdx, 488(%rsp)         # 8-byte Spill
	leal	-8(%rax), %edx
	movq	%rdx, 464(%rsp)         # 8-byte Spill
	leal	-4(%rax), %edx
	movq	%rdx, 448(%rsp)         # 8-byte Spill
	leal	-6(%rax), %edx
	movq	%rdx, 432(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rdx         # 8-byte Reload
	vbroadcastss	(%rdx,%r8,4), %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdx,%rcx,4), %xmm0
	vmovaps	%xmm0, 224(%rsp)        # 16-byte Spill
	vbroadcastss	(%rdx,%rsi,4), %xmm0
	vmovaps	%xmm0, 208(%rsp)        # 16-byte Spill
	vpabsd	%xmm1, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_3(%rip), %xmm0
	vmovaps	%xmm0, 368(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI159_4(%rip), %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_5(%rip), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI159_6(%rip), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	.align	16, 0x90
.LBB159_32:                             # %for dh.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	testl	%r12d, %r12d
	setne	1312(%rsp)              # 1-byte Folded Spill
	sete	1352(%rsp)              # 1-byte Folded Spill
	movq	984(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 1024(%rsp)        # 4-byte Spill
	andl	$1, %eax
	movl	%eax, 1456(%rsp)        # 4-byte Spill
	sete	1296(%rsp)              # 1-byte Folded Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI159_2(%rip), %xmm10 # xmm10 = [0,2,4,6]
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	768(%rsp), %xmm1        # 16-byte Reload
	vpextrd	$1, %xmm1, %r13d
	cltd
	idivl	%r13d
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	vmovd	%xmm1, %r8d
	cltd
	idivl	%r8d
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm1, %edi
	cltd
	idivl	%edi
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm1, %r11d
	cltd
	idivl	%r11d
	movl	%edx, 1184(%rsp)        # 4-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r14d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm10, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	vmovd	%r15d, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	vpinsrd	$1, %r14d, %xmm1, %xmm0
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	vpinsrd	$3, %ebp, %xmm0, %xmm0
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ebp
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vmovd	%esi, %xmm3
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpinsrd	$2, %ebx, %xmm3, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%edi
	movl	%edi, %r14d
	movl	%edx, %ecx
	vpinsrd	$3, %r15d, %xmm3, %xmm3
	vmovd	%esi, %xmm4
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ebp, %xmm4, %xmm4
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	400(%rsp), %xmm13       # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movl	1024(%rsp), %ebx        # 4-byte Reload
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	960(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpcmpeqd	%xmm6, %xmm6, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	944(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm6, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vmovdqa	736(%rsp), %xmm9        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm7
	vmovdqa	384(%rsp), %xmm15       # 16-byte Reload
	vpsubd	%xmm0, %xmm15, %xmm6
	vblendvps	%xmm7, %xmm0, %xmm6, %xmm0
	vmovdqa	336(%rsp), %xmm12       # 16-byte Reload
	vpaddd	%xmm12, %xmm0, %xmm0
	vmovdqa	720(%rsp), %xmm14       # 16-byte Reload
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm10, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vblendvps	%xmm5, %xmm0, %xmm2, %xmm0
	vmovdqa	752(%rsp), %xmm11       # 16-byte Reload
	vpmulld	%xmm11, %xmm0, %xmm2
	vpsrad	$31, %xmm3, %xmm0
	vpand	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm3
	vpinsrd	$2, %ecx, %xmm4, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	928(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpcmpeqd	%xmm7, %xmm7, %xmm7
	vmovdqa	912(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm3, %xmm9, %xmm5
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm5, %xmm3, %xmm6, %xmm3
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpminsd	%xmm14, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpmulld	%xmm11, %xmm3, %xmm4
	vmovdqa	%xmm4, 1280(%rsp)       # 16-byte Spill
	vpsubd	1376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovdqa	%xmm2, 992(%rsp)        # 16-byte Spill
	vpaddd	704(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovq	%xmm2, %rax
	movslq	%eax, %rcx
	vmovss	(%r9,%rcx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%ecx, %rax
	sarq	$32, %rcx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vmovdqa	1424(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm4, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm8
	vmovq	%xmm2, %rcx
	movslq	%ecx, %rax
	vmovss	(%r9,%rax,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rax
	sarq	$32, %rcx
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm2 # xmm2 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	sarq	$32, %rax
	vinsertps	$32, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovdqa	896(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm3, %xmm3
	vpxor	%xmm7, %xmm3, %xmm3
	vmovdqa	880(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpcmpgtd	%xmm0, %xmm9, %xmm4
	vpsubd	%xmm0, %xmm15, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r10), %ecx
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpmulld	%xmm11, %xmm0, %xmm0
	vmovdqa	%xmm0, 1152(%rsp)       # 16-byte Spill
	vinsertps	$48, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vpaddd	%xmm0, %xmm8, %xmm0
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%r9,%rcx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r9,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1],mem[0],xmm0[3]
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	cltq
	movq	1568(%rsp), %r15        # 8-byte Reload
	vmovups	12296(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovups	12312(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vmovups	12304(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vmovups	12320(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	sarq	$32, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovups	12288(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm10, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	1216(%rsp), %xmm7       # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovd	%xmm3, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$1, 1248(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$2, 1200(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpinsrd	$3, 1184(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpsrad	$31, %xmm7, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovd	%esi, %xmm7
	vpinsrd	$1, %ecx, %xmm7, %xmm7
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$2, %edi, %xmm7, %xmm3
	vpinsrd	$3, %edx, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm7
	vpand	%xmm13, %xmm7, %xmm7
	vpaddd	%xmm3, %xmm7, %xmm3
	vpcmpgtd	%xmm6, %xmm9, %xmm7
	vpsubd	%xmm6, %xmm15, %xmm4
	vblendvps	%xmm7, %xmm6, %xmm4, %xmm4
	vmovdqa	864(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	848(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	488(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm10, %xmm7, %xmm7
	vpminsd	%xmm14, %xmm7, %xmm7
	vpmaxsd	%xmm12, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm7, %xmm4
	vpmulld	%xmm11, %xmm4, %xmm2
	vmovdqa	%xmm2, 1200(%rsp)       # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm4
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	vpcmpgtd	%xmm3, %xmm9, %xmm4
	vpsubd	%xmm3, %xmm15, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovdqa	832(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	816(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	movq	496(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r10), %edx
	vmovd	%edx, %xmm6
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vpaddd	%xmm12, %xmm3, %xmm3
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm12, %xmm3, %xmm3
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm3, %xmm6, %xmm3
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm7 # xmm7 = xmm4[0,1,2],mem[0]
	vpmulld	%xmm11, %xmm3, %xmm2
	vmovdqa	%xmm2, 1184(%rsp)       # 16-byte Spill
	vpaddd	%xmm2, %xmm8, %xmm3
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%r9,%rdx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	movslq	%eax, %rdx
	sarq	$32, %rax
	vinsertps	$16, (%r9,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rax,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm10, %xmm4, %xmm4
	vpextrd	$1, %xmm4, %eax
	cltd
	idivl	%r13d
	movl	%ebx, %r13d
	movl	%edx, %ecx
	vmovd	%xmm4, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm4, %eax
	vpextrd	$3, %xmm4, %edi
	cltd
	idivl	%r14d
	vmovd	%esi, %xmm4
	vpinsrd	$1, %ecx, %xmm4, %xmm4
	vpinsrd	$2, %edx, %xmm4, %xmm4
	movl	%edi, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm6
	vpand	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm4
	vpcmpgtd	%xmm4, %xmm9, %xmm6
	vpsubd	%xmm4, %xmm15, %xmm9
	vblendvps	%xmm6, %xmm4, %xmm9, %xmm4
	vmovdqa	800(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm6
	vpxor	%xmm5, %xmm6, %xmm6
	vmovdqa	784(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm2, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vpminsd	%xmm14, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vblendvps	%xmm1, %xmm4, %xmm6, %xmm9
	vmovaps	416(%rsp), %xmm14       # 16-byte Reload
	vmulps	1168(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm3, %xmm0, %xmm6 # xmm6 = xmm0[0,2],xmm3[0,2]
	vmovaps	672(%rsp), %xmm13       # 16-byte Reload
	vsubps	%xmm13, %xmm6, %xmm6
	vmovaps	688(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm2, %xmm4
	vmulps	1136(%rsp), %xmm14, %xmm5 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1120(%rsp), %xmm8       # 16-byte Reload
	vshufps	$136, %xmm15, %xmm8, %xmm2 # xmm2 = xmm8[0,2],xmm15[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	1072(%rsp), %xmm14, %xmm10 # 16-byte Folded Reload
	vmovaps	1104(%rsp), %xmm12      # 16-byte Reload
	vshufps	$136, %xmm8, %xmm12, %xmm5 # xmm5 = xmm12[0,2],xmm8[0,2]
	vsubps	%xmm13, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm10, %xmm5
	vmovaps	368(%rsp), %xmm10       # 16-byte Reload
	vminps	%xmm10, %xmm4, %xmm6
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm6, %xmm6
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vsubps	%xmm6, %xmm2, %xmm2
	vminps	%xmm10, %xmm5, %xmm5
	vmaxps	%xmm4, %xmm5, %xmm5
	vsubps	%xmm6, %xmm5, %xmm5
	vshufps	$221, %xmm3, %xmm0, %xmm6 # xmm6 = xmm0[1,3],xmm3[1,3]
	vmulps	%xmm7, %xmm14, %xmm7
	vsubps	%xmm13, %xmm6, %xmm6
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm7, %xmm6, %xmm6
	vmulps	1248(%rsp), %xmm14, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm12, %xmm7 # xmm7 = xmm12[1,3],xmm8[1,3]
	vsubps	%xmm13, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm3, %xmm7, %xmm3
	vpmulld	%xmm11, %xmm9, %xmm0
	vmovdqa	%xmm0, 1168(%rsp)       # 16-byte Spill
	vpaddd	1424(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %rcx
	movslq	%ecx, %rdx
	vshufps	$221, %xmm15, %xmm8, %xmm8 # xmm8 = xmm8[1,3],xmm15[1,3]
	vpxor	%xmm11, %xmm11, %xmm11
	vmovss	(%r9,%rdx,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	sarq	$32, %rcx
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	sarq	$32, %rax
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movl	352(%rsp), %r12d        # 4-byte Reload
	movl	%r12d, %eax
	andl	%r13d, %eax
	vmovaps	1440(%rsp), %xmm7       # 16-byte Reload
	vandps	%xmm7, %xmm2, %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vandps	%xmm7, %xmm5, %xmm2
	vaddps	%xmm2, %xmm0, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vminps	%xmm10, %xmm6, %xmm5
	vmaxps	%xmm11, %xmm5, %xmm5
	vmulps	%xmm4, %xmm14, %xmm4
	vsubps	%xmm13, %xmm8, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm6
	vsubps	%xmm3, %xmm5, %xmm8
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm7, %xmm3, %xmm0
	vminps	%xmm10, %xmm6, %xmm3
	vmaxps	%xmm11, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vandps	%xmm7, %xmm3, %xmm5
	vmulps	1472(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovups	%ymm1, 1248(%rsp)       # 32-byte Spill
	vmovaps	%xmm1, %xmm13
	jne	.LBB159_34
# BB#33:                                # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB159_34:                             # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	movq	1016(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%r10), %eax
	cltq
	movq	%rax, %rcx
	vmovups	(%r15,%rax,4), %xmm6
	movq	%rax, %rdx
	vmovups	32(%r15,%rax,4), %xmm12
	orq	$2, %rax
	vmovups	(%r15,%rax,4), %xmm4
	vandps	%xmm7, %xmm8, %xmm2
	vaddps	%xmm5, %xmm0, %xmm9
	orq	$6, %rcx
	vmovups	(%r15,%rcx,4), %xmm5
	vmovdqa	1376(%rsp), %xmm0       # 16-byte Reload
	vmovdqa	1152(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm3
	orq	$4, %rdx
	vmovups	(%r15,%rdx,4), %xmm15
	vmovdqa	1280(%rsp), %xmm1       # 16-byte Reload
	vpsubd	%xmm0, %xmm1, %xmm8
	movb	1312(%rsp), %al         # 1-byte Reload
	andb	1296(%rsp), %al         # 1-byte Folded Reload
	vmovaps	%xmm7, %xmm14
	movb	%al, %bl
	vmovdqa	%xmm0, %xmm11
	jne	.LBB159_35
# BB#93:                                # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm13, %xmm14
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovdqa	%xmm3, 1312(%rsp)       # 16-byte Spill
	vmovdqa	%xmm8, 1280(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	jmp	.LBB159_94
	.align	16, 0x90
.LBB159_35:                             #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	vmovdqa	320(%rsp), %xmm2        # 16-byte Reload
	vmovaps	%xmm15, %xmm13
	vmovaps	%xmm6, %xmm15
	vmovaps	%xmm15, 1120(%rsp)      # 16-byte Spill
	vpaddd	992(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	224(%rsp), %xmm0        # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm5, %xmm4, %xmm7 # xmm7 = xmm4[0,2],xmm5[0,2]
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1152(%rsp)       # 16-byte Spill
	vmovaps	288(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm4, %xmm7, %xmm7
	vmovdqa	%xmm8, 1280(%rsp)       # 16-byte Spill
	vmovaps	304(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm7
	vpaddd	%xmm2, %xmm3, %xmm6
	vmovdqa	%xmm3, 1312(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm13, %xmm15, %xmm3 # xmm3 = xmm15[0,2],xmm13[0,2]
	vmovaps	%xmm13, %xmm15
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm2, %xmm8, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm0, %xmm6
	vshufps	$136, %xmm12, %xmm15, %xmm2 # xmm2 = xmm15[0,2],xmm12[0,2]
	vsubps	%xmm4, %xmm2, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm3, %xmm3
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm2
	vaddps	%xmm2, %xmm3, %xmm2
	vminps	%xmm10, %xmm7, %xmm3
	vmaxps	%xmm0, %xmm3, %xmm3
	vmovaps	1360(%rsp), %xmm0       # 16-byte Reload
	vfnmadd213ps	%xmm2, %xmm0, %xmm3
	vandps	%xmm14, %xmm3, %xmm2
	vaddps	1136(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm14
.LBB159_94:                             # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovdqa	1200(%rsp), %xmm1       # 16-byte Reload
	vmovdqa	1184(%rsp), %xmm2       # 16-byte Reload
	vmovdqa	1168(%rsp), %xmm3       # 16-byte Reload
	vmovaps	%xmm12, %xmm8
	vmulps	%xmm7, %xmm9, %xmm0
	vmovaps	%xmm0, %xmm13
	testb	%bl, %bl
	jne	.LBB159_96
# BB#95:                                # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB159_96:                             # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vpsubd	%xmm11, %xmm1, %xmm5
	vpsubd	%xmm11, %xmm2, %xmm1
	vpsubd	%xmm11, %xmm3, %xmm4
	movl	%r12d, %eax
	andl	%r13d, %eax
	jne	.LBB159_97
# BB#98:                                # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovdqa	%xmm5, 1184(%rsp)       # 16-byte Spill
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
	jmp	.LBB159_99
	.align	16, 0x90
.LBB159_97:                             #   in Loop: Header=BB159_32 Depth=1
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
	vmovdqa	320(%rsp), %xmm3        # 16-byte Reload
	vpaddd	%xmm3, %xmm5, %xmm2
	vmovdqa	%xmm5, 1184(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %rdx
	vmovq	%xmm2, %rsi
	vpaddd	%xmm3, %xmm1, %xmm2
	vmovdqa	%xmm1, 1296(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm2, %rbp
	vpaddd	%xmm3, %xmm4, %xmm2
	vmovdqa	%xmm4, 1200(%rsp)       # 16-byte Spill
	vpextrq	$1, %xmm2, %r14
	vmovq	%xmm2, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r11
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1104(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[1,3],mem[1,3]
	vmovss	(%r9,%r8,4), %xmm3      # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	224(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	288(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vmovaps	304(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm2, %xmm1, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vmovss	(%r9,%rbx,4), %xmm3     # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmulps	%xmm3, %xmm6, %xmm3
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm0, %xmm4 # xmm4 = xmm0[1,3],xmm15[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r14d, %rdx
	sarq	$32, %r14
	vshufps	$221, %xmm8, %xmm15, %xmm0 # xmm0 = xmm15[1,3],xmm8[1,3]
	vmovss	(%r9,%rax,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vminps	%xmm10, %xmm3, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm3, %xmm3
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm0, %xmm1, %xmm2
	vandps	1440(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vaddps	1216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm7, %xmm0, %xmm13
.LBB159_99:                             # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm7, %xmm9
	movq	640(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	cltq
	vmovups	24584(%r15,%rax,4), %xmm1
	vmovups	24600(%r15,%rax,4), %xmm8
	vmovups	24576(%r15,%rax,4), %xmm5
	vmovups	24592(%r15,%rax,4), %xmm15
	vmovups	24608(%r15,%rax,4), %xmm12
	movb	1352(%rsp), %r15b       # 1-byte Reload
	movb	%r15b, %al
	movl	1456(%rsp), %ecx        # 4-byte Reload
	andb	%cl, %al
	jne	.LBB159_100
# BB#101:                               # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1168(%rsp)       # 16-byte Spill
	movq	1560(%rsp), %rcx        # 8-byte Reload
	vmovaps	%xmm9, %xmm7
	jmp	.LBB159_102
	.align	16, 0x90
.LBB159_100:                            #   in Loop: Header=BB159_32 Depth=1
	vmovdqa	272(%rsp), %xmm7        # 16-byte Reload
	vpaddd	992(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	208(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vshufps	$136, %xmm8, %xmm1, %xmm6 # xmm6 = xmm1[0,2],xmm8[0,2]
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	240(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm6, %xmm1, %xmm6
	vmulps	%xmm6, %xmm0, %xmm11
	vpaddd	1312(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vmovaps	%xmm5, 1168(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm15[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm1, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vpaddd	1280(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm6     # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r9,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	%xmm6, %xmm4, %xmm6
	vshufps	$136, %xmm12, %xmm15, %xmm7 # xmm7 = xmm15[0,2],xmm12[0,2]
	vsubps	%xmm2, %xmm7, %xmm7
	vmulps	%xmm7, %xmm1, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vminps	%xmm10, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vminps	%xmm10, %xmm6, %xmm6
	vmaxps	%xmm1, %xmm6, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vminps	%xmm10, %xmm11, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vfnmadd213ps	%xmm5, %xmm1, %xmm0
	vandps	1440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1136(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm9, %xmm7
	vmulps	%xmm7, %xmm0, %xmm14
	movq	1560(%rsp), %rcx        # 8-byte Reload
.LBB159_102:                            # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovups	1248(%rsp), %ymm9       # 32-byte Reload
	movl	%r13d, %eax
	orl	%ecx, %eax
	andl	$1, %eax
	je	.LBB159_104
# BB#103:                               # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm14, %xmm9
.LBB159_104:                            # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	testl	%eax, %eax
	jne	.LBB159_106
# BB#105:                               #   in Loop: Header=BB159_32 Depth=1
	vmovdqa	272(%rsp), %xmm5        # 16-byte Reload
	vpaddd	1184(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm0, %rsi
	vpaddd	1296(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm0, %rbp
	vpaddd	1200(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r14
	vmovq	%xmm0, %rcx
	movslq	%esi, %r8
	sarq	$32, %rsi
	movslq	%edx, %r11
	sarq	$32, %rdx
	movslq	%ebp, %rbx
	sarq	$32, %rbp
	movslq	%edi, %rax
	sarq	$32, %rdi
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm8[1,3]
	vmovss	(%r9,%r8,4), %xmm2      # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	208(%rsp), %xmm6        # 16-byte Reload
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	240(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	256(%rsp), %xmm1        # 16-byte Reload
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm2, %xmm0, %xmm0
	vmovss	(%r9,%rbx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%xmm2, %xmm6, %xmm2
	vmovaps	1168(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm15, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm15[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm1, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	movslq	%ecx, %rax
	sarq	$32, %rcx
	movslq	%r14d, %rdx
	sarq	$32, %r14
	vshufps	$221, %xmm12, %xmm15, %xmm3 # xmm3 = xmm15[1,3],xmm12[1,3]
	vmovss	(%r9,%rax,4), %xmm4     # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r9,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmulps	%xmm4, %xmm6, %xmm4
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	%xmm3, %xmm1, %xmm3
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm10, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm0, %xmm0
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm1, %xmm2, %xmm2
	vminps	%xmm10, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vfnmadd213ps	%xmm1, %xmm2, %xmm0
	vandps	1440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1216(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	%xmm7, %xmm0, %xmm13
.LBB159_106:                            # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	movq	1536(%rsp), %rdx        # 8-byte Reload
	vmovups	1472(%rsp), %ymm1       # 32-byte Reload
	vmovaps	%xmm7, 1472(%rsp)       # 16-byte Spill
	movl	1456(%rsp), %eax        # 4-byte Reload
	andb	%al, %r15b
	jne	.LBB159_108
# BB#107:                               # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	%xmm13, %xmm1
.LBB159_108:                            # %for dh.s0.v10.v1016
                                        #   in Loop: Header=BB159_32 Depth=1
	vmovaps	.LCPI159_7(%rip), %ymm0 # ymm0 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm0, %ymm0
	vmovaps	.LCPI159_8(%rip), %ymm1 # ymm1 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm9, %ymm1, %ymm1
	vblendps	$170, %ymm0, %ymm1, %ymm0 # ymm0 = ymm1[0],ymm0[1],ymm1[2],ymm0[3],ymm1[4],ymm0[5],ymm1[6],ymm0[7]
	movslq	%r13d, %rax
	movq	656(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1528(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r10d
	addl	$-1, %edx
	movq	%rdx, 1536(%rsp)        # 8-byte Spill
	jne	.LBB159_32
.LBB159_109:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1576, %rsp             # imm = 0x628
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end159:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175, .Lfunc_end159-par_for_par_for___sharpi_f0.s0.v11.v14_dh.s0.v11.175

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI160_0:
	.long	1                       # 0x1
.LCPI160_1:
	.long	40                      # 0x28
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176: # @par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	4(%rdx), %r11d
	addl	$43, %r11d
	sarl	$3, %r11d
	testl	%r11d, %r11d
	jle	.LBB160_5
# BB#1:                                 # %for f4.s0.v10.v10.preheader
	movslq	12(%rdx), %rbx
	movq	16(%rdx), %rcx
	movq	32(%rdx), %rdi
	movq	48(%rdx), %rax
	movq	%rax, -8(%rsp)          # 8-byte Spill
	movq	64(%rdx), %rax
	movq	%rax, -16(%rsp)         # 8-byte Spill
	movq	80(%rdx), %rax
	movq	%rax, -24(%rsp)         # 8-byte Spill
	movq	%rbx, %rbp
	sarq	$63, %rbp
	movl	(%rdx), %r10d
	andl	$-32, %r10d
	leal	40(%r10), %r14d
	movl	%esi, %r12d
	andl	$63, %r12d
	imull	%r12d, %r14d
	movl	%ebx, %r15d
	sarl	$31, %r15d
	andl	%ebx, %r15d
	addl	$48, %r10d
	movslq	8(%rdx), %rdx
	addq	$3, %rdx
	imulq	%rdx, %r12
	andq	%rbx, %rbp
	subq	%rbp, %r12
	addl	$60, %esi
	xorl	%r13d, %r13d
	vbroadcastss	.LCPI160_0(%rip), %ymm0
	vpbroadcastd	.LCPI160_1(%rip), %ymm1
	.align	16, 0x90
.LBB160_2:                              # %for f4.s0.v10.v10
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB160_3 Depth 2
	leal	(,%r13,8), %ebp
	vpxor	%ymm2, %ymm2, %ymm2
	movl	$9, %ebx
	movl	%esi, %edx
	.align	16, 0x90
.LBB160_3:                              # %for sum.s1.r4$y
                                        #   Parent Loop BB160_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%edx, %r8d
	andl	$63, %r8d
	imull	%r10d, %r8d
	leal	(%rbp,%r8), %eax
	movslq	%eax, %r9
	vmovups	(%rcx,%r9,4), %ymm3
	vcmpnltps	(%rdi,%r9,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$1, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$2, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$3, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$4, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$5, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	movl	%r9d, %eax
	orl	$6, %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	orl	$7, %r9d
	movslq	%r9d, %rax
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	leal	8(%rbp,%r8), %eax
	cltq
	vmovups	(%rcx,%rax,4), %ymm3
	vcmpnltps	(%rdi,%rax,4), %ymm3, %ymm3
	vandps	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm2
	addl	$1, %edx
	addl	$-1, %ebx
	jne	.LBB160_3
# BB#4:                                 # %consume sum
                                        #   in Loop: Header=BB160_2 Depth=1
	leal	(%rbp,%r15), %eax
	addl	%r14d, %ebp
	vpcmpgtd	%ymm2, %ymm1, %ymm2
	movslq	%ebp, %rdx
	movq	-16(%rsp), %rbp         # 8-byte Reload
	vmovups	(%rbp,%rdx,4), %ymm3
	movq	-24(%rsp), %rbp         # 8-byte Reload
	vblendvps	%ymm2, (%rbp,%rdx,4), %ymm3, %ymm2
	cltq
	leaq	(%rax,%r12), %rax
	movq	-8(%rsp), %rdx          # 8-byte Reload
	vmovups	%ymm2, (%rdx,%rax,4)
	addq	$1, %r13
	cmpl	%r11d, %r13d
	jne	.LBB160_2
.LBB160_5:                              # %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end160:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176, .Lfunc_end160-par_for_par_for___sharpi_f0.s0.v11.v14_f4.s0.v11.176

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI161_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI161_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI161_9:
	.zero	16,255
.LCPI161_10:
	.zero	16
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI161_1:
	.long	1199570688              # float 65535
.LCPI161_3:
	.long	1065353216              # float 1
.LCPI161_4:
	.long	1048576000              # float 0.25
.LCPI161_5:
	.long	1042983595              # float 0.166666672
.LCPI161_6:
	.long	1045220557              # float 0.200000003
	.section	.rodata,"a",@progbits
	.align	32
.LCPI161_7:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
.LCPI161_8:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177: # @par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1912, %rsp             # imm = 0x778
	vmovss	(%rdx), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 88(%rsp)         # 4-byte Spill
	vmovss	4(%rdx), %xmm14         # xmm14 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movl	12(%rdx), %eax
	movl	%eax, 128(%rsp)         # 4-byte Spill
	movl	16(%rdx), %eax
	movl	%eax, 112(%rsp)         # 4-byte Spill
	movl	20(%rdx), %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movl	24(%rdx), %edi
	movslq	36(%rdx), %rax
	movq	%rax, 96(%rsp)          # 8-byte Spill
	movslq	40(%rdx), %rax
	movq	%rax, 144(%rsp)         # 8-byte Spill
	movl	44(%rdx), %eax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	movl	48(%rdx), %eax
	movq	%rax, 152(%rsp)         # 8-byte Spill
	movl	52(%rdx), %eax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	movl	56(%rdx), %eax
	movq	%rax, 848(%rsp)         # 8-byte Spill
	movslq	60(%rdx), %rax
	movq	%rax, 608(%rsp)         # 8-byte Spill
	movl	64(%rdx), %r10d
	vmovss	84(%rdx), %xmm15        # xmm15 = mem[0],zero,zero,zero
	cmpl	%esi, 32(%rdx)
	movl	68(%rdx), %ecx
	movl	72(%rdx), %eax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	movl	76(%rdx), %r8d
	movslq	80(%rdx), %rax
	movq	%rax, 136(%rsp)         # 8-byte Spill
	movq	88(%rdx), %rbp
	movq	%rbp, 1880(%rsp)        # 8-byte Spill
	movq	104(%rdx), %rbx
	movq	%rbx, 1736(%rsp)        # 8-byte Spill
	movq	120(%rdx), %rax
	movq	%rax, 1704(%rsp)        # 8-byte Spill
	movq	136(%rdx), %rax
	movq	%rax, 1904(%rsp)        # 8-byte Spill
	movq	152(%rdx), %rax
	movq	%rax, 64(%rsp)          # 8-byte Spill
	jle	.LBB161_34
# BB#1:                                 # %true_bb
	addl	$39, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB161_164
# BB#2:                                 # %for f7.s0.v10.v10.preheader
	movq	%rsi, %r11
	movq	%r11, 1800(%rsp)        # 8-byte Spill
	movl	%r11d, %eax
	subl	%r8d, %eax
	movq	%rax, 1776(%rsp)        # 8-byte Spill
	leal	(%rcx,%rcx), %esi
	movl	%esi, 1760(%rsp)        # 4-byte Spill
	movq	%rax, %rbx
	cltd
	idivl	%esi
	movl	%edx, %edi
	leal	-1(%rbx), %eax
	cltd
	idivl	%esi
	movl	%edx, %ebp
	leal	1(%rbx), %eax
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%ecx, %r13d
	sarl	$31, %r13d
	movq	%rcx, %rbx
	movq	%rbx, 1888(%rsp)        # 8-byte Spill
	andnl	%esi, %r13d, %ecx
	andl	%eax, %r13d
	orl	%ecx, %r13d
	movl	%edi, %eax
	sarl	$31, %eax
	andl	%r13d, %eax
	addl	%edi, %eax
	movl	%ebp, %esi
	sarl	$31, %esi
	andl	%r13d, %esi
	addl	%ebp, %esi
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%r13d, %ecx
	addl	%edx, %ecx
	leal	-1(%rbx,%rbx), %r12d
	movl	%r12d, 1856(%rsp)       # 4-byte Spill
	movl	%r12d, %r15d
	subl	%ecx, %r15d
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %r15d
	addl	%r8d, %r15d
	leal	-1(%r8,%rbx), %r9d
	cmpl	%r15d, %r9d
	cmovlel	%r9d, %r15d
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	leal	1(%r11), %ecx
	movq	%rcx, 584(%rsp)         # 8-byte Spill
	cmpl	%ecx, %r9d
	movl	%r9d, %edi
	cmovgl	%ecx, %edi
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	movl	%r12d, %r14d
	subl	%esi, %r14d
	cmpl	%esi, %ebx
	cmovgl	%esi, %r14d
	addl	%r8d, %r14d
	cmpl	%r14d, %r9d
	cmovlel	%r9d, %r14d
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	subl	%eax, %r12d
	cmpl	%eax, %ebx
	movq	%rbx, %rdx
	cmovgl	%eax, %r12d
	addl	%r8d, %r12d
	cmpl	%r12d, %r9d
	cmovlel	%r9d, %r12d
	cmpl	%r8d, %r12d
	cmovll	%r8d, %r12d
	movq	%r11, %rbx
	cmpl	%ebx, %r9d
	cmovlel	%r15d, %edi
	movl	%edi, 1808(%rsp)        # 4-byte Spill
	movl	%r9d, %ecx
	cmovgl	%ebx, %ecx
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	leal	(%r8,%rdx), %eax
	cmpl	%ebx, %eax
	movl	%eax, %edx
	cmovgl	%ebx, %edx
	cmovlel	%r12d, %ecx
	movl	%ecx, 1824(%rsp)        # 4-byte Spill
	addl	$-1, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	cmpl	%ebx, %eax
	cmovll	%r14d, %edx
	movl	%edx, 1744(%rsp)        # 4-byte Spill
	movq	48(%rsp), %r11          # 8-byte Reload
	movl	%r11d, %eax
	negl	%eax
	leal	(%r10,%r10), %esi
	cltd
	idivl	%esi
	movl	%r10d, %eax
	sarl	$31, %eax
	andnl	%esi, %eax, %edi
	negl	%esi
	andl	%eax, %esi
	orl	%edi, %esi
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	addl	%edx, %eax
	leal	-1(%r10,%r10), %edi
	subl	%eax, %edi
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	leal	(%r11,%r10), %eax
	leal	-1(%r11,%r10), %r10d
	addl	%r11d, %edi
	cmpl	%edi, %r10d
	cmovlel	%r10d, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	xorl	%ecx, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %r10d
	cmpl	%r11d, %r10d
	cmovll	%r11d, %r10d
	testl	%eax, %eax
	cmovlel	%edi, %r10d
	movq	40(%rsp), %rsi          # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	32(%rsp), %rcx          # 8-byte Reload
	leal	(%rcx,%rcx), %ebx
	cltd
	idivl	%ebx
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ebx, %eax, %ebp
	negl	%ebx
	andl	%eax, %ebx
	orl	%ebp, %ebx
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%ebx, %ebp
	addl	%edx, %ebp
	leal	-1(%rcx,%rcx), %eax
	subl	%ebp, %eax
	cmpl	%ebp, %ecx
	cmovgl	%ebp, %eax
	leal	(%rsi,%rcx), %edx
	leal	-1(%rsi,%rcx), %ebx
	addl	%esi, %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%edx, %edx
	movl	$0, %ecx
	cmovgl	%ecx, %ebx
	cmpl	%esi, %ebx
	cmovll	%esi, %ebx
	testl	%edx, %edx
	cmovlel	%eax, %ebx
	movq	144(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 576(%rsp)         # 8-byte Spill
	movq	1800(%rsp), %rdx        # 8-byte Reload
	movl	%edx, %ecx
	andl	$1, %ecx
	movl	%ecx, 568(%rsp)         # 4-byte Spill
	movl	%edx, %ecx
	andl	$63, %ecx
	movq	%rcx, 1840(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	cmovgl	%eax, %ebx
	movq	608(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ebp
	movq	848(%rsp), %rax         # 8-byte Reload
	imull	%eax, %ebp
	addl	%esi, %ebp
	testl	%r11d, %r11d
	cmovgl	%edi, %r10d
	movq	1776(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	1760(%rsp), %ecx        # 4-byte Reload
	idivl	%ecx
	movl	%edx, %edi
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%ecx
	movq	152(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%rsi), %eax
	vmovd	%eax, %xmm12
	vmovd	%ebp, %xmm1
	vmovd	%ebp, %xmm4
	movq	608(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm10
	movq	848(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rsi), %eax
	vmovd	%eax, %xmm6
	leal	-1(%rcx,%rsi), %eax
	vmovd	%eax, %xmm8
	vmovd	%ebx, %xmm2
	vmovd	%ebx, %xmm9
	leal	-2(%rcx,%rsi), %eax
	vmovd	%eax, %xmm11
	movq	136(%rsp), %rbp         # 8-byte Reload
	movl	%ebp, %eax
	imull	%r8d, %eax
	addl	%r11d, %eax
	movl	%edi, %ebx
	sarl	$31, %ebx
	andl	%r13d, %ebx
	addl	%edi, %ebx
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r13d, %edi
	movq	1880(%rsp), %r13        # 8-byte Reload
	addl	%edx, %edi
	leal	-2(%rcx), %edx
	vmovd	%edx, %xmm13
	movl	128(%rsp), %r11d        # 4-byte Reload
	andl	$-32, %r11d
	leal	-1(%r8), %edx
	movq	1800(%rsp), %rcx        # 8-byte Reload
	cmpl	%ecx, %edx
	movl	1808(%rsp), %esi        # 4-byte Reload
	cmovgl	%r15d, %esi
	movslq	%eax, %rdx
	movslq	%r10d, %rax
	subq	%rdx, %rax
	movq	%rax, 1808(%rsp)        # 8-byte Spill
	movslq	%esi, %rax
	imulq	%rbp, %rax
	movq	%rax, 1776(%rsp)        # 8-byte Spill
	cmpl	%ecx, %r8d
	movq	%rcx, %r10
	movl	1744(%rsp), %edx        # 4-byte Reload
	cmovgel	%r14d, %edx
	movl	1824(%rsp), %ecx        # 4-byte Reload
	cmovgl	%r12d, %ecx
	movq	1736(%rsp), %r15        # 8-byte Reload
	movslq	%edx, %rax
	imulq	%rbp, %rax
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	movq	152(%rsp), %rdx         # 8-byte Reload
	movq	848(%rsp), %rax         # 8-byte Reload
	leal	2(%rax,%rdx), %ebp
	vmovd	%ebp, %xmm0
	movl	1856(%rsp), %r14d       # 4-byte Reload
	movl	%r14d, %ebp
	subl	%edi, %ebp
	movq	1888(%rsp), %rsi        # 8-byte Reload
	cmpl	%edi, %esi
	cmovgl	%edi, %ebp
	addl	%r8d, %ebp
	cmpl	%ebp, %r9d
	cmovlel	%r9d, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	leal	-2(%r10), %r12d
	cmpl	%r12d, %r9d
	movl	%r9d, %edi
	cmovgl	%r12d, %edi
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	leal	2(%r8,%rsi), %edx
	cmpl	%r10d, %edx
	cmovlel	%ebp, %edi
	leal	2(%r8), %edx
	cmpl	%r10d, %edx
	cmovgl	%ebp, %edi
	vpsubd	%xmm1, %xmm2, %xmm5
	vmovss	.LCPI161_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm7, %xmm1, %xmm2
	vdivss	%xmm15, %xmm2, %xmm2
	movslq	%edi, %rdi
	movq	136(%rsp), %rdx         # 8-byte Reload
	imulq	%rdx, %rdi
	movq	%rdi, 1824(%rsp)        # 8-byte Spill
	vaddss	%xmm2, %xmm3, %xmm2
	movslq	%ecx, %rdi
	imulq	%rdx, %rdi
	movq	%rdx, %rcx
	leal	2(%rax), %edx
	vmovd	%edx, %xmm3
	movl	%r14d, %ebp
	subl	%ebx, %ebp
	movq	%rsi, %rdx
	cmpl	%ebx, %edx
	cmovgl	%ebx, %ebp
	leal	-2(%r8,%rdx), %edx
	addl	%r8d, %ebp
	cmpl	%ebp, %r9d
	cmovlel	%r9d, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	movl	%ebp, %ebx
	leal	2(%r10), %ebp
	cmpl	%ebp, %r9d
	cmovgl	%ebp, %r9d
	cmpl	%r8d, %r9d
	cmovll	%r8d, %r9d
	cmpl	%r10d, %edx
	leal	-2(%r8), %edx
	cmovlel	%ebx, %r9d
	cmpl	%r10d, %edx
	cmovgl	%ebx, %r9d
	movslq	%r9d, %rdx
	xorl	%r9d, %r9d
	imulq	%rcx, %rdx
	movq	1808(%rsp), %rax        # 8-byte Reload
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r8
	movq	1760(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r14
	leaq	(%rdi,%rax), %rcx
	movq	%rcx, 1888(%rsp)        # 8-byte Spill
	movq	832(%rsp), %rbx         # 8-byte Reload
	addl	$3, %ebx
	andl	$63, %ebp
	imull	%ebx, %ebp
	movq	%rbp, 552(%rsp)         # 8-byte Spill
	andl	$63, %r12d
	imull	%ebx, %r12d
	movq	%r12, 560(%rsp)         # 8-byte Spill
	leal	63(%r10), %esi
	andl	$63, %esi
	imull	%ebx, %esi
	movq	%rsi, 544(%rsp)         # 8-byte Spill
	movq	584(%rsp), %rsi         # 8-byte Reload
	andl	$63, %esi
	imull	%ebx, %esi
	movq	%rsi, 584(%rsp)         # 8-byte Spill
	movq	1824(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r12
	movq	1840(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %ebx
	movq	%rbx, 832(%rsp)         # 8-byte Spill
	leaq	(%rdx,%rax), %rax
	movq	96(%rsp), %rbp          # 8-byte Reload
	addq	$32, %rbp
	movq	%rsi, %rdx
	leal	10(%r10), %esi
	movl	112(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %esi
	addl	$64, %r11d
	imull	%r11d, %esi
	movq	%rsi, 528(%rsp)         # 8-byte Spill
	imulq	%rbp, %rdx
	leal	8(%r10), %esi
	subl	%ebx, %esi
	imull	%r11d, %esi
	movq	%rsi, 520(%rsp)         # 8-byte Spill
	leal	6(%r10), %esi
	subl	%ebx, %esi
	imull	%r11d, %esi
	movq	%rsi, 496(%rsp)         # 8-byte Spill
	movq	144(%rsp), %rsi         # 8-byte Reload
	movq	%rsi, %rbp
	sarq	$63, %rbp
	andq	%rsi, %rbp
	leal	7(%r10), %esi
	subl	%ebx, %esi
	imull	%r11d, %esi
	movq	%rsi, 480(%rsp)         # 8-byte Spill
	leal	9(%r10), %esi
	subl	%ebx, %esi
	imull	%r11d, %esi
	movq	%rsi, 464(%rsp)         # 8-byte Spill
	subq	%rbp, %rdx
	movq	%rdx, 536(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm12, %xmm12
	vmovdqa	%xmm12, 448(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm4, %xmm4
	vmovaps	%xmm4, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm10, %xmm4
	vmovaps	%xmm4, 1680(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm4
	vsubss	%xmm7, %xmm14, %xmm6
	vmovdqa	.LCPI161_0(%rip), %xmm7 # xmm7 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 416(%rsp)        # 16-byte Spill
	vmulss	%xmm6, %xmm1, %xmm1
	movq	848(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm10
	vmovdqa	%xmm10, 1392(%rsp)      # 16-byte Spill
	vdivss	%xmm1, %xmm15, %xmm1
	movq	152(%rsp), %rdx         # 8-byte Reload
	vmovd	%edx, %xmm4
	vbroadcastss	%xmm4, %xmm4
	vmovaps	%xmm4, 1376(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm6
	vmovdqa	%xmm6, 1360(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm4
	vmovaps	%xmm4, 400(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm11, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm4
	vmovdqa	%xmm4, 1856(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	leal	1(%rcx,%rdx), %ebp
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	leal	1(%rcx), %ebp
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	leal	-1(%rcx), %ebp
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	leal	-3(%rcx,%rdx), %ebp
	movq	%rcx, %rsi
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 272(%rsp)        # 16-byte Spill
	movq	576(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	subl	%esi, %ecx
	movq	%rcx, 256(%rsp)         # 8-byte Spill
	leal	-3(%rsi), %ebp
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	vpaddd	%xmm7, %xmm10, %xmm0
	vmovdqa	%xmm0, 224(%rsp)        # 16-byte Spill
	vpaddd	%xmm7, %xmm6, %xmm0
	vmovdqa	%xmm0, 208(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	leal	3(%rsi), %edx
	movq	%rdx, 192(%rsp)         # 8-byte Spill
	leal	1(%rsi), %edx
	movq	%rdx, 176(%rsp)         # 8-byte Spill
	leal	-1(%rsi), %edx
	movq	%rdx, 160(%rsp)         # 8-byte Spill
	leal	-2(%rsi), %edx
	movq	%rdx, 152(%rsp)         # 8-byte Spill
	leal	2(%rsi), %edx
	movq	%rdx, 144(%rsp)         # 8-byte Spill
	leal	3(%rcx), %edx
	movq	%rdx, 136(%rsp)         # 8-byte Spill
	leal	1(%rcx), %edx
	movq	%rdx, 128(%rsp)         # 8-byte Spill
	leal	-1(%rcx), %edx
	movq	%rdx, 112(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %edx
	movq	%rdx, 96(%rsp)          # 8-byte Spill
	leal	2(%rcx), %ecx
	movq	%rcx, 88(%rsp)          # 8-byte Spill
	movq	64(%rsp), %rbp          # 8-byte Reload
	vbroadcastss	(%rbp,%r8,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbp,%r14,4), %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	movq	1888(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rbp,%rcx,4), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbp,%r12,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbp,%rax,4), %xmm0
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vpabsd	%xmm12, %xmm0
	vmovdqa	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovdqa	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI161_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vbroadcastss	.LCPI161_3(%rip), %xmm0
	vmovaps	%xmm0, 1888(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_4(%rip), %xmm0
	vmovaps	%xmm0, 64(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI161_5(%rip), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_6(%rip), %xmm0
	vmovaps	%xmm0, 48(%rsp)         # 16-byte Spill
	.align	16, 0x90
.LBB161_3:                              # %for f7.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	%r9, 1288(%rsp)         # 8-byte Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm3, %xmm11
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r14d
	movl	%r14d, 992(%rsp)        # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r11d
	movl	%r11d, 1216(%rsp)       # 4-byte Spill
	cltd
	idivl	%r11d
	movl	%edx, %edi
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %esi
	movl	%esi, 1264(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r10d
	movl	%r10d, 1232(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r8d
	movl	%r8d, 1248(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1344(%rsp), %xmm12      # 16-byte Reload
	vpand	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm6
	vmovdqa	%xmm6, 1008(%rsp)       # 16-byte Spill
	vmovdqa	416(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	224(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1376(%rsp), %xmm14      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vmovdqa	1328(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	1392(%rsp), %xmm15      # 16-byte Reload
	vpaddd	%xmm15, %xmm0, %xmm0
	vmovdqa	1360(%rsp), %xmm13      # 16-byte Reload
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	vpaddd	%xmm11, %xmm6, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1680(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm7
	vpsubd	432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	400(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1904(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1184(%rsp)       # 16-byte Spill
	movq	88(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	368(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm14, %xmm2
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	1856(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	320(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm0, %xmm0
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm0, %xmm0
	vmovdqa	304(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm0, %xmm2, %xmm0
	vpcmpgtd	%xmm1, %xmm14, %xmm2
	vpsubd	%xmm1, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpaddd	%xmm15, %xmm1, %xmm1
	vpminsd	%xmm13, %xmm1, %xmm1
	vpmaxsd	%xmm15, %xmm1, %xmm1
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpmaxsd	%xmm15, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r8d
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	208(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	288(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm5
	vpcmpgtd	%xmm0, %xmm14, %xmm1
	vpsubd	%xmm0, %xmm4, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	584(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %rcx
	movq	%rcx, 1168(%rsp)        # 8-byte Spill
	vmovups	8(%r15,%rcx,4), %xmm1
	vmovaps	%xmm1, 1552(%rsp)       # 16-byte Spill
	vmovups	24(%r15,%rcx,4), %xmm1
	vmovaps	%xmm1, 1600(%rsp)       # 16-byte Spill
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %rsi
	vmovups	32(%r13,%rsi,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	48(%r13,%rsi,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %rdi
	movq	%rdi, 1152(%rsp)        # 8-byte Spill
	vmovups	8(%r15,%rdi,4), %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	vmovups	24(%r15,%rdi,4), %xmm1
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	movslq	%eax, %rbx
	vmovups	32(%r13,%rbx,4), %xmm1
	vmovaps	%xmm1, 1584(%rsp)       # 16-byte Spill
	vmovups	48(%r13,%rbx,4), %xmm1
	vmovaps	%xmm1, 1568(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%rcx,4), %xmm1
	vmovaps	%xmm1, 1824(%rsp)       # 16-byte Spill
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	vmovups	40(%r13,%rsi,4), %xmm2
	vmovaps	%xmm2, 1808(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%rdi,4), %xmm8
	vmovaps	%xmm8, 1520(%rsp)       # 16-byte Spill
	vmovups	40(%r13,%rbx,4), %xmm3
	vmovaps	%xmm3, 1536(%rsp)       # 16-byte Spill
	vmovups	32(%r15,%rdi,4), %xmm10
	vmovaps	%xmm10, 752(%rsp)       # 16-byte Spill
	vmovups	56(%r13,%rbx,4), %xmm9
	vmovaps	%xmm9, 736(%rsp)        # 16-byte Spill
	vmovups	32(%r15,%rcx,4), %xmm2
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovups	56(%r13,%rsi,4), %xmm2
	vmovaps	%xmm2, 1408(%rsp)       # 16-byte Spill
	idivl	%r14d
	movl	%edx, %ecx
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %r12d
	movl	%r12d, 1200(%rsp)       # 4-byte Spill
	vmovd	%r12d, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vmovd	%xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpaddd	%xmm11, %xmm7, %xmm7
	vpminsd	%xmm13, %xmm7, %xmm7
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vpmaxsd	%xmm15, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm7
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r8d
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %ebp, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm5
	vpand	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm1
	vpcmpgtd	%xmm1, %xmm14, %xmm5
	vpsubd	%xmm1, %xmm4, %xmm0
	vblendvps	%xmm5, %xmm1, %xmm0, %xmm0
	vmovdqa	272(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	.LCPI161_9(%rip), %xmm1, %xmm1
	vmovdqa	240(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm4, %xmm5
	vpor	%xmm1, %xmm5, %xmm1
	vmovdqa	1680(%rsp), %xmm2       # 16-byte Reload
	vpmulld	1744(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm15, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm15, %xmm0, %xmm0
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm13, %xmm4, %xmm4
	vpmaxsd	%xmm15, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm0
	vmovdqa	1856(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm5, %xmm4, %xmm1
	movq	%r9, %rcx
	vpextrq	$1, %xmm1, %r9
	vmovq	%xmm1, %r11
	vpmulld	%xmm2, %xmm7, %xmm1
	vmovdqa	%xmm2, %xmm15
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrq	$1, %xmm1, %r15
	vmovq	%xmm1, %r13
	vmovaps	1584(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1568(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1632(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	1664(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1648(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1184(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm2, %xmm7, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	1888(%rsp), %xmm11      # 16-byte Reload
	vminps	%xmm11, %xmm1, %xmm1
	vpxor	%xmm12, %xmm12, %xmm12
	vmaxps	%xmm12, %xmm1, %xmm1
	vmovaps	1472(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1456(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm14
	vshufps	$136, %xmm9, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm9[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm2, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vshufps	$136, %xmm10, %xmm8, %xmm4 # xmm4 = xmm8[0,2],xmm10[0,2]
	vmovaps	%xmm7, %xmm8
	vsubps	%xmm4, %xmm1, %xmm10
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1312(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm8, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vmovaps	1552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 1600(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm7
	vmovaps	1808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm13, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vmovaps	1824(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 1424(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm1
	vpmulld	%xmm15, %xmm0, %xmm0
	vpaddd	1856(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %r8
	movq	%r8, 1120(%rsp)         # 8-byte Spill
	vmovq	%xmm0, %rdi
	movq	%rdi, 1088(%rsp)        # 8-byte Spill
	movq	%r11, %r14
	sarq	$32, %r14
	movq	%r9, %rbp
	sarq	$32, %rbp
	movq	%r13, %rdx
	sarq	$32, %rdx
	movq	%r15, %r10
	sarq	$32, %r10
	orq	$6, %rsi
	orq	$6, %rbx
	movl	%r12d, %eax
	andl	$1, %eax
	movq	%rdi, %r12
	sarq	$32, %r12
	sarq	$32, %r8
	testl	%eax, %eax
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rcx), %eax
	cltq
	movq	832(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 1056(%rsp)        # 4-byte Spill
	movq	560(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 1104(%rsp)        # 4-byte Spill
	movq	496(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 1024(%rsp)        # 4-byte Spill
	movq	552(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %edi
	movl	%edi, 1072(%rsp)        # 4-byte Spill
	movq	528(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rcx), %ecx
	jne	.LBB161_4
# BB#5:                                 # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm1, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 688(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 704(%rsp)       # 16-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	jmp	.LBB161_6
	.align	16, 0x90
.LBB161_4:                              #   in Loop: Header=BB161_3 Depth=1
	vaddps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm1, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 688(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm7, 672(%rsp)        # 16-byte Spill
	vaddps	%xmm0, %xmm14, %xmm0
	vmovaps	%xmm14, 704(%rsp)       # 16-byte Spill
	vmulps	64(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
.LBB161_6:                              # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovdqa	%xmm0, 1744(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm7
	vmovaps	1504(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1488(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1472(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1456(%rsp), %xmm15      # 16-byte Reload
	movslq	%r11d, %rdi
	movq	1904(%rsp), %r11        # 8-byte Reload
	vmovss	(%r11,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r9d, %rdi
	vinsertps	$32, (%r11,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rbp,4), %xmm0, %xmm14 # xmm14 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm14, 1440(%rsp)      # 16-byte Spill
	movslq	%r13d, %rdi
	vmovss	(%r11,%rdi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r15d, %rdx
	vinsertps	$32, (%r11,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%r10,4), %xmm0, %xmm9 # xmm9 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm9, 1136(%rsp)       # 16-byte Spill
	movq	1880(%rsp), %r13        # 8-byte Reload
	vmovups	(%r13,%rsi,4), %xmm3
	vmovaps	%xmm3, 816(%rsp)        # 16-byte Spill
	vmovups	(%r13,%rbx,4), %xmm5
	vmovaps	%xmm5, 800(%rsp)        # 16-byte Spill
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm4
	vmulps	%xmm4, %xmm9, %xmm0
	vshufps	$221, %xmm1, %xmm10, %xmm1 # xmm1 = xmm10[1,3],xmm1[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm12
	vshufps	$221, %xmm15, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm15[1,3]
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vmovaps	1648(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm9, %xmm1
	vmovaps	1584(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1568(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm9
	vmulps	%xmm4, %xmm14, %xmm1
	vshufps	$221, 1808(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm1, %xmm15
	vmulps	%xmm0, %xmm14, %xmm1
	vshufps	$221, 1536(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm5[1,3],mem[1,3]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm1, %xmm2
	vmovups	32(%r13,%rax,4), %xmm4
	vmovaps	%xmm4, 1552(%rsp)       # 16-byte Spill
	vmovups	48(%r13,%rax,4), %xmm5
	vmovaps	%xmm5, 1504(%rsp)       # 16-byte Spill
	movslq	%ecx, %rdi
	movq	%rdi, 976(%rsp)         # 8-byte Spill
	vmovups	32(%r13,%rdi,4), %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm8, %xmm1
	vshufps	$136, %xmm5, %xmm4, %xmm5 # xmm5 = xmm4[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm14
	vmovups	48(%r13,%rdi,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	1776(%rsp), %xmm8, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm10
	movslq	1024(%rsp), %rdx        # 4-byte Folded Reload
	vmulps	1616(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
	vmovups	32(%r13,%rdx,4), %xmm0
	vmovaps	%xmm0, 928(%rsp)        # 16-byte Spill
	vmovups	48(%r13,%rdx,4), %xmm4
	vmovaps	%xmm4, 912(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm4, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm4[0,2]
	vsubps	%xmm6, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm1, %xmm4
	vmovups	40(%r13,%rax,4), %xmm1
	vmovaps	%xmm1, 1600(%rsp)       # 16-byte Spill
	vmovups	56(%r13,%rax,4), %xmm5
	vmovaps	%xmm5, 720(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm5[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	1760(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm7
	movq	1088(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r11,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%rdx, %r12
	movq	1120(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r11,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
	movq	%r11, %r8
	movslq	1056(%rsp), %rcx        # 4-byte Folded Reload
	vminps	%xmm11, %xmm12, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vmaxps	%xmm0, %xmm1, %xmm6
	vminps	%xmm11, %xmm9, %xmm1
	vmaxps	%xmm0, %xmm1, %xmm1
	vminps	%xmm11, %xmm15, %xmm3
	vmaxps	%xmm0, %xmm3, %xmm3
	vmovaps	%xmm3, 1584(%rsp)       # 16-byte Spill
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	movslq	1104(%rsp), %r14        # 4-byte Folded Reload
	movslq	1072(%rsp), %rsi        # 4-byte Folded Reload
	vminps	%xmm11, %xmm10, %xmm3
	vxorps	%xmm15, %xmm15, %xmm15
	vminps	%xmm11, %xmm4, %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, %xmm0
	vminps	%xmm0, %xmm7, %xmm2
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vminps	%xmm0, %xmm14, %xmm14
	movl	568(%rsp), %ebx         # 4-byte Reload
	testl	%ebx, %ebx
	movq	1736(%rsp), %r15        # 8-byte Reload
	movq	1168(%rsp), %rdx        # 8-byte Reload
	vmovups	(%r15,%rdx,4), %xmm4
	vmovaps	%xmm4, 768(%rsp)        # 16-byte Spill
	movq	1152(%rsp), %rdx        # 8-byte Reload
	vmovups	(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 784(%rsp)        # 16-byte Spill
	vmovups	8(%r15,%rcx,4), %xmm2
	vmovups	24(%r15,%rcx,4), %xmm5
	vshufps	$221, 1824(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm4[1,3],mem[1,3]
	vshufps	$221, 1520(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm0[1,3],mem[1,3]
	vmovups	16(%r15,%rcx,4), %xmm0
	vmovups	8(%r15,%r14,4), %xmm11
	vmovups	24(%r15,%r14,4), %xmm10
	vmovups	8(%r15,%rsi,4), %xmm8
	vmovups	24(%r15,%rsi,4), %xmm9
	vmovups	32(%r15,%rcx,4), %xmm13
	vmovups	16(%r15,%rsi,4), %xmm4
	vmovaps	%xmm4, 960(%rsp)        # 16-byte Spill
	vmovups	40(%r13,%rdi,4), %xmm4
	vmovaps	%xmm4, 1024(%rsp)       # 16-byte Spill
	vmovups	16(%r15,%r14,4), %xmm4
	vmovaps	%xmm4, 1056(%rsp)       # 16-byte Spill
	vmovups	40(%r13,%r12,4), %xmm4
	vmovaps	%xmm4, 1072(%rsp)       # 16-byte Spill
	je	.LBB161_8
# BB#7:                                 # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vxorps	%xmm4, %xmm4, %xmm4
	vmovaps	%xmm4, 1744(%rsp)       # 16-byte Spill
.LBB161_8:                              # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vsubps	1040(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vshufps	$136, %xmm5, %xmm2, %xmm4 # xmm4 = xmm2[0,2],xmm5[0,2]
	vmovaps	%xmm5, 1168(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1184(%rsp)       # 16-byte Spill
	vsubps	1472(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm8, %xmm2 # xmm2 = xmm8[0,2],xmm9[0,2]
	vmovaps	%xmm2, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 1456(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1472(%rsp)       # 16-byte Spill
	vmaxps	%xmm15, %xmm3, %xmm2
	vmovaps	%xmm2, 896(%rsp)        # 16-byte Spill
	vmovaps	1584(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm7, %xmm2, %xmm3
	vshufps	$136, %xmm10, %xmm11, %xmm2 # xmm2 = xmm11[0,2],xmm10[0,2]
	vmovaps	%xmm2, 1040(%rsp)       # 16-byte Spill
	vmovaps	1088(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm15, %xmm2, %xmm2
	vmovaps	%xmm2, 1088(%rsp)       # 16-byte Spill
	vmovaps	1120(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm12, %xmm2, %xmm5
	vshufps	$136, %xmm13, %xmm0, %xmm9 # xmm9 = xmm0[0,2],xmm13[0,2]
	vmovaps	1104(%rsp), %xmm2       # 16-byte Reload
	vmaxps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm14, %xmm7
	movl	1200(%rsp), %edi        # 4-byte Reload
	movl	%edi, %edx
	movq	1800(%rsp), %r10        # 8-byte Reload
	orl	%r10d, %edx
	testb	$1, %dl
	movq	1288(%rsp), %r9         # 8-byte Reload
	movl	1264(%rsp), %r11d       # 4-byte Reload
	je	.LBB161_9
# BB#10:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm2, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 1104(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)      # 16-byte Spill
	vmovaps	%xmm13, 592(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovaps	1664(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1632(%rsp), %xmm8       # 16-byte Reload
	jmp	.LBB161_11
	.align	16, 0x90
.LBB161_9:                              #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm2, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 1104(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)      # 16-byte Spill
	vmovaps	%xmm13, 592(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, %xmm14
	vmovaps	%xmm14, 608(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, %xmm12
	vmovaps	%xmm12, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, %xmm11
	vmovaps	944(%rsp), %xmm7        # 16-byte Reload
	vmulps	1312(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
	vmovaps	1808(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm4, 1584(%rsp)       # 16-byte Spill
	vshufps	$221, 1408(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	1632(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, %xmm5
	vmovaps	%xmm5, 640(%rsp)        # 16-byte Spill
	vmovaps	1664(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm13, %xmm8
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	1824(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1424(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm0[1,3],mem[1,3]
	vmovaps	1888(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	1648(%rsp), %xmm7, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm11, %xmm7
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 736(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm8, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vmovaps	1520(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 752(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vminps	%xmm2, %xmm0, %xmm0
	vmaxps	%xmm15, %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm14, %xmm1, %xmm4
	vaddps	%xmm0, %xmm4, %xmm0
	vmovaps	1584(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm0, %xmm12, %xmm0
	vaddps	%xmm0, %xmm5, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vmulps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
.LBB161_11:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	1840(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1616(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1776(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1040(%rsp), %xmm1       # 16-byte Reload
	vmovaps	896(%rsp), %xmm5        # 16-byte Reload
	vmovaps	880(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm5, %xmm12
	vsubps	%xmm1, %xmm0, %xmm5
	vmovaps	864(%rsp), %xmm0        # 16-byte Reload
	vsubps	%xmm9, %xmm0, %xmm10
	vsubps	%xmm4, %xmm7, %xmm9
	orq	$6, %rax
	testl	%edi, %ebx
	jne	.LBB161_12
# BB#13:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm9, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1424(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, 1408(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1616(%rsp)      # 16-byte Spill
	vmovaps	%xmm4, 1584(%rsp)       # 16-byte Spill
	vmovaps	1888(%rsp), %xmm4       # 16-byte Reload
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	.LBB161_14
	.align	16, 0x90
.LBB161_12:                             #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm4, 1584(%rsp)       # 16-byte Spill
	vmovaps	1760(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm3, %xmm2, %xmm0
	movq	976(%rsp), %rdx         # 8-byte Reload
	vmovaps	1024(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 56(%r13,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 32(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1888(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm7, %xmm7, %xmm7
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm13, %xmm2, %xmm1
	vmovaps	1072(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%r13,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm8, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	1056(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%r15,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm9, %xmm5, %xmm2
	vmovaps	%xmm9, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1408(%rsp)       # 16-byte Spill
	vaddps	%xmm12, %xmm2, %xmm2
	vmovaps	%xmm12, 1616(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm10, %xmm1
	vmovaps	%xmm10, 1424(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
.LBB161_14:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	1600(%rsp), %xmm1       # 16-byte Reload
	vmovaps	928(%rsp), %xmm2        # 16-byte Reload
	vmovaps	912(%rsp), %xmm10       # 16-byte Reload
	vmulps	1440(%rsp), %xmm14, %xmm0 # 16-byte Folded Reload
	vmovups	(%r13,%rax,4), %xmm9
	vshufps	$221, %xmm1, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm1[1,3]
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovups	(%r15,%rcx,4), %xmm5
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vmovaps	1152(%rsp), %xmm15      # 16-byte Reload
	vshufps	$221, %xmm15, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm15[1,3]
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1104(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm10, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm10[1,3]
	vmovaps	1136(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm13, %xmm11, %xmm2
	vsubps	%xmm8, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm7, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm10
	vmovaps	%xmm8, %xmm12
	vmovaps	1184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1168(%rsp), %xmm0, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm0[1,3],mem[1,3]
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm14, %xmm11, %xmm1
	vsubps	%xmm12, %xmm0, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1472(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1568(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1488(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	%xmm3, 1776(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm11, %xmm3
	vsubps	%xmm12, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm8, %xmm0, %xmm0
	andl	$1, %edi
	je	.LBB161_15
# BB#16:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, %xmm14
	vmovaps	%xmm5, 1504(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1552(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1568(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, %xmm9
	vmovaps	%xmm4, 1888(%rsp)       # 16-byte Spill
	vmovaps	1744(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB161_17
	.align	16, 0x90
.LBB161_15:                             #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm5, 1504(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1552(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1568(%rsp)       # 16-byte Spill
	vshufps	$221, 592(%rsp), %xmm15, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm15[1,3],mem[1,3]
	vmovaps	%xmm15, %xmm9
	vmulps	944(%rsp), %xmm14, %xmm2 # 16-byte Folded Reload
	vmovaps	1600(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 720(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm12, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm4, 1888(%rsp)       # 16-byte Spill
	vmaxps	%xmm7, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm4
	vaddps	%xmm1, %xmm0, %xmm2
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vaddps	%xmm2, %xmm10, %xmm2
	vmovaps	%xmm10, %xmm14
	vaddps	%xmm4, %xmm2, %xmm1
	vaddps	1760(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	48(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
.LBB161_17:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovdqa	1008(%rsp), %xmm0       # 16-byte Reload
	movl	992(%rsp), %ecx         # 4-byte Reload
	vmovaps	%xmm13, %xmm15
	vmovaps	%xmm12, %xmm8
	vmovaps	848(%rsp), %xmm1        # 16-byte Reload
	testl	%ebx, %ebx
	jne	.LBB161_19
# BB#18:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	1744(%rsp), %xmm11      # 16-byte Reload
.LBB161_19:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	movl	%r11d, %eax
	andl	$1, %eax
	vmovaps	%xmm7, %xmm10
	vmovaps	%xmm6, %xmm12
	jne	.LBB161_20
# BB#21:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
	jmp	.LBB161_22
	.align	16, 0x90
.LBB161_20:                             #   in Loop: Header=BB161_3 Depth=1
	vaddps	640(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	608(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	64(%rsp), %xmm1, %xmm13 # 16-byte Folded Reload
.LBB161_22:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	testl	%ebx, %ebx
	je	.LBB161_24
# BB#23:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vxorps	%xmm13, %xmm13, %xmm13
.LBB161_24:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	movq	96(%rsp), %rax          # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	.LCPI161_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	1232(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	1248(%rsp)              # 4-byte Folded Reload
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %ebp, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm5
	vpand	1344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm5, %xmm1
	vmovdqa	352(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm5
	vpxor	.LCPI161_9(%rip), %xmm5, %xmm5
	vmovdqa	336(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovdqa	1376(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm6
	vmovdqa	1328(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm1, %xmm4, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vmovdqa	1392(%rsp), %xmm0       # 16-byte Reload
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	1360(%rsp), %xmm7       # 16-byte Reload
	vpminsd	%xmm7, %xmm1, %xmm1
	vpmaxsd	%xmm0, %xmm1, %xmm1
	movq	152(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r9), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm6
	vpminsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm0, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm1, %xmm6, %xmm1
	vpmulld	1680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	1856(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovq	%xmm1, %rax
	movslq	%eax, %rcx
	vmovss	(%r8,%rcx,4), %xmm5     # xmm5 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r8,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0],mem[0],xmm5[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r8,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	sarq	$32, %rcx
	vinsertps	$48, (%r8,%rcx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],mem[0]
	vmovups	(%r15,%rsi,4), %xmm1
	movl	%r11d, %eax
	orl	%r10d, %eax
	testb	$1, %al
	jne	.LBB161_26
# BB#25:                                #   in Loop: Header=BB161_3 Depth=1
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1824(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vmovaps	816(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1808(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm0[0,2],mem[0,2]
	vmulps	1312(%rsp), %xmm5, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm12, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmovaps	1888(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm6, %xmm6
	vmaxps	%xmm10, %xmm6, %xmm6
	vsubps	%xmm2, %xmm6, %xmm2
	vmovaps	784(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1520(%rsp), %xmm4, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm4[0,2],mem[0,2]
	vmovaps	800(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1536(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm4[0,2],mem[0,2]
	vmulps	1648(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm7, %xmm7
	vmulps	%xmm7, %xmm12, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm0, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm6, %xmm4, %xmm4
	vaddps	704(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	688(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm2, %xmm2
	vaddps	672(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	656(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm2, %xmm13 # 16-byte Folded Reload
.LBB161_26:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	movq	1712(%rsp), %rdx        # 8-byte Reload
	testl	%r11d, %ebx
	je	.LBB161_28
# BB#27:                                #   in Loop: Header=BB161_3 Depth=1
	vshufps	$221, 960(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	movq	976(%rsp), %rax         # 8-byte Reload
	orq	$6, %rax
	vmovups	(%r13,%rax,4), %xmm2
	vshufps	$221, 1024(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1440(%rsp), %xmm6       # 16-byte Reload
	vmulps	1776(%rsp), %xmm6, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm2, %xmm2
	vmulps	%xmm2, %xmm12, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1888(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vmovups	(%r15,%r14,4), %xmm2
	vshufps	$221, 1056(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	orq	$6, %r12
	vmovups	(%r13,%r12,4), %xmm4
	vshufps	$221, 1072(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	%xmm15, %xmm6, %xmm6
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm0, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm2
	vaddps	1760(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm1, %xmm14, %xmm1
	vaddps	1472(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vaddps	1488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm0, %xmm13 # 16-byte Folded Reload
.LBB161_28:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	1616(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1424(%rsp), %xmm7       # 16-byte Reload
	movl	%r11d, %eax
	andl	$1, %eax
	je	.LBB161_29
# BB#30:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm15, 1616(%rsp)      # 16-byte Spill
	vmovaps	%xmm13, %xmm0
	vmovaps	1584(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1568(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	jmp	.LBB161_31
	.align	16, 0x90
.LBB161_29:                             #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm15, 1616(%rsp)      # 16-byte Spill
	vmovaps	1504(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm9[0,2]
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm1
	vmovaps	1552(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1600(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	1888(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmaxps	%xmm10, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vaddps	1088(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
	vaddps	1408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm7, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	48(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
	vmovaps	1584(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1568(%rsp), %xmm4       # 16-byte Reload
.LBB161_31:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm8, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1840(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1664(%rsp)      # 16-byte Spill
	testl	%ebx, %ebx
	jne	.LBB161_33
# BB#32:                                # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vmovaps	%xmm13, %xmm0
.LBB161_33:                             # %for f7.s0.v10.v10
                                        #   in Loop: Header=BB161_3 Depth=1
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm11, %xmm4, %xmm1
	vmovaps	.LCPI161_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI161_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r11d, %rax
	movq	536(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r9d
	addl	$-1, %edx
	movq	%rdx, 1712(%rsp)        # 8-byte Spill
	jne	.LBB161_3
	jmp	.LBB161_164
.LBB161_34:                             # %false_bb
	cmpl	%esi, 28(%rdx)
	movq	%rsi, 1800(%rsp)        # 8-byte Spill
	jle	.LBB161_39
# BB#35:                                # %true_bb1
	movq	%rbx, 1736(%rsp)        # 8-byte Spill
	movq	%rbp, 1880(%rsp)        # 8-byte Spill
	movq	%r10, (%rsp)            # 8-byte Spill
	movq	%r8, 8(%rsp)            # 8-byte Spill
	vmovss	%xmm15, 20(%rsp)        # 4-byte Spill
	vmovss	%xmm7, 24(%rsp)         # 4-byte Spill
	vmovss	%xmm14, 28(%rsp)        # 4-byte Spill
	movq	144(%rsp), %rax         # 8-byte Reload
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	movq	%rsi, 1392(%rsp)        # 8-byte Spill
	movq	848(%rsp), %rdx         # 8-byte Reload
	movq	%rdx, %r8
	movl	%r8d, %ebp
	subl	%esi, %ebp
	movl	%ebp, %eax
	sarl	$3, %eax
	leal	1(%rax), %ecx
	leal	4(%rbp), %ebx
	sarl	$3, %ebx
	cmpl	%ebx, %eax
	cmovgel	%ecx, %ebx
	movl	%ebx, -28(%rsp)         # 4-byte Spill
	leal	5(%rbp), %eax
	sarl	$3, %eax
	movq	%rax, -8(%rsp)          # 8-byte Spill
	cmpl	%eax, %ebx
	movl	%eax, %edx
	cmovgel	%ebx, %edx
	leal	6(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -32(%rsp)         # 4-byte Spill
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	leal	7(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, -36(%rsp)         # 4-byte Spill
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	addl	$9, %ebp
	sarl	$3, %ebp
	movq	%rbp, -24(%rsp)         # 8-byte Spill
	cmpl	%ebp, %edx
	cmovll	%ebp, %edx
	xorl	%r14d, %r14d
	testl	%edx, %edx
	cmovsl	%r14d, %edx
	leal	39(%rdi), %eax
	sarl	$3, %eax
	movl	%eax, 592(%rsp)         # 4-byte Spill
	cmpl	%edx, %eax
	cmovlel	%eax, %edx
	movl	%edx, 584(%rsp)         # 4-byte Spill
	movq	152(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	subl	%esi, %eax
	leal	(%r8,%rax), %ebp
	leal	(%r8,%rcx), %ecx
	movq	%rcx, -16(%rsp)         # 8-byte Spill
	subl	%esi, %ecx
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	sarl	$3, %ebp
	movq	%rbp, -56(%rsp)         # 8-byte Spill
	leal	-10(%r8,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -60(%rsp)         # 4-byte Spill
	leal	-1(%rbp), %ebp
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-9(%r8,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -64(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-7(%r8,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -68(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-6(%r8,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -72(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-5(%r8,%rax), %eax
	sarl	$3, %eax
	movl	%eax, -76(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-10(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -80(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-9(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -84(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-7(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -88(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-6(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -92(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-5(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -96(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-4(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -100(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-3(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -104(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-2(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -108(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -112(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -116(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	sarl	$3, %ecx
	movq	%rcx, -48(%rsp)         # 8-byte Spill
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	addl	$31, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	addl	$1, %ebp
	cmpl	%ebp, %edx
	cmovgel	%edx, %ebp
	movl	%ebp, -120(%rsp)        # 4-byte Spill
	testl	%edx, %edx
	jle	.LBB161_73
# BB#36:                                # %for f7.s0.v10.v104.preheader
	movq	48(%rsp), %r8           # 8-byte Reload
	movl	%r8d, %eax
	negl	%eax
	movq	(%rsp), %rdi            # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	(%r8,%rdi), %edx
	leal	-1(%rdi,%rdi), %esi
	subl	%eax, %esi
	cmpl	%eax, %edi
	cmovgl	%eax, %esi
	addl	%r8d, %esi
	leal	-1(%r8,%rdi), %r9d
	cmpl	%esi, %r9d
	cmovlel	%r9d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	testl	%edx, %edx
	cmovgl	%r14d, %r9d
	cmpl	%r8d, %r9d
	cmovll	%r8d, %r9d
	testl	%edx, %edx
	cmovlel	%esi, %r9d
	movq	40(%rsp), %rcx          # 8-byte Reload
	movl	%ecx, %eax
	negl	%eax
	movq	32(%rsp), %rbx          # 8-byte Reload
	leal	(%rbx,%rbx), %ebp
	cltd
	idivl	%ebp
	leal	-1(%rbx,%rbx), %edi
	movl	%ebx, %eax
	sarl	$31, %eax
	andnl	%ebp, %eax, %r10d
	negl	%ebp
	andl	%eax, %ebp
	orl	%r10d, %ebp
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	leal	(%rcx,%rbx), %ebp
	subl	%eax, %edi
	cmpl	%eax, %ebx
	leal	-1(%rcx,%rbx), %edx
	cmovgl	%eax, %edi
	addl	%ecx, %edi
	cmpl	%edi, %edx
	cmovlel	%edx, %edi
	cmpl	%ecx, %edi
	cmovll	%ecx, %edi
	testl	%ebp, %ebp
	cmovgl	%r14d, %edx
	cmpl	%ecx, %edx
	cmovll	%ecx, %edx
	testl	%ebp, %ebp
	cmovlel	%edi, %edx
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	3(%rax), %eax
	movl	%eax, 1888(%rsp)        # 4-byte Spill
	movq	1800(%rsp), %r12        # 8-byte Reload
	movl	%r12d, %r10d
	andl	$63, %r10d
	imull	%r10d, %eax
	movq	%rax, 568(%rsp)         # 8-byte Spill
	movl	%r12d, %eax
	andl	$1, %eax
	movl	%eax, 560(%rsp)         # 4-byte Spill
	movq	152(%rsp), %r11         # 8-byte Reload
	leal	(%r11,%r11), %ebp
	vmovd	%ebp, %xmm9
	testl	%ecx, %ecx
	cmovgl	%edi, %edx
	movq	608(%rsp), %r15         # 8-byte Reload
	movl	%r15d, %edi
	movq	848(%rsp), %r14         # 8-byte Reload
	imull	%r14d, %edi
	addl	%ecx, %edi
	vmovd	%edi, %xmm8
	vmovd	%edi, %xmm0
	movq	-16(%rsp), %r13         # 8-byte Reload
	leal	-1(%r13), %edi
	vmovd	%edi, %xmm10
	testl	%r8d, %r8d
	cmovgl	%esi, %r9d
	movq	136(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %esi
	movq	8(%rsp), %rax           # 8-byte Reload
	imull	%eax, %esi
	vmovd	%edx, %xmm11
	vmovd	%edx, %xmm12
	addl	%r8d, %esi
	leal	-2(%r13), %edx
	vmovd	%edx, %xmm13
	leal	-2(%r14), %edx
	vmovd	%edx, %xmm14
	leal	2(%r13), %edx
	vmovd	%edx, %xmm15
	leal	2(%r14), %edx
	vmovd	%edx, %xmm2
	leal	1(%r13), %edx
	vmovd	%edx, %xmm3
	leal	1(%r14), %edx
	vmovd	%edx, %xmm1
	movl	128(%rsp), %ebp         # 4-byte Reload
	andl	$-32, %ebp
	addl	$64, %ebp
	movl	%r12d, %eax
	subl	112(%rsp), %eax         # 4-byte Folded Reload
	leal	9(%rax), %edx
	imull	%ebp, %edx
	movq	%rdx, 544(%rsp)         # 8-byte Spill
	movslq	%r12d, %rdx
	leaq	1(%rdx), %r8
	imulq	%rcx, %r8
	leal	7(%rax), %ebx
	imull	%ebp, %ebx
	movq	%rbx, 536(%rsp)         # 8-byte Spill
	leal	6(%rax), %ebx
	imull	%ebp, %ebx
	movq	%rbx, 528(%rsp)         # 8-byte Spill
	leal	8(%rax), %ebx
	imull	%ebp, %ebx
	movq	%rbx, 520(%rsp)         # 8-byte Spill
	addl	$10, %eax
	imull	%ebp, %eax
	movq	%rax, 552(%rsp)         # 8-byte Spill
	leal	1(%r12), %eax
	andl	$63, %eax
	movl	1888(%rsp), %ebx        # 4-byte Reload
	imull	%ebx, %eax
	movq	%rax, 496(%rsp)         # 8-byte Spill
	movslq	%esi, %rax
	leaq	-1(%rdx), %rsi
	imulq	%rcx, %rsi
	subq	%rax, %r8
	subq	%rax, %rsi
	leal	63(%r12), %ebp
	andl	$63, %ebp
	imull	%ebx, %ebp
	movl	%ebx, %edi
	movq	%rbp, 480(%rsp)         # 8-byte Spill
	leaq	-2(%rdx), %rbp
	imulq	%rcx, %rbp
	movq	%rcx, %rbx
	imulq	%rdx, %rbx
	subq	%rax, %rbp
	addq	$2, %rdx
	imulq	%rcx, %rdx
	subq	%rax, %rbx
	subq	%rax, %rdx
	leal	62(%r12), %eax
	andl	$63, %eax
	imull	%edi, %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	leal	2(%r12), %eax
	xorl	%r12d, %r12d
	andl	$63, %eax
	imull	%edi, %eax
	movq	%rax, 448(%rsp)         # 8-byte Spill
	movq	96(%rsp), %rax          # 8-byte Reload
	leaq	32(%rax), %rax
	imulq	%rax, %r10
	movslq	%r9d, %rax
	addq	%rax, %r8
	addq	%rax, %rsi
	addq	%rax, %rbp
	addq	%rax, %rbx
	addq	%rax, %rdx
	movq	144(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rax
	sarq	$63, %rax
	andq	%rcx, %rax
	subq	%rax, %r10
	movq	%r10, 576(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm9, %xmm9
	vmovdqa	%xmm9, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 416(%rsp)        # 16-byte Spill
	vmovd	%r15d, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI161_0(%rip), %xmm4 # xmm4 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vmovd	%r14d, %xmm0
	vpbroadcastd	%xmm0, %xmm7
	vmovdqa	%xmm7, 192(%rsp)        # 16-byte Spill
	vpsubd	%xmm8, %xmm11, %xmm0
	vmovd	%r11d, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 1376(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm5
	vmovdqa	%xmm5, 1360(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm12, %xmm6
	vmovaps	%xmm6, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm6
	vmovdqa	%xmm6, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm6
	vmovdqa	%xmm5, %xmm8
	vpaddd	%xmm4, %xmm6, %xmm5
	vmovdqa	%xmm5, 352(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 1648(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm1, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	leal	-1(%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 272(%rsp)        # 16-byte Spill
	leal	-3(%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	leal	-3(%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	vpaddd	%xmm4, %xmm7, %xmm0
	vmovdqa	%xmm0, 224(%rsp)        # 16-byte Spill
	vpaddd	%xmm4, %xmm8, %xmm0
	vmovdqa	%xmm0, 208(%rsp)        # 16-byte Spill
	vmovss	.LCPI161_1(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm0
	vmovss	28(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	24(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm1, %xmm1
	vmulss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm2, %xmm0, %xmm0
	vmovss	20(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vdivss	%xmm1, %xmm2, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 1856(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	movq	64(%rsp), %rax          # 8-byte Reload
	vbroadcastss	(%rax,%r8,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rsi,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbp,4), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rdx,4), %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	vpabsd	%xmm9, %xmm0
	vmovdqa	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm9, %xmm0
	vmovdqa	%xmm0, 1328(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_3(%rip), %xmm0
	vmovaps	%xmm0, 1888(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_4(%rip), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI161_5(%rip), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_6(%rip), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB161_37:                             # %for f7.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movq	%r12, 1288(%rsp)        # 8-byte Spill
	movq	1392(%rsp), %r15        # 8-byte Reload
	leal	(%r15,%r12,8), %esi
	movl	%esi, 1120(%rsp)        # 4-byte Spill
	movl	%esi, %r11d
	movq	848(%rsp), %rax         # 8-byte Reload
	subl	%eax, %r11d
	movq	%r11, 1024(%rsp)        # 8-byte Spill
	vmovd	%r11d, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI161_2(%rip), %xmm14 # xmm14 = [0,2,4,6]
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	432(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r13d
	movl	%r13d, 1040(%rsp)       # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r14d
	movl	%r14d, 1056(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r10d
	movl	%r10d, 1072(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r9d
	movl	%r9d, 1088(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1344(%rsp), %xmm13      # 16-byte Reload
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm10
	vmovdqa	%xmm10, 1104(%rsp)      # 16-byte Spill
	vmovdqa	400(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	224(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1376(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm6, %xmm2
	vmovdqa	1328(%rsp), %xmm8       # 16-byte Reload
	vpsubd	%xmm0, %xmm8, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	1360(%rsp), %xmm4       # 16-byte Reload
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm14, %xmm10, %xmm2
	vpminsd	%xmm4, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1664(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm5
	vpsubd	416(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1904(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	leal	2(%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	368(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm1, %xmm1
	vpxor	%xmm9, %xmm1, %xmm1
	vpcmpeqd	%xmm9, %xmm9, %xmm9
	vmovdqa	352(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm6, %xmm2
	vpsubd	%xmm0, %xmm8, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	leal	2(%r15,%r12,8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpminsd	%xmm4, %xmm2, %xmm2
	vpmaxsd	%xmm7, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm5, %xmm0, %xmm0
	vpaddd	1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	leal	-1(%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	vmovd	%edx, %xmm1
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r10d
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	304(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm1, %xmm1
	vpxor	%xmm9, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	288(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm6, %xmm2
	vpsubd	%xmm0, %xmm8, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm2
	leal	-1(%r15,%r12,8), %eax
	vmovd	%eax, %xmm0
	leal	1(%r11), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm14, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebp
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovd	%edi, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	208(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	272(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	%xmm1, 1232(%rsp)       # 16-byte Spill
	vpcmpgtd	%xmm0, %xmm6, %xmm1
	vpsubd	%xmm0, %xmm8, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm9
	leal	1(%r15,%r12,8), %eax
	movl	%eax, 1584(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm5
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12,8), %eax
	movq	496(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12,8), %ecx
	movq	%r12, %r8
	movslq	%ecx, %r12
	movq	%r12, 1264(%rsp)        # 8-byte Spill
	movq	1736(%rsp), %rbp        # 8-byte Reload
	vmovups	8(%rbp,%r12,4), %xmm0
	vmovaps	%xmm0, 1520(%rsp)       # 16-byte Spill
	vmovups	24(%rbp,%r12,4), %xmm0
	vmovaps	%xmm0, 1488(%rsp)       # 16-byte Spill
	movslq	%eax, %rsi
	movq	1880(%rsp), %rdi        # 8-byte Reload
	vmovups	32(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vmovups	48(%rdi,%rsi,4), %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	movq	536(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8,8), %eax
	movq	480(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r8,8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 1248(%rsp)        # 8-byte Spill
	vmovups	8(%rbp,%rcx,4), %xmm0
	vmovaps	%xmm0, 1504(%rsp)       # 16-byte Spill
	vmovups	24(%rbp,%rcx,4), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	movslq	%eax, %rbx
	vmovups	32(%rdi,%rbx,4), %xmm0
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	vmovups	48(%rdi,%rbx,4), %xmm2
	vmovaps	%xmm2, 1152(%rsp)       # 16-byte Spill
	leal	3(%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	vmovups	16(%rbp,%r12,4), %xmm1
	vmovaps	%xmm1, 1776(%rsp)       # 16-byte Spill
	vmovdqa	%xmm4, %xmm1
	vmovups	40(%rdi,%rsi,4), %xmm3
	vmovaps	%xmm3, 1808(%rsp)       # 16-byte Spill
	vmovups	16(%rbp,%rcx,4), %xmm15
	vmovaps	%xmm15, 1536(%rsp)      # 16-byte Spill
	vmovups	40(%rdi,%rbx,4), %xmm3
	vmovaps	%xmm3, 1552(%rsp)       # 16-byte Spill
	vmovups	32(%rbp,%rcx,4), %xmm4
	vmovaps	%xmm4, 1440(%rsp)       # 16-byte Spill
	vmovups	56(%rdi,%rbx,4), %xmm11
	vmovaps	%xmm11, 752(%rsp)       # 16-byte Spill
	vmovups	32(%rbp,%r12,4), %xmm4
	vmovaps	%xmm4, 1424(%rsp)       # 16-byte Spill
	vmovups	56(%rdi,%rsi,4), %xmm4
	vmovaps	%xmm4, 1408(%rsp)       # 16-byte Spill
	idivl	%r13d
	movl	%edx, %ecx
	vpminsd	%xmm1, %xmm5, %xmm5
	vpmaxsd	%xmm7, %xmm5, %xmm5
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovaps	1232(%rsp), %xmm4       # 16-byte Reload
	vblendvps	%xmm4, %xmm9, %xmm5, %xmm4
	vpextrd	$2, %xmm0, %eax
	vpextrd	$3, %xmm0, %ebp
	cltd
	idivl	%r10d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm6, %xmm5
	vpsubd	%xmm0, %xmm8, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	256(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm5, %xmm5
	vpxor	.LCPI161_9(%rip), %xmm5, %xmm5
	vmovdqa	240(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm10, %xmm6, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm1, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	leal	3(%r15,%r8,8), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm14, %xmm6, %xmm6
	vpminsd	%xmm1, %xmm6, %xmm6
	vpmaxsd	%xmm7, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm0, %xmm6, %xmm0
	vmovdqa	1664(%rsp), %xmm9       # 16-byte Reload
	vpmulld	1680(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	vmovdqa	1648(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm5, %xmm14, %xmm5
	vpextrq	$1, %xmm5, %r13
	vmovq	%xmm5, %r15
	vpmulld	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm14, %xmm4
	movq	%r8, %rdx
	vpextrq	$1, %xmm4, %r12
	vmovq	%xmm4, %r9
	vmovaps	1744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm2[0,2]
	vmovaps	1632(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmovaps	1856(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	1616(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm12, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vmovaps	1888(%rsp), %xmm8       # 16-byte Reload
	vminps	%xmm8, %xmm4, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm4, %xmm4
	vmovaps	1504(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 1472(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm5, %xmm4, %xmm10
	vshufps	$136, %xmm11, %xmm3, %xmm4 # xmm4 = xmm3[0,2],xmm11[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	1760(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm2, %xmm13, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm1, %xmm4, %xmm4
	vshufps	$136, 1440(%rsp), %xmm15, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm15[0,2],mem[0,2]
	vsubps	%xmm5, %xmm4, %xmm11
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1568(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmovaps	1312(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm12, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm1, %xmm4, %xmm4
	vmovaps	1520(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm5, %xmm4, %xmm15
	vmovaps	1808(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1408(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm3, %xmm13, %xmm5
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm8, %xmm4, %xmm4
	vmaxps	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovaps	1776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1424(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[0,2],mem[0,2]
	vsubps	%xmm5, %xmm4, %xmm1
	vpaddd	%xmm0, %xmm14, %xmm0
	vpextrq	$1, %xmm0, %r8
	movq	%r8, 1216(%rsp)         # 8-byte Spill
	vmovq	%xmm0, %r14
	movq	%r14, 1168(%rsp)        # 8-byte Spill
	movq	%r15, %r11
	sarq	$32, %r11
	movq	%r13, %rbp
	sarq	$32, %rbp
	movq	%r9, %rcx
	sarq	$32, %rcx
	movq	%r12, %rdi
	sarq	$32, %rdi
	orq	$6, %rsi
	orq	$6, %rbx
	movl	1584(%rsp), %eax        # 4-byte Reload
	andl	$1, %eax
	sarq	$32, %r14
	sarq	$32, %r8
	testl	%eax, %eax
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdx,8), %eax
	movslq	%eax, %r10
	jne	.LBB161_38
# BB#43:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm1, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 672(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 688(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 704(%rsp)       # 16-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	jmp	.LBB161_44
	.align	16, 0x90
.LBB161_38:                             #   in Loop: Header=BB161_37 Depth=1
	vaddps	%xmm1, %xmm11, %xmm0
	vmovaps	%xmm1, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, 688(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm15, %xmm0
	vmovaps	%xmm15, 672(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm10, %xmm0
	vmovaps	%xmm10, 704(%rsp)       # 16-byte Spill
	vmulps	176(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB161_44:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovdqa	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, %xmm4
	vmovaps	1600(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1152(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1568(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1520(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1504(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1488(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1472(%rsp), %xmm14      # 16-byte Reload
	movslq	%r15d, %rdx
	movq	1904(%rsp), %rax        # 8-byte Reload
	vmovss	(%rax,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r13d, %rdx
	vinsertps	$32, (%rax,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%rbp,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 1456(%rsp)       # 16-byte Spill
	movslq	%r9d, %rdx
	vmovss	(%rax,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r12d, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm1, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm1[1,3]
	vmovaps	%xmm1, 1200(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm0, %xmm5
	vshufps	$221, %xmm11, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm11[1,3]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm5, %xmm13
	vshufps	$221, %xmm14, %xmm10, %xmm1 # xmm1 = xmm10[1,3],xmm14[1,3]
	vmovaps	%xmm1, 1184(%rsp)       # 16-byte Spill
	vmovaps	1616(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm0, %xmm0
	vmovaps	1744(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	vmulps	%xmm3, %xmm6, %xmm0
	movq	1880(%rsp), %rbp        # 8-byte Reload
	vmovups	(%rbp,%rsi,4), %xmm1
	vmovaps	%xmm1, 816(%rsp)        # 16-byte Spill
	vshufps	$221, 1808(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm10
	vmulps	%xmm5, %xmm6, %xmm0
	vmovups	(%rbp,%rbx,4), %xmm1
	vmovaps	%xmm1, 800(%rsp)        # 16-byte Spill
	vshufps	$221, 1552(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm14
	movq	1288(%rsp), %r12        # 8-byte Reload
	movq	552(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12,8), %ecx
	vmovups	32(%rbp,%r10,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	48(%rbp,%r10,4), %xmm5
	vmovaps	%xmm5, 1472(%rsp)       # 16-byte Spill
	movslq	%ecx, %r13
	vmovups	32(%rbp,%r13,4), %xmm3
	vmovaps	%xmm3, 1488(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm12, %xmm0
	vshufps	$136, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm5[0,2]
	vsubps	%xmm4, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm9
	vmovups	48(%rbp,%r13,4), %xmm0
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0,2],xmm0[0,2]
	vsubps	%xmm4, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	1824(%rsp), %xmm12, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm3, %xmm1
	movq	528(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12,8), %ecx
	movslq	%ecx, %r15
	vmulps	%xmm15, %xmm12, %xmm3
	vmovups	32(%rbp,%r15,4), %xmm0
	vmovaps	%xmm0, 912(%rsp)        # 16-byte Spill
	vmovups	48(%rbp,%r15,4), %xmm5
	vmovaps	%xmm5, 896(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm0, %xmm5 # xmm5 = xmm0[0,2],xmm5[0,2]
	vsubps	%xmm4, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm3, %xmm5
	vmovups	40(%rbp,%r10,4), %xmm0
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	vmovups	56(%rbp,%r10,4), %xmm3
	vmovaps	%xmm3, 736(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm3[0,2]
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	1760(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm7
	movq	1168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r14,4), %xmm3, %xmm4 # xmm4 = xmm3[0],mem[0],xmm3[2,3]
	movq	568(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12,8), %ecx
	movslq	%ecx, %rdx
	vminps	%xmm8, %xmm13, %xmm3
	vmovaps	%xmm8, %xmm0
	vxorps	%xmm13, %xmm13, %xmm13
	vmaxps	%xmm13, %xmm3, %xmm11
	vmovaps	1568(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm0, %xmm2, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm2
	vmovaps	%xmm2, 1152(%rsp)       # 16-byte Spill
	vminps	%xmm0, %xmm10, %xmm2
	vmaxps	%xmm13, %xmm2, %xmm2
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vminps	%xmm0, %xmm14, %xmm6
	vmaxps	%xmm13, %xmm6, %xmm2
	vmovaps	%xmm2, 1136(%rsp)       # 16-byte Spill
	movq	1216(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	464(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r12,8), %ecx
	vinsertps	$48, (%rax,%r8,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm2, 928(%rsp)        # 16-byte Spill
	movq	%rax, %r9
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r12,8), %esi
	movslq	%ecx, %rdi
	movslq	%esi, %r8
	vminps	%xmm0, %xmm1, %xmm6
	vminps	%xmm0, %xmm5, %xmm14
	vminps	%xmm0, %xmm7, %xmm1
	vmovaps	%xmm1, 1216(%rsp)       # 16-byte Spill
	vminps	%xmm0, %xmm9, %xmm1
	movl	560(%rsp), %r14d        # 4-byte Reload
	testl	%r14d, %r14d
	movq	1736(%rsp), %rsi        # 8-byte Reload
	movq	1264(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 768(%rsp)        # 16-byte Spill
	movq	1248(%rsp), %rax        # 8-byte Reload
	vmovups	(%rsi,%rax,4), %xmm2
	vmovaps	%xmm2, 784(%rsp)        # 16-byte Spill
	vmovups	8(%rsi,%rdx,4), %xmm7
	vmovups	24(%rsi,%rdx,4), %xmm10
	vshufps	$221, 1776(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vshufps	$221, 1536(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[1,3],mem[1,3]
	vmovups	16(%rsi,%rdx,4), %xmm3
	vmovups	8(%rsi,%rdi,4), %xmm15
	vmovups	24(%rsi,%rdi,4), %xmm12
	vmovups	8(%rsi,%r8,4), %xmm9
	vmovups	24(%rsi,%r8,4), %xmm8
	vmovups	32(%rsi,%rdx,4), %xmm5
	vmovups	16(%rsi,%r8,4), %xmm2
	vmovaps	%xmm2, 960(%rsp)        # 16-byte Spill
	vmovups	40(%rbp,%r13,4), %xmm2
	vmovaps	%xmm2, 976(%rsp)        # 16-byte Spill
	vmovups	16(%rsi,%rdi,4), %xmm2
	vmovaps	%xmm2, 992(%rsp)        # 16-byte Spill
	vmovups	40(%rbp,%r15,4), %xmm2
	vmovaps	%xmm2, 1008(%rsp)       # 16-byte Spill
	je	.LBB161_46
# BB#45:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%xmm2, 1680(%rsp)       # 16-byte Spill
.LBB161_46:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vsubps	1200(%rsp), %xmm11, %xmm11 # 16-byte Folded Reload
	vshufps	$136, %xmm10, %xmm7, %xmm2 # xmm2 = xmm7[0,2],xmm10[0,2]
	vmovaps	%xmm10, 1248(%rsp)      # 16-byte Spill
	vmovaps	%xmm7, 1264(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1568(%rsp)       # 16-byte Spill
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vsubps	1184(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vshufps	$136, %xmm8, %xmm9, %xmm2 # xmm2 = xmm9[0,2],xmm8[0,2]
	vmovaps	%xmm2, 864(%rsp)        # 16-byte Spill
	vmaxps	%xmm13, %xmm6, %xmm2
	vmovaps	%xmm2, 880(%rsp)        # 16-byte Spill
	vmovaps	1168(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm6
	vshufps	$136, %xmm12, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm12[0,2]
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmaxps	%xmm13, %xmm14, %xmm2
	vmovaps	1136(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm3, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 720(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm14 # xmm14 = xmm3[0,2],xmm5[0,2]
	vmovaps	1216(%rsp), %xmm3       # 16-byte Reload
	vmaxps	%xmm13, %xmm3, %xmm10
	vmaxps	%xmm13, %xmm1, %xmm1
	movl	1584(%rsp), %eax        # 4-byte Reload
	movl	%eax, %ecx
	movq	1800(%rsp), %rbx        # 8-byte Reload
	orl	%ebx, %ecx
	testb	$1, %cl
	je	.LBB161_47
# BB#48:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm12, 1168(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 1184(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1200(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, 1440(%rsp)      # 16-byte Spill
	vmovaps	1856(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1632(%rsp), %xmm13      # 16-byte Reload
	jmp	.LBB161_49
	.align	16, 0x90
.LBB161_47:                             #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm12, 1168(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 1184(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1200(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 1216(%rsp)       # 16-byte Spill
	vxorps	%xmm12, %xmm12, %xmm12
	vmovaps	928(%rsp), %xmm3        # 16-byte Reload
	vmovaps	%xmm1, %xmm8
	vmulps	1312(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm2, %xmm9
	vmovaps	1808(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1408(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1632(%rsp), %xmm13      # 16-byte Reload
	vsubps	%xmm13, %xmm2, %xmm2
	vmovaps	1856(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1776(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1888(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vmulps	1616(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmovaps	1552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 752(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm13, %xmm3, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vmovaps	1536(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1440(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm12, %xmm2, %xmm2
	vsubps	%xmm3, %xmm2, %xmm2
	vaddps	%xmm0, %xmm7, %xmm3
	vmovaps	%xmm0, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 752(%rsp)        # 16-byte Spill
	vaddps	%xmm2, %xmm3, %xmm2
	vaddps	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm6, 640(%rsp)        # 16-byte Spill
	vaddps	%xmm2, %xmm11, %xmm2
	vmovaps	%xmm11, 1440(%rsp)      # 16-byte Spill
	vaddps	%xmm2, %xmm1, %xmm1
	vmovaps	%xmm9, %xmm2
	vmulps	1296(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 1680(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, %xmm1
.LBB161_49:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	1600(%rsp), %xmm15      # 16-byte Reload
	movl	1040(%rsp), %ecx        # 4-byte Reload
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vmovaps	864(%rsp), %xmm5        # 16-byte Reload
	vsubps	%xmm5, %xmm3, %xmm3
	vsubps	%xmm0, %xmm2, %xmm6
	vsubps	%xmm14, %xmm10, %xmm9
	vsubps	1568(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
	orq	$6, %r10
	testl	%eax, %r14d
	jne	.LBB161_50
# BB#51:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1824(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1888(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1744(%rsp), %xmm9       # 16-byte Reload
	vmovaps	944(%rsp), %xmm3        # 16-byte Reload
	vmovaps	912(%rsp), %xmm7        # 16-byte Reload
	vmovaps	896(%rsp), %xmm14       # 16-byte Reload
	vxorps	%xmm8, %xmm8, %xmm8
	jmp	.LBB161_52
	.align	16, 0x90
.LBB161_50:                             #   in Loop: Header=BB161_37 Depth=1
	vmovaps	1824(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1760(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm10, %xmm2, %xmm0
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 56(%rbp,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 32(%rsi,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1888(%rsp), %xmm5       # 16-byte Reload
	vminps	%xmm5, %xmm0, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm15, %xmm2, %xmm1
	vmovaps	1008(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rbp,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	992(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 32(%rsi,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm7, %xmm6, %xmm2
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 1136(%rsp)       # 16-byte Spill
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm9, %xmm1
	vmovaps	%xmm9, 880(%rsp)        # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1744(%rsp), %xmm9       # 16-byte Reload
	vmovaps	944(%rsp), %xmm3        # 16-byte Reload
	vmovaps	912(%rsp), %xmm7        # 16-byte Reload
	vmovaps	896(%rsp), %xmm14       # 16-byte Reload
.LBB161_52:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmulps	1456(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovups	(%rbp,%r10,4), %xmm1
	vmovaps	%xmm1, 1424(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm9, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm9[1,3]
	vsubps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovups	(%rsi,%rdx,4), %xmm1
	vmovaps	%xmm1, 1408(%rsp)       # 16-byte Spill
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	1520(%rsp), %xmm6       # 16-byte Reload
	vshufps	$221, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm6[1,3]
	vsubps	%xmm1, %xmm0, %xmm11
	vmovaps	1184(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1168(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm14, %xmm7, %xmm1 # xmm1 = xmm7[1,3],xmm14[1,3]
	vmulps	%xmm15, %xmm3, %xmm2
	vsubps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm5, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm14
	vmovaps	1264(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1248(%rsp), %xmm0, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm7, 1760(%rsp)       # 16-byte Spill
	vmovaps	1504(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm12, %xmm3, %xmm1
	vsubps	%xmm13, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1200(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1488(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1232(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	%xmm10, %xmm3, %xmm3
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm5, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm3
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm7, %xmm0, %xmm7
	andl	$1, %eax
	je	.LBB161_53
# BB#54:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm7, 1264(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1488(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, 1504(%rsp)      # 16-byte Spill
	movq	%rdi, 1584(%rsp)        # 8-byte Spill
	vmovaps	%xmm9, 1744(%rsp)       # 16-byte Spill
	vxorps	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm5, 1888(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1856(%rsp)       # 16-byte Spill
	vmovaps	1680(%rsp), %xmm6       # 16-byte Reload
	jmp	.LBB161_55
	.align	16, 0x90
.LBB161_53:                             #   in Loop: Header=BB161_37 Depth=1
	movq	%rdi, 1584(%rsp)        # 8-byte Spill
	vshufps	$221, 720(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm6[1,3],mem[1,3]
	vmulps	928(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
	vshufps	$221, 736(%rsp), %xmm9, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm9[1,3],mem[1,3]
	vmovaps	%xmm9, 1744(%rsp)       # 16-byte Spill
	vsubps	%xmm13, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	%xmm4, 1856(%rsp)       # 16-byte Spill
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm5, %xmm1, %xmm1
	vmovaps	%xmm5, 1888(%rsp)       # 16-byte Spill
	vmaxps	%xmm8, %xmm1, %xmm1
	vxorps	%xmm8, %xmm8, %xmm8
	vsubps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm3, %xmm7, %xmm1
	vmovaps	%xmm7, 1264(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm14, %xmm1
	vmovaps	%xmm14, 1488(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm11, %xmm0
	vmovaps	%xmm11, 1504(%rsp)      # 16-byte Spill
	vmulps	160(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
.LBB161_55:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovdqa	192(%rsp), %xmm7        # 16-byte Reload
	movl	1120(%rsp), %r10d       # 4-byte Reload
	vmovdqa	1104(%rsp), %xmm3       # 16-byte Reload
	movl	1088(%rsp), %r11d       # 4-byte Reload
	movl	1072(%rsp), %ebx        # 4-byte Reload
	movl	1056(%rsp), %edi        # 4-byte Reload
	vmovaps	1440(%rsp), %xmm0       # 16-byte Reload
	testl	%r14d, %r14d
	jne	.LBB161_57
# BB#56:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	1680(%rsp), %xmm6       # 16-byte Reload
.LBB161_57:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	movl	%r10d, %eax
	andl	$1, %eax
	jne	.LBB161_58
# BB#59:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
	jmp	.LBB161_60
	.align	16, 0x90
.LBB161_58:                             #   in Loop: Header=BB161_37 Depth=1
	vaddps	752(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	640(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	176(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
.LBB161_60:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	movq	1024(%rsp), %rax        # 8-byte Reload
	testl	%r14d, %r14d
	je	.LBB161_62
# BB#61:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
.LBB161_62:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	addl	$-2, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI161_2(%rip), %xmm1 # xmm1 = [0,2,4,6]
	vpaddd	%xmm1, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%edi
	vmovd	%edx, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	vpinsrd	$1, %ecx, %xmm2, %xmm2
	vpinsrd	$2, %edx, %xmm2, %xmm2
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r11d
	vpinsrd	$3, %edx, %xmm2, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	1344(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	336(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm2, %xmm2
	vpxor	.LCPI161_9(%rip), %xmm2, %xmm2
	vmovdqa	320(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm4, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vmovdqa	1376(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vmovdqa	1328(%rsp), %xmm3       # 16-byte Reload
	vpsubd	%xmm0, %xmm3, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	1360(%rsp), %xmm14      # 16-byte Reload
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	movq	1392(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax,%r12,8), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm1, %xmm4, %xmm4
	vpminsd	%xmm14, %xmm4, %xmm4
	vpmaxsd	%xmm7, %xmm4, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	vpmulld	1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rbx
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r9,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r9,%rax,4), %xmm0, %xmm9 # xmm9 = xmm0[0,1,2],mem[0]
	movl	%r10d, %eax
	movq	1800(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	vmovups	(%rsi,%r8,4), %xmm12
	testb	$1, %al
	jne	.LBB161_64
# BB#63:                                #   in Loop: Header=BB161_37 Depth=1
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	816(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1808(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	1312(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm1, %xmm1
	vmovaps	1856(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	1888(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm1, %xmm1
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	784(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	800(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1552(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vmulps	1616(%rsp), %xmm9, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm2, %xmm4, %xmm4
	vmaxps	%xmm8, %xmm4, %xmm4
	vxorps	%xmm8, %xmm8, %xmm8
	vsubps	%xmm1, %xmm4, %xmm1
	vaddps	704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	688(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	656(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
.LBB161_64:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	movq	1584(%rsp), %rax        # 8-byte Reload
	vmovaps	1504(%rsp), %xmm10      # 16-byte Reload
	vmovaps	%xmm6, %xmm14
	testl	%r10d, %r14d
	jne	.LBB161_65
# BB#66:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	1824(%rsp), %xmm6       # 16-byte Reload
	jmp	.LBB161_67
	.align	16, 0x90
.LBB161_65:                             #   in Loop: Header=BB161_37 Depth=1
	vshufps	$221, 960(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm12[1,3],mem[1,3]
	orq	$6, %r13
	vmovups	(%rbp,%r13,4), %xmm1
	vshufps	$221, 976(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1456(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1824(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm5, %xmm4
	vsubps	%xmm13, %xmm1, %xmm1
	vmovaps	1856(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	1888(%rsp), %xmm2       # 16-byte Reload
	vminps	%xmm2, %xmm1, %xmm1
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovups	(%rsi,%rax,4), %xmm1
	vshufps	$221, 992(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	orq	$6, %r15
	vmovups	(%rbp,%r15,4), %xmm4
	vshufps	$221, 1008(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	%xmm15, %xmm5, %xmm5
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm2, %xmm4, %xmm4
	vmaxps	%xmm8, %xmm4, %xmm4
	vxorps	%xmm8, %xmm8, %xmm8
	vsubps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm10, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	1488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1264(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
.LBB161_67:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	movl	%r10d, %eax
	andl	$1, %eax
	je	.LBB161_68
# BB#69:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm6, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1600(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, %xmm0
	vmovaps	1760(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1840(%rsp), %xmm3       # 16-byte Reload
	jmp	.LBB161_70
	.align	16, 0x90
.LBB161_68:                             #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm6, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1600(%rsp)      # 16-byte Spill
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1840(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm9, %xmm2
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1744(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[0,2],mem[0,2]
	vsubps	%xmm13, %xmm4, %xmm4
	vmulps	1856(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm4, %xmm2, %xmm2
	vminps	1888(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmaxps	%xmm8, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	864(%rsp), %xmm1        # 16-byte Reload
	vaddps	1152(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vaddps	1136(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	880(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmulps	160(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	1760(%rsp), %xmm1       # 16-byte Reload
.LBB161_70:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm3, 1840(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1632(%rsp)      # 16-byte Spill
	testl	%r14d, %r14d
	jne	.LBB161_72
# BB#71:                                # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vmovaps	%xmm11, %xmm0
.LBB161_72:                             # %for f7.s0.v10.v104
                                        #   in Loop: Header=BB161_37 Depth=1
	vaddps	1568(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm14, %xmm1, %xmm1
	vmovaps	.LCPI161_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI161_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r10d, %rax
	movq	576(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r12
	cmpl	584(%rsp), %r12d        # 4-byte Folded Reload
	jne	.LBB161_37
.LBB161_73:                             # %end for f7.s0.v10.v105
	movq	1736(%rsp), %r13        # 8-byte Reload
	movl	584(%rsp), %eax         # 4-byte Reload
	cmpl	-120(%rsp), %eax        # 4-byte Folded Reload
	jge	.LBB161_100
# BB#74:                                # %for f7.s0.v10.v108.preheader
	movq	48(%rsp), %r11          # 8-byte Reload
	movl	%r11d, %eax
	negl	%eax
	movq	(%rsp), %rdi            # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	(%r11,%rdi), %edx
	leal	-1(%rdi,%rdi), %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%r11d, %ecx
	leal	-1(%r11,%rdi), %r15d
	cmpl	%ecx, %r15d
	cmovlel	%r15d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	xorl	%eax, %eax
	testl	%edx, %edx
	cmovgl	%eax, %r15d
	xorl	%r9d, %r9d
	cmpl	%r11d, %r15d
	cmovll	%r11d, %r15d
	testl	%edx, %edx
	cmovlel	%ecx, %r15d
	movq	40(%rsp), %rsi          # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	32(%rsp), %rbx          # 8-byte Reload
	leal	(%rbx,%rbx), %edi
	cltd
	idivl	%edi
	leal	-1(%rbx,%rbx), %eax
	movl	%ebx, %ebp
	sarl	$31, %ebp
	andnl	%edi, %ebp, %r8d
	negl	%edi
	andl	%ebp, %edi
	orl	%r8d, %edi
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%edi, %ebp
	addl	%edx, %ebp
	leal	(%rsi,%rbx), %edx
	subl	%ebp, %eax
	cmpl	%ebp, %ebx
	leal	-1(%rsi,%rbx), %r12d
	cmovgl	%ebp, %eax
	addl	%esi, %eax
	cmpl	%eax, %r12d
	cmovlel	%r12d, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%edx, %edx
	cmovgl	%r9d, %r12d
	cmpl	%esi, %r12d
	cmovll	%esi, %r12d
	testl	%edx, %edx
	cmovlel	%eax, %r12d
	movq	1800(%rsp), %r8         # 8-byte Reload
	movl	%r8d, %edx
	andl	$1, %edx
	movl	%edx, 1008(%rsp)        # 4-byte Spill
	movl	%r8d, %edx
	andl	$63, %edx
	movq	%rdx, 1680(%rsp)        # 8-byte Spill
	movq	%rdx, %rdi
	testl	%esi, %esi
	cmovgl	%eax, %r12d
	movq	608(%rsp), %rax         # 8-byte Reload
	movl	%eax, %edx
	movq	848(%rsp), %rax         # 8-byte Reload
	imull	%eax, %edx
	addl	%esi, %edx
	movl	%edx, 1888(%rsp)        # 4-byte Spill
	testl	%r11d, %r11d
	cmovgl	%ecx, %r15d
	movq	136(%rsp), %rbp         # 8-byte Reload
	movl	%ebp, %eax
	movq	8(%rsp), %rcx           # 8-byte Reload
	imull	%ecx, %eax
	addl	%r11d, %eax
	movl	128(%rsp), %ebx         # 4-byte Reload
	andl	$-32, %ebx
	movslq	%eax, %rcx
	movslq	%r8d, %rax
	leaq	1(%rax), %r9
	imulq	%rbp, %r9
	movslq	%r15d, %rsi
	subq	%rcx, %r9
	addq	%rsi, %r9
	leaq	-1(%rax), %r14
	imulq	%rbp, %r14
	subq	%rcx, %r14
	addq	%rsi, %r14
	leaq	-2(%rax), %r11
	imulq	%rbp, %r11
	subq	%rcx, %r11
	addq	%rsi, %r11
	movq	%rbp, %r10
	imulq	%rax, %r10
	subq	%rcx, %r10
	addq	%rsi, %r10
	addq	$2, %rax
	imulq	%rbp, %rax
	subq	%rcx, %rax
	movq	96(%rsp), %rcx          # 8-byte Reload
	leaq	32(%rcx), %rcx
	movq	%rdi, %rbp
	imulq	%rcx, %rbp
	addq	%rsi, %rax
	movq	144(%rsp), %rsi         # 8-byte Reload
	movq	%rsi, %rcx
	sarq	$63, %rcx
	andq	%rsi, %rcx
	subq	%rcx, %rbp
	movq	%rbp, 992(%rsp)         # 8-byte Spill
	movl	592(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	movq	-8(%rsp), %rsi          # 8-byte Reload
	movl	-28(%rsp), %ebp         # 4-byte Reload
	cmpl	%esi, %ebp
	cmovgel	%ebp, %esi
	movl	-32(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	movl	-36(%rsp), %ebp         # 4-byte Reload
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	movq	-24(%rsp), %rbp         # 8-byte Reload
	cmpl	%ebp, %esi
	cmovll	%ebp, %esi
	testl	%esi, %esi
	movl	$0, %edx
	cmovsl	%edx, %esi
	notl	%esi
	cmpl	%esi, %ecx
	cmovgel	%ecx, %esi
	leal	10(%r8), %ebp
	movl	112(%rsp), %ecx         # 4-byte Reload
	subl	%ecx, %ebp
	addl	$64, %ebx
	imull	%ebx, %ebp
	movq	%rbp, 1824(%rsp)        # 8-byte Spill
	leal	8(%r8), %ebp
	subl	%ecx, %ebp
	imull	%ebx, %ebp
	movq	%rbp, 1808(%rsp)        # 8-byte Spill
	leal	6(%r8), %ebp
	subl	%ecx, %ebp
	imull	%ebx, %ebp
	movq	%rbp, 1776(%rsp)        # 8-byte Spill
	leal	7(%r8), %edi
	subl	%ecx, %edi
	imull	%ebx, %edi
	leal	9(%r8), %r15d
	subl	%ecx, %r15d
	imull	%ebx, %r15d
	movq	64(%rsp), %rbp          # 8-byte Reload
	vbroadcastss	(%rbp,%r9,4), %xmm0
	vmovaps	%xmm0, 1504(%rsp)       # 16-byte Spill
	leal	2(%r8), %r9d
	andl	$63, %r9d
	movq	832(%rsp), %rcx         # 8-byte Reload
	leal	3(%rcx), %ecx
	imull	%ecx, %r9d
	movq	%r9, 1760(%rsp)         # 8-byte Spill
	leal	62(%r8), %edx
	andl	$63, %edx
	imull	%ecx, %edx
	movq	%rdx, 1744(%rsp)        # 8-byte Spill
	vbroadcastss	(%rbp,%r14,4), %xmm0
	vmovaps	%xmm0, 1488(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbp,%r10,4), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	leal	63(%r8), %r14d
	andl	$63, %r14d
	imull	%ecx, %r14d
	leal	1(%r8), %ebx
	andl	$63, %ebx
	imull	%ecx, %ebx
	vbroadcastss	(%rbp,%r11,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vbroadcastss	(%rbp,%rax,4), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	movq	%rdi, %rdx
	leal	(,%rsi,8), %eax
	movl	$-8, %r11d
	subl	%eax, %r11d
	movq	%r11, 976(%rsp)         # 8-byte Spill
	movq	1680(%rsp), %rax        # 8-byte Reload
	imull	%ecx, %eax
	movq	%rax, 1680(%rsp)        # 8-byte Spill
	movq	-56(%rsp), %rax         # 8-byte Reload
	negl	%eax
	movl	-60(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-64(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-108(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-112(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-116(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	-48(%rsp), %rax         # 8-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movq	1712(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	negl	%ecx
	movl	%esi, %eax
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	leal	1(%rsi,%rax), %esi
	movslq	1888(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 1520(%rsp)        # 8-byte Spill
	movslq	%r12d, %rax
	movq	%rax, 1888(%rsp)        # 8-byte Spill
	vmovss	.LCPI161_1(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm0, %xmm0
	vmovss	24(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm0, %xmm1
	vmovss	20(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	28(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm2, %xmm0, %xmm0
	vdivss	%xmm0, %xmm4, %xmm0
	vbroadcastss	%xmm0, %xmm9
	vmovaps	%xmm9, 1440(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm15
	vmovaps	%xmm15, 1456(%rsp)      # 16-byte Spill
	vbroadcastss	.LCPI161_3(%rip), %xmm4
	vbroadcastss	.LCPI161_4(%rip), %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI161_5(%rip), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_6(%rip), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	movq	608(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %r12
	movl	1008(%rsp), %r9d        # 4-byte Reload
	movq	1392(%rsp), %rax        # 8-byte Reload
	movl	%eax, %edi
	.align	16, 0x90
.LBB161_75:                             # %for f7.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movq	%rdi, 1264(%rsp)        # 8-byte Spill
	movl	%esi, 1288(%rsp)        # 4-byte Spill
	movq	%rbx, 1296(%rsp)        # 8-byte Spill
	movq	%r14, 1312(%rsp)        # 8-byte Spill
	movq	%r15, 1328(%rsp)        # 8-byte Spill
	movq	%rdx, 1344(%rsp)        # 8-byte Spill
	testl	%r9d, %r9d
	sete	1648(%rsp)              # 1-byte Folded Spill
	setne	1248(%rsp)              # 1-byte Folded Spill
	leal	(%r11,%rdi), %eax
	movl	%eax, 1856(%rsp)        # 4-byte Spill
	movl	%eax, %ecx
	andl	$1, %ecx
	movl	%ecx, 1664(%rsp)        # 4-byte Spill
	sete	1232(%rsp)              # 1-byte Folded Spill
	movslq	%eax, %r10
	leaq	-1(%r10), %rax
	movq	608(%rsp), %r8          # 8-byte Reload
	imulq	%r8, %rax
	movq	%rdx, %rdi
	movq	1520(%rsp), %rbp        # 8-byte Reload
	subq	%rbp, %rax
	addq	1888(%rsp), %rax        # 8-byte Folded Reload
	movq	1904(%rsp), %r9         # 8-byte Reload
	leaq	(%r9,%rax,4), %rcx
	leaq	(%rcx,%r12,4), %rdx
	leaq	(%rdx,%r12,4), %rsi
	vmovss	(%r9,%rax,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	1(%r10), %rax
	imulq	%r8, %rax
	vinsertps	$48, (%rsi,%r12,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm11, 1584(%rsp)      # 16-byte Spill
	subq	%rbp, %rax
	addq	1888(%rsp), %rax        # 8-byte Folded Reload
	leaq	(%r9,%rax,4), %rcx
	leaq	(%rcx,%r12,4), %rdx
	leaq	(%rdx,%r12,4), %rsi
	vmovss	(%r9,%rax,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%r12,4), %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm6, 1552(%rsp)       # 16-byte Spill
	leal	(%r11,%rbx), %eax
	cltq
	vmovups	8(%r13,%rax,4), %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vmovups	24(%r13,%rax,4), %xmm1
	vmovaps	%xmm1, 1568(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm1[1,3]
	vmovaps	1504(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm6, %xmm2
	leal	(%r11,%r15), %ecx
	movslq	%ecx, %rbx
	movq	1880(%rsp), %r15        # 8-byte Reload
	vmovups	32(%r15,%rbx,4), %xmm1
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	vmovups	48(%r15,%rbx,4), %xmm3
	vmovaps	%xmm3, 1376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[1,3],xmm3[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm1
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
	leal	(%r11,%r14), %ecx
	movslq	%ecx, %rcx
	vmovups	8(%r13,%rcx,4), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vmovups	24(%r13,%rcx,4), %xmm14
	vshufps	$221, %xmm14, %xmm0, %xmm5 # xmm5 = xmm0[1,3],xmm14[1,3]
	vmovaps	1488(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm6, %xmm6
	leal	(%r11,%rdi), %edx
	movslq	%edx, %rdx
	vmovups	32(%r15,%rdx,4), %xmm9
	vmovups	48(%r15,%rdx,4), %xmm13
	vshufps	$221, %xmm13, %xmm9, %xmm0 # xmm0 = xmm9[1,3],xmm13[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm5, %xmm0, %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmovups	40(%r15,%rbx,4), %xmm3
	vmovaps	%xmm3, 1360(%rsp)       # 16-byte Spill
	movq	%rbx, %rsi
	orq	$6, %rsi
	vmovups	(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm3[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm12, %xmm11, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovups	(%r13,%rax,4), %xmm2
	vmovaps	%xmm2, 1056(%rsp)       # 16-byte Spill
	vmovups	16(%r13,%rax,4), %xmm10
	vmovaps	%xmm10, 1200(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm10, %xmm2, %xmm6 # xmm6 = xmm2[1,3],xmm10[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 1136(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	orq	$6, %rsi
	vmovups	40(%r15,%rdx,4), %xmm2
	vmovaps	%xmm2, 1216(%rsp)       # 16-byte Spill
	vmovups	(%r15,%rsi,4), %xmm0
	vmovaps	%xmm0, 1040(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmulps	%xmm7, %xmm11, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm1, %xmm11
	vmovups	(%r13,%rcx,4), %xmm1
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	vmovups	16(%r13,%rcx,4), %xmm8
	vmovaps	%xmm8, 1184(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm8, %xmm1, %xmm6 # xmm6 = xmm1[1,3],xmm8[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovaps	1600(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm14[0,2]
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	movq	%r10, %rsi
	imulq	%r8, %rsi
	vshufps	$136, %xmm13, %xmm9, %xmm0 # xmm0 = xmm9[0,2],xmm13[0,2]
	subq	%rbp, %rsi
	addq	1888(%rsp), %rsi        # 8-byte Folded Reload
	leaq	(%r9,%rsi,4), %rdi
	leaq	(%rdi,%r12,4), %r14
	vmovss	(%r9,%rsi,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%r14,%r12,4), %rsi
	vinsertps	$32, (%r14,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rbp, %r11
	movq	1888(%rsp), %rbp        # 8-byte Reload
	leaq	2(%r10), %rdi
	imulq	%r8, %rdi
	vinsertps	$48, (%rsi,%r12,4), %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm5, 1408(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm7, %xmm5, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	subq	%r11, %rdi
	addq	%rbp, %rdi
	leaq	(%r9,%rdi,4), %rsi
	vmovss	(%r9,%rdi,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%rsi,%r12,4), %rsi
	vinsertps	$32, (%rsi,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rsi,%r12,4), %rsi
	vinsertps	$48, (%rsi,%r12,4), %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm6, 1600(%rsp)       # 16-byte Spill
	vmovups	56(%r15,%rdx,4), %xmm14
	vshufps	$136, %xmm14, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm14[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm7, %xmm6, %xmm2
	vmulps	%xmm1, %xmm2, %xmm9
	vmovaps	1632(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1568(%rsp), %xmm1, %xmm13 # 16-byte Folded Reload
                                        # xmm13 = xmm1[0,2],mem[0,2]
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	%xmm4, %xmm7
	vmovaps	1536(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1376(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm12, %xmm5, %xmm4
	vmulps	%xmm2, %xmm4, %xmm4
	vmovups	56(%r15,%rbx,4), %xmm2
	vmovaps	%xmm2, 1632(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm12, %xmm6, %xmm6
	vmulps	%xmm2, %xmm6, %xmm6
	leaq	-2(%r10), %rdx
	imulq	%r8, %rdx
	subq	%r11, %rdx
	addq	%rbp, %rdx
	leaq	(%r9,%rdx,4), %rsi
	vmovss	(%r9,%rdx,4), %xmm2     # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	leaq	(%rsi,%r12,4), %rdx
	movb	1648(%rsp), %sil        # 1-byte Reload
	vinsertps	$32, (%rdx,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leaq	(%rdx,%r12,4), %rdx
	vinsertps	$48, (%rdx,%r12,4), %xmm2, %xmm15 # xmm15 = xmm2[0,1,2],mem[0]
	leaq	3(%r10), %rdx
	imulq	%r8, %rdx
	movl	1664(%rsp), %ebp        # 4-byte Reload
	movb	%sil, %bl
	andb	%bpl, %bl
	vminps	%xmm7, %xmm0, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm2
	vminps	%xmm7, %xmm9, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm3
	vminps	%xmm7, %xmm4, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm4
	vminps	%xmm7, %xmm6, %xmm0
	vmaxps	%xmm1, %xmm0, %xmm0
	vmovups	32(%r13,%rcx,4), %xmm1
	vmovups	32(%r13,%rax,4), %xmm5
	vshufps	$136, %xmm1, %xmm8, %xmm11 # xmm11 = xmm8[0,2],xmm1[0,2]
	vshufps	$136, %xmm5, %xmm10, %xmm10 # xmm10 = xmm10[0,2],xmm5[0,2]
	jne	.LBB161_76
# BB#77:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm5, 1376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1568(%rsp)      # 16-byte Spill
	vmovaps	1104(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1136(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1152(%rsp), %xmm14      # 16-byte Reload
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	.LBB161_78
	.align	16, 0x90
.LBB161_76:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm5, 1376(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1568(%rsp)      # 16-byte Spill
	vmovaps	1152(%rsp), %xmm14      # 16-byte Reload
	vaddps	1168(%rsp), %xmm14, %xmm6 # 16-byte Folded Reload
	vmovaps	1104(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1136(%rsp), %xmm12      # 16-byte Reload
	vaddps	%xmm12, %xmm6, %xmm6
	vaddps	%xmm8, %xmm6, %xmm6
	vmulps	960(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
.LBB161_78:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	vmovaps	1120(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm9
	vsubps	%xmm11, %xmm3, %xmm11
	vsubps	%xmm13, %xmm4, %xmm13
	vsubps	%xmm10, %xmm0, %xmm1
	subq	1520(%rsp), %rdx        # 8-byte Folded Reload
	movl	1856(%rsp), %eax        # 4-byte Reload
	movq	1800(%rsp), %rcx        # 8-byte Reload
	orl	%ecx, %eax
	vmovaps	%xmm15, %xmm3
	andl	$1, %eax
	je	.LBB161_79
# BB#80:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vxorps	%xmm15, %xmm15, %xmm15
	jmp	.LBB161_81
	.align	16, 0x90
.LBB161_79:                             #   in Loop: Header=BB161_75 Depth=1
	vmulps	1504(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	1072(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1360(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm2[0,2],mem[0,2]
	vmovaps	1456(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vmovaps	1440(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm6
	vmulps	%xmm6, %xmm0, %xmm0
	vmovaps	1056(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 1200(%rsp), %xmm5, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm5[0,2],mem[0,2]
	vminps	%xmm7, %xmm0, %xmm0
	vxorps	%xmm15, %xmm15, %xmm15
	vmaxps	%xmm15, %xmm0, %xmm0
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	1488(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	vmovaps	1040(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 1216(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm2, %xmm5, %xmm5
	vmulps	%xmm5, %xmm4, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	1024(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1184(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm2[0,2],mem[0,2]
	vminps	%xmm7, %xmm5, %xmm5
	vmaxps	%xmm15, %xmm5, %xmm5
	vsubps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm5, %xmm9, %xmm5
	vaddps	%xmm11, %xmm5, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vaddps	%xmm0, %xmm13, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
.LBB161_81:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	addq	1888(%rsp), %rdx        # 8-byte Folded Reload
	testl	%eax, %eax
	je	.LBB161_82
# BB#83:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB161_84
	.align	16, 0x90
.LBB161_82:                             #   in Loop: Header=BB161_75 Depth=1
	vaddps	%xmm1, %xmm11, %xmm0
	vaddps	%xmm0, %xmm13, %xmm0
	vaddps	%xmm0, %xmm9, %xmm0
	vmulps	960(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
.LBB161_84:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	leaq	(%r9,%rdx,4), %rax
	vmovss	(%r9,%rdx,4), %xmm0     # xmm0 = mem[0],zero,zero,zero
	leaq	(%rax,%r12,4), %rcx
	vinsertps	$16, (%rax,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rcx,%r12,4), %rax
	vinsertps	$32, (%rcx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rax,%r12,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	andb	%bpl, %sil
	jne	.LBB161_85
# BB#86:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm1, 1168(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	vmovaps	1616(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB161_87
	.align	16, 0x90
.LBB161_85:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm3, 1152(%rsp)       # 16-byte Spill
	vmovaps	1616(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1200(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1632(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	1504(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmovaps	1456(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	1440(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm7, %xmm1, %xmm1
	vmaxps	%xmm15, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	1184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1568(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	1488(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm4, 1136(%rsp)       # 16-byte Spill
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm8, %xmm14, %xmm2
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm12, %xmm1
	vaddps	1168(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1168(%rsp)       # 16-byte Spill
.LBB161_87:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	movq	%r13, %rsi
	movl	1008(%rsp), %r9d        # 4-byte Reload
	movq	1680(%rsp), %rbp        # 8-byte Reload
	vmovaps	1584(%rsp), %xmm3       # 16-byte Reload
	movq	976(%rsp), %r11         # 8-byte Reload
	leal	(%r11,%rbp), %eax
	movslq	%eax, %r13
	vmovups	8(%rsi,%r13,4), %xmm1
	vmovups	24(%rsi,%r13,4), %xmm2
	vmovups	(%rsi,%r13,4), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vmovups	16(%rsi,%r13,4), %xmm4
	vmovaps	%xmm4, 1664(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm0, %xmm6 # xmm6 = xmm0[1,3],xmm4[1,3]
	vmovaps	1472(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm3, %xmm3
	movq	1808(%rsp), %rax        # 8-byte Reload
	leal	(%r11,%rax), %eax
	movslq	%eax, %rdx
	movq	%r15, %rdi
	vmovups	40(%rdi,%rdx,4), %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, %xmm4
	vmovups	32(%rdi,%rdx,4), %xmm10
	movq	%rdx, %rax
	orq	$6, %rax
	vmovups	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 1184(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[1,3],xmm4[1,3]
	vmovaps	1456(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm4
	vmovaps	1440(%rsp), %xmm9       # 16-byte Reload
	vmulps	%xmm4, %xmm9, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm7, %xmm3, %xmm3
	vmaxps	%xmm15, %xmm3, %xmm3
	vsubps	%xmm6, %xmm3, %xmm3
	vmovaps	%xmm3, 1536(%rsp)       # 16-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	leal	(%r11,%rax), %eax
	movslq	%eax, %r8
	vmovups	8(%rsi,%r8,4), %xmm4
	vmovaps	%xmm4, 1360(%rsp)       # 16-byte Spill
	vmovups	24(%rsi,%r8,4), %xmm13
	vmovaps	1552(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm11, %xmm3, %xmm6
	movq	1776(%rsp), %rax        # 8-byte Reload
	leal	(%r11,%rax), %eax
	movslq	%eax, %r15
	vmovaps	%xmm7, %xmm11
	vxorps	%xmm7, %xmm7, %xmm7
	vmovups	32(%rdi,%r15,4), %xmm14
	vmovaps	%xmm0, %xmm15
	vmovups	48(%rdi,%r15,4), %xmm12
	vshufps	$221, %xmm12, %xmm14, %xmm0 # xmm0 = xmm14[1,3],xmm12[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vshufps	$221, %xmm13, %xmm4, %xmm6 # xmm6 = xmm4[1,3],xmm13[1,3]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm7, %xmm0, %xmm0
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm2[0,2]
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vmovups	48(%rdi,%rdx,4), %xmm2
	vshufps	$221, %xmm2, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm2[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm5, %xmm3, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm3, %xmm6
	movq	1824(%rsp), %rax        # 8-byte Reload
	leal	(%r11,%rax), %eax
	cltq
	vmovups	32(%rdi,%rax,4), %xmm8
	vmovups	48(%rdi,%rax,4), %xmm0
	vshufps	$221, %xmm0, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	movq	1760(%rsp), %rbx        # 8-byte Reload
	leal	(%r11,%rbx), %ebx
	movslq	%ebx, %rbx
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm7, %xmm3, %xmm3
	vmovups	8(%rsi,%rbx,4), %xmm6
	vmovups	24(%rsi,%rbx,4), %xmm4
	vshufps	$221, %xmm4, %xmm6, %xmm7 # xmm7 = xmm6[1,3],xmm4[1,3]
	vsubps	%xmm7, %xmm3, %xmm3
	vmovaps	%xmm3, 1552(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,2],xmm2[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm9, %xmm2
	vmovaps	1408(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm5, %xmm7, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vshufps	$136, %xmm4, %xmm6, %xmm3 # xmm3 = xmm6[0,2],xmm4[0,2]
	vmovaps	%xmm3, 1216(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm8, %xmm0 # xmm0 = xmm8[0,2],xmm0[0,2]
	vxorps	%xmm8, %xmm8, %xmm8
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm1, %xmm13 # xmm13 = xmm1[0,2],xmm13[0,2]
	vshufps	$136, %xmm12, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm12[0,2]
	vmovaps	1616(%rsp), %xmm10      # 16-byte Reload
	vmulps	%xmm10, %xmm7, %xmm3
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm1, %xmm3, %xmm3
	vmovups	56(%rdi,%rdx,4), %xmm7
	vmovaps	1648(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm7[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	1600(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm4, %xmm6
	movl	%r9d, %edx
	andl	1856(%rsp), %edx        # 4-byte Folded Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm12
	vminps	%xmm11, %xmm3, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vminps	%xmm11, %xmm6, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	vmovaps	1376(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	1632(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vminps	%xmm11, %xmm2, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovups	32(%rsi,%r13,4), %xmm6
	vmovaps	1664(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, %xmm6, %xmm2, %xmm14 # xmm14 = xmm2[0,2],xmm6[0,2]
	vmovups	16(%rsi,%rbx,4), %xmm4
	vmovups	40(%rdi,%rax,4), %xmm1
	vmovups	16(%rsi,%r8,4), %xmm2
	vmovups	40(%rdi,%r15,4), %xmm3
	jne	.LBB161_88
# BB#89:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm1, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1584(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1376(%rsp)       # 16-byte Spill
	movq	1344(%rsp), %rdi        # 8-byte Reload
	movq	1312(%rsp), %r14        # 8-byte Reload
	vmovaps	1840(%rsp), %xmm10      # 16-byte Reload
	movb	1248(%rsp), %cl         # 1-byte Reload
	movb	1232(%rsp), %dl         # 1-byte Reload
	vmovaps	1088(%rsp), %xmm7       # 16-byte Reload
	jmp	.LBB161_90
	.align	16, 0x90
.LBB161_88:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm6, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1376(%rsp)       # 16-byte Spill
	vmovaps	1584(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm3, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1056(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1040(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1088(%rsp)      # 16-byte Spill
	vmovaps	%xmm2, %xmm13
	vmovaps	%xmm13, 1104(%rsp)      # 16-byte Spill
	vmulps	1840(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	movq	%rax, %rcx
	orq	$6, %rcx
	vmovups	(%rdi,%rcx,4), %xmm7
	vshufps	$221, %xmm1, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm1[1,3]
	vsubps	%xmm15, %xmm7, %xmm7
	vmulps	%xmm7, %xmm9, %xmm7
	vmulps	%xmm7, %xmm2, %xmm2
	vmovups	(%rsi,%rbx,4), %xmm7
	vshufps	$221, %xmm4, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm4[1,3]
	vminps	%xmm11, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm10, %xmm6, %xmm7
	movq	%r15, %rcx
	orq	$6, %rcx
	vmovups	(%rdi,%rcx,4), %xmm6
	vshufps	$221, %xmm3, %xmm6, %xmm6 # xmm6 = xmm6[1,3],xmm3[1,3]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmovups	(%rsi,%r8,4), %xmm7
	vshufps	$221, %xmm13, %xmm7, %xmm7 # xmm7 = xmm7[1,3],xmm13[1,3]
	vmovaps	1088(%rsp), %xmm13      # 16-byte Reload
	vminps	%xmm11, %xmm6, %xmm6
	vmaxps	%xmm8, %xmm6, %xmm6
	vsubps	%xmm7, %xmm6, %xmm6
	vaddps	1536(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vaddps	%xmm6, %xmm2, %xmm2
	vaddps	1568(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vmovaps	%xmm5, 1584(%rsp)       # 16-byte Spill
	vaddps	1552(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1424(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	movq	1344(%rsp), %rdi        # 8-byte Reload
	movq	1312(%rsp), %r14        # 8-byte Reload
	vmovaps	1840(%rsp), %xmm10      # 16-byte Reload
	movb	1248(%rsp), %cl         # 1-byte Reload
	movb	1232(%rsp), %dl         # 1-byte Reload
.LBB161_90:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	1216(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1408(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1360(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm2, %xmm12, %xmm8
	vsubps	1680(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vsubps	%xmm13, %xmm3, %xmm0
	vsubps	%xmm14, %xmm1, %xmm12
	andb	%dl, %cl
	jne	.LBB161_91
# BB#92:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB161_93
	.align	16, 0x90
.LBB161_91:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	1184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[0,2],mem[0,2]
	vsubps	%xmm15, %xmm5, %xmm5
	vmovaps	1152(%rsp), %xmm1       # 16-byte Reload
	vmulps	1472(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
	vmulps	%xmm5, %xmm9, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vminps	%xmm11, %xmm5, %xmm5
	vaddps	%xmm8, %xmm2, %xmm6
	vmovaps	1200(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1664(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm1[0,2],mem[0,2]
	vxorps	%xmm1, %xmm1, %xmm1
	vmaxps	%xmm1, %xmm5, %xmm5
	vaddps	%xmm6, %xmm0, %xmm6
	vsubps	%xmm7, %xmm5, %xmm5
	vaddps	%xmm12, %xmm6, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vmulps	944(%rsp), %xmm5, %xmm7 # 16-byte Folded Reload
.LBB161_93:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	movq	%rdi, %rdx
	testb	%cl, %cl
	jne	.LBB161_94
# BB#95:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	movq	1328(%rsp), %r15        # 8-byte Reload
	movq	1296(%rsp), %rbx        # 8-byte Reload
	movq	1264(%rsp), %rdi        # 8-byte Reload
	vmovaps	1168(%rsp), %xmm2       # 16-byte Reload
	jmp	.LBB161_96
	.align	16, 0x90
.LBB161_94:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	1600(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm10, %xmm6, %xmm5
	movq	1880(%rsp), %rcx        # 8-byte Reload
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 56(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,2],mem[0,2]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmovaps	1056(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 32(%rsi,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm1, %xmm3, %xmm3
	vsubps	%xmm5, %xmm3, %xmm3
	vmulps	1616(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vmovaps	1072(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, 56(%rcx,%r15,4), %xmm6, %xmm6 # xmm6 = xmm6[0,2],mem[0,2]
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vmovaps	1104(%rsp), %xmm6       # 16-byte Reload
	vshufps	$136, 32(%rsi,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,2],mem[0,2]
	vminps	%xmm11, %xmm5, %xmm5
	vmaxps	%xmm1, %xmm5, %xmm5
	vsubps	%xmm6, %xmm5, %xmm5
	vaddps	%xmm2, %xmm0, %xmm0
	vaddps	%xmm8, %xmm0, %xmm0
	vaddps	%xmm5, %xmm0, %xmm0
	vaddps	%xmm0, %xmm12, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vmulps	1424(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	movq	1328(%rsp), %r15        # 8-byte Reload
	movq	1296(%rsp), %rbx        # 8-byte Reload
	movq	1264(%rsp), %rdi        # 8-byte Reload
.LBB161_96:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	1536(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1568(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1552(%rsp), %xmm6       # 16-byte Reload
	vaddps	1680(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	andl	%r9d, 1856(%rsp)        # 4-byte Folded Spill
	jne	.LBB161_97
# BB#98:                                # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm11, %xmm4
	vmovaps	%xmm10, 1840(%rsp)      # 16-byte Spill
	movq	%rsi, %r13
	jmp	.LBB161_99
	.align	16, 0x90
.LBB161_97:                             #   in Loop: Header=BB161_75 Depth=1
	vmovaps	%xmm10, 1840(%rsp)      # 16-byte Spill
	movq	%rsi, %r13
	vmovaps	1664(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1120(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm2[1,3],mem[1,3]
	vmovaps	1136(%rsp), %xmm2       # 16-byte Reload
	vmulps	1472(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmovaps	1648(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm11, %xmm4
	vmaxps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm7, %xmm2, %xmm1
	vaddps	1584(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm13, %xmm1
	vmulps	944(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
.LBB161_99:                             # %for f7.s0.v10.v108
                                        #   in Loop: Header=BB161_75 Depth=1
	movl	1288(%rsp), %esi        # 4-byte Reload
	vmovaps	1632(%rsp), %xmm1       # 16-byte Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vmovaps	.LCPI161_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI161_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movq	992(%rsp), %rax         # 8-byte Reload
	leaq	(%r10,%rax), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	movq	1760(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	movq	1824(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	%rax, 1824(%rsp)        # 8-byte Spill
	movq	1808(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	%rax, 1808(%rsp)        # 8-byte Spill
	movq	1744(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	%rax, 1744(%rsp)        # 8-byte Spill
	movq	1776(%rsp), %rax        # 8-byte Reload
	addl	$8, %eax
	movq	%rax, 1776(%rsp)        # 8-byte Spill
	addl	$8, %r14d
	addl	$8, %edx
	addl	$8, %ebx
	addl	$8, %r15d
	addl	$8, %ebp
	movq	%rbp, 1680(%rsp)        # 8-byte Spill
	addl	$8, %edi
	addl	$-1, %esi
	jne	.LBB161_75
.LBB161_100:                            # %end for f7.s0.v10.v109
	movl	-120(%rsp), %eax        # 4-byte Reload
	cmpl	592(%rsp), %eax         # 4-byte Folded Reload
	vmovss	28(%rsp), %xmm7         # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	24(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmovss	20(%rsp), %xmm15        # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	movq	8(%rsp), %r9            # 8-byte Reload
	movq	(%rsp), %rdi            # 8-byte Reload
	jge	.LBB161_164
# BB#101:                               # %for f7.s0.v10.v1012.preheader
	movq	48(%rsp), %r15          # 8-byte Reload
	movl	%r15d, %eax
	negl	%eax
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rdi,%rdi), %ecx
	subl	%eax, %ecx
	cmpl	%eax, %edi
	cmovgl	%eax, %ecx
	addl	%r15d, %ecx
	leal	(%r15,%rdi), %eax
	leal	-1(%r15,%rdi), %r13d
	cmpl	%ecx, %r13d
	cmovlel	%r13d, %ecx
	cmpl	%r15d, %ecx
	cmovll	%r15d, %ecx
	xorl	%r8d, %r8d
	testl	%eax, %eax
	cmovgl	%r8d, %r13d
	cmpl	%r15d, %r13d
	cmovll	%r15d, %r13d
	testl	%eax, %eax
	cmovlel	%ecx, %r13d
	movq	40(%rsp), %rbx          # 8-byte Reload
	movl	%ebx, %eax
	negl	%eax
	movq	32(%rsp), %rsi          # 8-byte Reload
	leal	(%rsi,%rsi), %edi
	cltd
	idivl	%edi
	movl	%esi, %eax
	sarl	$31, %eax
	andnl	%edi, %eax, %ebp
	negl	%edi
	andl	%eax, %edi
	orl	%ebp, %edi
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%edi, %ebp
	addl	%edx, %ebp
	leal	-1(%rsi,%rsi), %eax
	subl	%ebp, %eax
	cmpl	%ebp, %esi
	cmovgl	%ebp, %eax
	leal	(%rbx,%rsi), %edx
	leal	-1(%rbx,%rsi), %r14d
	addl	%ebx, %eax
	cmpl	%eax, %r14d
	cmovlel	%r14d, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	testl	%edx, %edx
	cmovgl	%r8d, %r14d
	cmpl	%ebx, %r14d
	cmovll	%ebx, %r14d
	testl	%edx, %edx
	cmovlel	%eax, %r14d
	movq	1800(%rsp), %r11        # 8-byte Reload
	movl	%r11d, %edx
	andl	$1, %edx
	movl	%edx, 688(%rsp)         # 4-byte Spill
	movl	%r11d, %edx
	andl	$63, %edx
	movq	%rdx, 1808(%rsp)        # 8-byte Spill
	movq	%rdx, %rsi
	testl	%ebx, %ebx
	cmovgl	%eax, %r14d
	movq	608(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r12d
	movq	848(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r12d
	addl	%ebx, %r12d
	testl	%r15d, %r15d
	cmovgl	%ecx, %r13d
	movq	136(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r9d
	movslq	%r11d, %rbx
	leaq	1(%rbx), %rcx
	imulq	%rax, %rcx
	leaq	-1(%rbx), %rdx
	imulq	%rax, %rdx
	leaq	-2(%rbx), %rdi
	imulq	%rax, %rdi
	movq	%rax, %rbp
	imulq	%rbx, %rbp
	addl	%r15d, %r9d
	addq	$2, %rbx
	imulq	%rax, %rbx
	movslq	%r9d, %rax
	subq	%rax, %rcx
	subq	%rax, %rdx
	subq	%rax, %rdi
	subq	%rax, %rbp
	subq	%rax, %rbx
	movslq	%r13d, %rax
	addq	%rax, %rcx
	movq	%rcx, 1824(%rsp)        # 8-byte Spill
	addq	%rax, %rdx
	movq	%rdx, 1840(%rsp)        # 8-byte Spill
	addq	%rax, %rdi
	movq	%rdi, 1888(%rsp)        # 8-byte Spill
	addq	%rax, %rbp
	movq	%rbp, 1856(%rsp)        # 8-byte Spill
	addq	%rax, %rbx
	movq	96(%rsp), %rax          # 8-byte Reload
	addq	$32, %rax
	movq	%rsi, %rdx
	imulq	%rax, %rdx
	movq	144(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rax
	sarq	$63, %rax
	andq	%rcx, %rax
	subq	%rax, %rdx
	movq	%rdx, 672(%rsp)         # 8-byte Spill
	movq	-16(%rsp), %r15         # 8-byte Reload
	vmovd	%r15d, %xmm8
	movl	128(%rsp), %r10d        # 4-byte Reload
	andl	$-32, %r10d
	leal	-1(%r15), %r13d
	leal	-2(%r15), %eax
	movl	%eax, 1744(%rsp)        # 4-byte Spill
	leal	2(%r15), %eax
	movl	%eax, 1760(%rsp)        # 4-byte Spill
	leal	1(%r15), %eax
	movl	%eax, 1776(%rsp)        # 4-byte Spill
	addl	$-3, %r15d
	movq	-56(%rsp), %rax         # 8-byte Reload
	negl	%eax
	movl	-60(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-64(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-108(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-112(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-116(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	-48(%rsp), %rax         # 8-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movq	1712(%rsp), %rdx        # 8-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	negl	%edx
	movq	-8(%rsp), %rcx          # 8-byte Reload
	movl	-28(%rsp), %eax         # 4-byte Reload
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-32(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	-36(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	-24(%rsp), %rax         # 8-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	592(%rsp), %eax         # 4-byte Reload
	notl	%eax
	testl	%ecx, %ecx
	cmovsl	%r8d, %ecx
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	notl	%ecx
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leal	10(%r11), %r9d
	movl	112(%rsp), %esi         # 4-byte Reload
	subl	%esi, %r9d
	addl	$64, %r10d
	imull	%r10d, %r9d
	leal	8(%r11), %r8d
	subl	%esi, %r8d
	imull	%r10d, %r8d
	leal	6(%r11), %edx
	subl	%esi, %edx
	imull	%r10d, %edx
	movq	152(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rdi), %eax
	vmovd	%eax, %xmm9
	leal	7(%r11), %eax
	subl	%esi, %eax
	imull	%r10d, %eax
	leal	9(%r11), %ebp
	subl	%esi, %ebp
	imull	%r10d, %ebp
	vmovd	%r12d, %xmm1
	vmovd	%r12d, %xmm0
	movq	608(%rsp), %rsi         # 8-byte Reload
	vmovd	%esi, %xmm2
	vmovd	%r13d, %xmm6
	vmovd	%edi, %xmm14
	vmovd	%r14d, %xmm3
	vmovd	%r14d, %xmm12
	movl	1744(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm13
	movl	1760(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm10
	vpsubd	%xmm1, %xmm3, %xmm11
	vmovss	.LCPI161_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm1, %xmm5
	movq	832(%rsp), %r10         # 8-byte Reload
	addl	$3, %r10d
	leal	2(%r11), %r12d
	andl	$63, %r12d
	imull	%r10d, %r12d
	leal	62(%r11), %r14d
	andl	$63, %r14d
	imull	%r10d, %r14d
	vmulss	%xmm4, %xmm5, %xmm1
	vdivss	%xmm15, %xmm1, %xmm1
	leal	63(%r11), %r13d
	andl	$63, %r13d
	imull	%r10d, %r13d
	leal	1(%r11), %edi
	andl	$63, %edi
	imull	%r10d, %edi
	vaddss	%xmm1, %xmm3, %xmm1
	movl	1776(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm3
	movq	1808(%rsp), %rsi        # 8-byte Reload
	imull	%esi, %r10d
	leal	(%r12,%rcx,8), %esi
	movq	%rsi, 656(%rsp)         # 8-byte Spill
	movq	1736(%rsp), %r12        # 8-byte Reload
	leal	(%r9,%rcx,8), %esi
	movq	%rsi, 640(%rsp)         # 8-byte Spill
	leal	(%r8,%rcx,8), %esi
	movq	%rsi, 624(%rsp)         # 8-byte Spill
	leal	(%r14,%rcx,8), %esi
	movq	%rsi, 608(%rsp)         # 8-byte Spill
	leal	(%rdx,%rcx,8), %edx
	movq	%rdx, 584(%rsp)         # 8-byte Spill
	leal	(%r13,%rcx,8), %edx
	movq	%rdx, 576(%rsp)         # 8-byte Spill
	leal	(%rax,%rcx,8), %eax
	movq	%rax, 568(%rsp)         # 8-byte Spill
	leal	(%rdi,%rcx,8), %eax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	leal	(%rbp,%rcx,8), %eax
	movq	%rax, 552(%rsp)         # 8-byte Spill
	leal	(%r10,%rcx,8), %eax
	movq	%rax, 544(%rsp)         # 8-byte Spill
	movl	$0, %r10d
	movq	1392(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movq	%rdx, 536(%rsp)         # 8-byte Spill
	leal	3(%rax,%rcx,8), %esi
	movq	%rsi, 528(%rsp)         # 8-byte Spill
	leal	1(%rax,%rcx,8), %esi
	movq	%rsi, 520(%rsp)         # 8-byte Spill
	leal	-1(%rax,%rcx,8), %esi
	movq	%rsi, 496(%rsp)         # 8-byte Spill
	leal	-2(%rax,%rcx,8), %esi
	movq	%rsi, 480(%rsp)         # 8-byte Spill
	leal	2(%rax,%rcx,8), %eax
	movq	%rax, 464(%rsp)         # 8-byte Spill
	subl	%ecx, 592(%rsp)         # 4-byte Folded Spill
	vpbroadcastd	%xmm9, %xmm9
	vmovdqa	%xmm9, 448(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vsubss	%xmm4, %xmm7, %xmm2
	vmovdqa	.LCPI161_0(%rip), %xmm4 # xmm4 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm4, %xmm0, %xmm0
	vmovdqa	%xmm0, 416(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm6
	vmovdqa	%xmm6, 1456(%rsp)       # 16-byte Spill
	vmulss	%xmm2, %xmm5, %xmm0
	movq	848(%rsp), %rdi         # 8-byte Reload
	vmovd	%edi, %xmm2
	vpbroadcastd	%xmm2, %xmm7
	vmovdqa	%xmm7, 400(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm2
	vmovdqa	%xmm2, 1440(%rsp)       # 16-byte Spill
	vdivss	%xmm0, %xmm15, %xmm0
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm9, %xmm2
	vmovdqa	%xmm2, 1424(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm12, %xmm2
	vmovaps	%xmm2, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm11, %xmm2
	vmovdqa	%xmm2, 1408(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 352(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 336(%rsp)        # 16-byte Spill
	vmovd	%r15d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm2, 320(%rsp)        # 16-byte Spill
	vpaddd	%xmm4, %xmm6, %xmm2
	vmovdqa	%xmm2, 304(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	movl	$3, %eax
	subl	%edi, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 288(%rsp)         # 8-byte Spill
	movl	$1, %eax
	subl	%edi, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 272(%rsp)         # 8-byte Spill
	movl	$-2, %eax
	subl	%edi, %eax
	movl	$2, %ecx
	subl	%edi, %ecx
	movl	%edx, %esi
	subl	%edi, %esi
	movq	%rsi, 256(%rsp)         # 8-byte Spill
	movl	%edi, %esi
	notl	%esi
	leal	(%rsi,%rdx), %esi
	movq	%rsi, 240(%rsp)         # 8-byte Spill
	leal	(%rax,%rdx), %eax
	movq	%rax, 224(%rsp)         # 8-byte Spill
	leal	(%rcx,%rdx), %eax
	movq	%rax, 208(%rsp)         # 8-byte Spill
	movq	64(%rsp), %rax          # 8-byte Reload
	movq	1824(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	movq	1840(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	movq	1856(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1856(%rsp)       # 16-byte Spill
	movq	1888(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1552(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%rbx,4), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vpabsd	%xmm9, %xmm0
	vmovdqa	%xmm0, 1392(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_3(%rip), %xmm0
	vmovaps	%xmm0, 1888(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_4(%rip), %xmm0
	vmovaps	%xmm0, 192(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI161_5(%rip), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_6(%rip), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB161_102:                            # %for f7.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	movq	%r10, 1072(%rsp)        # 8-byte Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI161_2(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r8d
	movl	%r8d, 1288(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r11d
	movl	%r11d, 1296(%rsp)       # 4-byte Spill
	cltd
	idivl	%r11d
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r9d
	movl	%r9d, 1312(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r14d
	movl	%r14d, 1328(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1392(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movq	536(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 1344(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm9
	vmovdqa	%xmm9, 1056(%rsp)       # 16-byte Spill
	vmovdqa	416(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm11, %xmm9, %xmm2
	vmovdqa	1456(%rsp), %xmm8       # 16-byte Reload
	vpminsd	%xmm8, %xmm2, %xmm2
	vmovdqa	%xmm7, %xmm12
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vmovdqa	1440(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm3
	vmovdqa	%xmm4, %xmm5
	vmovdqa	1424(%rsp), %xmm7       # 16-byte Reload
	vpsubd	%xmm0, %xmm7, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vmovdqa	1680(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm6
	vpsubd	432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1904(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1264(%rsp)       # 16-byte Spill
	movq	208(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vpinsrd	$2, %ebp, %xmm1, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	368(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm8, %xmm2, %xmm2
	vpmaxsd	%xmm12, %xmm2, %xmm2
	vpcmpgtd	%xmm0, %xmm5, %xmm3
	vpsubd	%xmm0, %xmm7, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vmovdqa	1408(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	movq	240(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 1216(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, 1712(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, 1520(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1248(%rsp)        # 4-byte Spill
	movq	272(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 1200(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, 1168(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	movq	288(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movq	%r10, %rdi
	movslq	%eax, %r10
	movq	%r10, 1232(%rsp)        # 8-byte Spill
	vmovups	8(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovups	24(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1568(%rsp)       # 16-byte Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	movslq	%eax, %rsi
	movq	1880(%rsp), %rcx        # 8-byte Reload
	vmovups	32(%rcx,%rsi,4), %xmm1
	vmovaps	%xmm1, 1584(%rsp)       # 16-byte Spill
	vmovups	48(%rcx,%rsi,4), %xmm1
	vmovaps	%xmm1, 1648(%rsp)       # 16-byte Spill
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	movslq	%eax, %rbx
	movq	%rbx, 1184(%rsp)        # 8-byte Spill
	vmovups	8(%r12,%rbx,4), %xmm13
	vmovaps	%xmm13, 1040(%rsp)      # 16-byte Spill
	vmovups	24(%r12,%rbx,4), %xmm1
	vmovaps	%xmm1, 1600(%rsp)       # 16-byte Spill
	movq	568(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	movslq	%eax, %rbp
	vmovups	32(%rcx,%rbp,4), %xmm14
	vmovaps	%xmm14, 1024(%rsp)      # 16-byte Spill
	vmovups	48(%rcx,%rbp,4), %xmm1
	vmovaps	%xmm1, 1632(%rsp)       # 16-byte Spill
	vmovups	16(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1808(%rsp)       # 16-byte Spill
	vmovups	40(%rcx,%rsi,4), %xmm1
	vmovaps	%xmm1, 1824(%rsp)       # 16-byte Spill
	vmovups	16(%r12,%rbx,4), %xmm1
	vmovaps	%xmm1, 1840(%rsp)       # 16-byte Spill
	vmovups	40(%rcx,%rbp,4), %xmm15
	vmovaps	%xmm15, 1616(%rsp)      # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	vmovups	32(%r12,%rbx,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	56(%rcx,%rbp,4), %xmm3
	vmovaps	%xmm3, 816(%rsp)        # 16-byte Spill
	vmovups	32(%r12,%r10,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vmovups	56(%rcx,%rsi,4), %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	idivl	%r8d
	movl	%edx, %r8d
	vmovd	%xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vmovd	1712(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 1216(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 1520(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, 1248(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	%xmm5, %xmm1
	vpcmpgtd	%xmm0, %xmm1, %xmm4
	vpsubd	%xmm0, %xmm7, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm8, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vpaddd	%xmm12, %xmm0, %xmm0
	vpminsd	%xmm8, %xmm0, %xmm0
	vpmaxsd	%xmm12, %xmm0, %xmm0
	vmovdqa	336(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm5, %xmm5
	vblendvps	%xmm5, %xmm4, %xmm0, %xmm0
	vmovd	1168(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 1200(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, %r15d, %xmm4, %xmm4
	vpinsrd	$3, %r13d, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm1, %xmm5
	vpsubd	%xmm4, %xmm7, %xmm6
	vblendvps	%xmm5, %xmm4, %xmm6, %xmm4
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %ecx
	movl	%ecx, 1248(%rsp)        # 4-byte Spill
	vmovd	%ecx, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm8, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm4, %xmm4
	vpminsd	%xmm8, %xmm4, %xmm4
	vpmaxsd	%xmm12, %xmm4, %xmm4
	vmovdqa	304(%rsp), %xmm6        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm6, %xmm6
	vblendvps	%xmm6, %xmm5, %xmm4, %xmm4
	vmovd	%r10d, %xmm5
	vpinsrd	$1, %r8d, %xmm5, %xmm5
	vpinsrd	$2, %ebx, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm6
	vpand	%xmm10, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm5
	vpcmpgtd	%xmm5, %xmm1, %xmm6
	vpsubd	%xmm5, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm5, %xmm7, %xmm5
	vmovdqa	1680(%rsp), %xmm10      # 16-byte Reload
	vpmulld	%xmm10, %xmm0, %xmm0
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vpminsd	%xmm8, %xmm6, %xmm6
	vpmaxsd	%xmm12, %xmm6, %xmm6
	vpaddd	%xmm12, %xmm5, %xmm5
	vpminsd	%xmm8, %xmm5, %xmm5
	vpmaxsd	%xmm12, %xmm5, %xmm5
	vmovaps	1264(%rsp), %xmm12      # 16-byte Reload
	vmovdqa	320(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm7, %xmm6, %xmm5, %xmm9
	vmovdqa	%xmm2, %xmm8
	vpaddd	%xmm0, %xmm8, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %r12
	vpmulld	%xmm10, %xmm4, %xmm0
	vpaddd	%xmm0, %xmm8, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %r13
	vshufps	$136, 1632(%rsp), %xmm14, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm14[0,2],mem[0,2]
	vmovaps	1760(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm0, %xmm0
	vmovaps	1776(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1664(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm12, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	1888(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vshufps	$136, 1600(%rsp), %xmm13, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm13[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm13
	vshufps	$136, %xmm3, %xmm15, %xmm0 # xmm0 = xmm15[0,2],xmm3[0,2]
	vsubps	%xmm11, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1744(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm2, %xmm5, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1504(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm15
	vmovaps	1584(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm11, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1376(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm12, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovaps	1152(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1568(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm2[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm3
	vmovaps	1824(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm11, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm4, %xmm5, %xmm6
	vmulps	%xmm0, %xmm6, %xmm0
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovaps	1808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm1, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm1[0,2],mem[0,2]
	vsubps	%xmm6, %xmm0, %xmm1
	vpmulld	%xmm10, %xmm9, %xmm0
	vpaddd	%xmm0, %xmm8, %xmm0
	vpextrq	$1, %xmm0, %r11
	movq	%r11, 1200(%rsp)        # 8-byte Spill
	vmovq	%xmm0, %r10
	movq	%r10, 1168(%rsp)        # 8-byte Spill
	movq	%r12, %r14
	sarq	$32, %r14
	movq	%rax, %r15
	sarq	$32, %r15
	movq	%r13, %r8
	sarq	$32, %r8
	movq	%r9, %rbx
	sarq	$32, %rbx
	orq	$6, %rsi
	orq	$6, %rbp
	andl	$1, %ecx
	sarq	$32, %r10
	sarq	$32, %r11
	testl	%ecx, %ecx
	movq	624(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movq	%rdi, %rdx
	movslq	%ecx, %rdi
	movq	544(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdx), %ecx
	movl	%ecx, 1104(%rsp)        # 4-byte Spill
	movq	608(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdx), %ecx
	movl	%ecx, 1136(%rsp)        # 4-byte Spill
	movq	584(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdx), %ecx
	movl	%ecx, 1088(%rsp)        # 4-byte Spill
	movq	656(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdx), %ecx
	movl	%ecx, 1120(%rsp)        # 4-byte Spill
	movq	640(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdx), %ecx
	jne	.LBB161_103
# BB#104:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	movl	%ecx, 1216(%rsp)        # 4-byte Spill
	vmovaps	%xmm1, 736(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 768(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 784(%rsp)       # 16-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	jmp	.LBB161_105
	.align	16, 0x90
.LBB161_103:                            #   in Loop: Header=BB161_102 Depth=1
	movl	%ecx, 1216(%rsp)        # 4-byte Spill
	vaddps	%xmm1, %xmm15, %xmm0
	vmovaps	%xmm1, 736(%rsp)        # 16-byte Spill
	vmovaps	%xmm15, 768(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm3, %xmm0
	vmovaps	%xmm3, 752(%rsp)        # 16-byte Spill
	vaddps	%xmm0, %xmm13, %xmm0
	vmovaps	%xmm13, 784(%rsp)       # 16-byte Spill
	vmulps	192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB161_105:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovdqa	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmovaps	1040(%rsp), %xmm10      # 16-byte Reload
	vmovaps	%xmm2, %xmm13
	vmovaps	1600(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1024(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1584(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1568(%rsp), %xmm2       # 16-byte Reload
	movslq	%r12d, %rcx
	movq	1904(%rsp), %rdx        # 8-byte Reload
	vmovss	(%rdx,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r15,4), %xmm0, %xmm9 # xmm9 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm9, 1520(%rsp)       # 16-byte Spill
	movslq	%r13d, %rax
	vmovss	(%rdx,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r9d, %rax
	vinsertps	$32, (%rdx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%rbx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	movq	1880(%rsp), %rcx        # 8-byte Reload
	vmovups	(%rcx,%rsi,4), %xmm6
	vmovaps	%xmm6, 864(%rsp)        # 16-byte Spill
	vmovups	(%rcx,%rbp,4), %xmm5
	vmovaps	%xmm5, 880(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm13, %xmm0 # xmm0 = xmm13[1,3],xmm2[1,3]
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vmulps	%xmm4, %xmm3, %xmm0
	vshufps	$221, 1648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm8
	vshufps	$221, %xmm15, %xmm10, %xmm0 # xmm0 = xmm10[1,3],xmm15[1,3]
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vmovaps	1664(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm0
	vshufps	$221, 1632(%rsp), %xmm14, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm14[1,3],mem[1,3]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm10
	vmulps	%xmm4, %xmm9, %xmm0
	vshufps	$221, 1824(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[1,3],mem[1,3]
	vsubps	%xmm11, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm13
	vmulps	%xmm2, %xmm9, %xmm0
	vshufps	$221, 1616(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm5[1,3],mem[1,3]
	vsubps	%xmm11, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm0, %xmm9
	vmovups	32(%rcx,%rdi,4), %xmm3
	vmovaps	%xmm3, 1584(%rsp)       # 16-byte Spill
	vmovups	48(%rcx,%rdi,4), %xmm5
	vmovaps	%xmm5, 1568(%rsp)       # 16-byte Spill
	movslq	1216(%rsp), %r9         # 4-byte Folded Reload
	vmovups	32(%rcx,%r9,4), %xmm4
	vmovaps	%xmm4, 1600(%rsp)       # 16-byte Spill
	vmovaps	1856(%rsp), %xmm1       # 16-byte Reload
	vmulps	%xmm1, %xmm12, %xmm0
	vshufps	$136, %xmm5, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vmovups	48(%rcx,%r9,4), %xmm3
	vmovaps	%xmm3, 1216(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0,2],xmm3[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	1536(%rsp), %xmm12, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm5, %xmm5
	movslq	1088(%rsp), %rbp        # 4-byte Folded Reload
	movq	%rbp, 1024(%rsp)        # 8-byte Spill
	vmulps	1552(%rsp), %xmm12, %xmm3 # 16-byte Folded Reload
	vmovups	32(%rcx,%rbp,4), %xmm4
	vmovaps	%xmm4, 928(%rsp)        # 16-byte Spill
	vmovups	48(%rcx,%rbp,4), %xmm6
	vmovaps	%xmm6, 912(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm6[0,2]
	vsubps	%xmm11, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm3, %xmm4
	vmovups	40(%rcx,%rdi,4), %xmm3
	vmovaps	%xmm3, 1040(%rsp)       # 16-byte Spill
	vmovups	56(%rcx,%rdi,4), %xmm6
	vmovaps	%xmm6, 800(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm6, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm6[0,2]
	vsubps	%xmm11, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	1744(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm7, %xmm1
	movq	1168(%rsp), %rax        # 8-byte Reload
	cltq
	vmovss	(%rdx,%rax,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	1200(%rsp), %rax        # 8-byte Reload
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm3, 944(%rsp)        # 16-byte Spill
	movq	%rdx, %r14
	movslq	1104(%rsp), %rdx        # 4-byte Folded Reload
	vmovaps	1888(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm8, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm3, %xmm3
	vmovaps	%xmm3, 1632(%rsp)       # 16-byte Spill
	vminps	%xmm6, %xmm10, %xmm3
	vmaxps	%xmm2, %xmm3, %xmm10
	vminps	%xmm6, %xmm13, %xmm3
	vmaxps	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, 1168(%rsp)       # 16-byte Spill
	vminps	%xmm6, %xmm9, %xmm2
	vxorps	%xmm11, %xmm11, %xmm11
	vmaxps	%xmm11, %xmm2, %xmm2
	vmovaps	%xmm2, 1104(%rsp)       # 16-byte Spill
	movslq	1136(%rsp), %r15        # 4-byte Folded Reload
	movslq	1120(%rsp), %r11        # 4-byte Folded Reload
	vminps	%xmm6, %xmm5, %xmm2
	vminps	%xmm6, %xmm4, %xmm3
	vmovaps	%xmm3, 1120(%rsp)       # 16-byte Spill
	vminps	%xmm6, %xmm1, %xmm1
	vmovaps	%xmm1, 1136(%rsp)       # 16-byte Spill
	vminps	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	movl	688(%rsp), %ebx         # 4-byte Reload
	testl	%ebx, %ebx
	movq	1736(%rsp), %r12        # 8-byte Reload
	movq	1232(%rsp), %rax        # 8-byte Reload
	vmovups	(%r12,%rax,4), %xmm4
	vmovaps	%xmm4, 832(%rsp)        # 16-byte Spill
	movq	1184(%rsp), %rax        # 8-byte Reload
	vmovups	(%r12,%rax,4), %xmm1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	movq	%rcx, %rsi
	vmovups	8(%r12,%rdx,4), %xmm3
	vmovups	24(%r12,%rdx,4), %xmm7
	vshufps	$221, 1808(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm4[1,3],mem[1,3]
	vshufps	$221, 1840(%rsp), %xmm1, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm1[1,3],mem[1,3]
	vmovups	16(%r12,%rdx,4), %xmm4
	vmovups	8(%r12,%r15,4), %xmm15
	vmovups	24(%r12,%r15,4), %xmm14
	vmovups	8(%r12,%r11,4), %xmm13
	vmovups	24(%r12,%r11,4), %xmm9
	vmovups	32(%r12,%rdx,4), %xmm12
	vmovups	16(%r12,%r11,4), %xmm1
	vmovaps	%xmm1, 976(%rsp)        # 16-byte Spill
	vmovups	40(%rsi,%r9,4), %xmm1
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	vmovups	16(%r12,%r15,4), %xmm1
	vmovaps	%xmm1, 1008(%rsp)       # 16-byte Spill
	vmovups	40(%rsi,%rbp,4), %xmm1
	vmovaps	%xmm1, 1184(%rsp)       # 16-byte Spill
	je	.LBB161_107
# BB#106:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	%xmm1, 1712(%rsp)       # 16-byte Spill
.LBB161_107:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1632(%rsp), %xmm0       # 16-byte Reload
	vsubps	1152(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 1632(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm3, %xmm1 # xmm1 = xmm3[0,2],xmm7[0,2]
	vmovaps	%xmm7, 1200(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1232(%rsp)       # 16-byte Spill
	vsubps	1648(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm9[0,2]
	vmovaps	%xmm0, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 1264(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1152(%rsp)      # 16-byte Spill
	vmaxps	%xmm11, %xmm2, %xmm6
	vmovaps	1168(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm5, %xmm2, %xmm2
	vshufps	$136, %xmm14, %xmm15, %xmm7 # xmm7 = xmm15[0,2],xmm14[0,2]
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm11, %xmm0, %xmm5
	vmovaps	1104(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm8, %xmm3, %xmm3
	vshufps	$136, %xmm12, %xmm4, %xmm8 # xmm8 = xmm4[0,2],xmm12[0,2]
	vmovaps	1136(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm11, %xmm0, %xmm13
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm11, %xmm0, %xmm11
	movl	1248(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	1800(%rsp), %r8         # 8-byte Reload
	orl	%r8d, %eax
	testb	$1, %al
	movl	1344(%rsp), %r13d       # 4-byte Reload
	je	.LBB161_108
# BB#109:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm14, 1136(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 1168(%rsp)      # 16-byte Spill
	vmovaps	%xmm12, 704(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 1488(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1504(%rsp)      # 16-byte Spill
	vmovaps	%xmm4, %xmm14
	vmovaps	1536(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1888(%rsp), %xmm15      # 16-byte Reload
	jmp	.LBB161_110
	.align	16, 0x90
.LBB161_108:                            #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm14, 1136(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 1168(%rsp)      # 16-byte Spill
	vmovaps	%xmm12, 704(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, %xmm14
	vmovaps	944(%rsp), %xmm9        # 16-byte Reload
	vmulps	1376(%rsp), %xmm9, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm1, 1648(%rsp)       # 16-byte Spill
	vmovaps	1824(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1472(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vmovaps	1760(%rsp), %xmm1       # 16-byte Reload
	vsubps	%xmm1, %xmm4, %xmm4
	vmovaps	%xmm5, %xmm12
	vmovaps	1776(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	1808(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1488(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	1888(%rsp), %xmm15      # 16-byte Reload
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	.LCPI161_10(%rip), %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm0
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmulps	1664(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
	vmovaps	1616(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 816(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm12, %xmm5
	vmulps	%xmm4, %xmm0, %xmm0
	vmovaps	1840(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1504(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[1,3],mem[1,3]
	vmovaps	1648(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	.LCPI161_10(%rip), %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm3, %xmm10, %xmm4
	vmovaps	%xmm10, 1504(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm4, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vmovaps	%xmm2, 1488(%rsp)       # 16-byte Spill
	vaddps	1632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1712(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmovaps	1536(%rsp), %xmm9       # 16-byte Reload
.LBB161_110:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vxorps	%xmm12, %xmm12, %xmm12
	vmovaps	896(%rsp), %xmm2        # 16-byte Reload
	vsubps	%xmm2, %xmm6, %xmm6
	vsubps	%xmm7, %xmm5, %xmm3
	vsubps	%xmm8, %xmm13, %xmm5
	vsubps	%xmm1, %xmm11, %xmm4
	orq	$6, %rdi
	testl	%ecx, %ebx
	jne	.LBB161_111
# BB#112:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm4, 1744(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1120(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 1648(%rsp)       # 16-byte Spill
	vmovaps	1776(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1760(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1856(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1552(%rsp), %xmm2       # 16-byte Reload
	jmp	.LBB161_113
	.align	16, 0x90
.LBB161_111:                            #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm1, 1648(%rsp)       # 16-byte Spill
	vmovaps	1744(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm9, %xmm2, %xmm0
	vmovaps	992(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 56(%rsi,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1760(%rsp), %xmm11      # 16-byte Reload
	vsubps	%xmm11, %xmm1, %xmm1
	vmovaps	1776(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm11, %xmm10
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 32(%r12,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	1552(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm2, %xmm1
	movq	1024(%rsp), %rax        # 8-byte Reload
	vmovaps	1184(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%rsi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm8, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	1008(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%r12,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm12, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm4, %xmm3, %xmm2
	vmovaps	%xmm4, 1744(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1120(%rsp)       # 16-byte Spill
	vaddps	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm6, 1088(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vmovaps	%xmm7, %xmm2
	vaddps	%xmm1, %xmm5, %xmm1
	vmovaps	%xmm5, 1104(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmovaps	1856(%rsp), %xmm11      # 16-byte Reload
.LBB161_113:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1040(%rsp), %xmm13      # 16-byte Reload
	vmovaps	960(%rsp), %xmm3        # 16-byte Reload
	vmovaps	928(%rsp), %xmm5        # 16-byte Reload
	vmovaps	912(%rsp), %xmm7        # 16-byte Reload
	vmovaps	1152(%rsp), %xmm6       # 16-byte Reload
	vmulps	1520(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
	vmovups	(%rsi,%rdi,4), %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm13[1,3]
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovups	(%r12,%rdx,4), %xmm1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm0
	vshufps	$221, %xmm14, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm14[1,3]
	vxorps	%xmm4, %xmm4, %xmm4
	vsubps	%xmm1, %xmm0, %xmm12
	vmovaps	1168(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1136(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vshufps	$221, %xmm7, %xmm5, %xmm1 # xmm1 = xmm5[1,3],xmm7[1,3]
	vmulps	%xmm2, %xmm3, %xmm2
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm7
	vmovaps	1232(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1200(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vmovaps	1584(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1568(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm11, %xmm3, %xmm1
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm8, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vshufps	$221, 1264(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[1,3],mem[1,3]
	vmovaps	1600(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1216(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmulps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm8, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm4, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm3
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm4, %xmm0, %xmm0
	vsubps	%xmm5, %xmm0, %xmm6
	andl	$1, %ecx
	je	.LBB161_114
# BB#115:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm3, 1568(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1600(%rsp)      # 16-byte Spill
	vmovaps	1712(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB161_116
	.align	16, 0x90
.LBB161_114:                            #   in Loop: Header=BB161_102 Depth=1
	vshufps	$221, 704(%rsp), %xmm14, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm14[1,3],mem[1,3]
	vmulps	944(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
	vshufps	$221, 800(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm13[1,3],mem[1,3]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm8, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm3, %xmm6, %xmm1
	vmovaps	%xmm3, 1568(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm7, %xmm1
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm12, %xmm0
	vmovaps	%xmm12, 1600(%rsp)      # 16-byte Spill
	vmulps	176(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
.LBB161_116:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	movq	1072(%rsp), %r10        # 8-byte Reload
	vmovdqa	1056(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1632(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1488(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1744(%rsp), %xmm4       # 16-byte Reload
	testl	%ebx, %ebx
	jne	.LBB161_118
# BB#117:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1712(%rsp), %xmm12      # 16-byte Reload
.LBB161_118:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	movl	%r13d, %eax
	andl	$1, %eax
	jne	.LBB161_119
# BB#120:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm5, 1712(%rsp)       # 16-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	.LBB161_121
	.align	16, 0x90
.LBB161_119:                            #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm5, 1712(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm2, %xmm0, %xmm0
	vaddps	720(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	192(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
.LBB161_121:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm6, 1584(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1744(%rsp)       # 16-byte Spill
	testl	%ebx, %ebx
	je	.LBB161_123
# BB#122:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
.LBB161_123:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	movq	224(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI161_2(%rip), %xmm4 # xmm4 = [0,2,4,6]
	vpaddd	%xmm4, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	1288(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	1296(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	1312(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	1328(%rsp)              # 4-byte Folded Reload
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm2
	vpand	1392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	352(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm3, %xmm2, %xmm2
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm4, %xmm3, %xmm3
	vmovdqa	1456(%rsp), %xmm9       # 16-byte Reload
	vpminsd	%xmm9, %xmm3, %xmm3
	vmovdqa	400(%rsp), %xmm7        # 16-byte Reload
	vpmaxsd	%xmm7, %xmm3, %xmm3
	vmovdqa	1440(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm4
	vmovdqa	1424(%rsp), %xmm6       # 16-byte Reload
	vpsubd	%xmm0, %xmm6, %xmm6
	vblendvps	%xmm4, %xmm0, %xmm6, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpminsd	%xmm9, %xmm0, %xmm0
	vpmaxsd	%xmm7, %xmm0, %xmm0
	vblendvps	%xmm2, %xmm3, %xmm0, %xmm0
	vpmulld	1680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovq	%xmm0, %rax
	movslq	%eax, %rcx
	vmovss	(%r14,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r14,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vmovups	(%r12,%r11,4), %xmm3
	movl	%r13d, %eax
	orl	%r8d, %eax
	testb	$1, %al
	je	.LBB161_124
# BB#125:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
	jmp	.LBB161_126
	.align	16, 0x90
.LBB161_124:                            #   in Loop: Header=BB161_102 Depth=1
	vmovaps	832(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1808(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	864(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1824(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	1376(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	848(%rsp), %xmm1        # 16-byte Reload
	vshufps	$136, 1840(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	880(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1616(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vmulps	1664(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm8, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm15, %xmm4, %xmm4
	vmaxps	%xmm5, %xmm4, %xmm4
	vsubps	%xmm1, %xmm4, %xmm1
	vaddps	784(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	768(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	752(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	736(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1360(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
.LBB161_126:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1600(%rsp), %xmm6       # 16-byte Reload
	testl	%r13d, %ebx
	je	.LBB161_128
# BB#127:                               #   in Loop: Header=BB161_102 Depth=1
	vshufps	$221, 976(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm3[1,3],mem[1,3]
	orq	$6, %r9
	vmovups	(%rsi,%r9,4), %xmm1
	vshufps	$221, 992(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1520(%rsp), %xmm4       # 16-byte Reload
	vmulps	1536(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm8, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm5, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovups	(%r12,%r15,4), %xmm1
	vshufps	$221, 1008(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	movq	1024(%rsp), %rax        # 8-byte Reload
	orq	$6, %rax
	vmovups	(%rsi,%rax,4), %xmm3
	vshufps	$221, 1184(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmulps	1552(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm8, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm15, %xmm3, %xmm3
	vmaxps	%xmm5, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	%xmm1, %xmm6, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	1632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1568(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1360(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
.LBB161_128:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1856(%rsp), %xmm4       # 16-byte Reload
	movl	%r13d, %eax
	andl	$1, %eax
	je	.LBB161_129
# BB#130:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm1, %xmm0
	jmp	.LBB161_131
	.align	16, 0x90
.LBB161_129:                            #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1152(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm14[0,2]
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	1472(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm3, %xmm3 # xmm3 = xmm3[0,2],xmm13[0,2]
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm8, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm5, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	1744(%rsp), %xmm2       # 16-byte Reload
	vaddps	1088(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	1120(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	1104(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vmulps	176(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB161_131:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	1712(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm15, 1888(%rsp)      # 16-byte Spill
	vmovaps	%xmm4, 1856(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1760(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1776(%rsp)       # 16-byte Spill
	testl	%ebx, %ebx
	jne	.LBB161_133
# BB#132:                               # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vmovaps	%xmm1, %xmm0
.LBB161_133:                            # %for f7.s0.v10.v1012
                                        #   in Loop: Header=BB161_102 Depth=1
	vaddps	1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm12, %xmm2, %xmm1
	vmovaps	.LCPI161_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI161_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r13d, %rax
	movq	672(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r10d
	addl	$-1, 592(%rsp)          # 4-byte Folded Spill
	jne	.LBB161_102
	jmp	.LBB161_164
.LBB161_39:                             # %false_bb2
	addl	$39, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB161_164
# BB#40:                                # %for f7.s0.v10.v1016.preheader
	movq	1800(%rsp), %r12        # 8-byte Reload
	movl	%r12d, %eax
	subl	%r8d, %eax
	movq	%rax, 1776(%rsp)        # 8-byte Spill
	leal	(%rcx,%rcx), %esi
	movl	%esi, 1760(%rsp)        # 4-byte Spill
	movq	%rax, %rdi
	cltd
	idivl	%esi
	movl	%edx, %r9d
	leal	-1(%rdi), %eax
	cltd
	idivl	%esi
	movl	%edx, %ebp
	leal	1(%rdi), %eax
	cltd
	idivl	%esi
	movl	%esi, %eax
	negl	%eax
	movl	%ecx, %edi
	sarl	$31, %edi
	movq	%rcx, %rbx
	movq	%rbx, 1888(%rsp)        # 8-byte Spill
	andnl	%esi, %edi, %ecx
	andl	%eax, %edi
	orl	%ecx, %edi
	movl	%edi, 1808(%rsp)        # 4-byte Spill
	movl	%r9d, %ecx
	sarl	$31, %ecx
	andl	%edi, %ecx
	addl	%r9d, %ecx
	movl	%ebp, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	addl	%ebp, %eax
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%edi, %esi
	addl	%edx, %esi
	leal	-1(%r8,%rbx), %edx
	movl	%edx, 1824(%rsp)        # 4-byte Spill
	leal	1(%r12), %edi
	movq	%rdi, 624(%rsp)         # 8-byte Spill
	cmpl	%edi, %edx
	movl	%edx, %ebp
	cmovgl	%edi, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	leal	-1(%rbx,%rbx), %r15d
	movl	%r15d, %r13d
	subl	%esi, %r13d
	cmpl	%esi, %ebx
	cmovgl	%esi, %r13d
	addl	%r8d, %r13d
	cmpl	%r13d, %edx
	cmovlel	%edx, %r13d
	cmpl	%r8d, %r13d
	cmovll	%r8d, %r13d
	cmpl	%r12d, %edx
	cmovgl	%ebp, %r13d
	movl	%edx, %esi
	cmovgl	%r12d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	movl	%r15d, %edi
	subl	%ecx, %edi
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %edi
	addl	%r8d, %edi
	cmpl	%edi, %edx
	cmovlel	%edx, %edi
	movl	%edx, %ebp
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	leal	(%r8,%rbx), %ecx
	cmpl	%r12d, %ecx
	cmovgl	%esi, %edi
	movl	%edi, 1856(%rsp)        # 4-byte Spill
	movl	%ecx, %edx
	cmovgl	%r12d, %edx
	addl	$-1, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%r15d, %r11d
	subl	%eax, %r11d
	cmpl	%eax, %ebx
	cmovgl	%eax, %r11d
	addl	%r8d, %r11d
	cmpl	%r11d, %ebp
	cmovlel	%ebp, %r11d
	cmpl	%r8d, %r11d
	cmovll	%r8d, %r11d
	cmpl	%r12d, %ecx
	cmovgel	%edx, %r11d
	movq	48(%rsp), %r14          # 8-byte Reload
	movl	%r14d, %eax
	negl	%eax
	leal	(%r10,%r10), %ecx
	cltd
	idivl	%ecx
	movl	%r10d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r10,%r10), %edi
	subl	%eax, %edi
	cmpl	%eax, %r10d
	cmovgl	%eax, %edi
	leal	(%r14,%r10), %eax
	leal	-1(%r14,%r10), %r9d
	addl	%r14d, %edi
	cmpl	%edi, %r9d
	cmovlel	%r9d, %edi
	cmpl	%r14d, %edi
	cmovll	%r14d, %edi
	xorl	%r10d, %r10d
	testl	%eax, %eax
	cmovgl	%r10d, %r9d
	cmpl	%r14d, %r9d
	cmovll	%r14d, %r9d
	testl	%eax, %eax
	cmovlel	%edi, %r9d
	movq	40(%rsp), %rsi          # 8-byte Reload
	movl	%esi, %eax
	negl	%eax
	movq	32(%rsp), %rbp          # 8-byte Reload
	leal	(%rbp,%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%ebp, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %ebx
	negl	%ecx
	andl	%eax, %ecx
	orl	%ebx, %ecx
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	addl	%edx, %ebx
	leal	-1(%rbp,%rbp), %eax
	subl	%ebx, %eax
	cmpl	%ebx, %ebp
	cmovgl	%ebx, %eax
	leal	(%rsi,%rbp), %ecx
	leal	-1(%rsi,%rbp), %ebx
	addl	%esi, %eax
	cmpl	%eax, %ebx
	cmovlel	%ebx, %eax
	cmpl	%esi, %eax
	cmovll	%esi, %eax
	testl	%ecx, %ecx
	cmovgl	%r10d, %ebx
	cmpl	%esi, %ebx
	cmovll	%esi, %ebx
	testl	%ecx, %ecx
	cmovlel	%eax, %ebx
	movq	144(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 592(%rsp)         # 8-byte Spill
	movl	%r12d, %ecx
	andl	$1, %ecx
	movl	%ecx, 584(%rsp)         # 4-byte Spill
	movl	%r12d, %ecx
	andl	$63, %ecx
	movq	%rcx, 1840(%rsp)        # 8-byte Spill
	testl	%esi, %esi
	cmovgl	%eax, %ebx
	movq	608(%rsp), %r10         # 8-byte Reload
	movl	%r10d, %ebp
	movq	848(%rsp), %r12         # 8-byte Reload
	imull	%r12d, %ebp
	addl	%esi, %ebp
	testl	%r14d, %r14d
	cmovgl	%edi, %r9d
	movq	1776(%rsp), %rsi        # 8-byte Reload
	leal	2(%rsi), %eax
	cltd
	movl	1760(%rsp), %edi        # 4-byte Reload
	idivl	%edi
	movl	%edx, %ecx
	movq	%rsi, %rax
	addl	$-2, %eax
	cltd
	idivl	%edi
	movq	152(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rdi), %eax
	vmovd	%eax, %xmm12
	vmovd	%ebp, %xmm1
	vmovd	%ebp, %xmm4
	vmovd	%r10d, %xmm11
	movq	%r12, %rsi
	leal	(%rsi,%rdi), %eax
	vmovd	%eax, %xmm6
	leal	-1(%rsi,%rdi), %eax
	vmovd	%eax, %xmm8
	vmovd	%ebx, %xmm2
	vmovd	%ebx, %xmm9
	leal	-2(%rsi,%rdi), %eax
	movq	%rdi, %rbp
	vmovd	%eax, %xmm10
	movq	136(%rsp), %r12         # 8-byte Reload
	movl	%r12d, %eax
	imull	%r8d, %eax
	addl	%r14d, %eax
	movl	%ecx, %r14d
	sarl	$31, %r14d
	movl	1808(%rsp), %edi        # 4-byte Reload
	andl	%edi, %r14d
	addl	%ecx, %r14d
	movl	%edx, %ebx
	sarl	$31, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	leal	-2(%rsi), %ecx
	vmovd	%ecx, %xmm13
	leal	2(%rsi,%rbp), %ecx
	vmovd	%ecx, %xmm0
	movslq	%eax, %rcx
	movslq	%r9d, %rax
	subq	%rcx, %rax
	movslq	%r13d, %rcx
	movl	128(%rsp), %r13d        # 4-byte Reload
	andl	$-32, %r13d
	imulq	%r12, %rcx
	movq	%rcx, 1776(%rsp)        # 8-byte Spill
	movslq	%r11d, %r9
	imulq	%r12, %r9
	movq	1800(%rsp), %r11        # 8-byte Reload
	leal	-2(%r11), %edx
	movl	1824(%rsp), %ecx        # 4-byte Reload
	cmpl	%edx, %ecx
	movl	%ecx, %esi
	cmovgl	%edx, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	movl	%r15d, %ebp
	subl	%ebx, %ebp
	movq	1888(%rsp), %rdi        # 8-byte Reload
	cmpl	%ebx, %edi
	cmovgl	%ebx, %ebp
	addl	%r8d, %ebp
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	cmpl	%r8d, %ebp
	cmovll	%r8d, %ebp
	leal	2(%r8,%rdi), %ebx
	cmpl	%r11d, %ebx
	cmovgl	%esi, %ebp
	vpsubd	%xmm1, %xmm2, %xmm3
	vmovss	.LCPI161_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovss	88(%rsp), %xmm5         # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm5, %xmm1, %xmm1
	vmulss	%xmm7, %xmm1, %xmm2
	vdivss	%xmm15, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	movslq	%ebp, %rsi
	imulq	%r12, %rsi
	movq	%rsi, 1808(%rsp)        # 8-byte Spill
	movq	848(%rsp), %rsi         # 8-byte Reload
	leal	2(%rsi), %esi
	vmovd	%esi, %xmm5
	movslq	1856(%rsp), %rsi        # 4-byte Folded Reload
	movq	1880(%rsp), %r10        # 8-byte Reload
	imulq	%r12, %rsi
	leal	2(%r11), %ebp
	cmpl	%ebp, %ecx
	movl	%ecx, %ebx
	cmovgl	%ebp, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	subl	%r14d, %r15d
	cmpl	%r14d, %edi
	cmovgl	%r14d, %r15d
	addl	%r8d, %r15d
	cmpl	%r15d, %ecx
	cmovlel	%ecx, %r15d
	leal	-2(%r8,%rdi), %edi
	cmpl	%r8d, %r15d
	cmovll	%r8d, %r15d
	cmpl	%r11d, %edi
	cmovgl	%ebx, %r15d
	movslq	%r15d, %rdi
	xorl	%r15d, %r15d
	imulq	%r12, %rdi
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r8
	leaq	(%r9,%rax), %r9
	leaq	(%rsi,%rax), %r14
	movq	832(%rsp), %rbx         # 8-byte Reload
	addl	$3, %ebx
	andl	$63, %ebp
	imull	%ebx, %ebp
	movq	%rbp, 576(%rsp)         # 8-byte Spill
	andl	$63, %edx
	imull	%ebx, %edx
	movq	%rdx, 608(%rsp)         # 8-byte Spill
	leal	63(%r11), %edx
	andl	$63, %edx
	imull	%ebx, %edx
	movq	%rdx, 568(%rsp)         # 8-byte Spill
	movq	624(%rsp), %rdx         # 8-byte Reload
	andl	$63, %edx
	imull	%ebx, %edx
	movq	%rdx, 624(%rsp)         # 8-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rax), %r12
	movq	1840(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %ebx
	movq	%rbx, 832(%rsp)         # 8-byte Spill
	leaq	(%rdi,%rax), %rax
	movq	96(%rsp), %rdi          # 8-byte Reload
	addq	$32, %rdi
	movq	%rcx, %rdx
	leal	10(%r11), %esi
	movl	112(%rsp), %ebx         # 4-byte Reload
	subl	%ebx, %esi
	addl	$64, %r13d
	imull	%r13d, %esi
	movq	%rsi, 552(%rsp)         # 8-byte Spill
	imulq	%rdi, %rdx
	leal	8(%r11), %esi
	subl	%ebx, %esi
	imull	%r13d, %esi
	movq	%rsi, 544(%rsp)         # 8-byte Spill
	leal	6(%r11), %esi
	subl	%ebx, %esi
	imull	%r13d, %esi
	movq	%rsi, 536(%rsp)         # 8-byte Spill
	movq	144(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rdi
	sarq	$63, %rdi
	andq	%rcx, %rdi
	leal	7(%r11), %esi
	subl	%ebx, %esi
	imull	%r13d, %esi
	movq	%rsi, 528(%rsp)         # 8-byte Spill
	leal	9(%r11), %esi
	movq	1736(%rsp), %r11        # 8-byte Reload
	subl	%ebx, %esi
	imull	%r13d, %esi
	movq	%rsi, 520(%rsp)         # 8-byte Spill
	subq	%rdi, %rdx
	movq	%rdx, 560(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm12, %xmm12
	vmovdqa	%xmm12, 496(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm4, %xmm4
	vmovaps	%xmm4, 480(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm4
	vmovaps	%xmm4, 1664(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm4
	vsubss	%xmm7, %xmm14, %xmm6
	vmovdqa	.LCPI161_0(%rip), %xmm7 # xmm7 = [0,4294967294,4294967292,4294967290]
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 464(%rsp)        # 16-byte Spill
	vmulss	%xmm6, %xmm1, %xmm1
	movq	848(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm4
	vpbroadcastd	%xmm4, %xmm6
	vmovdqa	%xmm6, 1392(%rsp)       # 16-byte Spill
	vdivss	%xmm1, %xmm15, %xmm11
	movq	152(%rsp), %rdx         # 8-byte Reload
	vmovd	%edx, %xmm4
	vbroadcastss	%xmm4, %xmm4
	vmovaps	%xmm4, 1376(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm1
	vmovdqa	%xmm1, 1360(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm9, %xmm4
	vmovaps	%xmm4, 448(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 432(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm4
	vpaddd	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm4, 416(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vmovdqa	%xmm3, 1648(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	leal	1(%rcx,%rdx), %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	leal	1(%rcx), %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	leal	-1(%rcx), %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	leal	-3(%rcx,%rdx), %edi
	movq	%rcx, %rsi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	movq	592(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	subl	%esi, %ecx
	movq	%rcx, 304(%rsp)         # 8-byte Spill
	leal	-3(%rsi), %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	vpaddd	%xmm7, %xmm6, %xmm0
	vmovdqa	%xmm0, 272(%rsp)        # 16-byte Spill
	vpaddd	%xmm7, %xmm1, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm11, %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm2, %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	leal	3(%rsi), %edx
	movq	%rdx, 240(%rsp)         # 8-byte Spill
	leal	1(%rsi), %edx
	movq	%rdx, 224(%rsp)         # 8-byte Spill
	leal	-1(%rsi), %edx
	movq	%rdx, 208(%rsp)         # 8-byte Spill
	leal	-2(%rsi), %edx
	movq	%rdx, 192(%rsp)         # 8-byte Spill
	leal	2(%rsi), %edx
	movq	%rdx, 176(%rsp)         # 8-byte Spill
	leal	3(%rcx), %edx
	movq	%rdx, 160(%rsp)         # 8-byte Spill
	leal	1(%rcx), %edx
	movq	%rdx, 152(%rsp)         # 8-byte Spill
	leal	-1(%rcx), %edx
	movq	%rdx, 144(%rsp)         # 8-byte Spill
	leal	-2(%rcx), %edx
	movq	%rdx, 136(%rsp)         # 8-byte Spill
	leal	2(%rcx), %ecx
	movq	%rcx, 128(%rsp)         # 8-byte Spill
	movq	64(%rsp), %rdi          # 8-byte Reload
	vbroadcastss	(%rdi,%r8,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r9,4), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r14,4), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%r12,4), %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdi,%rax,4), %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vpabsd	%xmm12, %xmm0
	vmovdqa	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm12, %xmm0
	vmovdqa	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI161_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vbroadcastss	.LCPI161_3(%rip), %xmm0
	vmovaps	%xmm0, 1888(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_4(%rip), %xmm0
	vmovaps	%xmm0, 112(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI161_5(%rip), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI161_6(%rip), %xmm0
	vmovaps	%xmm0, 96(%rsp)         # 16-byte Spill
	.align	16, 0x90
.LBB161_41:                             # %for f7.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	movq	%r15, 1856(%rsp)        # 8-byte Spill
	movq	304(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm3, %xmm11
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	496(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r13d
	movl	%r13d, 992(%rsp)        # 4-byte Spill
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r14d
	movl	%r14d, 1264(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %edi
	movq	592(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %esi
	movl	%esi, 1288(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r12d
	movl	%r12d, 1248(%rsp)       # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r9d
	movl	%r9d, 1232(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1344(%rsp), %xmm2       # 16-byte Reload
	vpand	%xmm2, %xmm1, %xmm1
	vmovdqa	%xmm2, %xmm5
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm6
	vmovdqa	%xmm6, 976(%rsp)        # 16-byte Spill
	vmovdqa	464(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	272(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1376(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm2
	vmovdqa	1328(%rsp), %xmm13      # 16-byte Reload
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	1392(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm14, %xmm0, %xmm0
	vmovdqa	1360(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm14, %xmm0, %xmm0
	vpaddd	%xmm11, %xmm6, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm14, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1664(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm7
	vpsubd	480(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	448(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1904(%rsp), %rbx        # 8-byte Reload
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm5, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	432(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	%xmm8, %xmm1, %xmm1
	vmovdqa	416(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm4, %xmm2
	vpsubd	%xmm0, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm14, %xmm0, %xmm0
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm14, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%rbx,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebp
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	%xmm5, %xmm7
	vpand	%xmm7, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	368(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm0, %xmm0
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vpxor	%xmm5, %xmm0, %xmm0
	vmovdqa	352(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm0, %xmm2, %xmm0
	vpcmpgtd	%xmm1, %xmm4, %xmm2
	vpsubd	%xmm1, %xmm13, %xmm3
	vblendvps	%xmm2, %xmm1, %xmm3, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm14, %xmm1, %xmm1
	movq	208(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	movq	152(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm11, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r13d
	movl	%edx, %ecx
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpmaxsd	%xmm14, %xmm2, %xmm2
	vblendvps	%xmm0, %xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebp
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm7, %xmm1, %xmm1
	vmovdqa	%xmm7, %xmm3
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	256(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	336(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm7
	vpcmpgtd	%xmm0, %xmm4, %xmm1
	vpsubd	%xmm0, %xmm13, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm14, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm14, %xmm0, %xmm0
	movq	624(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rcx
	movq	%rcx, 960(%rsp)         # 8-byte Spill
	vmovups	8(%r11,%rcx,4), %xmm1
	vmovaps	%xmm1, 1520(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rcx,4), %xmm1
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %r8
	vmovups	32(%r10,%r8,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	48(%r10,%r8,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	movq	568(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rsi
	movq	%rsi, 1184(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rsi,4), %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rsi,4), %xmm1
	vmovaps	%xmm1, 1584(%rsp)       # 16-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	movslq	%eax, %rbx
	vmovups	32(%r10,%rbx,4), %xmm1
	vmovaps	%xmm1, 1568(%rsp)       # 16-byte Spill
	vmovups	48(%r10,%rbx,4), %xmm12
	vmovaps	%xmm12, 1152(%rsp)      # 16-byte Spill
	vmovups	16(%r11,%rcx,4), %xmm1
	vmovaps	%xmm1, 1808(%rsp)       # 16-byte Spill
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	vmovups	40(%r10,%r8,4), %xmm2
	vmovaps	%xmm2, 1776(%rsp)       # 16-byte Spill
	vmovups	16(%r11,%rsi,4), %xmm9
	vmovaps	%xmm9, 1536(%rsp)       # 16-byte Spill
	vmovups	40(%r10,%rbx,4), %xmm8
	vmovaps	%xmm8, 1552(%rsp)       # 16-byte Spill
	vmovups	32(%r11,%rsi,4), %xmm2
	vmovaps	%xmm2, 1440(%rsp)       # 16-byte Spill
	vmovups	56(%r10,%rbx,4), %xmm10
	vmovaps	%xmm10, 752(%rsp)       # 16-byte Spill
	vmovups	32(%r11,%rcx,4), %xmm2
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovups	56(%r10,%r8,4), %xmm2
	vmovaps	%xmm2, 1408(%rsp)       # 16-byte Spill
	idivl	%r13d
	movl	%edx, %ecx
	movq	224(%rsp), %rax         # 8-byte Reload
	movq	1856(%rsp), %rdx        # 8-byte Reload
	leal	(%rax,%rdx), %r15d
	movl	%r15d, 1200(%rsp)       # 4-byte Spill
	vmovd	%r15d, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpaddd	%xmm11, %xmm5, %xmm5
	vpminsd	%xmm15, %xmm5, %xmm5
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebp
	vpmaxsd	%xmm14, %xmm5, %xmm5
	vblendvps	%xmm7, %xmm0, %xmm5, %xmm5
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r9d
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %ebp, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm7
	vpand	%xmm3, %xmm7, %xmm7
	vpaddd	%xmm1, %xmm7, %xmm1
	vpcmpgtd	%xmm1, %xmm4, %xmm7
	vpsubd	%xmm1, %xmm13, %xmm0
	vblendvps	%xmm7, %xmm1, %xmm0, %xmm0
	vmovdqa	320(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	.LCPI161_9(%rip), %xmm1, %xmm1
	vmovdqa	288(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm4, %xmm7
	vpor	%xmm1, %xmm7, %xmm1
	vmovdqa	1664(%rsp), %xmm2       # 16-byte Reload
	vpmulld	1680(%rsp), %xmm2, %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm14, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm14, %xmm0, %xmm0
	movq	240(%rsp), %rax         # 8-byte Reload
	movq	1856(%rsp), %rcx        # 8-byte Reload
	leal	(%rax,%rcx), %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %xmm4
	vpaddd	%xmm11, %xmm4, %xmm4
	vpminsd	%xmm15, %xmm4, %xmm4
	vmovaps	1216(%rsp), %xmm15      # 16-byte Reload
	vpmaxsd	%xmm14, %xmm4, %xmm4
	vblendvps	%xmm1, %xmm0, %xmm4, %xmm13
	vmovdqa	1648(%rsp), %xmm3       # 16-byte Reload
	vpaddd	%xmm7, %xmm3, %xmm1
	vpextrq	$1, %xmm1, %r9
	vmovq	%xmm1, %r10
	vpmulld	%xmm2, %xmm5, %xmm1
	vmovdqa	%xmm2, %xmm0
	vpaddd	%xmm1, %xmm3, %xmm1
	vmovdqa	%xmm3, %xmm14
	vpextrq	$1, %xmm1, %r14
	movq	%r14, 1168(%rsp)        # 8-byte Spill
	vmovq	%xmm1, %r12
	vmovaps	1568(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm12, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm12[0,2]
	vmovaps	1616(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	1632(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1600(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	1888(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm1, %xmm1
	vpxor	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm1, %xmm1
	vmovaps	1472(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1584(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm12
	vshufps	$136, %xmm10, %xmm8, %xmm1 # xmm1 = xmm8[0,2],xmm10[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1744(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm2, %xmm11, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vshufps	$136, 1440(%rsp), %xmm9, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm9[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm8
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmovaps	1312(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm9
	vmovaps	1520(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm1, %xmm4, %xmm4 # xmm4 = xmm4[0,2],xmm1[0,2]
	vmovaps	%xmm1, %xmm10
	vsubps	%xmm4, %xmm9, %xmm9
	vmovaps	1776(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm2, %xmm11, %xmm4
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vmovaps	1808(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 1424(%rsp), %xmm3, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm3[0,2],mem[0,2]
	vsubps	%xmm4, %xmm1, %xmm1
	vpmulld	%xmm0, %xmm13, %xmm0
	vpaddd	%xmm0, %xmm14, %xmm0
	vpextrq	$1, %xmm0, %rcx
	movq	%rcx, 1136(%rsp)        # 8-byte Spill
	vmovq	%xmm0, %rdx
	movq	%rdx, 1024(%rsp)        # 8-byte Spill
	movq	%r10, %r11
	sarq	$32, %r11
	movq	%r9, %rbp
	sarq	$32, %rbp
	movq	%r12, %rsi
	sarq	$32, %rsi
	sarq	$32, %r14
	orq	$6, %r8
	orq	$6, %rbx
	movl	%r15d, %eax
	movq	1856(%rsp), %rdi        # 8-byte Reload
	andl	$1, %eax
	sarq	$32, %rdx
	movq	%rdx, %r15
	sarq	$32, %rcx
	movq	%rcx, 1120(%rsp)        # 8-byte Spill
	testl	%eax, %eax
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rdi), %eax
	movslq	%eax, %rdx
	movq	832(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %eax
	movl	%eax, 1088(%rsp)        # 4-byte Spill
	movq	608(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %eax
	movq	536(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movl	%ecx, 1040(%rsp)        # 4-byte Spill
	movq	576(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movl	%ecx, 1104(%rsp)        # 4-byte Spill
	movq	552(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rdi), %edi
	jne	.LBB161_42
# BB#134:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	movl	%eax, 912(%rsp)         # 4-byte Spill
	vmovaps	%xmm1, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 688(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 704(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 720(%rsp)       # 16-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	jmp	.LBB161_135
	.align	16, 0x90
.LBB161_42:                             #   in Loop: Header=BB161_41 Depth=1
	movl	%eax, 912(%rsp)         # 4-byte Spill
	vaddps	%xmm1, %xmm8, %xmm0
	vmovaps	%xmm1, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 704(%rsp)        # 16-byte Spill
	vaddps	%xmm0, %xmm9, %xmm0
	vmovaps	%xmm9, 688(%rsp)        # 16-byte Spill
	vaddps	%xmm0, %xmm12, %xmm0
	vmovaps	%xmm12, 720(%rsp)       # 16-byte Spill
	vmulps	112(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB161_135:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovdqa	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	1824(%rsp), %xmm14      # 16-byte Reload
	movq	960(%rsp), %r13         # 8-byte Reload
	vmovaps	1520(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm10, %xmm4
	vmovaps	1504(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1488(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1472(%rsp), %xmm11      # 16-byte Reload
	movq	%r15, %rax
	movslq	%r10d, %rcx
	movq	1904(%rsp), %r15        # 8-byte Reload
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r9d, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%rbp,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm13, 1456(%rsp)      # 16-byte Spill
	movslq	%r12d, %rcx
	vmovss	(%r15,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	1168(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r14,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm3, 1168(%rsp)       # 16-byte Spill
	movq	1880(%rsp), %r10        # 8-byte Reload
	vmovups	(%r10,%r8,4), %xmm9
	vmovaps	%xmm9, 816(%rsp)        # 16-byte Spill
	vmovups	(%r10,%rbx,4), %xmm10
	vmovaps	%xmm10, 800(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm4[1,3]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm4
	vmulps	%xmm4, %xmm3, %xmm0
	vshufps	$221, %xmm6, %xmm8, %xmm1 # xmm1 = xmm8[1,3],xmm6[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	%xmm5, %xmm6
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1008(%rsp)       # 16-byte Spill
	vshufps	$221, 1584(%rsp), %xmm11, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm11[1,3],mem[1,3]
	vmovaps	%xmm0, 1056(%rsp)       # 16-byte Spill
	vmovaps	1600(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm0, %xmm3, %xmm1
	vmovaps	1568(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1152(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm6, %xmm2
	vmulps	%xmm2, %xmm1, %xmm12
	vmulps	%xmm4, %xmm13, %xmm1
	vshufps	$221, 1776(%rsp), %xmm9, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm9[1,3],mem[1,3]
	vsubps	%xmm7, %xmm3, %xmm3
	vmulps	%xmm3, %xmm6, %xmm3
	vmulps	%xmm3, %xmm1, %xmm8
	vmulps	%xmm0, %xmm13, %xmm1
	vshufps	$221, 1552(%rsp), %xmm10, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm10[1,3],mem[1,3]
	vsubps	%xmm7, %xmm4, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vmulps	%xmm4, %xmm1, %xmm2
	movq	%rdx, %rbp
	vmovups	32(%r10,%rbp,4), %xmm3
	vmovaps	%xmm3, 1504(%rsp)       # 16-byte Spill
	vmovups	48(%r10,%rbp,4), %xmm5
	vmovaps	%xmm5, 1488(%rsp)       # 16-byte Spill
	movslq	%edi, %r8
	vmovups	32(%r10,%r8,4), %xmm0
	vmovaps	%xmm0, 1520(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm15, %xmm1
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm11
	vmovups	48(%r10,%r8,4), %xmm1
	vmovaps	%xmm1, 1472(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	1760(%rsp), %xmm15, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm0
	movslq	1040(%rsp), %rbx        # 4-byte Folded Reload
	vmulps	%xmm14, %xmm15, %xmm1
	vmovups	32(%r10,%rbx,4), %xmm3
	vmovaps	%xmm3, 944(%rsp)        # 16-byte Spill
	vmovups	48(%r10,%rbx,4), %xmm5
	vmovaps	%xmm5, 928(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm3, %xmm5 # xmm5 = xmm3[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm5, %xmm1, %xmm3
	vmovups	40(%r10,%rbp,4), %xmm1
	vmovaps	%xmm1, 1584(%rsp)       # 16-byte Spill
	vmovups	56(%r10,%rbp,4), %xmm5
	vmovaps	%xmm5, 736(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm5[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vmulps	1744(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm5, %xmm4
	movq	1024(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1136(%rsp), %rcx        # 8-byte Reload
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1120(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	movq	%r15, %r14
	movslq	1088(%rsp), %rcx        # 4-byte Folded Reload
	vmovaps	1888(%rsp), %xmm7       # 16-byte Reload
	vmovaps	1008(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm7, %xmm1, %xmm1
	vxorps	%xmm15, %xmm15, %xmm15
	vmaxps	%xmm15, %xmm1, %xmm14
	vminps	%xmm7, %xmm12, %xmm1
	vmaxps	%xmm15, %xmm1, %xmm9
	vminps	%xmm7, %xmm8, %xmm5
	vmaxps	%xmm15, %xmm5, %xmm1
	vmovaps	%xmm1, 1216(%rsp)       # 16-byte Spill
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm15, %xmm2, %xmm1
	vmovaps	%xmm1, 1568(%rsp)       # 16-byte Spill
	movslq	912(%rsp), %rdx         # 4-byte Folded Reload
	movq	%rdx, 1120(%rsp)        # 8-byte Spill
	movslq	1104(%rsp), %r9         # 4-byte Folded Reload
	vminps	%xmm7, %xmm0, %xmm10
	vminps	%xmm7, %xmm3, %xmm0
	vmovaps	%xmm0, 1152(%rsp)       # 16-byte Spill
	vminps	%xmm7, %xmm4, %xmm0
	vmovaps	%xmm0, 1136(%rsp)       # 16-byte Spill
	vminps	%xmm7, %xmm11, %xmm11
	movl	584(%rsp), %esi         # 4-byte Reload
	testl	%esi, %esi
	movq	1736(%rsp), %r11        # 8-byte Reload
	vmovups	(%r11,%r13,4), %xmm4
	vmovaps	%xmm4, 768(%rsp)        # 16-byte Spill
	movq	1184(%rsp), %rdi        # 8-byte Reload
	vmovups	(%r11,%rdi,4), %xmm3
	vmovaps	%xmm3, 784(%rsp)        # 16-byte Spill
	vmovups	8(%r11,%rcx,4), %xmm5
	vmovups	24(%r11,%rcx,4), %xmm6
	vshufps	$221, 1808(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm4[1,3],mem[1,3]
	vshufps	$221, 1536(%rsp), %xmm3, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm3[1,3],mem[1,3]
	vmovups	16(%r11,%rcx,4), %xmm3
	vmovups	8(%r11,%rdx,4), %xmm12
	vmovups	24(%r11,%rdx,4), %xmm13
	vmovups	8(%r11,%r9,4), %xmm2
	vmovups	24(%r11,%r9,4), %xmm1
	vmovups	32(%r11,%rcx,4), %xmm0
	vmovups	16(%r11,%r9,4), %xmm4
	vmovaps	%xmm4, 1040(%rsp)       # 16-byte Spill
	vmovups	40(%r10,%r8,4), %xmm4
	vmovaps	%xmm4, 1088(%rsp)       # 16-byte Spill
	vmovups	16(%r11,%rdx,4), %xmm4
	vmovaps	%xmm4, 1104(%rsp)       # 16-byte Spill
	vmovups	40(%r10,%rbx,4), %xmm4
	vmovaps	%xmm4, 1184(%rsp)       # 16-byte Spill
	je	.LBB161_137
# BB#136:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vxorps	%xmm4, %xmm4, %xmm4
	vmovaps	%xmm4, 1680(%rsp)       # 16-byte Spill
.LBB161_137:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vsubps	1072(%rsp), %xmm14, %xmm14 # 16-byte Folded Reload
	vshufps	$136, %xmm6, %xmm5, %xmm4 # xmm4 = xmm5[0,2],xmm6[0,2]
	vmovaps	%xmm6, 1008(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1072(%rsp)       # 16-byte Spill
	vsubps	1056(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
	vshufps	$136, %xmm1, %xmm2, %xmm5 # xmm5 = xmm2[0,2],xmm1[0,2]
	vmovaps	%xmm5, 848(%rsp)        # 16-byte Spill
	vmaxps	%xmm15, %xmm10, %xmm5
	vmovaps	%xmm5, 880(%rsp)        # 16-byte Spill
	vmovaps	1216(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vshufps	$136, %xmm13, %xmm12, %xmm6 # xmm6 = xmm12[0,2],xmm13[0,2]
	vmovaps	%xmm6, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 960(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1056(%rsp)      # 16-byte Spill
	vmovaps	1152(%rsp), %xmm6       # 16-byte Reload
	vmaxps	%xmm15, %xmm6, %xmm6
	vmovaps	%xmm6, 896(%rsp)        # 16-byte Spill
	vmovaps	1568(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm8, %xmm6, %xmm6
	vshufps	$136, %xmm0, %xmm3, %xmm7 # xmm7 = xmm3[0,2],xmm0[0,2]
	vmovaps	%xmm7, 912(%rsp)        # 16-byte Spill
	vmovaps	1136(%rsp), %xmm7       # 16-byte Reload
	vmaxps	%xmm15, %xmm7, %xmm10
	vmaxps	%xmm15, %xmm11, %xmm12
	movl	1200(%rsp), %edi        # 4-byte Reload
	movl	%edi, %edx
	movq	1800(%rsp), %rax        # 8-byte Reload
	orl	%eax, %edx
	testb	$1, %dl
	movq	1856(%rsp), %r15        # 8-byte Reload
	movl	1288(%rsp), %r12d       # 4-byte Reload
	vxorps	%xmm13, %xmm13, %xmm13
	je	.LBB161_138
# BB#139:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 1136(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1408(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, %xmm2
	vmovaps	1632(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1616(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1824(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1888(%rsp), %xmm8       # 16-byte Reload
	jmp	.LBB161_140
	.align	16, 0x90
.LBB161_138:                            #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm1, 1152(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, %xmm2
	vmovaps	1024(%rsp), %xmm0       # 16-byte Reload
	vmulps	1312(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	%xmm4, 1568(%rsp)       # 16-byte Spill
	vmovaps	1776(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1408(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	1616(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm4, %xmm4
	vmovaps	1632(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm3, %xmm4, %xmm3
	vmovaps	1808(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1424(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmovaps	%xmm5, %xmm1
	vmovaps	%xmm1, 656(%rsp)        # 16-byte Spill
	vmovaps	1888(%rsp), %xmm8       # 16-byte Reload
	vminps	%xmm8, %xmm3, %xmm3
	vmaxps	%xmm13, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vmulps	1600(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 752(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm4, %xmm0, %xmm0
	vmovaps	1536(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1440(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm13, %xmm0, %xmm0
	vsubps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm6, %xmm9, %xmm4
	vmovaps	%xmm6, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 1136(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm4, %xmm0
	vmovaps	1568(%rsp), %xmm4       # 16-byte Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm14, %xmm0
	vmovaps	%xmm14, 1408(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm3, %xmm0
	vmulps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	1824(%rsp), %xmm13      # 16-byte Reload
.LBB161_140:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	912(%rsp), %xmm0        # 16-byte Reload
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vmovaps	864(%rsp), %xmm5        # 16-byte Reload
	vmovaps	848(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm6, %xmm3, %xmm9
	vsubps	%xmm5, %xmm1, %xmm3
	vsubps	%xmm0, %xmm10, %xmm7
	vsubps	%xmm4, %xmm12, %xmm6
	orq	$6, %rbp
	testl	%edi, %esi
	vmovaps	%xmm2, %xmm12
	jne	.LBB161_141
# BB#142:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm9, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 1424(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1568(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm7       # 16-byte Reload
	vmovaps	1584(%rsp), %xmm5       # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	928(%rsp), %xmm6        # 16-byte Reload
	vmovaps	1072(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1008(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1056(%rsp), %xmm3       # 16-byte Reload
	vmovaps	960(%rsp), %xmm4        # 16-byte Reload
	vxorps	%xmm14, %xmm14, %xmm14
	jmp	.LBB161_143
	.align	16, 0x90
.LBB161_141:                            #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm4, 1568(%rsp)       # 16-byte Spill
	vmovaps	1744(%rsp), %xmm2       # 16-byte Reload
	vmulps	1760(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 56(%r10,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	1040(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%r11,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm8, %xmm0, %xmm0
	vxorps	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	%xmm13, %xmm2, %xmm1
	vmovaps	1184(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 56(%r10,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	movq	1120(%rsp), %rdx        # 8-byte Reload
	vmovaps	1104(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%r11,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm6, %xmm3, %xmm2
	vmovaps	%xmm3, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 880(%rsp)        # 16-byte Spill
	vaddps	%xmm9, %xmm2, %xmm2
	vmovaps	%xmm9, 912(%rsp)        # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vmovaps	%xmm7, 1424(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm7       # 16-byte Reload
	vmovaps	1584(%rsp), %xmm5       # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	928(%rsp), %xmm6        # 16-byte Reload
	vmovaps	1072(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1008(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1056(%rsp), %xmm3       # 16-byte Reload
	vmovaps	960(%rsp), %xmm4        # 16-byte Reload
.LBB161_143:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmulps	1456(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vmovups	(%r10,%rbp,4), %xmm1
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovups	(%r11,%rcx,4), %xmm1
	vmovaps	%xmm1, 1056(%rsp)       # 16-byte Spill
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm12, %xmm5
	vshufps	$221, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm5[1,3]
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1744(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm3, %xmm0 # xmm0 = xmm3[1,3],xmm4[1,3]
	vshufps	$221, %xmm6, %xmm2, %xmm1 # xmm1 = xmm2[1,3],xmm6[1,3]
	vmovaps	1168(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm13, %xmm12, %xmm2
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm8, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vmovaps	%xmm5, %xmm13
	vsubps	%xmm0, %xmm1, %xmm5
	vshufps	$221, %xmm10, %xmm9, %xmm10 # xmm10 = xmm9[1,3],xmm10[1,3]
	vmovaps	%xmm10, 1440(%rsp)      # 16-byte Spill
	vmovaps	1504(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm7, %xmm12, %xmm1
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm11, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1216(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1152(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1520(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1472(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1760(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm4, %xmm12, %xmm3
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	vminps	%xmm8, %xmm0, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vsubps	%xmm10, %xmm0, %xmm10
	andl	$1, %edi
	vmovaps	%xmm4, %xmm12
	movq	%rbx, %r13
	je	.LBB161_144
# BB#145:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 1520(%rsp)      # 16-byte Spill
	vmovaps	1680(%rsp), %xmm9       # 16-byte Reload
	movl	992(%rsp), %ecx         # 4-byte Reload
	vmovaps	%xmm5, %xmm13
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1136(%rsp), %xmm1       # 16-byte Reload
	movq	%rax, %rbx
	vmovaps	1744(%rsp), %xmm3       # 16-byte Reload
	jmp	.LBB161_146
	.align	16, 0x90
.LBB161_144:                            #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vshufps	$221, 640(%rsp), %xmm13, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm13[1,3],mem[1,3]
	vmovaps	%xmm13, 1520(%rsp)      # 16-byte Spill
	vmulps	1024(%rsp), %xmm7, %xmm2 # 16-byte Folded Reload
	vmovaps	1584(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 736(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm15, %xmm3, %xmm3
	vmulps	%xmm3, %xmm11, %xmm3
	vmulps	%xmm2, %xmm3, %xmm2
	vminps	%xmm8, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm1, %xmm10, %xmm2
	vaddps	%xmm2, %xmm5, %xmm2
	vaddps	%xmm0, %xmm2, %xmm1
	vmovaps	1744(%rsp), %xmm2       # 16-byte Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vmulps	96(%rsp), %xmm1, %xmm9  # 16-byte Folded Reload
	movl	992(%rsp), %ecx         # 4-byte Reload
	vmovaps	%xmm5, %xmm13
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1136(%rsp), %xmm1       # 16-byte Reload
	movq	%rax, %rbx
	vmovaps	%xmm2, %xmm3
.LBB161_146:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	testl	%esi, %esi
	jne	.LBB161_148
# BB#147:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	1680(%rsp), %xmm9       # 16-byte Reload
.LBB161_148:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	movl	%r12d, %eax
	andl	$1, %eax
	jne	.LBB161_149
# BB#150:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	.LBB161_151
	.align	16, 0x90
.LBB161_149:                            #   in Loop: Header=BB161_41 Depth=1
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	656(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	752(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	112(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
.LBB161_151:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovdqa	976(%rsp), %xmm0        # 16-byte Reload
	vmovaps	%xmm3, 1744(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1888(%rsp)       # 16-byte Spill
	testl	%esi, %esi
	je	.LBB161_153
# BB#152:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
.LBB161_153:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	.LCPI161_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vpaddd	%xmm3, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	1264(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	1248(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	1232(%rsp)              # 4-byte Folded Reload
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpinsrd	$2, %ebp, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm5
	vpand	1344(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm5, %xmm1
	vmovdqa	400(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm5
	vpxor	.LCPI161_9(%rip), %xmm5, %xmm5
	vmovdqa	384(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vmovdqa	1376(%rsp), %xmm4       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm4, %xmm6
	vmovdqa	1328(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm1, %xmm4, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vmovdqa	1392(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm1, %xmm1
	vmovdqa	1360(%rsp), %xmm7       # 16-byte Reload
	vpminsd	%xmm7, %xmm1, %xmm1
	vpmaxsd	%xmm4, %xmm1, %xmm1
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r15), %eax
	vmovd	%eax, %xmm6
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm6
	vpminsd	%xmm7, %xmm6, %xmm6
	vpmaxsd	%xmm4, %xmm6, %xmm6
	vblendvps	%xmm5, %xmm1, %xmm6, %xmm1
	vpmulld	1664(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	1648(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovq	%xmm1, %rax
	movslq	%eax, %rcx
	vmovss	(%r14,%rcx,4), %xmm5    # xmm5 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rcx
	sarq	$32, %rax
	vinsertps	$16, (%r14,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0],mem[0],xmm5[2,3]
	movslq	%ecx, %rax
	vinsertps	$32, (%r14,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	sarq	$32, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm1, %xmm8 # xmm8 = xmm1[0,1,2],mem[0]
	vmovups	(%r11,%r9,4), %xmm1
	movl	%r12d, %eax
	orl	%ebx, %eax
	vmovaps	%xmm10, %xmm0
	vmovaps	%xmm12, %xmm14
	testb	$1, %al
	je	.LBB161_154
# BB#155:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	movq	1712(%rsp), %rdx        # 8-byte Reload
	vxorps	%xmm10, %xmm10, %xmm10
	jmp	.LBB161_156
	.align	16, 0x90
.LBB161_154:                            #   in Loop: Header=BB161_41 Depth=1
	vmovaps	768(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1808(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	816(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm4, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm4[0,2],mem[0,2]
	vmulps	1312(%rsp), %xmm8, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vmovaps	1888(%rsp), %xmm10      # 16-byte Reload
	vminps	%xmm10, %xmm6, %xmm6
	vxorps	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm6, %xmm6
	vsubps	%xmm2, %xmm6, %xmm2
	vmovaps	784(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1536(%rsp), %xmm4, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm4[0,2],mem[0,2]
	vmovaps	800(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1552(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm4[0,2],mem[0,2]
	vmulps	1600(%rsp), %xmm8, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm15, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm4, %xmm4
	vminps	%xmm10, %xmm4, %xmm4
	vxorps	%xmm10, %xmm10, %xmm10
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm6, %xmm4, %xmm4
	vaddps	720(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	704(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vaddps	%xmm4, %xmm2, %xmm2
	vaddps	688(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	672(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	movq	1712(%rsp), %rdx        # 8-byte Reload
.LBB161_156:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	1824(%rsp), %xmm5       # 16-byte Reload
	testl	%r12d, %esi
	je	.LBB161_158
# BB#157:                               #   in Loop: Header=BB161_41 Depth=1
	vshufps	$221, 1040(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	orq	$6, %r8
	vmovups	(%r10,%r8,4), %xmm2
	vshufps	$221, 1088(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vmovaps	1456(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm14, %xmm6, %xmm4
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmovaps	1888(%rsp), %xmm7       # 16-byte Reload
	vminps	%xmm7, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm1
	movq	1120(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm2
	vshufps	$221, 1104(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	orq	$6, %r13
	vmovups	(%r10,%r13,4), %xmm4
	vshufps	$221, 1184(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	%xmm5, %xmm6, %xmm6
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm7, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm2, %xmm4, %xmm2
	vaddps	1744(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm1, %xmm13, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	1504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1296(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
.LBB161_158:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	1424(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1840(%rsp), %xmm12      # 16-byte Reload
	movl	%r12d, %eax
	andl	$1, %eax
	je	.LBB161_159
# BB#160:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm14, 1760(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, %xmm0
	jmp	.LBB161_161
	.align	16, 0x90
.LBB161_159:                            #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm14, 1760(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, 1824(%rsp)       # 16-byte Spill
	vmovaps	1056(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmulps	%xmm12, %xmm8, %xmm1
	vmovaps	1072(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1584(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm15, %xmm4, %xmm4
	vmulps	%xmm4, %xmm11, %xmm4
	vmulps	%xmm4, %xmm1, %xmm1
	vminps	1888(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmaxps	%xmm10, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	880(%rsp), %xmm1        # 16-byte Reload
	vaddps	912(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	896(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm6, %xmm1, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	96(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
.LBB161_161:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	1568(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1440(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm12, 1840(%rsp)      # 16-byte Spill
	vmovaps	%xmm15, 1616(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, 1632(%rsp)      # 16-byte Spill
	testl	%esi, %esi
	jne	.LBB161_163
# BB#162:                               # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vmovaps	%xmm2, %xmm0
.LBB161_163:                            # %for f7.s0.v10.v1016
                                        #   in Loop: Header=BB161_41 Depth=1
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm9, %xmm4, %xmm1
	vmovaps	.LCPI161_7(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI161_8(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r12d, %rax
	movq	560(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r15d
	addl	$-1, %edx
	movq	%rdx, 1712(%rsp)        # 8-byte Spill
	jne	.LBB161_41
.LBB161_164:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1912, %rsp             # imm = 0x778
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end161:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177, .Lfunc_end161-par_for_par_for___sharpi_f0.s0.v11.v14_f7.s0.v11.177

	.section	.rodata.cst16,"aM",@progbits,16
	.align	16
.LCPI162_0:
	.long	0                       # 0x0
	.long	4294967294              # 0xfffffffe
	.long	4294967292              # 0xfffffffc
	.long	4294967290              # 0xfffffffa
.LCPI162_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
.LCPI162_9:
	.zero	16,255
.LCPI162_10:
	.zero	16
	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI162_1:
	.long	1199570688              # float 65535
.LCPI162_3:
	.long	1065353216              # float 1
.LCPI162_4:
	.long	1045220557              # float 0.200000003
.LCPI162_5:
	.long	1042983595              # float 0.166666672
.LCPI162_6:
	.long	1048576000              # float 0.25
	.section	.rodata,"a",@progbits
	.align	32
.LCPI162_7:
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.zero	4
.LCPI162_8:
	.zero	4
	.long	0                       # 0x0
	.zero	4
	.long	1                       # 0x1
	.zero	4
	.long	2                       # 0x2
	.zero	4
	.long	3                       # 0x3
	.section	.text.par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178,@function
par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178: # @par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1864, %rsp             # imm = 0x748
	vmovss	(%rdx), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 60(%rsp)         # 4-byte Spill
	vmovss	4(%rdx), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 120(%rsp)        # 4-byte Spill
	vmovss	8(%rdx), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 112(%rsp)        # 4-byte Spill
	movl	12(%rdx), %eax
	movl	%eax, 96(%rsp)          # 4-byte Spill
	movl	16(%rdx), %eax
	movl	%eax, 64(%rsp)          # 4-byte Spill
	movl	20(%rdx), %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	movslq	24(%rdx), %rax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	movl	28(%rdx), %edi
	movslq	40(%rdx), %rax
	movq	%rax, 128(%rsp)         # 8-byte Spill
	movl	44(%rdx), %eax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	movl	48(%rdx), %eax
	movq	%rax, 136(%rsp)         # 8-byte Spill
	movl	52(%rdx), %eax
	movq	%rax, 24(%rsp)          # 8-byte Spill
	movl	56(%rdx), %eax
	movq	%rax, 272(%rsp)         # 8-byte Spill
	movslq	60(%rdx), %rax
	movq	%rax, 560(%rsp)         # 8-byte Spill
	movl	64(%rdx), %ebp
	vmovss	84(%rdx), %xmm8         # xmm8 = mem[0],zero,zero,zero
	cmpl	%esi, 36(%rdx)
	movl	68(%rdx), %ecx
	movl	72(%rdx), %eax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	movl	76(%rdx), %r8d
	movslq	80(%rdx), %rax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	movq	88(%rdx), %r9
	movq	104(%rdx), %r13
	movq	%r13, 1736(%rsp)        # 8-byte Spill
	movq	120(%rdx), %rax
	movq	%rax, 1704(%rsp)        # 8-byte Spill
	movq	136(%rdx), %rax
	movq	%rax, 1696(%rsp)        # 8-byte Spill
	movq	152(%rdx), %rax
	movq	%rax, 80(%rsp)          # 8-byte Spill
	jle	.LBB162_34
# BB#1:                                 # %true_bb
	movq	%rbp, 8(%rsp)           # 8-byte Spill
	movq	%rsi, %r13
	addl	$39, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB162_160
# BB#2:                                 # %for f8.s0.v10.v10.preheader
	movq	%r13, 1744(%rsp)        # 8-byte Spill
	movl	%r13d, %r10d
	subl	%r8d, %r10d
	movq	%r10, 1632(%rsp)        # 8-byte Spill
	leal	1(%r10), %eax
	leal	(%rcx,%rcx), %r12d
	movl	%r12d, 1648(%rsp)       # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%edx, %edi
	leal	-1(%r10), %eax
	cltd
	idivl	%r12d
	movl	%r12d, %eax
	negl	%eax
	movl	%ecx, %ebp
	sarl	$31, %ebp
	movq	%rcx, %rsi
	movq	%rsi, 1840(%rsp)        # 8-byte Spill
	andnl	%r12d, %ebp, %ecx
	andl	%eax, %ebp
	orl	%ecx, %ebp
	movl	%ebp, 1680(%rsp)        # 4-byte Spill
	movl	%edi, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edi, %eax
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%ebp, %ecx
	addl	%edx, %ecx
	movq	%r8, %r11
	leal	-1(%rsi,%rsi), %r15d
	movq	%r9, 1752(%rsp)         # 8-byte Spill
	movl	%r15d, %edx
	subl	%ecx, %edx
	cmpl	%ecx, %esi
	cmovgl	%ecx, %edx
	addl	%r11d, %edx
	leal	-1(%r11,%rsi), %r14d
	cmpl	%edx, %r14d
	cmovlel	%r14d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, 1776(%rsp)        # 4-byte Spill
	movl	%edx, %r9d
	leal	(%r11,%rsi), %ecx
	movq	%rsi, %rdi
	cmpl	%r13d, %ecx
	movl	%ecx, %edx
	cmovgl	%r13d, %edx
	addl	$-1, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, %esi
	movl	%r15d, %edx
	subl	%eax, %edx
	cmpl	%eax, %edi
	movq	%rdi, %r8
	cmovgl	%eax, %edx
	addl	%r11d, %edx
	cmpl	%edx, %r14d
	cmovlel	%r14d, %edx
	cmpl	%r11d, %edx
	cmovll	%r11d, %edx
	movl	%edx, 1824(%rsp)        # 4-byte Spill
	leal	1(%r13), %eax
	movq	%rax, 584(%rsp)         # 8-byte Spill
	cmpl	%eax, %r14d
	movl	%r14d, %edi
	cmovgl	%eax, %edi
	cmpl	%r11d, %edi
	cmovll	%r11d, %edi
	cmpl	%r13d, %r14d
	movl	%r14d, %ebx
	cmovgl	%r13d, %ebx
	cmovlel	%edx, %edi
	movl	%edi, 1808(%rsp)        # 4-byte Spill
	cmpl	%r13d, %ecx
	cmovll	%r9d, %esi
	movl	%esi, 1760(%rsp)        # 4-byte Spill
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ebp, %eax
	addl	%edx, %eax
	movl	%r15d, %ebp
	subl	%eax, %ebp
	cmpl	%eax, %r8d
	cmovgl	%eax, %ebp
	addl	%r11d, %ebp
	cmpl	%ebp, %r14d
	cmovlel	%r14d, %ebp
	cmpl	%r11d, %ebp
	cmovll	%r11d, %ebp
	cmpl	%r11d, %ebx
	cmovll	%r11d, %ebx
	cmpl	%r13d, %ecx
	cmovlel	%ebp, %ebx
	movl	%ebx, 1664(%rsp)        # 4-byte Spill
	movl	$2, %eax
	movq	32(%rsp), %rbx          # 8-byte Reload
	subl	%ebx, %eax
	movq	8(%rsp), %rsi           # 8-byte Reload
	leal	(%rsi,%rsi), %ecx
	cltd
	idivl	%ecx
	movl	%esi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	negl	%ecx
	andl	%eax, %ecx
	orl	%edi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rsi,%rsi), %r9d
	subl	%eax, %r9d
	cmpl	%eax, %esi
	cmovgl	%eax, %r9d
	leal	(%rbx,%rsi), %eax
	leal	-1(%rbx,%rsi), %r12d
	addl	%ebx, %r9d
	cmpl	%r9d, %r12d
	cmovlel	%r12d, %r9d
	cmpl	%ebx, %r9d
	cmovll	%ebx, %r9d
	cmpl	$3, %eax
	movl	$2, %r10d
	cmovgel	%r10d, %r12d
	cmpl	%ebx, %r12d
	cmovll	%ebx, %r12d
	cmpl	$3, %eax
	cmovll	%r9d, %r12d
	movl	$2, %eax
	movq	24(%rsp), %r8           # 8-byte Reload
	subl	%r8d, %eax
	movq	16(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%ecx, %esi
	addl	%edx, %esi
	leal	-1(%rdi,%rdi), %eax
	subl	%esi, %eax
	cmpl	%esi, %edi
	cmovgl	%esi, %eax
	leal	(%r8,%rdi), %ecx
	leal	-1(%r8,%rdi), %edx
	addl	%r8d, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	cmpl	$3, %ecx
	cmovll	%edx, %r10d
	cmpl	%r8d, %r10d
	cmovll	%r8d, %r10d
	cmpl	$3, %ecx
	cmovll	%eax, %r10d
	movq	128(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 576(%rsp)         # 8-byte Spill
	movl	%r13d, %ecx
	andl	$1, %ecx
	movl	%ecx, 568(%rsp)         # 4-byte Spill
	movl	%r13d, %ecx
	andl	$63, %ecx
	movq	%rcx, 1792(%rsp)        # 8-byte Spill
	cmpl	$2, %r8d
	cmovgl	%eax, %r10d
	movq	560(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ecx
	movq	272(%rsp), %rsi         # 8-byte Reload
	imull	%esi, %ecx
	addl	%r8d, %ecx
	cmpl	$2, %ebx
	cmovgl	%r9d, %r12d
	movq	1632(%rsp), %rdi        # 8-byte Reload
	leal	-2(%rdi), %eax
	cltd
	movl	1648(%rsp), %r8d        # 4-byte Reload
	idivl	%r8d
	movl	%edx, %r9d
	movq	%rdi, %rax
	addl	$2, %eax
	cltd
	idivl	%r8d
	movq	136(%rsp), %rdi         # 8-byte Reload
	leal	(%rdi,%rdi), %eax
	vmovd	%eax, %xmm11
	vmovd	%ecx, %xmm2
	vmovd	%ecx, %xmm12
	movq	560(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm10
	leal	-1(%rsi,%rdi), %eax
	vmovd	%eax, %xmm6
	leal	-1(%rsi), %eax
	vmovd	%eax, %xmm13
	vmovd	%r10d, %xmm4
	vmovd	%r10d, %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	leal	1(%rsi,%rdi), %eax
	vmovd	%eax, %xmm15
	movq	40(%rsp), %r8           # 8-byte Reload
	movl	%r8d, %ecx
	imull	%r11d, %ecx
	addl	%ebx, %ecx
	movl	%r9d, %eax
	sarl	$31, %eax
	movl	1680(%rsp), %edi        # 4-byte Reload
	andl	%edi, %eax
	addl	%r9d, %eax
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%edi, %esi
	addl	%edx, %esi
	movl	96(%rsp), %edi          # 4-byte Reload
	andl	$-32, %edi
	movl	%edi, %edx
	shll	$5, %edx
	movq	%rdx, 1680(%rsp)        # 8-byte Spill
	cmpl	%r13d, %r11d
	movl	1664(%rsp), %edx        # 4-byte Reload
	cmovgl	%ebp, %edx
	movl	1760(%rsp), %r9d        # 4-byte Reload
	cmovgel	1776(%rsp), %r9d        # 4-byte Folded Reload
	movq	1752(%rsp), %r10        # 8-byte Reload
	movslq	%ecx, %rcx
	movslq	%r12d, %rbx
	subq	%rcx, %rbx
	movslq	%edx, %rcx
	imulq	%r8, %rcx
	movq	%rcx, 1776(%rsp)        # 8-byte Spill
	movl	%r15d, %ecx
	subl	%esi, %ecx
	movq	1840(%rsp), %r12        # 8-byte Reload
	cmpl	%esi, %r12d
	cmovgl	%esi, %ecx
	addl	%r11d, %ecx
	cmpl	%ecx, %r14d
	cmovlel	%r14d, %ecx
	cmpl	%r11d, %ecx
	cmovll	%r11d, %ecx
	leal	2(%r13), %edx
	cmpl	%edx, %r14d
	movl	%r14d, %esi
	cmovgl	%edx, %esi
	cmpl	%r11d, %esi
	cmovll	%r11d, %esi
	leal	-2(%r11,%r12), %ebp
	cmpl	%r13d, %ebp
	cmovlel	%ecx, %esi
	leal	-2(%r11), %ebp
	cmpl	%r13d, %ebp
	cmovgl	%ecx, %esi
	movslq	%esi, %rcx
	imulq	%r8, %rcx
	movq	%rcx, 1760(%rsp)        # 8-byte Spill
	movq	272(%rsp), %rbp         # 8-byte Reload
	leal	1(%rbp), %ecx
	vmovd	%ecx, %xmm14
	subl	%eax, %r15d
	cmpl	%eax, %r12d
	cmovgl	%eax, %r15d
	leal	2(%r11,%r12), %eax
	addl	%r11d, %r15d
	cmpl	%r15d, %r14d
	cmovlel	%r14d, %r15d
	cmpl	%r11d, %r15d
	cmovll	%r11d, %r15d
	leal	-2(%r13), %ecx
	cmpl	%ecx, %r14d
	cmovgl	%ecx, %r14d
	cmpl	%r11d, %r14d
	cmovll	%r11d, %r14d
	cmpl	%r13d, %eax
	cmovlel	%r15d, %r14d
	leal	2(%r11), %eax
	cmpl	%r13d, %eax
	cmovgl	%r15d, %r14d
	movslq	%r14d, %r14
	imulq	%r8, %r14
	movslq	%r9d, %rax
	imulq	%r8, %rax
	addl	$-1, %r11d
	cmpl	%r13d, %r11d
	movl	1808(%rsp), %esi        # 4-byte Reload
	cmovgl	1824(%rsp), %esi        # 4-byte Folded Reload
	movslq	%esi, %r9
	imulq	%r8, %r9
	movq	136(%rsp), %rsi         # 8-byte Reload
	leal	2(%rbp,%rsi), %esi
	movq	%rbp, %r15
	vmovd	%esi, %xmm9
	vpsubd	%xmm2, %xmm4, %xmm2
	vmovss	.LCPI162_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovss	60(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm1, %xmm4
	vmovss	112(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmulss	%xmm7, %xmm4, %xmm1
	vdivss	%xmm8, %xmm1, %xmm1
	movq	832(%rsp), %rsi         # 8-byte Reload
	addl	$3, %esi
	movq	584(%rsp), %rbp         # 8-byte Reload
	andl	$63, %ebp
	imull	%esi, %ebp
	movq	%rbp, 584(%rsp)         # 8-byte Spill
	leal	63(%r13), %ebp
	andl	$63, %ebp
	imull	%esi, %ebp
	movq	%rbp, 544(%rsp)         # 8-byte Spill
	andl	$63, %ecx
	imull	%esi, %ecx
	movq	%rcx, 552(%rsp)         # 8-byte Spill
	andl	$63, %edx
	imull	%esi, %edx
	movq	%rdx, 560(%rsp)         # 8-byte Spill
	vaddss	%xmm1, %xmm3, %xmm1
	movq	1792(%rsp), %rcx        # 8-byte Reload
	imull	%ecx, %esi
	movq	%rsi, 832(%rsp)         # 8-byte Spill
	leal	2(%r15), %esi
	vmovd	%esi, %xmm3
	movq	48(%rsp), %rsi          # 8-byte Reload
	addq	$32, %rsi
	movq	%rcx, %r8
	imulq	%rsi, %r8
	movq	1776(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	%rcx, 1808(%rsp)        # 8-byte Spill
	movq	1760(%rsp), %rcx        # 8-byte Reload
	leaq	(%rcx,%rbx), %rcx
	movq	%rcx, 1840(%rsp)        # 8-byte Spill
	leal	9(%r13), %ecx
	movl	64(%rsp), %edx          # 4-byte Reload
	subl	%edx, %ecx
	addl	$64, %edi
	imull	%edi, %ecx
	leaq	(%r14,%rbx), %rsi
	movq	%rsi, 1824(%rsp)        # 8-byte Spill
	leaq	(%rax,%rbx), %r12
	leal	7(%r13), %ebp
	subl	%edx, %ebp
	imull	%edi, %ebp
	leal	6(%r13), %eax
	subl	%edx, %eax
	imull	%edi, %eax
	addq	%rbx, %r9
	leal	10(%r13), %esi
	subl	%edx, %esi
	leal	8(%r13), %r14d
	subl	%edx, %r14d
	imull	%edi, %esi
	imull	%edi, %r14d
	movq	128(%rsp), %rdi         # 8-byte Reload
	movq	%rdi, %rdx
	sarq	$63, %rdx
	andq	%rdi, %rdx
	subq	%rdx, %r8
	movq	%r8, 536(%rsp)          # 8-byte Spill
	movq	1680(%rsp), %rdx        # 8-byte Reload
	leal	(%rdx,%rdx,2), %edx
	movq	1736(%rsp), %r11        # 8-byte Reload
	addl	%edx, %ecx
	movq	%rcx, 528(%rsp)         # 8-byte Spill
	addl	%edx, %ebp
	movq	%rbp, 520(%rsp)         # 8-byte Spill
	addl	%edx, %eax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	addl	%edx, %esi
	movq	%rsi, 504(%rsp)         # 8-byte Spill
	addl	%edx, %r14d
	movq	%r14, 496(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm11, %xmm11
	vmovdqa	%xmm11, 480(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm12, %xmm5
	vmovaps	%xmm5, 464(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, %xmm0
	vbroadcastss	%xmm10, %xmm5
	vmovaps	%xmm5, 1680(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm6, %xmm6
	vmovdqa	%xmm6, 96(%rsp)         # 16-byte Spill
	vmovss	120(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm7, %xmm5, %xmm10
	vmovdqa	.LCPI162_0(%rip), %xmm5 # xmm5 = [0,4294967294,4294967292,4294967290]
	vpbroadcastd	%xmm13, %xmm7
	vmovdqa	%xmm6, %xmm12
	vpaddd	%xmm5, %xmm7, %xmm6
	vmovdqa	%xmm6, 448(%rsp)        # 16-byte Spill
	vmulss	%xmm10, %xmm4, %xmm4
	vmovd	%r15d, %xmm6
	vpbroadcastd	%xmm6, %xmm8
	vmovdqa	%xmm8, 1280(%rsp)       # 16-byte Spill
	vdivss	%xmm4, %xmm0, %xmm4
	movq	136(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 1264(%rsp)       # 16-byte Spill
	vbroadcastss	1648(%rsp), %xmm6 # 16-byte Folded Reload
	vmovaps	%xmm6, 432(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm6
	vpaddd	%xmm5, %xmm6, %xmm6
	vmovdqa	%xmm6, 416(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm6
	vmovdqa	%xmm12, %xmm14
	vpaddd	%xmm5, %xmm6, %xmm6
	vmovdqa	%xmm6, 400(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 1664(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	leal	(%r15,%rcx), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	leal	-2(%r15,%rcx), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	leal	-2(%r15), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	leal	-3(%r15,%rcx), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	movq	576(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	subl	%r15d, %eax
	movq	%rax, 288(%rsp)         # 8-byte Spill
	leal	-3(%r15), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm5, %xmm0, %xmm0
	vmovdqa	%xmm0, 272(%rsp)        # 16-byte Spill
	vpaddd	%xmm5, %xmm14, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	vpaddd	%xmm5, %xmm8, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm4, %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	movq	%rcx, %rdx
	leal	3(%rdx), %ecx
	movq	%rcx, 224(%rsp)         # 8-byte Spill
	leal	2(%rdx), %ecx
	movq	%rcx, 208(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 192(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %ecx
	movq	%rcx, 176(%rsp)         # 8-byte Spill
	leal	1(%rdx), %ecx
	movq	%rcx, 160(%rsp)         # 8-byte Spill
	leal	3(%rax), %ecx
	movq	%rcx, 144(%rsp)         # 8-byte Spill
	leal	2(%rax), %ecx
	movq	%rcx, 136(%rsp)         # 8-byte Spill
	leal	-2(%rax), %ecx
	movq	%rcx, 128(%rsp)         # 8-byte Spill
	leal	-1(%rax), %ecx
	movq	%rcx, 120(%rsp)         # 8-byte Spill
	leal	1(%rax), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	xorl	%r8d, %r8d
	movq	80(%rsp), %rdx          # 8-byte Reload
	movq	1808(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	movq	1840(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	movq	1824(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rdx,%rax,4), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%r12,4), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vbroadcastss	(%rdx,%r9,4), %xmm0
	vmovaps	%xmm0, 1584(%rsp)       # 16-byte Spill
	vpabsd	%xmm11, %xmm0
	vmovdqa	%xmm0, 1248(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm11, %xmm0
	vmovdqa	%xmm0, 1232(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI162_2(%rip), %xmm11 # xmm11 = [0,2,4,6]
	vbroadcastss	.LCPI162_3(%rip), %xmm0
	vmovaps	%xmm0, 1216(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_4(%rip), %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI162_5(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_6(%rip), %xmm0
	vmovaps	%xmm0, 64(%rsp)         # 16-byte Spill
	.align	16, 0x90
.LBB162_3:                              # %for f8.s0.v10.v10
                                        # =>This Inner Loop Header: Depth=1
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	movslq	%eax, %r13
	vmovups	8(%r11,%r13,4), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%r13,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm11, %xmm12
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	480(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r15d
	movl	%r15d, 1136(%rsp)       # 4-byte Spill
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r14d
	movl	%r14d, 1160(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %esi
	movl	%esi, 1184(%rsp)        # 4-byte Spill
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %ebx
	movl	%ebx, 1172(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r9d
	movl	%r9d, 1176(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1248(%rsp), %xmm8       # 16-byte Reload
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm11
	vmovdqa	%xmm11, 1040(%rsp)      # 16-byte Spill
	vmovdqa	256(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1264(%rsp), %xmm9       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm9, %xmm2
	vmovdqa	%xmm9, %xmm10
	vmovdqa	1232(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	1280(%rsp), %xmm9       # 16-byte Reload
	vpaddd	%xmm9, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm0, %xmm0
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	movl	%eax, 1568(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm12, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1680(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm6
	vpsubd	464(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	movq	1696(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	416(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	400(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm10, %xmm2
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm9, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm0, %xmm0
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm12, %xmm2, %xmm2
	vpminsd	%xmm14, %xmm2, %xmm2
	vpmaxsd	%xmm9, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm6, %xmm0, %xmm0
	vpaddd	1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	movq	288(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, 1760(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1296(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 1104(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, 1088(%rsp)        # 4-byte Spill
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, 1072(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1056(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebp
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	movslq	%eax, %rcx
	vmovups	24608(%r10,%rcx,4), %xmm1
	vmovaps	%xmm1, 1552(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rcx,4), %xmm1
	vmovaps	%xmm1, 1376(%rsp)       # 16-byte Spill
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	cltq
	movq	%rax, 1488(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1776(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1392(%rsp)       # 16-byte Spill
	movq	504(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	cltq
	movq	%rax, 1504(%rsp)        # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1808(%rsp)       # 16-byte Spill
	vmovdqa	%xmm14, %xmm6
	vmovups	24624(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1424(%rsp)       # 16-byte Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	cltq
	movq	%rax, 1520(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1792(%rsp)       # 16-byte Spill
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	cltq
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1408(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1440(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm12, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	vmovups	16(%r11,%r13,4), %xmm1
	vmovaps	%xmm1, 1840(%rsp)       # 16-byte Spill
	vmovups	32(%r11,%r13,4), %xmm1
	vmovaps	%xmm1, 1328(%rsp)       # 16-byte Spill
	vmovups	24616(%r10,%rcx,4), %xmm15
	vmovups	24632(%r10,%rcx,4), %xmm1
	vmovaps	%xmm1, 1312(%rsp)       # 16-byte Spill
	vmovups	(%r11,%r13,4), %xmm13
	vmovaps	%xmm13, 768(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rcx,4), %xmm14
	vmovaps	%xmm14, 784(%rsp)       # 16-byte Spill
	idivl	%r15d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovd	1296(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 1760(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, 1104(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$3, 1088(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpsrad	$31, %xmm2, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm2
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	vmovdqa	%xmm10, %xmm3
	vpcmpgtd	%xmm2, %xmm3, %xmm5
	vpsubd	%xmm2, %xmm4, %xmm7
	vblendvps	%xmm5, %xmm2, %xmm7, %xmm2
	vmovdqa	352(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm5
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vpxor	%xmm10, %xmm5, %xmm5
	vmovdqa	240(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm9, %xmm2, %xmm2
	vpminsd	%xmm6, %xmm2, %xmm2
	vpmaxsd	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm12, %xmm11, %xmm7
	vpminsd	%xmm6, %xmm7, %xmm7
	vpmaxsd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm2, %xmm7, %xmm2
	vpextrd	$3, %xmm0, %eax
	vmovd	1056(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 1072(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, %r12d, %xmm0, %xmm0
	vpinsrd	$3, %ebp, %xmm0, %xmm0
	cltd
	idivl	%r9d
	vpsrad	$31, %xmm0, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm0
	vpcmpgtd	%xmm0, %xmm3, %xmm5
	vpsubd	%xmm0, %xmm4, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm0
	vmovdqa	336(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpcmpeqd	%xmm10, %xmm10, %xmm10
	vmovdqa	320(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm1, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm9, %xmm0, %xmm0
	vpminsd	%xmm6, %xmm0, %xmm0
	vpmaxsd	%xmm9, %xmm0, %xmm0
	movq	208(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm12, %xmm7, %xmm7
	vpminsd	%xmm6, %xmm7, %xmm7
	vpmaxsd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm0, %xmm7, %xmm0
	vmovd	%edi, %xmm5
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpinsrd	$2, %ebx, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm8, %xmm7, %xmm7
	vpaddd	%xmm5, %xmm7, %xmm5
	vpcmpgtd	%xmm5, %xmm3, %xmm7
	vpsubd	%xmm5, %xmm4, %xmm1
	vblendvps	%xmm7, %xmm5, %xmm1, %xmm1
	vmovdqa	304(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vmovdqa	272(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	%xmm11, %xmm7, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm9, %xmm1, %xmm1
	vpminsd	%xmm6, %xmm1, %xmm1
	vpmaxsd	%xmm9, %xmm1, %xmm1
	movq	224(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm12, %xmm7, %xmm7
	vpminsd	%xmm6, %xmm7, %xmm7
	vmovaps	%xmm15, %xmm8
	vpmaxsd	%xmm9, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vmovdqa	1680(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm9, %xmm2, %xmm2
	vmovdqa	1664(%rsp), %xmm12      # 16-byte Reload
	vpaddd	%xmm2, %xmm12, %xmm2
	vpextrq	$1, %xmm2, %r13
	vpmulld	%xmm9, %xmm0, %xmm0
	vmovq	%xmm2, %r10
	vpaddd	%xmm0, %xmm12, %xmm0
	vpextrq	$1, %xmm0, %r9
	vmovq	%xmm0, %rax
	vshufps	$221, %xmm8, %xmm14, %xmm0 # xmm0 = xmm14[1,3],xmm8[1,3]
	vmovaps	1632(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm0, %xmm0
	vmovaps	1648(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	816(%rsp), %xmm7        # 16-byte Reload
	vmulps	1824(%rsp), %xmm7, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1216(%rsp), %xmm10      # 16-byte Reload
	vminps	%xmm10, %xmm0, %xmm0
	vpxor	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vshufps	$221, 1840(%rsp), %xmm13, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm13[1,3],mem[1,3]
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	1456(%rsp), %xmm3       # 16-byte Reload
	vmulps	1600(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm2, %xmm0
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm0
	vmovaps	1536(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1792(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm2, %xmm0, %xmm11
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm7, %xmm3, %xmm2
	vmulps	%xmm0, %xmm2, %xmm0
	vmovaps	1808(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1424(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	1616(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vminps	%xmm10, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vmovaps	1776(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1392(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm3[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm14
	vmovaps	1344(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1472(%rsp), %xmm13      # 16-byte Reload
	vpmulld	%xmm9, %xmm1, %xmm1
	vpxor	%xmm9, %xmm9, %xmm9
	vpaddd	%xmm1, %xmm12, %xmm1
	vpextrq	$1, %xmm1, %rcx
	vmovq	%xmm1, %rdx
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vmovss	(%rsi,%rdi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%ecx, %rdx
	vinsertps	$32, (%rsi,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	sarq	$32, %rcx
	vinsertps	$48, (%rsi,%rcx,4), %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],mem[0]
	movq	%rsi, %r11
	movq	%r10, %rbp
	sarq	$32, %rbp
	movq	%r13, %rdi
	sarq	$32, %rdi
	movq	%rax, %rbx
	sarq	$32, %rbx
	movq	%r9, %rdx
	sarq	$32, %rdx
	movl	1568(%rsp), %ecx        # 4-byte Reload
	andl	$1, %ecx
	testl	%ecx, %ecx
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vshufps	$221, %xmm5, %xmm13, %xmm1 # xmm1 = xmm13[1,3],xmm5[1,3]
	vsubps	%xmm1, %xmm0, %xmm0
	movq	544(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r8), %r14d
	movq	520(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r8), %ecx
	movq	584(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r8), %r15d
	movq	528(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r8), %r12d
	jne	.LBB162_4
# BB#5:                                 # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm6, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 688(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 704(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm3
	vmovaps	1840(%rsp), %xmm0       # 16-byte Reload
	jmp	.LBB162_6
	.align	16, 0x90
.LBB162_4:                              #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, %xmm3
	vmovaps	%xmm3, 672(%rsp)        # 16-byte Spill
	vmulps	%xmm6, %xmm7, %xmm0
	vmovaps	%xmm6, 592(%rsp)        # 16-byte Spill
	vshufps	$221, 1312(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm8[1,3],mem[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1328(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm2[1,3],mem[1,3]
	vminps	%xmm10, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm14, %xmm3, %xmm1
	vmovaps	%xmm14, 688(%rsp)       # 16-byte Spill
	vaddps	%xmm11, %xmm1, %xmm1
	vmovaps	%xmm11, 704(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	1296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	80(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm3
	vmovaps	%xmm2, %xmm0
.LBB162_6:                              # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm8, %xmm12
	vmovaps	1584(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1456(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1440(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1424(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1408(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1392(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1376(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 800(%rsp)       # 16-byte Spill
	movslq	%r10d, %rsi
	vmovss	(%r11,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r13d, %rsi
	vinsertps	$32, (%r11,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rdi,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, 1104(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm5, %xmm13, %xmm0 # xmm0 = xmm13[0,2],xmm5[0,2]
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	cltq
	vmovss	(%r11,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r9d, %rax
	vinsertps	$32, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rdx,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm5, 1344(%rsp)       # 16-byte Spill
	movq	%r11, %r13
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmulps	%xmm7, %xmm4, %xmm1
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm0, %xmm1, %xmm13
	vmovaps	1776(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm2, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm2[0,2]
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vmulps	1616(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovaps	1808(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm8, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm8[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmovaps	%xmm7, %xmm2
	vmulps	%xmm1, %xmm0, %xmm8
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1792(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	vmulps	1600(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm9, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm9[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm14
	vmulps	%xmm2, %xmm5, %xmm0
	vshufps	$136, 1312(%rsp), %xmm12, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm12[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm9
	movslq	%ecx, %rdx
	movq	1752(%rsp), %r10        # 8-byte Reload
	vmovups	24608(%r10,%rdx,4), %xmm4
	vmovaps	%xmm4, 976(%rsp)        # 16-byte Spill
	movslq	%r12d, %rax
	vmovups	24608(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1424(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm2
	vmovaps	%xmm2, 1408(%rsp)       # 16-byte Spill
	vmulps	%xmm6, %xmm11, %xmm0
	vshufps	$221, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm2[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm2
	vmovups	24624(%r10,%rdx,4), %xmm1
	vmovaps	%xmm1, 960(%rsp)        # 16-byte Spill
	vmovaps	1360(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm0
	vshufps	$221, %xmm1, %xmm4, %xmm1 # xmm1 = xmm4[1,3],xmm1[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmulps	%xmm1, %xmm0, %xmm5
	vmovups	24616(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1552(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 752(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	1824(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm6
	vmovups	24616(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 1792(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rdx,4), %xmm1
	vmovaps	%xmm1, 720(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm15, %xmm0, %xmm0
	vmulps	%xmm0, %xmm3, %xmm0
	vmulps	%xmm7, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm3
	vminps	%xmm10, %xmm8, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vminps	%xmm10, %xmm14, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vminps	%xmm10, %xmm9, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	movslq	%r15d, %rsi
	vminps	%xmm10, %xmm2, %xmm4
	vminps	%xmm10, %xmm5, %xmm2
	vminps	%xmm10, %xmm6, %xmm6
	vminps	%xmm10, %xmm3, %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vminps	%xmm10, %xmm13, %xmm3
	vmaxps	%xmm8, %xmm3, %xmm13
	movl	568(%rsp), %r14d        # 4-byte Reload
	testl	%r14d, %r14d
	vmovups	24632(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vmovups	24632(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	movq	1736(%rsp), %r11        # 8-byte Reload
	vmovups	8(%r11,%rsi,4), %xmm9
	vmovups	24(%r11,%rsi,4), %xmm14
	vmovups	16(%r11,%rsi,4), %xmm3
	vmovups	32(%r11,%rsi,4), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmovups	(%r11,%rsi,4), %xmm0
	vmovups	8(%r11,%rcx,4), %xmm11
	vmovups	24(%r11,%rcx,4), %xmm5
	vmovups	16(%r11,%rcx,4), %xmm7
	vmovups	32(%r11,%rcx,4), %xmm1
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
	vmovups	(%r11,%rcx,4), %xmm12
	movq	1488(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 992(%rsp)        # 16-byte Spill
	movq	1504(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1008(%rsp)       # 16-byte Spill
	movq	1520(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1024(%rsp)       # 16-byte Spill
	movq	1120(%rsp), %r12        # 8-byte Reload
	vmovups	24616(%r10,%r12,4), %xmm15
	vmovaps	%xmm15, 1056(%rsp)      # 16-byte Spill
	je	.LBB162_8
# BB#7:                                 # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	%xmm1, 1760(%rsp)       # 16-byte Spill
.LBB162_8:                              # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vshufps	$221, %xmm14, %xmm9, %xmm1 # xmm1 = xmm9[1,3],xmm14[1,3]
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vmaxps	%xmm8, %xmm4, %xmm1
	vmovaps	%xmm1, 880(%rsp)        # 16-byte Spill
	vmovaps	1088(%rsp), %xmm1       # 16-byte Reload
	vsubps	1776(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vshufps	$221, %xmm5, %xmm11, %xmm1 # xmm1 = xmm11[1,3],xmm5[1,3]
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 928(%rsp)        # 16-byte Spill
	vmovaps	%xmm11, 1088(%rsp)      # 16-byte Spill
	vmaxps	%xmm8, %xmm2, %xmm1
	vmovaps	%xmm1, 896(%rsp)        # 16-byte Spill
	vmovaps	1312(%rsp), %xmm1       # 16-byte Reload
	vsubps	1392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vshufps	$221, %xmm3, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm3[1,3]
	vmovaps	%xmm2, 1312(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 736(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 1808(%rsp)       # 16-byte Spill
	vmaxps	%xmm8, %xmm6, %xmm5
	vmovaps	1376(%rsp), %xmm0       # 16-byte Reload
	vsubps	1072(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
	vshufps	$221, %xmm7, %xmm12, %xmm0 # xmm0 = xmm12[1,3],xmm7[1,3]
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm8, %xmm0, %xmm11
	vmovaps	1472(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm13, %xmm3
	movl	1568(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	1744(%rsp), %r9         # 8-byte Reload
	orl	%r9d, %eax
	testb	$1, %al
	movl	1184(%rsp), %r15d       # 4-byte Reload
	vxorps	%xmm13, %xmm13, %xmm13
	je	.LBB162_9
# BB#10:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm7, 1776(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1376(%rsp)      # 16-byte Spill
	vmovaps	%xmm9, 1392(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 1328(%rsp)      # 16-byte Spill
	vmovaps	1648(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1632(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1584(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm10, %xmm9
	jmp	.LBB162_11
	.align	16, 0x90
.LBB162_9:                              #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm7, 1776(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1376(%rsp)      # 16-byte Spill
	vmovaps	%xmm9, 1392(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1328(%rsp)      # 16-byte Spill
	movq	1488(%rsp), %rax        # 8-byte Reload
	vmovaps	992(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 32(%r11,%rax,4), %xmm0, %xmm8 # xmm8 = xmm0[0,2],mem[0,2]
	vmovaps	1344(%rsp), %xmm0       # 16-byte Reload
	vmulps	1616(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	movq	1504(%rsp), %rax        # 8-byte Reload
	vmovaps	%xmm5, %xmm7
	vmovaps	1008(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%r10,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vmovaps	1632(%rsp), %xmm15      # 16-byte Reload
	vsubps	%xmm15, %xmm5, %xmm5
	vmovaps	1648(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm10, %xmm9
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm13, %xmm2, %xmm2
	vsubps	%xmm8, %xmm2, %xmm8
	vmulps	1600(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	1056(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 24632(%r10,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vsubps	%xmm15, %xmm5, %xmm5
	vmulps	%xmm5, %xmm12, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	movq	1520(%rsp), %rax        # 8-byte Reload
	vmovaps	1024(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, 32(%r11,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,2],mem[0,2]
	vminps	%xmm9, %xmm2, %xmm2
	vmaxps	%xmm13, %xmm2, %xmm2
	vsubps	%xmm5, %xmm2, %xmm2
	vaddps	%xmm1, %xmm3, %xmm5
	vmovaps	%xmm3, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 640(%rsp)        # 16-byte Spill
	vaddps	%xmm5, %xmm4, %xmm5
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vaddps	%xmm2, %xmm5, %xmm2
	vmovaps	%xmm7, %xmm5
	vaddps	%xmm2, %xmm6, %xmm2
	vmovaps	%xmm6, 624(%rsp)        # 16-byte Spill
	vaddps	%xmm2, %xmm8, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	1584(%rsp), %xmm8       # 16-byte Reload
.LBB162_11:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	912(%rsp), %xmm0        # 16-byte Reload
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vmovaps	880(%rsp), %xmm2        # 16-byte Reload
	vmovaps	864(%rsp), %xmm3        # 16-byte Reload
	vmovaps	1312(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1072(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm0, %xmm2, %xmm2
	vsubps	%xmm3, %xmm1, %xmm6
	vsubps	%xmm4, %xmm5, %xmm0
	vsubps	%xmm7, %xmm11, %xmm13
	testl	%ecx, %r14d
	jne	.LBB162_12
# BB#13:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm13, 912(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm6, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1312(%rsp)       # 16-byte Spill
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1808(%rsp), %xmm5       # 16-byte Reload
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm4        # 16-byte Reload
	vmovaps	1776(%rsp), %xmm14      # 16-byte Reload
	vmovaps	944(%rsp), %xmm10       # 16-byte Reload
	jmp	.LBB162_14
	.align	16, 0x90
.LBB162_12:                             #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm0, %xmm7
	vmovaps	%xmm7, 1072(%rsp)       # 16-byte Spill
	vmovaps	592(%rsp), %xmm4        # 16-byte Reload
	vmulps	%xmm4, %xmm8, %xmm0
	vmovaps	%xmm13, 912(%rsp)       # 16-byte Spill
	vmovaps	1552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1808(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 1536(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm5[1,3],mem[1,3]
	vminps	%xmm9, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmaxps	%xmm3, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	1360(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm4, %xmm11, %xmm1
	vmovaps	%xmm6, %xmm4
	vmovaps	%xmm4, 1216(%rsp)       # 16-byte Spill
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmovaps	%xmm2, %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vshufps	$221, 1440(%rsp), %xmm6, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm6[1,3],mem[1,3]
	vsubps	%xmm15, %xmm2, %xmm2
	vmulps	%xmm2, %xmm12, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1776(%rsp), %xmm14      # 16-byte Reload
	vmovaps	944(%rsp), %xmm10       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm14, %xmm2 # xmm2 = xmm14[1,3],xmm10[1,3]
	vminps	%xmm9, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm13, %xmm4, %xmm2
	vmovaps	1584(%rsp), %xmm8       # 16-byte Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vmovaps	%xmm11, %xmm2
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	1760(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm4        # 16-byte Reload
.LBB162_14:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmovaps	928(%rsp), %xmm7        # 16-byte Reload
	vmovaps	%xmm5, 1808(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm7[0,2]
	vshufps	$136, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm4[0,2]
	vmovaps	1104(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm2, %xmm11, %xmm4
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vminps	%xmm9, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm4
	vmovaps	1344(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm2, %xmm13, %xmm0
	vshufps	$136, 1440(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, %xmm10, %xmm14, %xmm1 # xmm1 = xmm14[0,2],xmm10[0,2]
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm10
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1408(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	%xmm8, %xmm11, %xmm2
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm9, %xmm1, %xmm1
	vmaxps	%xmm3, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm2
	vmulps	%xmm8, %xmm13, %xmm0
	vmovaps	1552(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm15, %xmm1, %xmm1
	vmulps	%xmm1, %xmm12, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1536(%rsp), %xmm5, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm5[0,2],mem[0,2]
	vminps	%xmm9, %xmm0, %xmm0
	vmaxps	%xmm3, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	andl	$1, %ecx
	vmovaps	%xmm15, %xmm8
	je	.LBB162_15
# BB#16:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm0, %xmm15
	vmovaps	%xmm10, 1568(%rsp)      # 16-byte Spill
	vmovaps	%xmm4, 1536(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1776(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, 1792(%rsp)       # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	vmovaps	1760(%rsp), %xmm10      # 16-byte Reload
	jmp	.LBB162_17
	.align	16, 0x90
.LBB162_15:                             #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm14, 1776(%rsp)      # 16-byte Spill
	vmovaps	%xmm6, 1792(%rsp)       # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	vmovaps	%xmm0, %xmm1
	vaddps	%xmm1, %xmm10, %xmm0
	vmovaps	%xmm1, %xmm15
	vmovaps	%xmm10, 1568(%rsp)      # 16-byte Spill
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm4, %xmm0
	vmovaps	%xmm4, 1536(%rsp)       # 16-byte Spill
	vmulps	64(%rsp), %xmm0, %xmm10 # 16-byte Folded Reload
.LBB162_17:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovdqa	96(%rsp), %xmm14        # 16-byte Reload
	vmovdqa	1040(%rsp), %xmm4       # 16-byte Reload
	testl	%r14d, %r14d
	jne	.LBB162_19
# BB#18:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	1760(%rsp), %xmm10      # 16-byte Reload
.LBB162_19:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI162_2(%rip), %xmm5 # xmm5 = [0,2,4,6]
	vpaddd	%xmm5, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	1136(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	1160(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	movl	%r15d, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	1172(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	andl	$1, %ebx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	1176(%rsp)              # 4-byte Folded Reload
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	1248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm1
	vpxor	.LCPI162_9(%rip), %xmm1, %xmm1
	vmovdqa	368(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vmovdqa	1264(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vmovdqa	1232(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	1280(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm4, %xmm0, %xmm0
	vpminsd	%xmm14, %xmm0, %xmm0
	vpmaxsd	%xmm4, %xmm0, %xmm0
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r8), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm5, %xmm3, %xmm3
	vmovdqa	%xmm5, %xmm11
	vpminsd	%xmm14, %xmm3, %xmm3
	vpmaxsd	%xmm4, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	1680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm2, %xmm13
	testl	%ebx, %ebx
	jne	.LBB162_20
# BB#21:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
	jmp	.LBB162_22
	.align	16, 0x90
.LBB162_20:                             #   in Loop: Header=BB162_3 Depth=1
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1840(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmulps	816(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovaps	784(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 800(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm9, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	608(%rsp), %xmm2        # 16-byte Reload
	vaddps	656(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vaddps	640(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm3, %xmm0
	vmulps	80(%rsp), %xmm0, %xmm3  # 16-byte Folded Reload
.LBB162_22:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	movq	1712(%rsp), %rdx        # 8-byte Reload
	vmovaps	1296(%rsp), %xmm2       # 16-byte Reload
	testl	%r14d, %r14d
	je	.LBB162_24
# BB#23:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
.LBB162_24:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	movl	%r15d, %eax
	orl	%r9d, %eax
	testb	$1, %al
	jne	.LBB162_26
# BB#25:                                #   in Loop: Header=BB162_3 Depth=1
	movq	1488(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm0
	vshufps	$221, 992(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movq	1504(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%r10,%rax,4), %xmm3
	vshufps	$221, 1008(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	1824(%rsp), %xmm5       # 16-byte Reload
	vmulps	1616(%rsp), %xmm5, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm9, %xmm3, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	movq	1520(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm3
	vshufps	$221, 1024(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovups	24600(%r10,%r12,4), %xmm4
	vshufps	$221, 1056(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	1600(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm4, %xmm4
	vmulps	%xmm4, %xmm12, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vminps	%xmm9, %xmm4, %xmm4
	vmaxps	%xmm6, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vaddps	%xmm3, %xmm2, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vaddps	704(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	688(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
.LBB162_26:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	1568(%rsp), %xmm5       # 16-byte Reload
	testl	%r15d, %r14d
	je	.LBB162_28
# BB#27:                                #   in Loop: Header=BB162_3 Depth=1
	vmovaps	720(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1792(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm8, %xmm0, %xmm0
	vmovaps	752(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1552(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[0,2],mem[0,2]
	vmulps	1360(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm12, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vsubps	%xmm8, %xmm3, %xmm3
	vmulps	1584(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm12, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	1328(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm2[0,2],mem[0,2]
	vminps	%xmm9, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vmaxps	%xmm2, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	736(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1808(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm9, %xmm1, %xmm1
	vmaxps	%xmm2, %xmm1, %xmm1
	vaddps	1536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	%xmm0, %xmm5, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm13, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
.LBB162_28:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	848(%rsp), %xmm1        # 16-byte Reload
	vmovaps	1472(%rsp), %xmm2       # 16-byte Reload
	movl	%r15d, %eax
	andl	$1, %eax
	je	.LBB162_29
# BB#30:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm3, %xmm0
	jmp	.LBB162_31
	.align	16, 0x90
.LBB162_29:                             #   in Loop: Header=BB162_3 Depth=1
	vmovaps	1216(%rsp), %xmm0       # 16-byte Reload
	vaddps	1312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1072(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	912(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	64(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
.LBB162_31:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm9, 1216(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1648(%rsp)      # 16-byte Spill
	testl	%r14d, %r14d
	jne	.LBB162_33
# BB#32:                                # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vmovaps	%xmm3, %xmm0
.LBB162_33:                             # %for f8.s0.v10.v10
                                        #   in Loop: Header=BB162_3 Depth=1
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm10, %xmm1, %xmm1
	vmovaps	.LCPI162_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI162_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r15d, %rax
	movq	536(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r8d
	addl	$-1, %edx
	movq	%rdx, 1712(%rsp)        # 8-byte Spill
	jne	.LBB162_3
	jmp	.LBB162_160
.LBB162_34:                             # %false_bb
	cmpl	%esi, 32(%rdx)
	movq	%rsi, 1744(%rsp)        # 8-byte Spill
	jle	.LBB162_39
# BB#35:                                # %true_bb1
	movq	%rbp, 8(%rsp)           # 8-byte Spill
	movq	%r8, -8(%rsp)           # 8-byte Spill
	vmovss	%xmm8, 4(%rsp)          # 4-byte Spill
	movq	%r9, 1752(%rsp)         # 8-byte Spill
	movq	128(%rsp), %rax         # 8-byte Reload
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	movq	%rsi, 1280(%rsp)        # 8-byte Spill
	movq	272(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ebp
	subl	%esi, %ebp
	movl	%ebp, %eax
	sarl	$3, %eax
	leal	1(%rax), %ecx
	leal	4(%rbp), %ebx
	sarl	$3, %ebx
	cmpl	%ebx, %eax
	cmovgel	%ecx, %ebx
	movl	%ebx, -36(%rsp)         # 4-byte Spill
	leal	5(%rbp), %eax
	sarl	$3, %eax
	movq	%rax, -24(%rsp)         # 8-byte Spill
	cmpl	%eax, %ebx
	cmovgel	%ebx, %eax
	leal	6(%rbp), %ecx
	sarl	$3, %ecx
	movl	%ecx, -40(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	leal	7(%rbp), %ecx
	sarl	$3, %ecx
	movl	%ecx, -44(%rsp)         # 4-byte Spill
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	addl	$9, %ebp
	sarl	$3, %ebp
	movq	%rbp, -32(%rsp)         # 8-byte Spill
	cmpl	%ebp, %eax
	cmovll	%ebp, %eax
	xorl	%ebx, %ebx
	testl	%eax, %eax
	cmovnsl	%eax, %ebx
	leal	39(%rdi), %eax
	sarl	$3, %eax
	movl	%eax, 584(%rsp)         # 4-byte Spill
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movl	%ebx, 576(%rsp)         # 4-byte Spill
	movq	136(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %eax
	subl	%esi, %eax
	leal	(%rdx,%rax), %ebp
	leal	(%rdx,%rcx), %ecx
	movq	%rcx, -16(%rsp)         # 8-byte Spill
	subl	%esi, %ecx
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	sarl	$3, %ebp
	movq	%rbp, -64(%rsp)         # 8-byte Spill
	leal	-1(%rbp), %ebp
	leal	-10(%rdx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -68(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-9(%rdx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -72(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-7(%rdx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -76(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-6(%rdx,%rax), %esi
	sarl	$3, %esi
	movl	%esi, -80(%rsp)         # 4-byte Spill
	cmpl	%ebp, %esi
	cmovlel	%esi, %ebp
	leal	-5(%rdx,%rax), %eax
	sarl	$3, %eax
	movl	%eax, -84(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-10(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -88(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-9(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -92(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-7(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -96(%rsp)         # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-6(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -100(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-5(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -104(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-4(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -108(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-3(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -112(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-2(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -116(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	-1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -120(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	leal	1(%rcx), %eax
	sarl	$3, %eax
	movl	%eax, -124(%rsp)        # 4-byte Spill
	cmpl	%ebp, %eax
	cmovlel	%eax, %ebp
	sarl	$3, %ecx
	movq	%rcx, -56(%rsp)         # 8-byte Spill
	cmpl	%ebp, %ecx
	cmovlel	%ecx, %ebp
	addl	$31, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	cmpl	%ebp, %edi
	cmovlel	%edi, %ebp
	addl	$1, %ebp
	cmpl	%ebp, %ebx
	cmovgel	%ebx, %ebp
	movl	%ebp, -128(%rsp)        # 4-byte Spill
	testl	%ebx, %ebx
	jle	.LBB162_72
# BB#36:                                # %for f8.s0.v10.v104.preheader
	movl	$2, %eax
	movq	32(%rsp), %r8           # 8-byte Reload
	subl	%r8d, %eax
	movq	8(%rsp), %rbp           # 8-byte Reload
	leal	(%rbp,%rbp), %ecx
	cltd
	idivl	%ecx
	movl	$2, %esi
	movl	%ebp, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	negl	%ecx
	andl	%eax, %ecx
	orl	%edi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	(%r8,%rbp), %ecx
	leal	-1(%rbp,%rbp), %r11d
	subl	%eax, %r11d
	cmpl	%eax, %ebp
	cmovgl	%eax, %r11d
	addl	%r8d, %r11d
	leal	-1(%r8,%rbp), %r14d
	cmpl	%r11d, %r14d
	cmovlel	%r14d, %r11d
	cmpl	%r8d, %r11d
	cmovll	%r8d, %r11d
	cmpl	$3, %ecx
	cmovgel	%esi, %r14d
	cmpl	%r8d, %r14d
	cmovll	%r8d, %r14d
	cmpl	$3, %ecx
	cmovll	%r11d, %r14d
	movl	$2, %eax
	movq	24(%rsp), %rbx          # 8-byte Reload
	subl	%ebx, %eax
	movq	16(%rsp), %rdi          # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	leal	-1(%rdi,%rdi), %ebp
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %r10d
	negl	%ecx
	andl	%eax, %ecx
	orl	%r10d, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	(%rbx,%rdi), %ecx
	subl	%eax, %ebp
	cmpl	%eax, %edi
	leal	-1(%rbx,%rdi), %edx
	cmovgl	%eax, %ebp
	addl	%ebx, %ebp
	cmpl	%ebp, %edx
	cmovlel	%edx, %ebp
	cmpl	%ebx, %ebp
	cmovll	%ebx, %ebp
	cmpl	$3, %ecx
	cmovll	%edx, %esi
	cmpl	%ebx, %esi
	cmovll	%ebx, %esi
	cmpl	$3, %ecx
	cmovll	%ebp, %esi
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	3(%rax), %eax
	movl	%eax, 1840(%rsp)        # 4-byte Spill
	movq	1744(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %r15d
	andl	$63, %r15d
	imull	%r15d, %eax
	movq	%rax, 552(%rsp)         # 8-byte Spill
	movl	%r10d, %eax
	andl	$1, %eax
	movl	%eax, 176(%rsp)         # 4-byte Spill
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%rax), %eax
	vmovd	%eax, %xmm9
	cmpl	$2, %ebx
	cmovgl	%ebp, %esi
	movq	560(%rsp), %rax         # 8-byte Reload
	movq	272(%rsp), %r12         # 8-byte Reload
	imull	%r12d, %eax
	addl	%ebx, %eax
	vmovd	%eax, %xmm8
	vmovd	%eax, %xmm1
	movq	-16(%rsp), %rcx         # 8-byte Reload
	leal	-1(%rcx), %eax
	vmovd	%eax, %xmm12
	cmpl	$2, %r8d
	cmovgl	%r11d, %r14d
	movq	40(%rsp), %r11          # 8-byte Reload
	movl	%r11d, %edx
	movq	-8(%rsp), %rax          # 8-byte Reload
	imull	%eax, %edx
	leal	-1(%r12), %eax
	vmovd	%eax, %xmm6
	vmovd	%esi, %xmm10
	vmovd	%esi, %xmm11
	addl	%r8d, %edx
	leal	1(%rcx), %eax
	vmovd	%eax, %xmm2
	leal	1(%r12), %eax
	vmovd	%eax, %xmm0
	leal	2(%rcx), %eax
	vmovd	%eax, %xmm13
	leal	2(%r12), %eax
	vmovd	%eax, %xmm14
	leal	-2(%rcx), %eax
	vmovd	%eax, %xmm5
	movl	96(%rsp), %esi          # 4-byte Reload
	andl	$-32, %esi
	movl	%esi, %eax
	shll	$5, %eax
	addl	$64, %esi
	movl	%r10d, %ecx
	subl	64(%rsp), %ecx          # 4-byte Folded Reload
	leal	8(%rcx), %edi
	imull	%esi, %edi
	leal	(%rax,%rax,2), %ebp
	addl	%ebp, %edi
	movq	%rdi, 536(%rsp)         # 8-byte Spill
	movslq	%edx, %r9
	movslq	%r10d, %rdx
	leal	10(%rcx), %r8d
	imull	%esi, %r8d
	leal	6(%rcx), %edi
	imull	%esi, %edi
	leal	7(%rcx), %eax
	imull	%esi, %eax
	addl	$9, %ecx
	imull	%esi, %ecx
	movq	%r11, %rbx
	imulq	%rdx, %rbx
	subq	%r9, %rbx
	addl	%ebp, %r8d
	movq	%r8, 528(%rsp)          # 8-byte Spill
	addl	%ebp, %edi
	movq	%rdi, 520(%rsp)         # 8-byte Spill
	addl	%ebp, %eax
	movq	%rax, 512(%rsp)         # 8-byte Spill
	addl	%ebp, %ecx
	movq	%rcx, 544(%rsp)         # 8-byte Spill
	leaq	2(%rdx), %rbp
	imulq	%r11, %rbp
	subq	%r9, %rbp
	leal	2(%r10), %eax
	andl	$63, %eax
	movl	1840(%rsp), %edi        # 4-byte Reload
	imull	%edi, %eax
	movq	%rax, 504(%rsp)         # 8-byte Spill
	leaq	-2(%rdx), %rax
	imulq	%r11, %rax
	leal	62(%r10), %ecx
	andl	$63, %ecx
	imull	%edi, %ecx
	movq	%rcx, 496(%rsp)         # 8-byte Spill
	leaq	-1(%rdx), %rcx
	imulq	%r11, %rcx
	leal	63(%r10), %esi
	andl	$63, %esi
	imull	%edi, %esi
	movq	%rsi, 480(%rsp)         # 8-byte Spill
	subq	%r9, %rax
	addq	$1, %rdx
	imulq	%r11, %rdx
	subq	%r9, %rcx
	subq	%r9, %rdx
	leal	1(%r10), %esi
	andl	$63, %esi
	imull	%edi, %esi
	movq	%rsi, 464(%rsp)         # 8-byte Spill
	movslq	%r14d, %rsi
	addq	%rsi, %rbx
	addq	%rsi, %rbp
	addq	%rsi, %rax
	addq	%rsi, %rcx
	addq	%rsi, %rdx
	movq	48(%rsp), %rsi          # 8-byte Reload
	leaq	32(%rsi), %rsi
	imulq	%rsi, %r15
	movq	128(%rsp), %rdi         # 8-byte Reload
	movq	%rdi, %rsi
	sarq	$63, %rsi
	andq	%rdi, %rsi
	subq	%rsi, %r15
	movq	%r15, 568(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm9, %xmm4
	vmovdqa	%xmm4, 448(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 432(%rsp)        # 16-byte Spill
	movq	560(%rsp), %rsi         # 8-byte Reload
	vmovd	%esi, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 1600(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm3
	vmovdqa	%xmm3, 1264(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI162_0(%rip), %xmm1 # xmm1 = [0,4294967294,4294967292,4294967290]
	vpbroadcastd	%xmm6, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm6
	vmovdqa	%xmm6, 416(%rsp)        # 16-byte Spill
	vmovd	%r12d, %xmm6
	vpbroadcastd	%xmm6, %xmm7
	vmovdqa	%xmm7, 400(%rsp)        # 16-byte Spill
	movq	136(%rsp), %rsi         # 8-byte Reload
	vmovd	%esi, %xmm6
	vbroadcastss	%xmm6, %xmm6
	vmovaps	%xmm6, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm11, %xmm6
	vmovaps	%xmm6, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm2
	vmovdqa	%xmm2, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	vpsubd	%xmm8, %xmm10, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 1584(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm13, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	movq	-16(%rsp), %rdi         # 8-byte Reload
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm5, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 288(%rsp)        # 16-byte Spill
	leal	-2(%r12), %esi
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	leal	-3(%rdi), %esi
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	leal	-3(%r12), %esi
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm0, 224(%rsp)        # 16-byte Spill
	vpaddd	%xmm1, %xmm3, %xmm0
	vmovdqa	%xmm0, 208(%rsp)        # 16-byte Spill
	vpaddd	%xmm1, %xmm7, %xmm0
	vmovdqa	%xmm0, 192(%rsp)        # 16-byte Spill
	vmovdqa	%xmm7, %xmm11
	vmovss	.LCPI162_1(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	60(%rsp), %xmm3         # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm0, %xmm0
	vmovss	120(%rsp), %xmm1        # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	112(%rsp), %xmm2        # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm1, %xmm1
	vmulss	%xmm1, %xmm0, %xmm1
	vmulss	%xmm2, %xmm0, %xmm0
	vmovss	4(%rsp), %xmm2          # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vdivss	%xmm1, %xmm2, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 1824(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1808(%rsp)       # 16-byte Spill
	xorl	%r11d, %r11d
	movq	80(%rsp), %rsi          # 8-byte Reload
	vbroadcastss	(%rsi,%rbx,4), %xmm0
	vmovaps	%xmm0, 800(%rsp)        # 16-byte Spill
	vbroadcastss	(%rsi,%rbp,4), %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1552(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rcx,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rdx,4), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vpabsd	%xmm4, %xmm0
	vmovdqa	%xmm0, 1232(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm4, %xmm0
	vmovdqa	%xmm0, 1216(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI162_2(%rip), %xmm3 # xmm3 = [0,2,4,6]
	vbroadcastss	.LCPI162_3(%rip), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_4(%rip), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI162_5(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_6(%rip), %xmm0
	vmovaps	%xmm0, 144(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB162_37:                             # %for f8.s0.v10.v104
                                        # =>This Inner Loop Header: Depth=1
	movq	1280(%rsp), %r9         # 8-byte Reload
	leal	(%r9,%r11,8), %esi
	movl	%esi, 1184(%rsp)        # 4-byte Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	vmovups	8(%r13,%rax,4), %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	vmovups	24(%r13,%rax,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	movl	%esi, %ebx
	movq	272(%rsp), %rax         # 8-byte Reload
	subl	%eax, %ebx
	movq	%rbx, 992(%rsp)         # 8-byte Spill
	leal	1(%rbx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm3, %xmm8
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r10d
	movl	%r10d, 1008(%rsp)       # 4-byte Spill
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r12d
	movl	%r12d, 1172(%rsp)       # 4-byte Spill
	cltd
	idivl	%r12d
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r15d
	cltd
	idivl	%r15d
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r14d
	movl	%r14d, 1176(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1232(%rsp), %xmm9       # 16-byte Reload
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm6
	vmovdqa	%xmm6, 1024(%rsp)       # 16-byte Spill
	vmovdqa	208(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vmovdqa	416(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1248(%rsp), %xmm15      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm15, %xmm2
	vmovdqa	1216(%rsp), %xmm7       # 16-byte Reload
	vpsubd	%xmm0, %xmm7, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	vmovdqa	%xmm11, %xmm13
	vpaddd	%xmm13, %xmm0, %xmm0
	vmovdqa	1264(%rsp), %xmm10      # 16-byte Reload
	vpminsd	%xmm10, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	movq	%r13, %r8
	leal	1(%r9,%r11,8), %eax
	movl	%eax, 1160(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1600(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm5
	vpsubd	432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	movq	1696(%rsp), %r13        # 8-byte Reload
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	leal	-1(%rbx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	368(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm1, %xmm1
	vpxor	%xmm3, %xmm1, %xmm1
	vmovdqa	352(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm15, %xmm2
	vpsubd	%xmm0, %xmm7, %xmm4
	vblendvps	%xmm2, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm10, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	leal	-1(%r9,%r11,8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm8, %xmm2, %xmm2
	vpminsd	%xmm10, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm5, %xmm0, %xmm0
	vpaddd	1584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rsi
	sarq	$32, %rax
	vmovss	(%r13,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1792(%rsp)       # 16-byte Spill
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, 1616(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 1136(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, 1120(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1104(%rsp)        # 4-byte Spill
	leal	2(%rbx), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r10d
	movl	%edx, 1088(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, 1056(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, 1040(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	movq	1760(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r8,%rax,4), %xmm2
	vmovaps	%xmm2, 1776(%rsp)       # 16-byte Spill
	vmovups	32(%r8,%rax,4), %xmm0
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	vmovups	(%r8,%rax,4), %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	leal	3(%rbx), %eax
	vmovd	%eax, %xmm0
	movq	536(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	movslq	%eax, %rsi
	movq	1752(%rsp), %rbp        # 8-byte Reload
	vmovups	24608(%rbp,%rsi,4), %xmm1
	vmovaps	%xmm1, 1520(%rsp)       # 16-byte Spill
	vmovups	24624(%rbp,%rsi,4), %xmm1
	vmovaps	%xmm1, 1376(%rsp)       # 16-byte Spill
	movq	504(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	movq	%rax, 1424(%rsp)        # 8-byte Spill
	vmovups	8(%r8,%rax,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	24(%r8,%rax,4), %xmm1
	vmovaps	%xmm1, 1648(%rsp)       # 16-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	vmovups	24608(%rbp,%rax,4), %xmm1
	vmovaps	%xmm1, 1632(%rsp)       # 16-byte Spill
	vmovups	24624(%rbp,%rax,4), %xmm1
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	movslq	%eax, %rcx
	movq	%rcx, 1456(%rsp)        # 8-byte Spill
	vmovups	8(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	movslq	%eax, %rbx
	movq	%rbx, 1472(%rsp)        # 8-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	vmovups	24(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 1680(%rsp)       # 16-byte Spill
	vmovups	24608(%rbp,%rbx,4), %xmm1
	vmovaps	%xmm1, 1664(%rsp)       # 16-byte Spill
	vmovups	24624(%rbp,%rbx,4), %xmm14
	vmovaps	%xmm14, 1072(%rsp)      # 16-byte Spill
	vmovups	24616(%rbp,%rsi,4), %xmm11
	vmovaps	%xmm11, 1760(%rsp)      # 16-byte Spill
	vmovups	24632(%rbp,%rsi,4), %xmm1
	vmovaps	%xmm1, 1296(%rsp)       # 16-byte Spill
	vmovups	24600(%rbp,%rsi,4), %xmm1
	vmovaps	%xmm1, 784(%rsp)        # 16-byte Spill
	movq	%rbp, %r8
	idivl	%r10d
	movl	1160(%rsp), %r10d       # 4-byte Reload
	movl	%edx, %ebp
	vmovd	%xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	vmovd	1136(%rsp), %xmm4       # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 1616(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$2, 1120(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpinsrd	$3, 1104(%rsp), %xmm4, %xmm4 # 4-byte Folded Reload
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm4
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%r15d, %r12d
	movl	%edx, %ecx
	vpcmpgtd	%xmm4, %xmm15, %xmm5
	vpsubd	%xmm4, %xmm7, %xmm3
	vblendvps	%xmm5, %xmm4, %xmm3, %xmm3
	vmovdqa	304(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm4, %xmm4
	vpcmpeqd	%xmm12, %xmm12, %xmm12
	vpxor	%xmm12, %xmm4, %xmm4
	vmovdqa	192(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpaddd	%xmm13, %xmm3, %xmm3
	vpminsd	%xmm10, %xmm3, %xmm3
	vpmaxsd	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm8, %xmm6, %xmm5
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm3, %xmm5, %xmm3
	vpextrd	$3, %xmm0, %eax
	vmovd	1056(%rsp), %xmm0       # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 1088(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$2, 1040(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	vpinsrd	$3, %edi, %xmm0, %xmm0
	cltd
	idivl	%r14d
	vpsrad	$31, %xmm0, %xmm4
	vpand	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm0, %xmm4, %xmm0
	vpcmpgtd	%xmm0, %xmm15, %xmm4
	vpsubd	%xmm0, %xmm7, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovdqa	288(%rsp), %xmm4        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm4, %xmm4
	vpxor	%xmm12, %xmm4, %xmm4
	vpcmpeqd	%xmm12, %xmm12, %xmm12
	vmovdqa	256(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm5, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm10, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	leal	2(%r9,%r11,8), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpminsd	%xmm10, %xmm5, %xmm5
	vpmaxsd	%xmm13, %xmm5, %xmm5
	vblendvps	%xmm4, %xmm0, %xmm5, %xmm0
	vmovd	%ebx, %xmm4
	vpinsrd	$1, %ebp, %xmm4, %xmm4
	vpinsrd	$2, %ecx, %xmm4, %xmm4
	vpinsrd	$3, %edx, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vmovaps	%xmm2, %xmm9
	vpaddd	%xmm4, %xmm5, %xmm4
	vpcmpgtd	%xmm4, %xmm15, %xmm5
	vpsubd	%xmm4, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm4, %xmm7, %xmm4
	vmovdqa	240(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm5, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vmovdqa	224(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	%xmm6, %xmm7, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm13, %xmm4, %xmm4
	vpminsd	%xmm10, %xmm4, %xmm4
	vpmaxsd	%xmm13, %xmm4, %xmm4
	leal	3(%r9,%r11,8), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm8, %xmm7, %xmm7
	vpminsd	%xmm10, %xmm7, %xmm7
	vpmaxsd	%xmm13, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm4, %xmm7, %xmm12
	vmovdqa	1600(%rsp), %xmm15      # 16-byte Reload
	vpmulld	%xmm15, %xmm3, %xmm3
	vmovdqa	1584(%rsp), %xmm2       # 16-byte Reload
	vpaddd	%xmm3, %xmm2, %xmm3
	vpextrq	$1, %xmm3, %r15
	vpmulld	%xmm15, %xmm0, %xmm0
	vmovq	%xmm3, %rsi
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm2, %xmm4
	vpextrq	$1, %xmm0, %r14
	vmovq	%xmm0, %rdx
	vshufps	$221, %xmm11, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm11[1,3]
	vmovaps	1808(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm0, %xmm0
	vmovaps	1824(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	800(%rsp), %xmm2        # 16-byte Reload
	vmulps	1792(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	1840(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm0, %xmm0
	vpxor	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	1328(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[1,3],xmm9[1,3]
	vsubps	%xmm3, %xmm0, %xmm9
	vmovaps	1664(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm14[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1392(%rsp), %xmm5       # 16-byte Reload
	vmulps	1552(%rsp), %xmm5, %xmm3 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm3, %xmm0
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	1488(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1680(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm3, %xmm0, %xmm11
	vmovaps	1520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm2, %xmm5, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	1632(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1536(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	1568(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm1, %xmm3, %xmm3
	vmaxps	%xmm8, %xmm3, %xmm3
	vmovaps	1504(%rsp), %xmm5       # 16-byte Reload
	vshufps	$221, 1648(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vsubps	%xmm5, %xmm3, %xmm13
	vpmulld	%xmm15, %xmm12, %xmm3
	vmovaps	1408(%rsp), %xmm14      # 16-byte Reload
	vpaddd	%xmm3, %xmm4, %xmm3
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm3, %rcx
	movslq	%ecx, %rdi
	sarq	$32, %rcx
	vmovss	(%r13,%rdi,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%r13,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	sarq	$32, %rax
	vinsertps	$48, (%r13,%rax,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	%rsi, %rdi
	sarq	$32, %rdi
	movq	%r15, %rcx
	sarq	$32, %rcx
	movq	%rdx, %rbx
	sarq	$32, %rbx
	movq	%r14, %rax
	sarq	$32, %rax
	movl	%r10d, %ebp
	andl	$1, %ebp
	testl	%ebp, %ebp
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovaps	1312(%rsp), %xmm15      # 16-byte Reload
	vshufps	$221, %xmm15, %xmm14, %xmm3 # xmm3 = xmm14[1,3],xmm15[1,3]
	vsubps	%xmm3, %xmm0, %xmm0
	jne	.LBB162_38
# BB#43:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm5, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 688(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 704(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, 720(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 816(%rsp)        # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	movl	176(%rsp), %r9d         # 4-byte Reload
	vmovaps	1344(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm1, %xmm5
	movl	%r12d, %ebp
	vmovaps	1776(%rsp), %xmm11      # 16-byte Reload
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	1392(%rsp), %xmm13      # 16-byte Reload
	vmovaps	1760(%rsp), %xmm10      # 16-byte Reload
	vmovaps	976(%rsp), %xmm12       # 16-byte Reload
	jmp	.LBB162_44
	.align	16, 0x90
.LBB162_38:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm3, 816(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, %xmm4
	vmovaps	%xmm4, 672(%rsp)        # 16-byte Spill
	vmulps	%xmm5, %xmm2, %xmm0
	vmovaps	%xmm5, 592(%rsp)        # 16-byte Spill
	vmovaps	1760(%rsp), %xmm10      # 16-byte Reload
	vshufps	$221, 1296(%rsp), %xmm10, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm10[1,3],mem[1,3]
	vsubps	%xmm6, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	vmovaps	976(%rsp), %xmm12       # 16-byte Reload
	vmovaps	%xmm9, %xmm5
	vmovaps	1776(%rsp), %xmm9       # 16-byte Reload
	vshufps	$221, %xmm12, %xmm9, %xmm3 # xmm3 = xmm9[1,3],xmm12[1,3]
	vminps	%xmm1, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vaddps	%xmm13, %xmm4, %xmm3
	vmovaps	%xmm13, 688(%rsp)       # 16-byte Spill
	vaddps	%xmm11, %xmm3, %xmm3
	vmovaps	%xmm11, 704(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm3, %xmm0
	vaddps	%xmm0, %xmm5, %xmm0
	vmovaps	%xmm5, 720(%rsp)        # 16-byte Spill
	vmulps	160(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	movl	176(%rsp), %r9d         # 4-byte Reload
	vmovaps	1344(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm1, %xmm5
	movl	%r12d, %ebp
	vmovaps	%xmm9, %xmm11
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	1392(%rsp), %xmm13      # 16-byte Reload
.LBB162_44:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	1376(%rsp), %xmm1       # 16-byte Reload
	vmovaps	%xmm10, 1760(%rsp)      # 16-byte Spill
	vmovaps	%xmm11, 1776(%rsp)      # 16-byte Spill
	movslq	%esi, %rsi
	vmovss	(%r13,%rsi,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r15d, %rsi
	vinsertps	$32, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm3, 1136(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm15, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm15[0,2]
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	movslq	%edx, %rcx
	movslq	%r14d, %rdx
	vmovss	(%r13,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r13,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, 1312(%rsp)       # 16-byte Spill
	movq	%r13, %r12
	vmovaps	1520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmulps	%xmm2, %xmm3, %xmm1
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 1520(%rsp)       # 16-byte Spill
	vmovaps	1504(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vmulps	1568(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	1632(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmovaps	1488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vmulps	1552(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovaps	1664(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1072(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmulps	%xmm2, %xmm4, %xmm0
	vshufps	$136, 1296(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm10[0,2],mem[0,2]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm15
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	cltq
	vmovups	24608(%r8,%rax,4), %xmm3
	vmovaps	%xmm3, 960(%rsp)        # 16-byte Spill
	movq	544(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r11,8), %ecx
	movslq	%ecx, %rcx
	vmovups	24608(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovups	24624(%r8,%rcx,4), %xmm4
	vmovaps	%xmm4, 1488(%rsp)       # 16-byte Spill
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm2, %xmm13, %xmm0
	vshufps	$221, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm4[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm14
	vmovups	24624(%r8,%rax,4), %xmm1
	vmovaps	%xmm1, 944(%rsp)        # 16-byte Spill
	vmulps	%xmm8, %xmm13, %xmm0
	vshufps	$221, %xmm1, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm1[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm10
	vmovups	24616(%r8,%rcx,4), %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rcx,4), %xmm1
	vmovaps	%xmm1, 768(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	1792(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm2, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm2
	vmovups	24616(%r8,%rax,4), %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vmovups	24600(%r8,%rax,4), %xmm1
	vmovaps	%xmm1, 752(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm8, %xmm4, %xmm1
	vmulps	%xmm0, %xmm1, %xmm6
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm5, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmovaps	1376(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm5, %xmm0, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm0
	vmovaps	%xmm0, 1120(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm12, %xmm11, %xmm0 # xmm0 = xmm11[0,2],xmm12[0,2]
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vminps	%xmm5, %xmm15, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	movq	480(%rsp), %rdx         # 8-byte Reload
	leal	(%rdx,%r11,8), %edx
	movslq	%edx, %rdx
	movq	464(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r11,8), %esi
	movslq	%esi, %rsi
	vminps	%xmm5, %xmm14, %xmm1
	vminps	%xmm5, %xmm10, %xmm4
	vminps	%xmm5, %xmm2, %xmm14
	vminps	%xmm5, %xmm6, %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vmovaps	1520(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm5, %xmm0, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm10
	testl	%r9d, %r9d
	vmovups	24632(%r8,%rax,4), %xmm2
	vmovaps	%xmm2, 928(%rsp)        # 16-byte Spill
	vmovups	24632(%r8,%rcx,4), %xmm2
	vmovaps	%xmm2, 1520(%rsp)       # 16-byte Spill
	movq	1736(%rsp), %r13        # 8-byte Reload
	vmovups	8(%r13,%rsi,4), %xmm8
	vmovups	24(%r13,%rsi,4), %xmm13
	vmovups	16(%r13,%rsi,4), %xmm2
	vmovups	32(%r13,%rsi,4), %xmm3
	vmovaps	%xmm3, 1536(%rsp)       # 16-byte Spill
	vmovups	(%r13,%rsi,4), %xmm6
	vmovups	8(%r13,%rdx,4), %xmm0
	vmovups	24(%r13,%rdx,4), %xmm12
	vmovups	16(%r13,%rdx,4), %xmm7
	vmovups	32(%r13,%rdx,4), %xmm3
	vmovaps	%xmm3, 912(%rsp)        # 16-byte Spill
	vmovups	(%r13,%rdx,4), %xmm11
	movq	1424(%rsp), %rcx        # 8-byte Reload
	vmovups	16(%r13,%rcx,4), %xmm3
	vmovaps	%xmm3, 976(%rsp)        # 16-byte Spill
	movq	1440(%rsp), %rcx        # 8-byte Reload
	vmovups	24616(%r8,%rcx,4), %xmm5
	movq	1456(%rsp), %rcx        # 8-byte Reload
	vmovups	16(%r13,%rcx,4), %xmm3
	vmovaps	%xmm3, 1040(%rsp)       # 16-byte Spill
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r8,%rax,4), %xmm15
	vmovaps	%xmm15, 1056(%rsp)      # 16-byte Spill
	movq	%r8, %r15
	je	.LBB162_46
# BB#45:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
	vmovaps	%xmm3, 1616(%rsp)       # 16-byte Spill
.LBB162_46:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vshufps	$221, %xmm13, %xmm8, %xmm3 # xmm3 = xmm8[1,3],xmm13[1,3]
	vmovaps	%xmm3, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 1376(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1392(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm1, %xmm1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vmovaps	1104(%rsp), %xmm1       # 16-byte Reload
	vsubps	1648(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vshufps	$221, %xmm12, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm12[1,3]
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 896(%rsp)       # 16-byte Spill
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm4, %xmm8
	vmovaps	1120(%rsp), %xmm0       # 16-byte Reload
	vsubps	1632(%rsp), %xmm0, %xmm4 # 16-byte Folded Reload
	vmovaps	%xmm6, 736(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm2, %xmm6, %xmm15 # xmm15 = xmm6[1,3],xmm2[1,3]
	vmovaps	%xmm2, 1648(%rsp)       # 16-byte Spill
	vmaxps	%xmm9, %xmm14, %xmm14
	vmovaps	1296(%rsp), %xmm0       # 16-byte Reload
	vsubps	1088(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vshufps	$221, %xmm7, %xmm11, %xmm13 # xmm13 = xmm11[1,3],xmm7[1,3]
	vmovaps	1072(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm9, %xmm0, %xmm12
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm10, %xmm1
	movl	%r10d, %eax
	movq	1744(%rsp), %r14        # 8-byte Reload
	orl	%r14d, %eax
	testb	$1, %al
	movl	1184(%rsp), %r8d        # 4-byte Reload
	je	.LBB162_47
# BB#48:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm7, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 624(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm3, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 1296(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)      # 16-byte Spill
	vmovaps	1840(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB162_49
	.align	16, 0x90
.LBB162_47:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm7, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)      # 16-byte Spill
	movq	1424(%rsp), %rax        # 8-byte Reload
	vmovaps	976(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 32(%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vmovaps	%xmm1, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, %xmm10
	vmovaps	%xmm10, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, %xmm2
	vmovaps	%xmm2, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, 1088(%rsp)       # 16-byte Spill
	vmovaps	1312(%rsp), %xmm8       # 16-byte Reload
	vmulps	1568(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	movq	1440(%rsp), %rax        # 8-byte Reload
	vmovaps	%xmm4, %xmm7
	vmovaps	%xmm7, 640(%rsp)        # 16-byte Spill
	vshufps	$136, 24632(%r15,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,2],mem[0,2]
	vmovaps	%xmm5, 1296(%rsp)       # 16-byte Spill
	vmovaps	1808(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm4, %xmm4
	vmovaps	1824(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vmovaps	1840(%rsp), %xmm11      # 16-byte Reload
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	1616(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmulps	1552(%rsp), %xmm8, %xmm3 # 16-byte Folded Reload
	vmovaps	1088(%rsp), %xmm8       # 16-byte Reload
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovaps	1056(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 24632(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm6, %xmm4, %xmm4
	vmulps	%xmm4, %xmm0, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovaps	1040(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 32(%r13,%rax,4), %xmm0, %xmm4 # xmm4 = xmm0[0,2],mem[0,2]
	vminps	%xmm11, %xmm3, %xmm3
	vmaxps	%xmm9, %xmm3, %xmm3
	vsubps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm7, %xmm1, %xmm4
	vaddps	%xmm4, %xmm2, %xmm4
	vaddps	%xmm3, %xmm4, %xmm3
	vaddps	%xmm3, %xmm10, %xmm3
	vaddps	%xmm3, %xmm5, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
.LBB162_49:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	movq	992(%rsp), %rax         # 8-byte Reload
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	1680(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1664(%rsp), %xmm5       # 16-byte Reload
	vmovaps	880(%rsp), %xmm0        # 16-byte Reload
	vmovaps	864(%rsp), %xmm2        # 16-byte Reload
	vmovaps	848(%rsp), %xmm3        # 16-byte Reload
	vsubps	%xmm0, %xmm3, %xmm10
	vsubps	%xmm2, %xmm8, %xmm6
	vsubps	%xmm15, %xmm14, %xmm4
	vsubps	%xmm13, %xmm12, %xmm7
	testl	%r10d, %r9d
	jne	.LBB162_50
# BB#51:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm4, 880(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1088(%rsp)      # 16-byte Spill
	vmovaps	1824(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1808(%rsp), %xmm7       # 16-byte Reload
	vmovaps	1344(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1024(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1648(%rsp), %xmm3       # 16-byte Reload
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	928(%rsp), %xmm13       # 16-byte Reload
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vmovaps	896(%rsp), %xmm4        # 16-byte Reload
	vmovaps	1632(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm6, %xmm15
	vmovaps	912(%rsp), %xmm5        # 16-byte Reload
	jmp	.LBB162_52
	.align	16, 0x90
.LBB162_50:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm7, 864(%rsp)        # 16-byte Spill
	vmovaps	592(%rsp), %xmm2        # 16-byte Reload
	vmulps	1360(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vshufps	$221, 1520(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1808(%rsp), %xmm12      # 16-byte Reload
	vsubps	%xmm12, %xmm1, %xmm1
	vmovaps	1824(%rsp), %xmm14      # 16-byte Reload
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1648(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, 1536(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[1,3],mem[1,3]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	1344(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm1
	vmovaps	928(%rsp), %xmm13       # 16-byte Reload
	vshufps	$221, %xmm13, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm13[1,3]
	vsubps	%xmm12, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1632(%rsp), %xmm8       # 16-byte Reload
	vmovaps	912(%rsp), %xmm5        # 16-byte Reload
	vshufps	$221, %xmm5, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm5[1,3]
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm7, %xmm6, %xmm2
	vmovaps	%xmm12, %xmm7
	vmovaps	%xmm15, %xmm12
	vmovaps	%xmm6, 1072(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm4, %xmm1
	vmovaps	%xmm4, 880(%rsp)        # 16-byte Spill
	vaddps	%xmm1, %xmm10, %xmm1
	vmovaps	%xmm10, 1088(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vmovaps	1024(%rsp), %xmm15      # 16-byte Reload
	vmovaps	960(%rsp), %xmm1        # 16-byte Reload
	vmovaps	944(%rsp), %xmm2        # 16-byte Reload
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vmovaps	896(%rsp), %xmm4        # 16-byte Reload
.LBB162_52:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vshufps	$136, %xmm4, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm4[0,2]
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	1136(%rsp), %xmm10      # 16-byte Reload
	vmovaps	%xmm12, %xmm6
	vmulps	%xmm6, %xmm10, %xmm2
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm12
	vmovaps	1312(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm6, %xmm4, %xmm0
	vmovaps	1664(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, %xmm5, %xmm8, %xmm1 # xmm1 = xmm8[0,2],xmm5[0,2]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm6
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmovaps	1360(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm10, %xmm2
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm11, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm2
	vmulps	%xmm5, %xmm4, %xmm0
	vmovaps	%xmm5, %xmm13
	vmovaps	1680(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1520(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm14, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1536(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[0,2],mem[0,2]
	vminps	%xmm11, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm10
	andl	$1, %r10d
	je	.LBB162_53
# BB#54:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm6, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1536(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1632(%rsp)       # 16-byte Spill
	vmovaps	1616(%rsp), %xmm12      # 16-byte Reload
	jmp	.LBB162_55
	.align	16, 0x90
.LBB162_53:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm8, 1632(%rsp)       # 16-byte Spill
	vaddps	%xmm10, %xmm6, %xmm0
	vmovaps	%xmm6, 1520(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm12, %xmm0
	vmovaps	%xmm12, 1536(%rsp)      # 16-byte Spill
	vmulps	144(%rsp), %xmm0, %xmm12 # 16-byte Folded Reload
.LBB162_55:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	movl	1008(%rsp), %ecx        # 4-byte Reload
	vmovaps	%xmm2, %xmm8
	vmovaps	%xmm3, 1648(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1840(%rsp)      # 16-byte Spill
	testl	%r9d, %r9d
	jne	.LBB162_57
# BB#56:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	1616(%rsp), %xmm12      # 16-byte Reload
.LBB162_57:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	addl	$-2, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI162_2(%rip), %xmm4 # xmm4 = [0,2,4,6]
	vpaddd	%xmm4, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	1172(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	movl	%r8d, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ebp
	andl	$1, %ebx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	1176(%rsp)              # 4-byte Folded Reload
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	1232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	336(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm15, %xmm1, %xmm1
	vpxor	.LCPI162_9(%rip), %xmm1, %xmm1
	vmovdqa	320(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm15, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1248(%rsp), %xmm2       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm2, %xmm2
	vmovdqa	1216(%rsp), %xmm5       # 16-byte Reload
	vpsubd	%xmm0, %xmm5, %xmm5
	vblendvps	%xmm2, %xmm0, %xmm5, %xmm0
	vmovdqa	400(%rsp), %xmm6        # 16-byte Reload
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	1264(%rsp), %xmm5       # 16-byte Reload
	vpminsd	%xmm5, %xmm0, %xmm0
	vpmaxsd	%xmm6, %xmm0, %xmm0
	movq	1280(%rsp), %rax        # 8-byte Reload
	leal	-2(%rax,%r11,8), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	%xmm4, %xmm3
	vpminsd	%xmm5, %xmm2, %xmm2
	vpmaxsd	%xmm6, %xmm2, %xmm2
	vmovdqa	%xmm6, %xmm11
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	1600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r12,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r12,%rax,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	testl	%ebx, %ebx
	jne	.LBB162_58
# BB#59:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	.LBB162_60
	.align	16, 0x90
.LBB162_58:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmulps	800(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
	vmovaps	784(%rsp), %xmm4        # 16-byte Reload
	vshufps	$136, 1760(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm4[0,2],mem[0,2]
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm14, %xmm5
	vmulps	%xmm5, %xmm2, %xmm2
	vminps	1840(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	vmovaps	608(%rsp), %xmm2        # 16-byte Reload
	vaddps	656(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	640(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm2, %xmm0
	vmulps	160(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
.LBB162_60:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	movq	%r15, %rcx
	vmovaps	1296(%rsp), %xmm4       # 16-byte Reload
	testl	%r9d, %r9d
	je	.LBB162_62
# BB#61:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
.LBB162_62:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	movl	%r8d, %eax
	orl	%r14d, %eax
	testb	$1, %al
	jne	.LBB162_64
# BB#63:                                #   in Loop: Header=BB162_37 Depth=1
	movq	1424(%rsp), %rax        # 8-byte Reload
	vmovups	(%r13,%rax,4), %xmm0
	vshufps	$221, 976(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movq	1440(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%rcx,%rax,4), %xmm2
	vshufps	$221, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm4[1,3]
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmulps	1568(%rsp), %xmm6, %xmm5 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmovaps	1840(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm4, %xmm2, %xmm2
	vmaxps	%xmm9, %xmm2, %xmm2
	vsubps	%xmm0, %xmm2, %xmm0
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovups	(%r13,%rax,4), %xmm2
	vshufps	$221, 1040(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%rcx,%rax,4), %xmm5
	vshufps	$221, 1056(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmulps	1552(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm7, %xmm5, %xmm5
	vmulps	%xmm5, %xmm14, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vminps	%xmm4, %xmm5, %xmm5
	vmaxps	%xmm9, %xmm5, %xmm5
	vsubps	%xmm2, %xmm5, %xmm2
	vaddps	720(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm2, %xmm0, %xmm0
	vaddps	704(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	688(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
.LBB162_64:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	testl	%r8d, %r9d
	je	.LBB162_66
# BB#65:                                #   in Loop: Header=BB162_37 Depth=1
	vmovaps	752(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmovaps	768(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1680(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmulps	1344(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm0, %xmm14, %xmm0
	vmulps	%xmm0, %xmm5, %xmm0
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm13, %xmm1, %xmm1
	vmulps	%xmm2, %xmm14, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	1120(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1632(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vmovaps	1840(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm2, %xmm0, %xmm0
	vmovaps	736(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vaddps	1536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	1520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm8, %xmm0
	vaddps	%xmm0, %xmm10, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
.LBB162_66:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	816(%rsp), %xmm1        # 16-byte Reload
	vmovaps	1408(%rsp), %xmm4       # 16-byte Reload
	movl	%r8d, %eax
	andl	$1, %eax
	je	.LBB162_67
# BB#68:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm2, %xmm0
	jmp	.LBB162_69
	.align	16, 0x90
.LBB162_67:                             #   in Loop: Header=BB162_37 Depth=1
	vmovaps	1072(%rsp), %xmm0       # 16-byte Reload
	vaddps	1088(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	864(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	144(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB162_69:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm7, 1808(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 1824(%rsp)      # 16-byte Spill
	movq	%rcx, 1752(%rsp)        # 8-byte Spill
	testl	%r9d, %r9d
	jne	.LBB162_71
# BB#70:                                # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vmovaps	%xmm2, %xmm0
.LBB162_71:                             # %for f8.s0.v10.v104
                                        #   in Loop: Header=BB162_37 Depth=1
	vaddps	%xmm0, %xmm4, %xmm0
	vaddps	%xmm12, %xmm1, %xmm1
	vmovaps	.LCPI162_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI162_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r8d, %rax
	movq	568(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addq	$1, %r11
	cmpl	576(%rsp), %r11d        # 4-byte Folded Reload
	jne	.LBB162_37
.LBB162_72:                             # %end for f8.s0.v10.v105
	movq	%r13, 1736(%rsp)        # 8-byte Spill
	movl	576(%rsp), %eax         # 4-byte Reload
	cmpl	-128(%rsp), %eax        # 4-byte Folded Reload
	movq	1752(%rsp), %rsi        # 8-byte Reload
	jge	.LBB162_98
# BB#73:                                # %for f8.s0.v10.v108.preheader
	movl	$2, %eax
	movq	32(%rsp), %r9           # 8-byte Reload
	subl	%r9d, %eax
	movq	8(%rsp), %rdi           # 8-byte Reload
	leal	(%rdi,%rdi), %ecx
	cltd
	idivl	%ecx
	movl	$2, %ebp
	movl	%edi, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	negl	%ecx
	andl	%eax, %ecx
	orl	%esi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	(%r9,%rdi), %edx
	leal	-1(%rdi,%rdi), %esi
	subl	%eax, %esi
	cmpl	%eax, %edi
	cmovgl	%eax, %esi
	addl	%r9d, %esi
	leal	-1(%r9,%rdi), %r11d
	cmpl	%esi, %r11d
	cmovlel	%r11d, %esi
	cmpl	%r9d, %esi
	cmovll	%r9d, %esi
	cmpl	$3, %edx
	cmovgel	%ebp, %r11d
	cmpl	%r9d, %r11d
	cmovll	%r9d, %r11d
	cmpl	$3, %edx
	cmovll	%esi, %r11d
	movl	$2, %eax
	movq	24(%rsp), %rbx          # 8-byte Reload
	subl	%ebx, %eax
	movq	16(%rsp), %rcx          # 8-byte Reload
	leal	(%rcx,%rcx), %edi
	cltd
	idivl	%edi
	leal	-1(%rcx,%rcx), %eax
	movl	%ecx, %ebp
	sarl	$31, %ebp
	andnl	%edi, %ebp, %r8d
	negl	%edi
	andl	%ebp, %edi
	orl	%r8d, %edi
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%edi, %ebp
	addl	%edx, %ebp
	leal	(%rbx,%rcx), %edx
	subl	%ebp, %eax
	cmpl	%ebp, %ecx
	leal	-1(%rbx,%rcx), %edi
	cmovgl	%ebp, %eax
	addl	%ebx, %eax
	cmpl	%eax, %edi
	cmovlel	%edi, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	$3, %edx
	movl	$2, %ecx
	cmovll	%edi, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	$3, %edx
	cmovll	%eax, %ecx
	movq	1744(%rsp), %r10        # 8-byte Reload
	movl	%r10d, %edx
	andl	$1, %edx
	movl	%edx, 1172(%rsp)        # 4-byte Spill
	movl	%r10d, %edx
	andl	$63, %edx
	movq	%rdx, 1176(%rsp)        # 8-byte Spill
	movq	%rdx, %rdi
	cmpl	$2, %ebx
	cmovgl	%eax, %ecx
	movl	%ecx, 1840(%rsp)        # 4-byte Spill
	movq	560(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ecx
	movq	272(%rsp), %rax         # 8-byte Reload
	imull	%eax, %ecx
	addl	%ebx, %ecx
	movl	%ecx, 1808(%rsp)        # 4-byte Spill
	cmpl	$2, %r9d
	cmovgl	%esi, %r11d
	movq	40(%rsp), %rdx          # 8-byte Reload
	movl	%edx, %eax
	movq	-8(%rsp), %rsi          # 8-byte Reload
	imull	%esi, %eax
	addl	%r9d, %eax
	movl	96(%rsp), %ebx          # 4-byte Reload
	andl	$-32, %ebx
	movl	%ebx, %r9d
	shll	$5, %r9d
	movslq	%eax, %rsi
	movslq	%r10d, %rax
	movq	%rdx, %r15
	imulq	%rax, %r15
	movslq	%r11d, %rcx
	subq	%rsi, %r15
	addq	%rcx, %r15
	leaq	2(%rax), %rbp
	imulq	%rdx, %rbp
	subq	%rsi, %rbp
	leaq	-2(%rax), %r11
	imulq	%rdx, %r11
	addq	%rcx, %rbp
	subq	%rsi, %r11
	addq	%rcx, %r11
	leaq	-1(%rax), %r14
	imulq	%rdx, %r14
	subq	%rsi, %r14
	addq	%rcx, %r14
	addq	$1, %rax
	imulq	%rdx, %rax
	subq	%rsi, %rax
	addq	%rcx, %rax
	movq	48(%rsp), %rcx          # 8-byte Reload
	leaq	32(%rcx), %rcx
	movq	%rdi, %rsi
	imulq	%rcx, %rsi
	movq	128(%rsp), %rdx         # 8-byte Reload
	movq	%rdx, %rcx
	sarq	$63, %rcx
	andq	%rdx, %rcx
	subq	%rcx, %rsi
	movq	%rsi, 1160(%rsp)        # 8-byte Spill
	movl	584(%rsp), %edx         # 4-byte Reload
	notl	%edx
	movq	-24(%rsp), %rcx         # 8-byte Reload
	movl	-36(%rsp), %esi         # 4-byte Reload
	cmpl	%ecx, %esi
	movl	%ecx, %r8d
	cmovgel	%esi, %r8d
	movl	-40(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	movl	-44(%rsp), %esi         # 4-byte Reload
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	movq	-32(%rsp), %rsi         # 8-byte Reload
	cmpl	%esi, %r8d
	cmovll	%esi, %r8d
	xorl	%esi, %esi
	testl	%r8d, %r8d
	cmovsl	%esi, %r8d
	notl	%r8d
	cmpl	%r8d, %edx
	cmovgel	%edx, %r8d
	leal	9(%r10), %r13d
	movl	64(%rsp), %edx          # 4-byte Reload
	subl	%edx, %r13d
	addl	$64, %ebx
	imull	%ebx, %r13d
	leal	7(%r10), %ecx
	subl	%edx, %ecx
	imull	%ebx, %ecx
	leal	6(%r10), %r12d
	subl	%edx, %r12d
	imull	%ebx, %r12d
	movq	80(%rsp), %rsi          # 8-byte Reload
	vbroadcastss	(%rsi,%r15,4), %xmm13
	vmovaps	%xmm13, 960(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rbp,4), %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	leal	10(%r10), %r15d
	subl	%edx, %r15d
	imull	%ebx, %r15d
	leal	8(%r10), %ebp
	subl	%edx, %ebp
	imull	%ebx, %ebp
	vbroadcastss	(%rsi,%r11,4), %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	leal	1(%r10), %edi
	andl	$63, %edi
	movq	832(%rsp), %rdx         # 8-byte Reload
	leal	3(%rdx), %ebx
	imull	%ebx, %edi
	leal	63(%r10), %r11d
	andl	$63, %r11d
	imull	%ebx, %r11d
	vbroadcastss	(%rsi,%r14,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	vbroadcastss	(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	leal	62(%r10), %esi
	andl	$63, %esi
	imull	%ebx, %esi
	leal	2(%r10), %r10d
	andl	$63, %r10d
	imull	%ebx, %r10d
	movq	1176(%rsp), %rdx        # 8-byte Reload
	imull	%ebx, %edx
	leal	(%r9,%r9,2), %eax
	xorl	%ebx, %ebx
	addl	%eax, %r13d
	addl	%eax, %ecx
	addl	%eax, %r12d
	addl	%eax, %r15d
	addl	%eax, %ebp
	leal	(,%r8,8), %eax
	subl	%eax, %edi
	subl	%eax, %r13d
	subl	%eax, %r11d
	subl	%eax, %ecx
	subl	%eax, %esi
	subl	%eax, %r12d
	subl	%eax, %r10d
	subl	%eax, %r15d
	subl	%eax, %ebp
	subl	%eax, %edx
	movq	%rdx, %r9
	movq	1280(%rsp), %rdx        # 8-byte Reload
	subl	%eax, %edx
	addl	$-8, %edi
	movq	%rdi, 1056(%rsp)        # 8-byte Spill
	addl	$-8, %r13d
	movq	%r13, 1136(%rsp)        # 8-byte Spill
	addl	$-8, %r11d
	movq	%r11, 1040(%rsp)        # 8-byte Spill
	addl	$-8, %ecx
	movq	%rcx, 1120(%rsp)        # 8-byte Spill
	addl	$-8, %esi
	movq	%rsi, 1024(%rsp)        # 8-byte Spill
	addl	$-8, %r12d
	movq	%r12, 1104(%rsp)        # 8-byte Spill
	addl	$-8, %r10d
	movq	%r10, 1008(%rsp)        # 8-byte Spill
	addl	$-8, %r15d
	movq	%r15, 1088(%rsp)        # 8-byte Spill
	addl	$-8, %ebp
	movq	%rbp, 1072(%rsp)        # 8-byte Spill
	movq	%r9, %rax
	addl	$-8, %eax
	movq	%rax, 1176(%rsp)        # 8-byte Spill
	addl	$-8, %edx
	movq	%rdx, 992(%rsp)         # 8-byte Spill
	movq	-64(%rsp), %rax         # 8-byte Reload
	negl	%eax
	movl	-68(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-76(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-84(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-92(%rsp), %edx         # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-100(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-108(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-112(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-116(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movl	-120(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movl	-124(%rsp), %edx        # 4-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	movq	-56(%rsp), %rax         # 8-byte Reload
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	movq	1712(%rsp), %rdx        # 8-byte Reload
	notl	%edx
	cmpl	%edx, %eax
	cmovgel	%eax, %edx
	negl	%edx
	movl	%r8d, %eax
	notl	%eax
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	leal	1(%r8,%rax), %edx
	movslq	1808(%rsp), %r14        # 4-byte Folded Reload
	movl	1172(%rsp), %r8d        # 4-byte Reload
	movslq	1840(%rsp), %rax        # 4-byte Folded Reload
	movq	%rax, 976(%rsp)         # 8-byte Spill
	movq	1752(%rsp), %rsi        # 8-byte Reload
	vmovss	.LCPI162_1(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	vmovss	60(%rsp), %xmm2         # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm2, %xmm0, %xmm0
	vmovss	112(%rsp), %xmm3        # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vmulss	%xmm3, %xmm0, %xmm1
	vmovss	4(%rsp), %xmm5          # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmovss	120(%rsp), %xmm2        # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vsubss	%xmm3, %xmm2, %xmm2
	vmulss	%xmm2, %xmm0, %xmm0
	vdivss	%xmm0, %xmm5, %xmm0
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm1, %xmm10
	vbroadcastss	.LCPI162_3(%rip), %xmm14
	vbroadcastss	.LCPI162_4(%rip), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI162_5(%rip), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_6(%rip), %xmm0
	vmovaps	%xmm0, 928(%rsp)        # 16-byte Spill
	movq	560(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%rax), %r15
	.align	16, 0x90
.LBB162_74:                             # %for f8.s0.v10.v108
                                        # =>This Inner Loop Header: Depth=1
	movq	%rbx, 1584(%rsp)        # 8-byte Spill
	movl	%edx, 1456(%rsp)        # 4-byte Spill
	testl	%r8d, %r8d
	sete	%r12b
	setne	1424(%rsp)              # 1-byte Folded Spill
	movq	992(%rsp), %rax         # 8-byte Reload
	movq	%rbx, %rdx
	leal	(%rax,%rdx), %eax
	movl	%eax, 1440(%rsp)        # 4-byte Spill
	movl	%eax, %ecx
	andl	$1, %ecx
	movl	%ecx, 1664(%rsp)        # 4-byte Spill
	sete	1408(%rsp)              # 1-byte Folded Spill
	movslq	%eax, %r9
	movq	%r9, %rax
	movq	560(%rsp), %r8          # 8-byte Reload
	imulq	%r8, %rax
	subq	%r14, %rax
	movq	976(%rsp), %r13         # 8-byte Reload
	addq	%r13, %rax
	movq	1696(%rsp), %r10        # 8-byte Reload
	leaq	(%r10,%rax,4), %rcx
	leaq	(%rcx,%r15,4), %rbp
	leaq	(%rbp,%r15,4), %rbx
	vmovss	(%r10,%rax,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbp,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	2(%r9), %rbp
	imulq	%r8, %rbp
	vinsertps	$48, (%rbx,%r15,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, 1376(%rsp)       # 16-byte Spill
	subq	%r14, %rbp
	addq	%r13, %rbp
	leaq	(%r10,%rbp,4), %r11
	leaq	(%r11,%r15,4), %rdi
	leaq	(%rdi,%r15,4), %rax
	movq	%rax, 1808(%rsp)        # 8-byte Spill
	vmulps	%xmm13, %xmm4, %xmm0
	movq	1072(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	movslq	%eax, %rbx
	vmovups	24608(%rsi,%rbx,4), %xmm2
	vmovaps	%xmm2, 1520(%rsp)       # 16-byte Spill
	vmovups	24624(%rsi,%rbx,4), %xmm1
	vmovaps	%xmm1, 1536(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0,2],xmm1[0,2]
	vsubps	%xmm10, %xmm1, %xmm1
	vmovaps	1840(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm0, 1552(%rsp)       # 16-byte Spill
	movq	1008(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	cltq
	movq	%rax, 1344(%rsp)        # 8-byte Spill
	movq	1736(%rsp), %rcx        # 8-byte Reload
	vmovups	8(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1504(%rsp)       # 16-byte Spill
	vmovups	24(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm1[0,2]
	vmovaps	1648(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm5, %xmm4, %xmm2
	movq	1088(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	cltq
	movq	%rax, 1328(%rsp)        # 8-byte Spill
	vmovups	24608(%rsi,%rax,4), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	vmovups	24624(%rsi,%rax,4), %xmm11
	vshufps	$136, %xmm11, %xmm0, %xmm3 # xmm3 = xmm0[0,2],xmm11[0,2]
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm3, %xmm2, %xmm2
	vminps	%xmm14, %xmm2, %xmm2
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm2, %xmm2
	vsubps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	movq	1024(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	cltq
	movq	%rax, 1312(%rsp)        # 8-byte Spill
	vmovups	8(%rcx,%rax,4), %xmm1
	vmovups	24(%rcx,%rax,4), %xmm3
	vmovaps	1632(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm8, %xmm4, %xmm2
	movq	1104(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %eax
	cltq
	movq	%rax, 1296(%rsp)        # 8-byte Spill
	vmovups	24608(%rsi,%rax,4), %xmm15
	vmovaps	%xmm13, %xmm9
	vmovups	24624(%rsi,%rax,4), %xmm12
	vshufps	$136, %xmm12, %xmm15, %xmm4 # xmm4 = xmm15[0,2],xmm12[0,2]
	vsubps	%xmm10, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vshufps	$136, %xmm3, %xmm1, %xmm4 # xmm4 = xmm1[0,2],xmm3[0,2]
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 1776(%rsp)       # 16-byte Spill
	vmovss	(%r10,%rbp,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rcx, %r11
	vinsertps	$32, (%rdi,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	1808(%rsp), %rax        # 8-byte Reload
	vinsertps	$48, (%rax,%r15,4), %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm4, 1360(%rsp)       # 16-byte Spill
	vmovups	24616(%rsi,%rbx,4), %xmm13
	vmovups	24632(%rsi,%rbx,4), %xmm2
	vmovaps	%xmm2, 1232(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm13, %xmm2 # xmm2 = xmm13[0,2],xmm2[0,2]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm9, %xmm4, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	movq	1176(%rsp), %rax        # 8-byte Reload
	leal	(%rax,%rdx), %ecx
	movslq	%ecx, %rbp
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm6, %xmm2, %xmm2
	vmovups	16(%r11,%rbp,4), %xmm6
	vmovups	32(%r11,%rbp,4), %xmm4
	vmovaps	%xmm4, 1216(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm4, %xmm6, %xmm4 # xmm4 = xmm6[0,2],xmm4[0,2]
	vsubps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm2, 1760(%rsp)       # 16-byte Spill
	leaq	-1(%r9), %rcx
	imulq	%r8, %rcx
	subq	%r14, %rcx
	addq	%r13, %rcx
	leaq	(%r10,%rcx,4), %rdx
	leaq	(%rdx,%r15,4), %rdi
	leaq	(%rdi,%r15,4), %rax
	vmovss	(%r10,%rcx,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rax,%r15,4), %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm4, 1792(%rsp)       # 16-byte Spill
	vmovups	24600(%rsi,%rbx,4), %xmm2
	vmovaps	%xmm2, 1248(%rsp)       # 16-byte Spill
	movq	%rsi, %rbx
	vshufps	$221, %xmm13, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm13[1,3]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm9, %xmm4, %xmm4
	vmulps	%xmm2, %xmm4, %xmm2
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vmovaps	%xmm1, 1264(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm12, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm12[1,3]
	leaq	1(%r9), %rcx
	imulq	%r8, %rcx
	subq	%r14, %rcx
	addq	%r13, %rcx
	leaq	(%r10,%rcx,4), %rdx
	leaq	(%rdx,%r15,4), %rsi
	leaq	(%rsi,%r15,4), %rdi
	vmovss	(%r10,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rsi,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rdi,%r15,4), %xmm3, %xmm15 # xmm15 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm15, 1568(%rsp)      # 16-byte Spill
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm8, %xmm15, %xmm3
	vxorps	%xmm8, %xmm8, %xmm8
	vmulps	%xmm1, %xmm3, %xmm1
	vmovups	8(%r11,%rbp,4), %xmm3
	vmovups	24(%r11,%rbp,4), %xmm4
	vshufps	$136, %xmm4, %xmm3, %xmm12 # xmm12 = xmm3[0,2],xmm4[0,2]
	vmovaps	%xmm12, 1392(%rsp)      # 16-byte Spill
	vshufps	$221, %xmm4, %xmm3, %xmm0 # xmm0 = xmm3[1,3],xmm4[1,3]
	vmovaps	%xmm0, 1808(%rsp)       # 16-byte Spill
	vmovaps	1520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1536(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm0[1,3],mem[1,3]
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm9, %xmm15, %xmm4
	vmulps	%xmm3, %xmm4, %xmm0
	vmovaps	%xmm0, 1520(%rsp)       # 16-byte Spill
	vmovaps	1504(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmovaps	1472(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, %xmm11, %xmm0, %xmm0 # xmm0 = xmm0[1,3],xmm11[1,3]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmulps	%xmm5, %xmm15, %xmm3
	vmulps	%xmm0, %xmm3, %xmm0
	leaq	-2(%r9), %rcx
	imulq	%r8, %rcx
	subq	%r14, %rcx
	addq	%r13, %rcx
	leaq	(%r10,%rcx,4), %rdx
	leaq	(%rdx,%r15,4), %rsi
	vmovss	(%r10,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rsi,%r15,4), %rcx
	vinsertps	$32, (%rsi,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movl	1664(%rsp), %esi        # 4-byte Reload
	vinsertps	$48, (%rcx,%r15,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	leaq	3(%r9), %rcx
	imulq	%r8, %rcx
	subq	%r14, %rcx
	addq	%r13, %rcx
	leaq	(%r10,%rcx,4), %rdx
	vmovss	(%r10,%rcx,4), %xmm3    # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	leaq	(%rdx,%r15,4), %rcx
	vinsertps	$32, (%rcx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leaq	(%rcx,%r15,4), %rcx
	vinsertps	$48, (%rcx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	movb	%r12b, %cl
	andb	%sil, %cl
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm15
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm4
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm3
	vmovaps	1552(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm12, %xmm0, %xmm2
	vmovups	(%r11,%rbp,4), %xmm0
	vshufps	$221, %xmm6, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm6[1,3]
	jne	.LBB162_75
# BB#76:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm2, 1552(%rsp)       # 16-byte Spill
	vmovaps	%xmm9, %xmm12
	vmovaps	%xmm6, 1184(%rsp)       # 16-byte Spill
	vmovaps	%xmm5, 1200(%rsp)       # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	.LBB162_77
	.align	16, 0x90
.LBB162_75:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm6, 1184(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm6, %xmm0, %xmm11 # xmm11 = xmm0[0,2],xmm6[0,2]
	vmulps	%xmm9, %xmm5, %xmm0
	vmovaps	%xmm9, %xmm12
	vmovaps	%xmm5, 1200(%rsp)       # 16-byte Spill
	vmovaps	1248(%rsp), %xmm5       # 16-byte Reload
	vshufps	$136, %xmm13, %xmm5, %xmm5 # xmm5 = xmm5[0,2],xmm13[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm11, %xmm0, %xmm0
	vaddps	1680(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
	vmovaps	%xmm2, 1552(%rsp)       # 16-byte Spill
	vaddps	1776(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	1760(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm5, %xmm0
	vmulps	944(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB162_77:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	movl	1172(%rsp), %r8d        # 4-byte Reload
	movl	1440(%rsp), %r10d       # 4-byte Reload
	movq	1344(%rsp), %rdi        # 8-byte Reload
	movq	1328(%rsp), %rbp        # 8-byte Reload
	movq	1312(%rsp), %r13        # 8-byte Reload
	movq	1296(%rsp), %rax        # 8-byte Reload
	vmovaps	%xmm7, %xmm9
	vmovaps	1264(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1536(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1520(%rsp), %xmm5       # 16-byte Reload
	vsubps	%xmm1, %xmm15, %xmm15
	vsubps	%xmm0, %xmm4, %xmm4
	vsubps	%xmm2, %xmm3, %xmm0
	vminps	%xmm14, %xmm5, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	1808(%rsp), %xmm1, %xmm7 # 16-byte Folded Reload
	vmovups	16(%r11,%rdi,4), %xmm3
	vmovups	24616(%rbx,%rbp,4), %xmm8
	vmovups	16(%r11,%r13,4), %xmm11
	vmovups	24616(%rbx,%rax,4), %xmm2
	movl	%r10d, %ecx
	movq	1744(%rsp), %rdx        # 8-byte Reload
	orl	%edx, %ecx
	andl	$1, %ecx
	je	.LBB162_78
# BB#79:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm3, 1504(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1536(%rsp)      # 16-byte Spill
	vxorps	%xmm6, %xmm6, %xmm6
	movq	%rbx, %rdx
	jmp	.LBB162_80
	.align	16, 0x90
.LBB162_78:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm2, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1536(%rsp)      # 16-byte Spill
	vmovups	(%r11,%rdi,4), %xmm1
	vshufps	$221, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm3[1,3]
	vmovaps	%xmm3, 1504(%rsp)       # 16-byte Spill
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmulps	1648(%rsp), %xmm6, %xmm3 # 16-byte Folded Reload
	vmovups	24600(%rbx,%rbp,4), %xmm5
	vshufps	$221, %xmm8, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm8[1,3]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm9, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	.LCPI162_10(%rip), %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vmulps	1632(%rsp), %xmm6, %xmm3 # 16-byte Folded Reload
	vmovups	24600(%rbx,%rax,4), %xmm5
	movq	%rbx, %rdx
	vshufps	$221, %xmm2, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm2[1,3]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm9, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmovups	(%r11,%r13,4), %xmm5
	vshufps	$221, %xmm11, %xmm5, %xmm5 # xmm5 = xmm5[1,3],xmm11[1,3]
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	.LCPI162_10(%rip), %xmm3, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vsubps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm3, %xmm15, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vaddps	%xmm1, %xmm4, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vaddps	%xmm1, %xmm0, %xmm1
	vmulps	1600(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm1, 1248(%rsp)       # 16-byte Spill
.LBB162_80:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	testl	%ecx, %ecx
	je	.LBB162_81
# BB#82:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	.LBB162_83
	.align	16, 0x90
.LBB162_81:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1184(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmulps	1472(%rsp), %xmm12, %xmm3 # 16-byte Folded Reload
	vshufps	$221, 1232(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm13[1,3],mem[1,3]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm9, %xmm5
	vmulps	%xmm3, %xmm5, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vmaxps	%xmm6, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vaddps	%xmm0, %xmm7, %xmm0
	vaddps	%xmm4, %xmm0, %xmm0
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vmulps	944(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB162_83:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1376(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1680(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1776(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1760(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1552(%rsp), %xmm7       # 16-byte Reload
	andb	%sil, %r12b
	jne	.LBB162_84
# BB#85:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
	movq	%rdx, %rsi
	vxorps	%xmm8, %xmm8, %xmm8
	vmovaps	%xmm2, %xmm6
	jmp	.LBB162_86
	.align	16, 0x90
.LBB162_84:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm2, %xmm6
	vmulps	1648(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
	movq	%rdx, %rsi
	vshufps	$136, 24632(%rsi,%rbp,4), %xmm8, %xmm1 # xmm1 = xmm8[0,2],mem[0,2]
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 32(%r11,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,2],mem[0,2]
	vminps	%xmm14, %xmm0, %xmm0
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmulps	1632(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
	vmovaps	1520(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 24632(%rsi,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vmovaps	1536(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%r11,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,2],mem[0,2]
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm4, %xmm7, %xmm2
	vaddps	%xmm2, %xmm11, %xmm2
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm5, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1232(%rsp)       # 16-byte Spill
.LBB162_86:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm9, 1840(%rsp)       # 16-byte Spill
	movq	1040(%rsp), %rax        # 8-byte Reload
	movq	1584(%rsp), %rdi        # 8-byte Reload
	leal	(%rax,%rdi), %eax
	cltq
	vmovups	8(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1520(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm1[0,2]
	vmovaps	1616(%rsp), %xmm12      # 16-byte Reload
	vmulps	%xmm12, %xmm3, %xmm1
	movq	1120(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movslq	%ecx, %rdx
	vmovups	24608(%rsi,%rdx,4), %xmm4
	vmovaps	%xmm4, 1488(%rsp)       # 16-byte Spill
	vmovups	24624(%rsi,%rdx,4), %xmm2
	vmovaps	%xmm2, 1504(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm4, %xmm2 # xmm2 = xmm4[0,2],xmm2[0,2]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vmovups	16(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1552(%rsp)       # 16-byte Spill
	vmovups	32(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm0[0,2]
	vmulps	%xmm12, %xmm6, %xmm1
	vmovups	24616(%rsi,%rdx,4), %xmm13
	vmovups	24632(%rsi,%rdx,4), %xmm2
	vmovaps	%xmm2, 1312(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm2, %xmm13, %xmm2 # xmm2 = xmm13[0,2],xmm2[0,2]
	vsubps	%xmm10, %xmm2, %xmm2
	vmulps	%xmm2, %xmm9, %xmm2
	vmulps	%xmm2, %xmm1, %xmm1
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovaps	1824(%rsp), %xmm15      # 16-byte Reload
	vmulps	%xmm15, %xmm3, %xmm0
	movq	1136(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movslq	%ecx, %rbp
	vmovups	24608(%rsi,%rbp,4), %xmm1
	vmovups	24624(%rsi,%rbp,4), %xmm2
	vshufps	$136, %xmm2, %xmm1, %xmm5 # xmm5 = xmm1[0,2],xmm2[0,2]
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm9, %xmm5
	vmulps	%xmm5, %xmm0, %xmm0
	movq	1056(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rdi), %ecx
	movq	%rdi, %rbx
	movslq	%ecx, %rdi
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovups	8(%r11,%rdi,4), %xmm5
	vmovups	24(%r11,%rdi,4), %xmm4
	vshufps	$136, %xmm4, %xmm5, %xmm3 # xmm3 = xmm5[0,2],xmm4[0,2]
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmulps	%xmm15, %xmm6, %xmm0
	vmovups	24616(%rsi,%rbp,4), %xmm11
	vmovups	24632(%rsi,%rbp,4), %xmm3
	vmovaps	%xmm3, 1296(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm11, %xmm3 # xmm3 = xmm11[0,2],xmm3[0,2]
	vsubps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm3, %xmm9, %xmm3
	vmulps	%xmm3, %xmm0, %xmm0
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vmovups	16(%r11,%rdi,4), %xmm7
	vmovups	32(%r11,%rdi,4), %xmm3
	vmovaps	%xmm3, 1264(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm3, %xmm7, %xmm3 # xmm3 = xmm7[0,2],xmm3[0,2]
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm4, %xmm5, %xmm0 # xmm0 = xmm5[1,3],xmm4[1,3]
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm2, %xmm1, %xmm0 # xmm0 = xmm1[1,3],xmm2[1,3]
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmovaps	1568(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm15, %xmm3, %xmm1
	vmulps	%xmm0, %xmm1, %xmm2
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vmovaps	1488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	vmulps	%xmm12, %xmm3, %xmm1
	vsubps	%xmm10, %xmm0, %xmm0
	vmulps	%xmm0, %xmm9, %xmm0
	vmulps	%xmm0, %xmm1, %xmm3
	vmovups	24600(%rsi,%rbp,4), %xmm0
	vshufps	$221, %xmm11, %xmm0, %xmm1 # xmm1 = xmm0[1,3],xmm11[1,3]
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm9, %xmm1
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm15, %xmm6, %xmm4
	vmulps	%xmm1, %xmm4, %xmm5
	vmulps	%xmm12, %xmm6, %xmm4
	vmovups	24600(%rsi,%rdx,4), %xmm1
	vshufps	$221, %xmm13, %xmm1, %xmm6 # xmm6 = xmm1[1,3],xmm13[1,3]
	vsubps	%xmm10, %xmm6, %xmm6
	vmulps	%xmm6, %xmm9, %xmm6
	vmulps	%xmm6, %xmm4, %xmm6
	vminps	%xmm14, %xmm2, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vmovaps	%xmm2, 1504(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm3, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vmovaps	%xmm2, 1520(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm5, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm2
	vmovaps	%xmm2, 1792(%rsp)       # 16-byte Spill
	vminps	%xmm14, %xmm6, %xmm2
	vmaxps	%xmm8, %xmm2, %xmm5
	movl	%r8d, %ecx
	andl	%r10d, %ecx
	vmovups	(%r11,%rdi,4), %xmm9
	vshufps	$221, %xmm7, %xmm9, %xmm2 # xmm2 = xmm9[1,3],xmm7[1,3]
	vmovaps	%xmm2, 1568(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, %xmm6
	vmovups	(%r11,%rax,4), %xmm15
	vmovaps	1552(%rsp), %xmm3       # 16-byte Reload
	vshufps	$221, %xmm3, %xmm15, %xmm4 # xmm4 = xmm15[1,3],xmm3[1,3]
	jne	.LBB162_87
# BB#88:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm5, 1360(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1488(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1344(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, %xmm5
	movl	1456(%rsp), %edx        # 4-byte Reload
	movb	1424(%rsp), %al         # 1-byte Reload
	movb	1408(%rsp), %cl         # 1-byte Reload
	vmovaps	1248(%rsp), %xmm9       # 16-byte Reload
	jmp	.LBB162_89
	.align	16, 0x90
.LBB162_87:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm5, 1360(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, 1488(%rsp)       # 16-byte Spill
	vmovaps	1200(%rsp), %xmm2       # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm6
	vshufps	$136, %xmm11, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm11[0,2]
	vmovaps	%xmm11, 1344(%rsp)      # 16-byte Spill
	vsubps	%xmm10, %xmm0, %xmm0
	vmovaps	1840(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm6, %xmm0
	vshufps	$136, %xmm7, %xmm9, %xmm6 # xmm6 = xmm9[0,2],xmm7[0,2]
	vminps	%xmm14, %xmm0, %xmm0
	vmaxps	%xmm8, %xmm0, %xmm0
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm12, %xmm2, %xmm6
	vshufps	$136, %xmm13, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm13[0,2]
	vsubps	%xmm10, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vshufps	$136, %xmm3, %xmm15, %xmm6 # xmm6 = xmm15[0,2],xmm3[0,2]
	vmovaps	%xmm3, %xmm5
	vminps	%xmm14, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm6, %xmm1, %xmm1
	vaddps	1776(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	1760(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	1680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1600(%rsp), %xmm0, %xmm9 # 16-byte Folded Reload
	movl	1456(%rsp), %edx        # 4-byte Reload
	movb	1424(%rsp), %al         # 1-byte Reload
	movb	1408(%rsp), %cl         # 1-byte Reload
.LBB162_89:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1376(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1536(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1792(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1568(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1520(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1504(%rsp), %xmm4       # 16-byte Reload
	vsubps	%xmm0, %xmm4, %xmm11
	vsubps	%xmm1, %xmm6, %xmm8
	vsubps	%xmm3, %xmm2, %xmm15
	vmovaps	1360(%rsp), %xmm0       # 16-byte Reload
	vsubps	1488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	andb	%cl, %al
	je	.LBB162_91
# BB#90:                                #   in Loop: Header=BB162_74 Depth=1
	vaddps	%xmm11, %xmm8, %xmm2
	vaddps	%xmm15, %xmm2, %xmm2
	vaddps	%xmm0, %xmm2, %xmm2
	vmulps	928(%rsp), %xmm2, %xmm9 # 16-byte Folded Reload
.LBB162_91:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1824(%rsp), %xmm2       # 16-byte Reload
	vmovaps	%xmm5, %xmm4
	vmovaps	1344(%rsp), %xmm3       # 16-byte Reload
	testb	%al, %al
	jne	.LBB162_92
# BB#93:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	960(%rsp), %xmm13       # 16-byte Reload
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1232(%rsp), %xmm3       # 16-byte Reload
	jmp	.LBB162_94
	.align	16, 0x90
.LBB162_92:                             #   in Loop: Header=BB162_74 Depth=1
	vshufps	$221, 1264(%rsp), %xmm7, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm7[1,3],mem[1,3]
	vshufps	$221, 1296(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	1472(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm2, %xmm5
	vsubps	%xmm10, %xmm3, %xmm3
	vmovaps	1840(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm3, %xmm7, %xmm3
	vmulps	%xmm5, %xmm3, %xmm3
	vminps	%xmm14, %xmm3, %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vmaxps	%xmm5, %xmm3, %xmm3
	vsubps	%xmm1, %xmm3, %xmm1
	vshufps	$221, 1328(%rsp), %xmm4, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm4[1,3],mem[1,3]
	vshufps	$221, 1312(%rsp), %xmm13, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm13[1,3],mem[1,3]
	vmulps	%xmm6, %xmm12, %xmm6
	vsubps	%xmm10, %xmm5, %xmm5
	vmulps	%xmm5, %xmm7, %xmm5
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	%xmm14, %xmm5, %xmm5
	vmaxps	.LCPI162_10(%rip), %xmm5, %xmm5
	vsubps	%xmm3, %xmm5, %xmm3
	vaddps	%xmm0, %xmm8, %xmm0
	vaddps	%xmm3, %xmm0, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vaddps	%xmm0, %xmm11, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	1600(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
	vmovaps	960(%rsp), %xmm13       # 16-byte Reload
	vmovaps	1392(%rsp), %xmm0       # 16-byte Reload
.LBB162_94:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	1776(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1760(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1680(%rsp), %xmm6       # 16-byte Reload
	vmovaps	1664(%rsp), %xmm1       # 16-byte Reload
	vaddps	%xmm9, %xmm0, %xmm0
	andl	%r8d, %r10d
	jne	.LBB162_95
# BB#96:                                # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm2, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1616(%rsp)      # 16-byte Spill
	jmp	.LBB162_97
	.align	16, 0x90
.LBB162_95:                             #   in Loop: Header=BB162_74 Depth=1
	vmovaps	%xmm2, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1616(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm5, %xmm1
	vaddps	%xmm1, %xmm6, %xmm1
	vaddps	%xmm1, %xmm4, %xmm1
	vmulps	928(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
.LBB162_97:                             # %for f8.s0.v10.v108
                                        #   in Loop: Header=BB162_74 Depth=1
	vaddps	1808(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
	vmovaps	.LCPI162_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vmovaps	.LCPI162_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movq	1160(%rsp), %rax        # 8-byte Reload
	leaq	(%r9,%rax), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %ebx
	addl	$-1, %edx
	jne	.LBB162_74
.LBB162_98:                             # %end for f8.s0.v10.v109
	movl	-128(%rsp), %eax        # 4-byte Reload
	cmpl	584(%rsp), %eax         # 4-byte Folded Reload
	vmovss	120(%rsp), %xmm4        # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vmovss	112(%rsp), %xmm7        # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vmovss	4(%rsp), %xmm8          # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	movq	-8(%rsp), %r8           # 8-byte Reload
	movq	8(%rsp), %rbp           # 8-byte Reload
	jge	.LBB162_160
# BB#99:                                # %for f8.s0.v10.v1012.preheader
	movl	$2, %eax
	movq	32(%rsp), %r9           # 8-byte Reload
	subl	%r9d, %eax
	leal	(%rbp,%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%ebp, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	negl	%ecx
	andl	%eax, %ecx
	orl	%edi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%rbp,%rbp), %r14d
	subl	%eax, %r14d
	cmpl	%eax, %ebp
	cmovgl	%eax, %r14d
	addl	%r9d, %r14d
	leal	(%r9,%rbp), %eax
	leal	-1(%r9,%rbp), %r10d
	cmpl	%r14d, %r10d
	cmovlel	%r10d, %r14d
	cmpl	%r9d, %r14d
	cmovll	%r9d, %r14d
	cmpl	$3, %eax
	movl	$2, %r11d
	cmovgel	%r11d, %r10d
	cmpl	%r9d, %r10d
	cmovll	%r9d, %r10d
	cmpl	$3, %eax
	cmovll	%r14d, %r10d
	movl	$2, %eax
	movq	24(%rsp), %rbx          # 8-byte Reload
	subl	%ebx, %eax
	movq	16(%rsp), %rcx          # 8-byte Reload
	leal	(%rcx,%rcx), %ebp
	cltd
	idivl	%ebp
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ebp, %eax, %edi
	negl	%ebp
	andl	%eax, %ebp
	orl	%edi, %ebp
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%ebp, %edi
	addl	%edx, %edi
	leal	-1(%rcx,%rcx), %eax
	subl	%edi, %eax
	cmpl	%edi, %ecx
	cmovgl	%edi, %eax
	leal	(%rbx,%rcx), %edx
	leal	-1(%rbx,%rcx), %ecx
	addl	%ebx, %eax
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	cmpl	$3, %edx
	cmovgel	%r11d, %ecx
	cmpl	%ebx, %ecx
	cmovll	%ebx, %ecx
	cmpl	$3, %edx
	cmovll	%eax, %ecx
	movl	%ecx, %edx
	movq	1744(%rsp), %rbp        # 8-byte Reload
	movl	%ebp, %ecx
	andl	$1, %ecx
	movl	%ecx, 624(%rsp)         # 4-byte Spill
	movl	%ebp, %ecx
	andl	$63, %ecx
	movq	%rcx, 1680(%rsp)        # 8-byte Spill
	cmpl	$2, %ebx
	cmovgl	%eax, %edx
	movl	%edx, 1840(%rsp)        # 4-byte Spill
	movq	560(%rsp), %rdi         # 8-byte Reload
	movl	%edi, %r11d
	movq	272(%rsp), %rax         # 8-byte Reload
	imull	%eax, %r11d
	addl	%ebx, %r11d
	cmpl	$2, %r9d
	cmovgl	%r14d, %r10d
	movq	40(%rsp), %rax          # 8-byte Reload
	imull	%eax, %r8d
	movslq	%ebp, %r12
	movq	%rax, %rdx
	imulq	%r12, %rdx
	leaq	2(%r12), %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r14
	addl	%r9d, %r8d
	leaq	-2(%r12), %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r9
	leaq	-1(%r12), %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r15
	addq	$1, %r12
	imulq	%rax, %r12
	movslq	%r8d, %rax
	movq	%rdx, %rbx
	subq	%rax, %rbx
	subq	%rax, %r14
	subq	%rax, %r9
	subq	%rax, %r15
	subq	%rax, %r12
	movq	48(%rsp), %rax          # 8-byte Reload
	addq	$32, %rax
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	movslq	%r10d, %rax
	addq	%rax, %rbx
	movq	%rbx, 1760(%rsp)        # 8-byte Spill
	addq	%rax, %r14
	movq	%r14, 1776(%rsp)        # 8-byte Spill
	addq	%rax, %r9
	movq	%r9, 1792(%rsp)         # 8-byte Spill
	addq	%rax, %r15
	movq	%r15, 1808(%rsp)        # 8-byte Spill
	addq	%rax, %r12
	movq	128(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %rax
	sarq	$63, %rax
	andq	%rcx, %rax
	subq	%rax, %rdx
	movq	%rdx, 608(%rsp)         # 8-byte Spill
	movq	-16(%rsp), %r15         # 8-byte Reload
	vmovd	%r15d, %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	movl	96(%rsp), %edx          # 4-byte Reload
	andl	$-32, %edx
	movl	%edx, %eax
	shll	$5, %eax
	movq	%rax, 1664(%rsp)        # 8-byte Spill
	leal	-1(%r15), %eax
	movl	%eax, 1616(%rsp)        # 4-byte Spill
	leal	1(%r15), %r8d
	leal	2(%r15), %eax
	movl	%eax, 1648(%rsp)        # 4-byte Spill
	leal	-2(%r15), %eax
	movl	%eax, 1632(%rsp)        # 4-byte Spill
	addl	$-3, %r15d
	movq	-64(%rsp), %rax         # 8-byte Reload
	negl	%eax
	movl	-68(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-72(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-76(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-80(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-84(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-88(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-92(%rsp), %ecx         # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-96(%rsp), %eax         # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-100(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-104(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-108(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-112(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-116(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movl	-120(%rsp), %eax        # 4-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movl	-124(%rsp), %ecx        # 4-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	movq	-56(%rsp), %rax         # 8-byte Reload
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movq	1712(%rsp), %rcx        # 8-byte Reload
	notl	%ecx
	cmpl	%ecx, %eax
	cmovgel	%eax, %ecx
	negl	%ecx
	movq	-24(%rsp), %rax         # 8-byte Reload
	movl	-36(%rsp), %ebx         # 4-byte Reload
	cmpl	%eax, %ebx
	cmovgel	%ebx, %eax
	movl	-40(%rsp), %ebx         # 4-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	-44(%rsp), %ebx         # 4-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movq	-32(%rsp), %rbx         # 8-byte Reload
	cmpl	%ebx, %eax
	cmovll	%ebx, %eax
	movl	584(%rsp), %ebx         # 4-byte Reload
	notl	%ebx
	xorl	%r10d, %r10d
	testl	%eax, %eax
	cmovsl	%r10d, %eax
	notl	%eax
	cmpl	%eax, %ebx
	cmovgel	%ebx, %eax
	notl	%eax
	cmpl	%eax, %ecx
	cmovgel	%ecx, %eax
	movq	%rsi, 1752(%rsp)        # 8-byte Spill
	leal	9(%rbp), %r14d
	movl	64(%rsp), %ecx          # 4-byte Reload
	subl	%ecx, %r14d
	addl	$64, %edx
	imull	%edx, %r14d
	leal	7(%rbp), %ebx
	subl	%ecx, %ebx
	imull	%edx, %ebx
	leal	6(%rbp), %r9d
	subl	%ecx, %r9d
	imull	%edx, %r9d
	movq	136(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%rsi), %r13d
	vmovd	%r13d, %xmm14
	vmovd	%r11d, %xmm2
	vmovd	%r11d, %xmm1
	vmovd	%edi, %xmm13
	movl	1616(%rsp), %edi        # 4-byte Reload
	vmovd	%edi, %xmm0
	vmovd	%esi, %xmm10
	vmovd	%r8d, %xmm12
	movl	1648(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm11
	movl	1632(%rsp), %esi        # 4-byte Reload
	vmovd	%esi, %xmm15
	leal	10(%rbp), %r11d
	subl	%ecx, %r11d
	imull	%edx, %r11d
	leal	8(%rbp), %r13d
	subl	%ecx, %r13d
	imull	%edx, %r13d
	vmovd	1840(%rsp), %xmm3       # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpsubd	%xmm2, %xmm3, %xmm9
	vmovss	.LCPI162_1(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vmovss	60(%rsp), %xmm6         # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vsubss	%xmm6, %xmm3, %xmm5
	vmulss	%xmm7, %xmm5, %xmm3
	movq	832(%rsp), %rcx         # 8-byte Reload
	addl	$3, %ecx
	leal	1(%rbp), %r8d
	andl	$63, %r8d
	imull	%ecx, %r8d
	leal	63(%rbp), %edi
	andl	$63, %edi
	imull	%ecx, %edi
	vdivss	%xmm8, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm6
	vmovd	%r15d, %xmm3
	leal	62(%rbp), %esi
	andl	$63, %esi
	imull	%ecx, %esi
	leal	2(%rbp), %edx
	andl	$63, %edx
	imull	%ecx, %edx
	movq	1680(%rsp), %rbp        # 8-byte Reload
	imull	%ebp, %ecx
	movq	1664(%rsp), %rbp        # 8-byte Reload
	leal	(%rbp,%rbp,2), %r15d
	addl	%r15d, %r14d
	addl	%r15d, %ebx
	addl	%r15d, %r9d
	addl	%r15d, %r11d
	addl	%r15d, %r13d
	leal	(%r8,%rax,8), %ebp
	movq	%rbp, 592(%rsp)         # 8-byte Spill
	leal	(%r14,%rax,8), %ebp
	movq	%rbp, 576(%rsp)         # 8-byte Spill
	movq	1752(%rsp), %r15        # 8-byte Reload
	leal	(%rdi,%rax,8), %edi
	movq	%rdi, 568(%rsp)         # 8-byte Spill
	leal	(%rbx,%rax,8), %edi
	movq	%rdi, 560(%rsp)         # 8-byte Spill
	leal	(%rsi,%rax,8), %esi
	movq	%rsi, 552(%rsp)         # 8-byte Spill
	leal	(%r9,%rax,8), %esi
	movq	%rsi, 544(%rsp)         # 8-byte Spill
	leal	(%rdx,%rax,8), %edx
	movq	%rdx, 536(%rsp)         # 8-byte Spill
	leal	(%r11,%rax,8), %edx
	movq	%rdx, 528(%rsp)         # 8-byte Spill
	movq	1736(%rsp), %rbx        # 8-byte Reload
	leal	(%r13,%rax,8), %edx
	movq	%rdx, 520(%rsp)         # 8-byte Spill
	leal	(%rcx,%rax,8), %ecx
	movq	%rcx, 512(%rsp)         # 8-byte Spill
	movq	1280(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rax,8), %edx
	movq	%rdx, 504(%rsp)         # 8-byte Spill
	leal	3(%rcx,%rax,8), %esi
	movq	%rsi, 496(%rsp)         # 8-byte Spill
	leal	2(%rcx,%rax,8), %esi
	movq	%rsi, 480(%rsp)         # 8-byte Spill
	leal	-2(%rcx,%rax,8), %esi
	movq	%rsi, 464(%rsp)         # 8-byte Spill
	leal	-1(%rcx,%rax,8), %esi
	movq	%rsi, 448(%rsp)         # 8-byte Spill
	leal	1(%rcx,%rax,8), %ecx
	movq	%rcx, 432(%rsp)         # 8-byte Spill
	subl	%eax, 584(%rsp)         # 4-byte Folded Spill
	vpbroadcastd	%xmm14, %xmm2
	vmovdqa	%xmm2, 416(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 400(%rsp)        # 16-byte Spill
	vmovaps	%xmm8, %xmm14
	vbroadcastss	%xmm13, %xmm1
	vmovaps	%xmm1, 1680(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm0, 1328(%rsp)       # 16-byte Spill
	vsubss	%xmm7, %xmm4, %xmm4
	vmovdqa	.LCPI162_0(%rip), %xmm1 # xmm1 = [0,4294967294,4294967292,4294967290]
	vmulss	%xmm4, %xmm5, %xmm5
	vpaddd	%xmm1, %xmm0, %xmm4
	vmovdqa	%xmm4, 384(%rsp)        # 16-byte Spill
	vdivss	%xmm5, %xmm14, %xmm0
	movq	272(%rsp), %rcx         # 8-byte Reload
	vmovd	%ecx, %xmm5
	vbroadcastss	%xmm5, %xmm4
	vmovaps	%xmm4, 1312(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm10, %xmm5
	vmovaps	%xmm5, 1296(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm5
	vmovdqa	%xmm5, 368(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm11, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm5
	vmovdqa	%xmm5, 352(%rsp)        # 16-byte Spill
	vpbroadcastd	1824(%rsp), %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm5, %xmm5
	vmovdqa	%xmm5, 336(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm15, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm5
	vmovdqa	%xmm5, 320(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm1, %xmm3, %xmm1
	vmovdqa	%xmm1, 304(%rsp)        # 16-byte Spill
	vpcmpeqd	%xmm1, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovdqa	%xmm1, 1280(%rsp)       # 16-byte Spill
	movl	1840(%rsp), %eax        # 4-byte Reload
	vmovd	%eax, %xmm1
	vbroadcastss	%xmm1, %xmm1
	vmovaps	%xmm1, 288(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm1
	vmovdqa	%xmm1, 1664(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm0, %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm6, %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	movl	$3, %eax
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 256(%rsp)         # 8-byte Spill
	movl	$2, %eax
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 240(%rsp)         # 8-byte Spill
	movl	%edx, %eax
	subl	%ecx, %eax
	movq	%rax, 224(%rsp)         # 8-byte Spill
	movl	$-2, %eax
	subl	%ecx, %eax
	leal	(%rax,%rdx), %eax
	movq	%rax, 208(%rsp)         # 8-byte Spill
	movl	$1, %eax
	subl	%ecx, %eax
	notl	%ecx
	leal	(%rcx,%rdx), %ecx
	movq	%rcx, 272(%rsp)         # 8-byte Spill
	leal	(%rax,%rdx), %eax
	movq	%rax, 192(%rsp)         # 8-byte Spill
	movq	80(%rsp), %rax          # 8-byte Reload
	movq	1760(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	movq	1776(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	movq	1792(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1616(%rsp)       # 16-byte Spill
	movq	1808(%rsp), %rcx        # 8-byte Reload
	vbroadcastss	(%rax,%rcx,4), %xmm0
	vmovaps	%xmm0, 1584(%rsp)       # 16-byte Spill
	vbroadcastss	(%rax,%r12,4), %xmm0
	vmovaps	%xmm0, 1600(%rsp)       # 16-byte Spill
	vpabsd	%xmm2, %xmm0
	vmovdqa	%xmm0, 1264(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI162_2(%rip), %xmm12 # xmm12 = [0,2,4,6]
	vbroadcastss	.LCPI162_3(%rip), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_4(%rip), %xmm0
	vmovaps	%xmm0, 176(%rsp)        # 16-byte Spill
	vbroadcastss	.LCPI162_5(%rip), %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_6(%rip), %xmm0
	vmovaps	%xmm0, 160(%rsp)        # 16-byte Spill
	.align	16, 0x90
.LBB162_100:                            # %for f8.s0.v10.v1012
                                        # =>This Inner Loop Header: Depth=1
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movslq	%eax, %r8
	vmovups	8(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm12, %xmm7
	vpaddd	%xmm7, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	416(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r12d
	movl	%r12d, 1176(%rsp)       # 4-byte Spill
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %r14d
	movl	%r14d, 1184(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r11d
	movl	%r11d, 1200(%rsp)       # 4-byte Spill
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r9d
	movl	%r9d, 1216(%rsp)        # 4-byte Spill
	cltd
	idivl	%r9d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1264(%rsp), %xmm10      # 16-byte Reload
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movq	504(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movl	%eax, 1232(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm9
	vmovdqa	%xmm9, 1056(%rsp)       # 16-byte Spill
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	movq	432(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %r13d
	movl	%r13d, 1172(%rsp)       # 4-byte Spill
	vmovd	%r13d, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vmovdqa	1328(%rsp), %xmm15      # 16-byte Reload
	vpminsd	%xmm15, %xmm2, %xmm2
	vmovdqa	1312(%rsp), %xmm13      # 16-byte Reload
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vmovdqa	1296(%rsp), %xmm11      # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm11, %xmm3
	vmovdqa	%xmm11, %xmm6
	vmovdqa	1280(%rsp), %xmm11      # 16-byte Reload
	vpsubd	%xmm0, %xmm11, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vmovdqa	1680(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm5
	vpsubd	400(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	288(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	movq	1696(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1488(%rsp)       # 16-byte Spill
	movq	272(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vpinsrd	$2, %ebp, %xmm1, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	368(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	movq	448(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm7, %xmm2, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm0, %xmm6, %xmm3
	vpsubd	%xmm0, %xmm11, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vblendvps	%xmm1, %xmm2, %xmm0, %xmm0
	vpmulld	%xmm5, %xmm0, %xmm0
	vpaddd	1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1808(%rsp)       # 16-byte Spill
	movq	224(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r9d
	vmovd	%edi, %xmm0
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vmovdqa	336(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm9, %xmm2
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm3
	vpsubd	%xmm1, %xmm11, %xmm4
	vblendvps	%xmm3, %xmm1, %xmm4, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	movq	240(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm7, %xmm3, %xmm3
	vpextrd	$1, %xmm3, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vmovd	%xmm3, %eax
	cltd
	idivl	%r14d
	vblendvps	%xmm0, %xmm2, %xmm1, %xmm0
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmovd	%edx, %xmm0
	vpextrd	$2, %xmm3, %eax
	cltd
	idivl	%r11d
	vpinsrd	$1, %ecx, %xmm0, %xmm0
	vpinsrd	$2, %edx, %xmm0, %xmm0
	vpextrd	$3, %xmm3, %eax
	cltd
	idivl	%r9d
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	movq	480(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vmovdqa	%xmm1, 1120(%rsp)       # 16-byte Spill
	vpcmpgtd	%xmm0, %xmm6, %xmm1
	vmovdqa	%xmm6, %xmm4
	vpsubd	%xmm0, %xmm11, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpaddd	%xmm13, %xmm0, %xmm2
	movq	520(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movslq	%eax, %rcx
	vmovups	24608(%r15,%rcx,4), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vmovups	24624(%r15,%rcx,4), %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	movq	536(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	cltq
	movq	%rax, 1160(%rsp)        # 8-byte Spill
	vmovups	8(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	vmovups	24(%rbx,%rax,4), %xmm0
	vmovaps	%xmm0, 1472(%rsp)       # 16-byte Spill
	movq	528(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	cltq
	movq	%rax, 1568(%rsp)        # 8-byte Spill
	vmovups	24608(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	vmovups	24624(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movslq	%eax, %rdi
	movq	%rdi, 1136(%rsp)        # 8-byte Spill
	vmovups	8(%rbx,%rdi,4), %xmm0
	vmovaps	%xmm0, 1792(%rsp)       # 16-byte Spill
	movq	544(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	movslq	%eax, %rbp
	movq	%rbp, 1552(%rsp)        # 8-byte Spill
	movq	256(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	vmovups	24(%rbx,%rdi,4), %xmm14
	vmovaps	%xmm14, 1104(%rsp)      # 16-byte Spill
	vmovups	24608(%r15,%rbp,4), %xmm0
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmovups	24624(%r15,%rbp,4), %xmm0
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vmovups	16(%rbx,%r8,4), %xmm12
	vmovaps	%xmm12, 1504(%rsp)      # 16-byte Spill
	vmovups	32(%rbx,%r8,4), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vmovups	24616(%r15,%rcx,4), %xmm6
	vmovaps	%xmm6, 1520(%rsp)       # 16-byte Spill
	vmovups	24632(%r15,%rcx,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%r8,4), %xmm8
	vmovaps	%xmm8, 784(%rsp)        # 16-byte Spill
	vmovups	24600(%r15,%rcx,4), %xmm3
	vmovaps	%xmm3, 800(%rsp)        # 16-byte Spill
	idivl	%r12d
	movl	%edx, %ecx
	vpminsd	%xmm15, %xmm2, %xmm2
	vpmaxsd	%xmm13, %xmm2, %xmm2
	vmovd	%xmm1, %eax
	cltd
	idivl	%r14d
	movl	%edx, %edi
	vmovdqa	320(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm0, %xmm5
	vblendvps	%xmm5, 1120(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebp
	vpextrd	$3, %xmm1, %eax
	vmovd	%edi, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	cltd
	idivl	%r9d
	vpinsrd	$2, %ebp, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm1
	vpcmpgtd	%xmm1, %xmm4, %xmm5
	vpsubd	%xmm1, %xmm11, %xmm0
	vblendvps	%xmm5, %xmm1, %xmm0, %xmm0
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm1
	vpminsd	%xmm15, %xmm1, %xmm1
	vpmaxsd	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm13, %xmm0, %xmm0
	vpminsd	%xmm15, %xmm0, %xmm0
	vpmaxsd	%xmm13, %xmm0, %xmm0
	vmovdqa	304(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm5, %xmm5
	vblendvps	%xmm5, %xmm1, %xmm0, %xmm0
	vmovdqa	1680(%rsp), %xmm9       # 16-byte Reload
	vpmulld	1712(%rsp), %xmm9, %xmm1 # 16-byte Folded Reload
	vmovdqa	1664(%rsp), %xmm11      # 16-byte Reload
	vpaddd	%xmm1, %xmm11, %xmm1
	vpextrq	$1, %xmm1, %r12
	vpmulld	%xmm9, %xmm2, %xmm2
	vmovq	%xmm1, %r15
	vpaddd	%xmm2, %xmm11, %xmm1
	vpextrq	$1, %xmm1, %r8
	vmovq	%xmm1, %r14
	vshufps	$221, %xmm6, %xmm3, %xmm1 # xmm1 = xmm3[1,3],xmm6[1,3]
	vmovaps	1824(%rsp), %xmm6       # 16-byte Reload
	vsubps	%xmm6, %xmm1, %xmm1
	vmovaps	1648(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	816(%rsp), %xmm3        # 16-byte Reload
	vmulps	1808(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1840(%rsp), %xmm15      # 16-byte Reload
	vminps	%xmm15, %xmm1, %xmm1
	vpxor	%xmm10, %xmm10, %xmm10
	vmaxps	%xmm10, %xmm1, %xmm1
	vshufps	$221, %xmm12, %xmm8, %xmm2 # xmm2 = xmm8[1,3],xmm12[1,3]
	vsubps	%xmm2, %xmm1, %xmm8
	vmovaps	1760(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1776(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	1488(%rsp), %xmm4       # 16-byte Reload
	vmulps	1616(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm15, %xmm1, %xmm1
	vmaxps	%xmm10, %xmm1, %xmm1
	vmovaps	1792(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm14, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm14[1,3]
	vsubps	%xmm2, %xmm1, %xmm14
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm3, %xmm4, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1424(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1392(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm6, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	1632(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vminps	%xmm15, %xmm2, %xmm2
	vmaxps	%xmm10, %xmm2, %xmm2
	vmovaps	1408(%rsp), %xmm4       # 16-byte Reload
	vshufps	$221, 1472(%rsp), %xmm4, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm4[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm13
	vmovaps	1536(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1376(%rsp), %xmm4       # 16-byte Reload
	vpmulld	%xmm9, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm11, %xmm0
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%eax, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	sarq	$32, %rax
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	movq	%rsi, %r11
	movq	%r15, %r9
	sarq	$32, %r9
	movq	%r12, %rdi
	sarq	$32, %rdi
	movq	%r14, %rax
	sarq	$32, %rax
	movq	%r8, %rsi
	sarq	$32, %rsi
	movl	%r13d, %ecx
	andl	$1, %ecx
	testl	%ecx, %ecx
	vminps	%xmm15, %xmm1, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vshufps	$221, %xmm12, %xmm4, %xmm1 # xmm1 = xmm4[1,3],xmm12[1,3]
	vsubps	%xmm1, %xmm0, %xmm9
	movq	568(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r10), %ebx
	movq	560(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r10), %ebp
	movq	592(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r10), %r13d
	movq	576(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r10), %edx
	jne	.LBB162_101
# BB#102:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm2, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 704(%rsp)        # 16-byte Spill
	vmovaps	%xmm13, 832(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 720(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 736(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	.LBB162_103
	.align	16, 0x90
.LBB162_101:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm1, 848(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 704(%rsp)        # 16-byte Spill
	vmulps	%xmm2, %xmm3, %xmm0
	vmovaps	%xmm2, 640(%rsp)        # 16-byte Spill
	vmovaps	1520(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm6, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1504(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vminps	%xmm15, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm13, %xmm9, %xmm1
	vmovaps	%xmm13, 832(%rsp)       # 16-byte Spill
	vaddps	%xmm14, %xmm1, %xmm1
	vmovaps	%xmm14, 720(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	%xmm8, 736(%rsp)        # 16-byte Spill
	vmulps	176(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB162_103:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vxorps	%xmm14, %xmm14, %xmm14
	vmovaps	1488(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1472(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1440(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1424(%rsp), %xmm9       # 16-byte Reload
	vmovaps	1408(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1392(%rsp), %xmm13      # 16-byte Reload
	movslq	%r15d, %rcx
	vmovss	(%r11,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r12d, %rcx
	vinsertps	$32, (%r11,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rdi,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm2, 1120(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm12, %xmm4, %xmm0 # xmm0 = xmm4[0,2],xmm12[0,2]
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	movslq	%r14d, %rcx
	vmovss	(%r11,%rcx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movslq	%r8d, %rax
	vinsertps	$32, (%r11,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%rsi,4), %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm4, 1376(%rsp)       # 16-byte Spill
	movq	%r11, %r9
	vshufps	$136, %xmm5, %xmm1, %xmm0 # xmm0 = xmm1[0,2],xmm5[0,2]
	vmulps	%xmm3, %xmm2, %xmm1
	vsubps	%xmm6, %xmm0, %xmm0
	vmulps	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm6, %xmm5
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	vshufps	$136, %xmm8, %xmm11, %xmm0 # xmm0 = xmm11[0,2],xmm8[0,2]
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmulps	1632(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vshufps	$136, %xmm13, %xmm9, %xmm1 # xmm1 = xmm9[0,2],xmm13[0,2]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm1, %xmm0, %xmm8
	vmovaps	1792(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1104(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmulps	1616(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
	vmovaps	1760(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm12
	vmulps	%xmm3, %xmm4, %xmm1
	vmovaps	1520(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm0[0,2],mem[0,2]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm9
	movslq	%ebp, %rax
	movq	1752(%rsp), %r11        # 8-byte Reload
	vmovups	24608(%r11,%rax,4), %xmm3
	vmovaps	%xmm3, 976(%rsp)        # 16-byte Spill
	movslq	%edx, %rdx
	vmovups	24608(%r11,%rdx,4), %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vmovups	24624(%r11,%rdx,4), %xmm2
	vmovaps	%xmm2, 1424(%rsp)       # 16-byte Spill
	vmovaps	1600(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm6, %xmm10, %xmm1
	vshufps	$221, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm5, %xmm2, %xmm2
	vmulps	%xmm2, %xmm7, %xmm2
	vmulps	%xmm2, %xmm1, %xmm2
	vmovups	24624(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 960(%rsp)        # 16-byte Spill
	vmovaps	1584(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm11, %xmm10, %xmm1
	vshufps	$221, %xmm0, %xmm3, %xmm4 # xmm4 = xmm3[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm4, %xmm4
	vmulps	%xmm4, %xmm7, %xmm4
	vmulps	%xmm4, %xmm1, %xmm3
	vmovups	24616(%r11,%rdx,4), %xmm0
	vmovaps	%xmm0, 1792(%rsp)       # 16-byte Spill
	vmovups	24600(%r11,%rdx,4), %xmm1
	vmovaps	%xmm1, 768(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm0[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmovaps	1808(%rsp), %xmm0       # 16-byte Reload
	vmulps	%xmm6, %xmm0, %xmm4
	vmulps	%xmm1, %xmm4, %xmm6
	vmovups	24616(%r11,%rax,4), %xmm1
	vmovaps	%xmm1, 1488(%rsp)       # 16-byte Spill
	vmovups	24600(%r11,%rax,4), %xmm4
	vmovaps	%xmm4, 752(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm1, %xmm4, %xmm1 # xmm1 = xmm4[1,3],xmm1[1,3]
	vsubps	%xmm5, %xmm1, %xmm1
	vmulps	%xmm1, %xmm7, %xmm1
	vmulps	%xmm11, %xmm0, %xmm4
	vmulps	%xmm1, %xmm4, %xmm5
	vminps	%xmm15, %xmm8, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm0
	vmovaps	%xmm0, 1072(%rsp)       # 16-byte Spill
	vminps	%xmm15, %xmm12, %xmm0
	vmaxps	%xmm14, %xmm0, %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, %xmm0
	vmovaps	1504(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 1360(%rsp), %xmm4, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm4[0,2],mem[0,2]
	vmovaps	%xmm1, 1760(%rsp)       # 16-byte Spill
	vminps	%xmm0, %xmm9, %xmm4
	vmaxps	%xmm14, %xmm4, %xmm1
	vmovaps	%xmm1, 1360(%rsp)       # 16-byte Spill
	movslq	%ebx, %rcx
	movslq	%r13d, %rsi
	movq	%r11, %r15
	vminps	%xmm0, %xmm2, %xmm4
	vminps	%xmm0, %xmm3, %xmm2
	vminps	%xmm0, %xmm6, %xmm6
	vminps	%xmm0, %xmm5, %xmm12
	vmovaps	1456(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm0, %xmm1, %xmm3
	vmaxps	%xmm14, %xmm3, %xmm8
	movl	624(%rsp), %r11d        # 4-byte Reload
	testl	%r11d, %r11d
	vmovups	24632(%r15,%rax,4), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vmovups	24632(%r15,%rdx,4), %xmm0
	vmovaps	%xmm0, 1456(%rsp)       # 16-byte Spill
	movq	1736(%rsp), %rbx        # 8-byte Reload
	vmovups	8(%rbx,%rsi,4), %xmm15
	vmovups	24(%rbx,%rsi,4), %xmm9
	vmovups	16(%rbx,%rsi,4), %xmm0
	vmovups	32(%rbx,%rsi,4), %xmm3
	vmovaps	%xmm3, 1472(%rsp)       # 16-byte Spill
	vmovups	(%rbx,%rsi,4), %xmm5
	vmovups	8(%rbx,%rcx,4), %xmm11
	vmovups	24(%rbx,%rcx,4), %xmm1
	vmovups	16(%rbx,%rcx,4), %xmm3
	vmovups	32(%rbx,%rcx,4), %xmm7
	vmovaps	%xmm7, 928(%rsp)        # 16-byte Spill
	vmovups	(%rbx,%rcx,4), %xmm10
	movq	1160(%rsp), %r12        # 8-byte Reload
	vmovups	16(%rbx,%r12,4), %xmm7
	vmovaps	%xmm7, 992(%rsp)        # 16-byte Spill
	movq	1568(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r15,%rax,4), %xmm7
	vmovaps	%xmm7, 1008(%rsp)       # 16-byte Spill
	movq	1136(%rsp), %r13        # 8-byte Reload
	vmovups	16(%rbx,%r13,4), %xmm7
	vmovaps	%xmm7, 1024(%rsp)       # 16-byte Spill
	movq	1552(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r15,%rax,4), %xmm13
	vmovaps	%xmm13, 1040(%rsp)      # 16-byte Spill
	je	.LBB162_105
# BB#104:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vxorps	%xmm7, %xmm7, %xmm7
	vmovaps	%xmm7, 1712(%rsp)       # 16-byte Spill
.LBB162_105:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vshufps	$221, %xmm9, %xmm15, %xmm7 # xmm7 = xmm15[1,3],xmm9[1,3]
	vmovaps	%xmm7, 896(%rsp)        # 16-byte Spill
	vmovaps	%xmm9, 1392(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 1408(%rsp)      # 16-byte Spill
	vmaxps	%xmm14, %xmm4, %xmm4
	vmovaps	%xmm4, 880(%rsp)        # 16-byte Spill
	vmovaps	1072(%rsp), %xmm4       # 16-byte Reload
	vsubps	1088(%rsp), %xmm4, %xmm7 # 16-byte Folded Reload
	vshufps	$221, %xmm1, %xmm11, %xmm4 # xmm4 = xmm11[1,3],xmm1[1,3]
	vmovaps	%xmm4, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm1, 1072(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1088(%rsp)      # 16-byte Spill
	vmaxps	%xmm14, %xmm2, %xmm1
	vmovaps	%xmm1, 912(%rsp)        # 16-byte Spill
	vmovaps	1344(%rsp), %xmm1       # 16-byte Reload
	vsubps	1104(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vmovaps	%xmm0, 1776(%rsp)       # 16-byte Spill
	vshufps	$221, %xmm0, %xmm5, %xmm2 # xmm2 = xmm5[1,3],xmm0[1,3]
	vmovaps	%xmm5, 1344(%rsp)       # 16-byte Spill
	vmaxps	%xmm14, %xmm6, %xmm13
	vmovaps	1360(%rsp), %xmm0       # 16-byte Reload
	vsubps	1760(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vshufps	$221, %xmm3, %xmm10, %xmm15 # xmm15 = xmm10[1,3],xmm3[1,3]
	vmovaps	%xmm3, 1760(%rsp)       # 16-byte Spill
	vmaxps	%xmm14, %xmm12, %xmm9
	vmovaps	1536(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm8, %xmm4
	movl	1172(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	1744(%rsp), %r8         # 8-byte Reload
	orl	%r8d, %eax
	testb	$1, %al
	movl	1232(%rsp), %r14d       # 4-byte Reload
	vmovaps	%xmm2, %xmm12
	je	.LBB162_106
# BB#107:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm1, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm7, 688(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 1104(%rsp)      # 16-byte Spill
	vmovaps	1648(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB162_108
	.align	16, 0x90
.LBB162_106:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm10, 1104(%rsp)      # 16-byte Spill
	vmovaps	992(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 32(%rbx,%r12,4), %xmm0, %xmm8 # xmm8 = xmm0[0,2],mem[0,2]
	vmovaps	1376(%rsp), %xmm3       # 16-byte Reload
	vmulps	1632(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	movq	1568(%rsp), %rax        # 8-byte Reload
	vmovaps	1008(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm1, %xmm10
	vmovaps	%xmm10, 672(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, %xmm1
	vmovaps	%xmm1, 688(%rsp)        # 16-byte Spill
	vshufps	$136, 24632(%r15,%rax,4), %xmm0, %xmm7 # xmm7 = xmm0[0,2],mem[0,2]
	vmovaps	1824(%rsp), %xmm2       # 16-byte Reload
	vsubps	%xmm2, %xmm7, %xmm7
	vmovaps	1648(%rsp), %xmm11      # 16-byte Reload
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmovaps	1840(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm6, %xmm6
	vmaxps	%xmm14, %xmm6, %xmm6
	vsubps	%xmm8, %xmm6, %xmm8
	vmulps	1616(%rsp), %xmm3, %xmm6 # 16-byte Folded Reload
	movq	1552(%rsp), %rax        # 8-byte Reload
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 24632(%r15,%rax,4), %xmm3, %xmm7 # xmm7 = xmm3[0,2],mem[0,2]
	vsubps	%xmm2, %xmm7, %xmm7
	vmulps	%xmm7, %xmm11, %xmm7
	vmulps	%xmm7, %xmm6, %xmm6
	vmovaps	1024(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 32(%rbx,%r13,4), %xmm2, %xmm7 # xmm7 = xmm2[0,2],mem[0,2]
	vminps	%xmm0, %xmm6, %xmm6
	vmaxps	%xmm14, %xmm6, %xmm6
	vsubps	%xmm7, %xmm6, %xmm6
	vaddps	%xmm10, %xmm4, %xmm7
	vaddps	%xmm7, %xmm1, %xmm7
	vaddps	%xmm6, %xmm7, %xmm6
	vaddps	%xmm6, %xmm5, %xmm6
	vaddps	%xmm6, %xmm8, %xmm2
	vmulps	1248(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
.LBB162_108:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	1792(%rsp), %xmm3       # 16-byte Reload
	vmovaps	912(%rsp), %xmm0        # 16-byte Reload
	vmovaps	896(%rsp), %xmm1        # 16-byte Reload
	vmovaps	880(%rsp), %xmm2        # 16-byte Reload
	vmovaps	864(%rsp), %xmm6        # 16-byte Reload
	vsubps	%xmm1, %xmm2, %xmm2
	vsubps	%xmm6, %xmm0, %xmm14
	vsubps	%xmm12, %xmm13, %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vsubps	%xmm15, %xmm9, %xmm12
	testl	%ecx, %r11d
	jne	.LBB162_109
# BB#110:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm12, 880(%rsp)       # 16-byte Spill
	vmovaps	%xmm14, 896(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 912(%rsp)        # 16-byte Spill
	vmovaps	1824(%rsp), %xmm3       # 16-byte Reload
	vmovaps	1584(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1600(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1840(%rsp), %xmm4       # 16-byte Reload
	vxorps	%xmm6, %xmm6, %xmm6
	vmovaps	1776(%rsp), %xmm15      # 16-byte Reload
	vmovaps	1760(%rsp), %xmm13      # 16-byte Reload
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm2        # 16-byte Reload
	vmovaps	944(%rsp), %xmm12       # 16-byte Reload
	vmovaps	928(%rsp), %xmm10       # 16-byte Reload
	jmp	.LBB162_111
	.align	16, 0x90
.LBB162_109:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm4, 656(%rsp)        # 16-byte Spill
	vmovaps	%xmm5, 864(%rsp)        # 16-byte Spill
	vmovaps	1600(%rsp), %xmm9       # 16-byte Reload
	vmovaps	%xmm12, 880(%rsp)       # 16-byte Spill
	vmovaps	640(%rsp), %xmm8        # 16-byte Reload
	vmulps	%xmm8, %xmm9, %xmm0
	vmovaps	%xmm14, 896(%rsp)       # 16-byte Spill
	vshufps	$221, 1456(%rsp), %xmm3, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm3[1,3],mem[1,3]
	vmovaps	%xmm2, %xmm5
	vmovaps	%xmm5, 912(%rsp)        # 16-byte Spill
	vmovaps	1824(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1776(%rsp), %xmm15      # 16-byte Reload
	vshufps	$221, 1472(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[1,3],mem[1,3]
	vmovaps	1840(%rsp), %xmm4       # 16-byte Reload
	vminps	%xmm4, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	1584(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm8, %xmm7, %xmm1
	vmovaps	944(%rsp), %xmm8        # 16-byte Reload
	vmovaps	1488(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, %xmm8, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm8[1,3]
	vsubps	%xmm3, %xmm2, %xmm2
	vmulps	%xmm2, %xmm11, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1760(%rsp), %xmm13      # 16-byte Reload
	vmovaps	928(%rsp), %xmm10       # 16-byte Reload
	vshufps	$221, %xmm10, %xmm13, %xmm2 # xmm2 = xmm13[1,3],xmm10[1,3]
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm12, %xmm14, %xmm2
	vmovaps	%xmm8, %xmm12
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	1360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmovaps	%xmm7, %xmm14
	vmovaps	%xmm9, %xmm5
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1712(%rsp)       # 16-byte Spill
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm2        # 16-byte Reload
.LBB162_111:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1072(%rsp), %xmm7       # 16-byte Reload
	vmovaps	%xmm15, 1776(%rsp)      # 16-byte Spill
	vshufps	$136, %xmm7, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm7[0,2]
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmovaps	1120(%rsp), %xmm8       # 16-byte Reload
	vmulps	%xmm14, %xmm8, %xmm2
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm9
	vmovaps	1376(%rsp), %xmm7       # 16-byte Reload
	vmulps	%xmm14, %xmm7, %xmm0
	vmovaps	1488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm12, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm12[0,2]
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, %xmm10, %xmm13, %xmm1 # xmm1 = xmm13[0,2],xmm10[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm12
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1392(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1440(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1424(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	%xmm5, %xmm8, %xmm2
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm4, %xmm1, %xmm1
	vmaxps	%xmm6, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm2
	vmulps	%xmm5, %xmm7, %xmm0
	vmovaps	1792(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1456(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm3, %xmm1, %xmm1
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1472(%rsp), %xmm15, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm15[0,2],mem[0,2]
	vminps	%xmm4, %xmm0, %xmm0
	vmaxps	%xmm6, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm1
	andl	$1, %ecx
	vmovaps	%xmm3, %xmm8
	je	.LBB162_112
# BB#113:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 1440(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1472(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, %xmm10
	vmovaps	%xmm13, 1760(%rsp)      # 16-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	1712(%rsp), %xmm15      # 16-byte Reload
	jmp	.LBB162_114
	.align	16, 0x90
.LBB162_112:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm5, %xmm10
	vmovaps	%xmm13, 1760(%rsp)      # 16-byte Spill
	vmovaps	%xmm2, %xmm0
	vmovaps	%xmm0, 1440(%rsp)       # 16-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%xmm1, 1456(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm12, %xmm1
	vmovaps	%xmm12, 1472(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm1
	vaddps	%xmm1, %xmm9, %xmm1
	vmulps	160(%rsp), %xmm1, %xmm15 # 16-byte Folded Reload
.LBB162_114:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovdqa	1056(%rsp), %xmm5       # 16-byte Reload
	vmovaps	%xmm4, 1840(%rsp)       # 16-byte Spill
	testl	%r11d, %r11d
	jne	.LBB162_116
# BB#115:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	1712(%rsp), %xmm15      # 16-byte Reload
.LBB162_116:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	movq	208(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vmovdqa	.LCPI162_2(%rip), %xmm4 # xmm4 = [0,2,4,6]
	vpaddd	%xmm4, %xmm1, %xmm1
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	1176(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm1, %eax
	cltd
	idivl	1184(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	movl	%r14d, %esi
	andl	$1, %esi
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	1200(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	vmovd	%edi, %xmm3
	vpinsrd	$1, %ecx, %xmm3, %xmm3
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	1216(%rsp)              # 4-byte Folded Reload
	vpinsrd	$2, %ebp, %xmm3, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm3
	vpand	1264(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm3, %xmm1
	vmovdqa	352(%rsp), %xmm0        # 16-byte Reload
	vpcmpgtd	%xmm5, %xmm0, %xmm3
	movq	464(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r10), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %xmm5
	vpaddd	%xmm4, %xmm5, %xmm5
	vmovdqa	%xmm4, %xmm12
	vmovdqa	1328(%rsp), %xmm4       # 16-byte Reload
	vpminsd	%xmm4, %xmm5, %xmm5
	vmovdqa	1312(%rsp), %xmm0       # 16-byte Reload
	vpmaxsd	%xmm0, %xmm5, %xmm5
	vmovdqa	1296(%rsp), %xmm6       # 16-byte Reload
	vpcmpgtd	%xmm1, %xmm6, %xmm6
	vmovdqa	1280(%rsp), %xmm7       # 16-byte Reload
	vpsubd	%xmm1, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm1, %xmm7, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vpminsd	%xmm4, %xmm1, %xmm1
	vpmaxsd	%xmm0, %xmm1, %xmm1
	vblendvps	%xmm3, %xmm5, %xmm1, %xmm1
	vpmulld	1680(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	1664(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rdi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r9,%rdx,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r9,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rax,4), %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],mem[0]
	testl	%esi, %esi
	jne	.LBB162_117
# BB#118:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
	jmp	.LBB162_119
	.align	16, 0x90
.LBB162_117:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1504(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vmulps	816(%rsp), %xmm3, %xmm5 # 16-byte Folded Reload
	vmovaps	800(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1520(%rsp), %xmm0, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm0[0,2],mem[0,2]
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm5, %xmm5
	vminps	1840(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vmaxps	%xmm2, %xmm5, %xmm5
	vsubps	%xmm1, %xmm5, %xmm1
	vmovaps	656(%rsp), %xmm0        # 16-byte Reload
	vaddps	688(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vaddps	672(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	864(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm5, %xmm1
	vmulps	176(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
.LBB162_119:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	832(%rsp), %xmm2        # 16-byte Reload
	testl	%r11d, %r11d
	je	.LBB162_121
# BB#120:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
.LBB162_121:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	movl	%r14d, %eax
	orl	%r8d, %eax
	testb	$1, %al
	jne	.LBB162_123
# BB#122:                               #   in Loop: Header=BB162_100 Depth=1
	vmovups	(%rbx,%r12,4), %xmm1
	vshufps	$221, 992(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	movq	1568(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%r15,%rax,4), %xmm5
	vshufps	$221, 1008(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	vmovaps	1808(%rsp), %xmm7       # 16-byte Reload
	vmulps	1632(%rsp), %xmm7, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm6, %xmm5
	vmovaps	1840(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm5, %xmm5
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm5, %xmm5
	vsubps	%xmm1, %xmm5, %xmm1
	vmovups	(%rbx,%r13,4), %xmm5
	vshufps	$221, 1024(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm5[1,3],mem[1,3]
	movq	1552(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%r15,%rax,4), %xmm6
	vshufps	$221, 1040(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[1,3],mem[1,3]
	vmulps	1616(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vsubps	%xmm8, %xmm6, %xmm6
	vmulps	%xmm6, %xmm11, %xmm6
	vmulps	%xmm6, %xmm7, %xmm6
	vminps	%xmm0, %xmm6, %xmm6
	vmaxps	%xmm4, %xmm6, %xmm6
	vsubps	%xmm5, %xmm6, %xmm5
	vaddps	736(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vaddps	%xmm5, %xmm1, %xmm1
	vaddps	720(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	704(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vmulps	1248(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
.LBB162_123:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	1472(%rsp), %xmm7       # 16-byte Reload
	testl	%r14d, %r11d
	je	.LBB162_125
# BB#124:                               #   in Loop: Header=BB162_100 Depth=1
	vmovaps	752(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[0,2],mem[0,2]
	vsubps	%xmm8, %xmm1, %xmm1
	vmovaps	768(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1792(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[0,2],mem[0,2]
	vmulps	%xmm14, %xmm3, %xmm6
	vmulps	%xmm1, %xmm11, %xmm1
	vmulps	%xmm1, %xmm6, %xmm1
	vsubps	%xmm8, %xmm5, %xmm5
	vmulps	%xmm10, %xmm3, %xmm3
	vmulps	%xmm5, %xmm11, %xmm5
	vmulps	%xmm5, %xmm3, %xmm3
	vmovaps	1104(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1760(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[0,2],mem[0,2]
	vmovaps	1840(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm0, %xmm1, %xmm1
	vpxor	%xmm4, %xmm4, %xmm4
	vmaxps	%xmm4, %xmm1, %xmm1
	vsubps	%xmm5, %xmm1, %xmm1
	vmovaps	1344(%rsp), %xmm2       # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm2, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm2[0,2],mem[0,2]
	vminps	%xmm0, %xmm3, %xmm3
	vmaxps	%xmm4, %xmm3, %xmm3
	vaddps	%xmm1, %xmm9, %xmm1
	vsubps	%xmm5, %xmm3, %xmm3
	vaddps	%xmm1, %xmm7, %xmm1
	vaddps	%xmm1, %xmm3, %xmm1
	vaddps	1440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vaddps	1456(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
	vmulps	1248(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
.LBB162_125:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	848(%rsp), %xmm1        # 16-byte Reload
	vmovaps	1536(%rsp), %xmm3       # 16-byte Reload
	movl	%r14d, %eax
	andl	$1, %eax
	je	.LBB162_126
# BB#127:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm14, 1584(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, %xmm0
	jmp	.LBB162_128
	.align	16, 0x90
.LBB162_126:                            #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm14, 1584(%rsp)      # 16-byte Spill
	vmovaps	896(%rsp), %xmm0        # 16-byte Reload
	vaddps	912(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1360(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	160(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
.LBB162_128:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm10, 1600(%rsp)      # 16-byte Spill
	vmovaps	%xmm8, 1824(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 1648(%rsp)      # 16-byte Spill
	testl	%r11d, %r11d
	jne	.LBB162_130
# BB#129:                               # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vmovaps	%xmm5, %xmm0
.LBB162_130:                            # %for f8.s0.v10.v1012
                                        #   in Loop: Header=BB162_100 Depth=1
	vaddps	%xmm0, %xmm3, %xmm0
	vaddps	%xmm15, %xmm1, %xmm1
	vmovaps	.LCPI162_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI162_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r14d, %rax
	movq	608(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r10d
	addl	$-1, 584(%rsp)          # 4-byte Folded Spill
	jne	.LBB162_100
	jmp	.LBB162_160
.LBB162_39:                             # %false_bb2
	movq	%rbp, 8(%rsp)           # 8-byte Spill
	addl	$39, %edi
	sarl	$3, %edi
	movq	%rdi, 1712(%rsp)        # 8-byte Spill
	testl	%edi, %edi
	jle	.LBB162_160
# BB#40:                                # %for f8.s0.v10.v1016.preheader
	movq	1744(%rsp), %r14        # 8-byte Reload
	movl	%r14d, %r12d
	subl	%r8d, %r12d
	movq	%r12, 1680(%rsp)        # 8-byte Spill
	leal	1(%r12), %eax
	leal	(%rcx,%rcx), %ebp
	movl	%ebp, 1824(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebp
	movl	%edx, %edi
	leal	-1(%r12), %eax
	cltd
	idivl	%ebp
	movl	%ebp, %eax
	negl	%eax
	movl	%ecx, %r10d
	sarl	$31, %r10d
	movq	%rcx, 1840(%rsp)        # 8-byte Spill
	andnl	%ebp, %r10d, %esi
	andl	%eax, %r10d
	orl	%esi, %r10d
	movl	%r10d, 1760(%rsp)       # 4-byte Spill
	movl	%edi, %esi
	sarl	$31, %esi
	andl	%r10d, %esi
	addl	%edi, %esi
	movl	%edx, %edi
	sarl	$31, %edi
	andl	%r10d, %edi
	addl	%edx, %edi
	leal	(%r8,%rcx), %r15d
	cmpl	%r14d, %r15d
	movl	%r15d, %eax
	cmovgl	%r14d, %eax
	addl	$-1, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	leal	-1(%rcx,%rcx), %r11d
	movq	%r9, 1752(%rsp)         # 8-byte Spill
	movl	%r11d, %edx
	subl	%edi, %edx
	cmpl	%edi, %ecx
	cmovgl	%edi, %edx
	addl	%r8d, %edx
	leal	-1(%r8,%rcx), %r13d
	movq	%rcx, %rbx
	cmpl	%edx, %r13d
	cmovlel	%r13d, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%edx, %ecx
	leal	1(%r14), %edi
	movq	%rdi, 584(%rsp)         # 8-byte Spill
	cmpl	%edi, %r13d
	movl	%r13d, %edx
	cmovgl	%edi, %edx
	cmpl	%r8d, %edx
	cmovll	%r8d, %edx
	movl	%r11d, %edi
	subl	%esi, %edi
	cmpl	%esi, %ebx
	movq	%rbx, %rbp
	cmovgl	%esi, %edi
	addl	%r8d, %edi
	cmpl	%edi, %r13d
	cmovlel	%r13d, %edi
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	movq	%r14, %rbx
	cmpl	%ebx, %r13d
	cmovgl	%edx, %edi
	movl	%edi, 1776(%rsp)        # 4-byte Spill
	movl	%r13d, %edi
	cmovgl	%ebx, %edi
	cmpl	%ebx, %r15d
	cmovgel	%eax, %ecx
	movl	%ecx, 1808(%rsp)        # 4-byte Spill
	movl	%r12d, %eax
	cltd
	idivl	1824(%rsp)              # 4-byte Folded Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	addl	%edx, %eax
	cmpl	%r8d, %edi
	cmovll	%r8d, %edi
	movl	%r11d, %esi
	subl	%eax, %esi
	cmpl	%eax, %ebp
	cmovgl	%eax, %esi
	addl	%r8d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	cmpl	%ebx, %r15d
	movq	%rbx, %r12
	cmovgl	%edi, %esi
	movl	$2, %eax
	movq	32(%rsp), %r15          # 8-byte Reload
	subl	%r15d, %eax
	movq	8(%rsp), %r10           # 8-byte Reload
	leal	(%r10,%r10), %ecx
	cltd
	idivl	%ecx
	movl	%r10d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	negl	%ecx
	andl	%eax, %ecx
	orl	%edi, %ecx
	movl	%edx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	addl	%edx, %eax
	leal	-1(%r10,%r10), %r9d
	subl	%eax, %r9d
	cmpl	%eax, %r10d
	cmovgl	%eax, %r9d
	leal	(%r15,%r10), %eax
	leal	-1(%r15,%r10), %r10d
	addl	%r15d, %r9d
	cmpl	%r9d, %r10d
	cmovlel	%r10d, %r9d
	cmpl	%r15d, %r9d
	cmovll	%r15d, %r9d
	cmpl	$3, %eax
	movl	$2, %ebx
	cmovgel	%ebx, %r10d
	cmpl	%r15d, %r10d
	cmovll	%r15d, %r10d
	cmpl	$3, %eax
	cmovll	%r9d, %r10d
	movl	$2, %eax
	movq	24(%rsp), %rdi          # 8-byte Reload
	subl	%edi, %eax
	movq	16(%rsp), %r14          # 8-byte Reload
	leal	(%r14,%r14), %ecx
	cltd
	idivl	%ecx
	movl	%r14d, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %ebp
	negl	%ecx
	andl	%eax, %ecx
	orl	%ebp, %ecx
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%ecx, %ebp
	addl	%edx, %ebp
	leal	-1(%r14,%r14), %eax
	subl	%ebp, %eax
	cmpl	%ebp, %r14d
	cmovgl	%ebp, %eax
	leal	(%rdi,%r14), %ecx
	leal	-1(%rdi,%r14), %edx
	addl	%edi, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cmpl	%edi, %eax
	cmovll	%edi, %eax
	cmpl	$3, %ecx
	cmovll	%edx, %ebx
	cmpl	%edi, %ebx
	cmovll	%edi, %ebx
	cmpl	$3, %ecx
	cmovll	%eax, %ebx
	movq	128(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andl	%edx, %ecx
	movq	%rcx, 576(%rsp)         # 8-byte Spill
	movl	%r12d, %ecx
	andl	$1, %ecx
	movl	%ecx, 568(%rsp)         # 4-byte Spill
	movl	%r12d, %ecx
	andl	$63, %ecx
	movq	%rcx, 1792(%rsp)        # 8-byte Spill
	cmpl	$2, %edi
	cmovgl	%eax, %ebx
	movq	560(%rsp), %rax         # 8-byte Reload
	movl	%eax, %ebp
	movq	272(%rsp), %r14         # 8-byte Reload
	imull	%r14d, %ebp
	addl	%edi, %ebp
	cmpl	$2, %r15d
	cmovgl	%r9d, %r10d
	movq	1680(%rsp), %rdi        # 8-byte Reload
	leal	-2(%rdi), %eax
	cltd
	movl	1824(%rsp), %r9d        # 4-byte Reload
	idivl	%r9d
	movl	%edx, %r12d
	movq	%rdi, %rax
	addl	$2, %eax
	cltd
	idivl	%r9d
	movq	136(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%rcx), %eax
	vmovd	%eax, %xmm2
	vmovd	%ebp, %xmm0
	vmovd	%ebp, %xmm5
	movq	560(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm6
	movq	40(%rsp), %r9           # 8-byte Reload
	movl	%r9d, %eax
	imull	%r8d, %eax
	leal	-1(%r14,%rcx), %edi
	vmovd	%edi, %xmm12
	leal	-1(%r14), %edi
	vmovd	%edi, %xmm11
	addl	%r15d, %eax
	movl	%r12d, %edi
	sarl	$31, %edi
	movl	1760(%rsp), %r15d       # 4-byte Reload
	andl	%r15d, %edi
	addl	%r12d, %edi
	movl	%edx, %ebp
	sarl	$31, %ebp
	andl	%r15d, %ebp
	addl	%edx, %ebp
	cltq
	movslq	%r10d, %r15
	subq	%rax, %r15
	movl	96(%rsp), %r14d         # 4-byte Reload
	andl	$-32, %r14d
	movslq	%esi, %rax
	imulq	%r9, %rax
	movq	%rax, 1680(%rsp)        # 8-byte Spill
	movl	%r14d, %eax
	shll	$5, %eax
	movq	%rax, 1824(%rsp)        # 8-byte Spill
	movq	1744(%rsp), %rcx        # 8-byte Reload
	leal	2(%rcx), %edx
	cmpl	%edx, %r13d
	movl	%r13d, %eax
	cmovgl	%edx, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	movl	%r11d, %esi
	subl	%ebp, %esi
	movq	1840(%rsp), %r10        # 8-byte Reload
	cmpl	%ebp, %r10d
	cmovgl	%ebp, %esi
	addl	%r8d, %esi
	cmpl	%esi, %r13d
	cmovlel	%r13d, %esi
	cmpl	%r8d, %esi
	cmovll	%r8d, %esi
	leal	-2(%r8,%r10), %ebp
	cmpl	%ecx, %ebp
	cmovgl	%eax, %esi
	movslq	%esi, %rax
	imulq	%r9, %rax
	movq	%rax, 1760(%rsp)        # 8-byte Spill
	vmovd	%ebx, %xmm1
	vmovd	%ebx, %xmm13
	leal	-2(%rcx), %ebp
	cmpl	%ebp, %r13d
	movl	%r13d, %eax
	cmovgl	%ebp, %eax
	cmpl	%r8d, %eax
	cmovll	%r8d, %eax
	subl	%edi, %r11d
	cmpl	%edi, %r10d
	cmovgl	%edi, %r11d
	addl	%r8d, %r11d
	cmpl	%r11d, %r13d
	cmovlel	%r13d, %r11d
	leal	2(%r8,%r10), %edi
	cmpl	%r8d, %r11d
	cmovll	%r8d, %r11d
	cmpl	%ecx, %edi
	movq	%rcx, %rdi
	cmovgl	%eax, %r11d
	movslq	%r11d, %r13
	movq	1736(%rsp), %r11        # 8-byte Reload
	imulq	%r9, %r13
	movq	136(%rsp), %rsi         # 8-byte Reload
	movq	272(%rsp), %r8          # 8-byte Reload
	leal	1(%r8,%rsi), %eax
	vmovd	%eax, %xmm14
	movslq	1808(%rsp), %rbx        # 4-byte Folded Reload
	movq	1752(%rsp), %r10        # 8-byte Reload
	imulq	%r9, %rbx
	leal	1(%r8), %eax
	vmovaps	%xmm8, %xmm15
	vmovd	%eax, %xmm10
	movslq	1776(%rsp), %r12        # 4-byte Folded Reload
	imulq	%r9, %r12
	leal	2(%r8,%rsi), %eax
	vmovd	%eax, %xmm9
	vpsubd	%xmm0, %xmm1, %xmm8
	vmovss	.LCPI162_1(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vmovss	60(%rsp), %xmm4         # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vsubss	%xmm4, %xmm1, %xmm1
	vmovss	112(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmulss	%xmm0, %xmm1, %xmm3
	vdivss	%xmm15, %xmm3, %xmm3
	movq	832(%rsp), %rax         # 8-byte Reload
	addl	$3, %eax
	movq	584(%rsp), %rsi         # 8-byte Reload
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 584(%rsp)         # 8-byte Spill
	movq	%rdi, %r9
	leal	63(%r9), %esi
	andl	$63, %esi
	imull	%eax, %esi
	movq	%rsi, 544(%rsp)         # 8-byte Spill
	andl	$63, %ebp
	imull	%eax, %ebp
	movq	%rbp, 552(%rsp)         # 8-byte Spill
	andl	$63, %edx
	imull	%eax, %edx
	movq	%rdx, 560(%rsp)         # 8-byte Spill
	vaddss	%xmm3, %xmm4, %xmm3
	movq	1792(%rsp), %rdi        # 8-byte Reload
	imull	%edi, %eax
	movq	%rax, 832(%rsp)         # 8-byte Spill
	leal	2(%r8), %eax
	vmovd	%eax, %xmm4
	movq	48(%rsp), %rax          # 8-byte Reload
	addq	$32, %rax
	imulq	%rax, %rdi
	movq	1680(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r15), %rax
	movq	%rax, 1808(%rsp)        # 8-byte Spill
	movq	1760(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r15), %rax
	movq	%rax, 1840(%rsp)        # 8-byte Spill
	movq	%r9, %rax
	leal	9(%rax), %edx
	movl	64(%rsp), %esi          # 4-byte Reload
	subl	%esi, %edx
	addl	$64, %r14d
	imull	%r14d, %edx
	leaq	(%r13,%r15), %rbp
	leaq	(%rbx,%r15), %r9
	leal	7(%rax), %ebx
	subl	%esi, %ebx
	imull	%r14d, %ebx
	leal	6(%rax), %r13d
	subl	%esi, %r13d
	imull	%r14d, %r13d
	addq	%r15, %r12
	leal	10(%rax), %r15d
	subl	%esi, %r15d
	leal	8(%rax), %eax
	subl	%esi, %eax
	imull	%r14d, %r15d
	imull	%r14d, %eax
	movq	128(%rsp), %rsi         # 8-byte Reload
	movq	%rsi, %rcx
	sarq	$63, %rcx
	andq	%rsi, %rcx
	subq	%rcx, %rdi
	movq	%rdi, 536(%rsp)         # 8-byte Spill
	movq	1824(%rsp), %rcx        # 8-byte Reload
	leal	(%rcx,%rcx,2), %ecx
	addl	%ecx, %edx
	movq	%rdx, 528(%rsp)         # 8-byte Spill
	addl	%ecx, %ebx
	movq	%rbx, 520(%rsp)         # 8-byte Spill
	addl	%ecx, %r13d
	movq	%r13, 512(%rsp)         # 8-byte Spill
	addl	%ecx, %r15d
	movq	%r15, 504(%rsp)         # 8-byte Spill
	addl	%ecx, %eax
	movq	%rax, 496(%rsp)         # 8-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vmovdqa	%xmm2, 480(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm5, %xmm5
	vmovaps	%xmm5, 464(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm6, %xmm5
	vmovaps	%xmm5, 1616(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm12, %xmm12
	vmovdqa	%xmm12, 1280(%rsp)      # 16-byte Spill
	vmovss	120(%rsp), %xmm5        # 4-byte Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vsubss	%xmm0, %xmm5, %xmm5
	vmovdqa	.LCPI162_0(%rip), %xmm6 # xmm6 = [0,4294967294,4294967292,4294967290]
	vpbroadcastd	%xmm11, %xmm7
	vpaddd	%xmm6, %xmm7, %xmm0
	vmovdqa	%xmm0, 448(%rsp)        # 16-byte Spill
	vmulss	%xmm5, %xmm1, %xmm1
	vmovd	%r8d, %xmm5
	vpbroadcastd	%xmm5, %xmm7
	vmovdqa	%xmm7, 1264(%rsp)       # 16-byte Spill
	vdivss	%xmm1, %xmm15, %xmm1
	movq	136(%rsp), %rax         # 8-byte Reload
	vmovd	%eax, %xmm5
	vbroadcastss	%xmm5, %xmm0
	vmovaps	%xmm0, 1248(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm13, %xmm0
	vmovaps	%xmm0, 432(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm14, %xmm5
	vpaddd	%xmm6, %xmm5, %xmm0
	vmovdqa	%xmm0, 416(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm10, %xmm5
	vpaddd	%xmm6, %xmm5, %xmm0
	vmovdqa	%xmm0, 400(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm8, %xmm0
	vmovdqa	%xmm0, 1600(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm9, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 384(%rsp)        # 16-byte Spill
	vpbroadcastd	%xmm4, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 368(%rsp)        # 16-byte Spill
	leal	(%r8,%rax), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 352(%rsp)        # 16-byte Spill
	leal	-2(%r8,%rax), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 336(%rsp)        # 16-byte Spill
	leal	-2(%r8), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 320(%rsp)        # 16-byte Spill
	leal	-3(%r8,%rax), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 304(%rsp)        # 16-byte Spill
	movq	576(%rsp), %rdx         # 8-byte Reload
	movl	%edx, %eax
	subl	%r8d, %eax
	movq	%rax, 288(%rsp)         # 8-byte Spill
	leal	-3(%r8), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm0, 272(%rsp)        # 16-byte Spill
	vpaddd	%xmm6, %xmm12, %xmm0
	vmovdqa	%xmm0, 256(%rsp)        # 16-byte Spill
	vpaddd	%xmm6, %xmm7, %xmm0
	vmovdqa	%xmm0, 240(%rsp)        # 16-byte Spill
	vbroadcastss	%xmm1, %xmm0
	vmovaps	%xmm0, 1584(%rsp)       # 16-byte Spill
	vbroadcastss	%xmm3, %xmm0
	vmovaps	%xmm0, 1824(%rsp)       # 16-byte Spill
	leal	3(%rdx), %ecx
	movq	%rcx, 224(%rsp)         # 8-byte Spill
	leal	2(%rdx), %ecx
	movq	%rcx, 208(%rsp)         # 8-byte Spill
	leal	-2(%rdx), %ecx
	movq	%rcx, 192(%rsp)         # 8-byte Spill
	leal	-1(%rdx), %ecx
	movq	%rcx, 176(%rsp)         # 8-byte Spill
	leal	1(%rdx), %ecx
	movq	%rcx, 160(%rsp)         # 8-byte Spill
	leal	3(%rax), %ecx
	movq	%rcx, 144(%rsp)         # 8-byte Spill
	leal	2(%rax), %ecx
	movq	%rcx, 136(%rsp)         # 8-byte Spill
	leal	-2(%rax), %ecx
	movq	%rcx, 128(%rsp)         # 8-byte Spill
	leal	-1(%rax), %ecx
	movq	%rcx, 120(%rsp)         # 8-byte Spill
	leal	1(%rax), %eax
	movq	%rax, 112(%rsp)         # 8-byte Spill
	xorl	%r13d, %r13d
	movq	80(%rsp), %rcx          # 8-byte Reload
	movq	1808(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 816(%rsp)        # 16-byte Spill
	movq	1840(%rsp), %rax        # 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %xmm0
	vmovaps	%xmm0, 1568(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%rbp,4), %xmm0
	vmovaps	%xmm0, 1552(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r9,4), %xmm0
	vmovaps	%xmm0, 1536(%rsp)       # 16-byte Spill
	vbroadcastss	(%rcx,%r12,4), %xmm0
	vmovaps	%xmm0, 1520(%rsp)       # 16-byte Spill
	vpabsd	%xmm2, %xmm0
	vmovdqa	%xmm0, 1232(%rsp)       # 16-byte Spill
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa	%xmm0, 1216(%rsp)       # 16-byte Spill
	vmovdqa	.LCPI162_2(%rip), %xmm8 # xmm8 = [0,2,4,6]
	vbroadcastss	.LCPI162_3(%rip), %xmm0
	vmovaps	%xmm0, 1808(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_4(%rip), %xmm0
	vmovaps	%xmm0, 96(%rsp)         # 16-byte Spill
	vbroadcastss	.LCPI162_5(%rip), %xmm0
	vmovaps	%xmm0, 1200(%rsp)       # 16-byte Spill
	vbroadcastss	.LCPI162_6(%rip), %xmm0
	vmovaps	%xmm0, 80(%rsp)         # 16-byte Spill
	.align	16, 0x90
.LBB162_41:                             # %for f8.s0.v10.v1016
                                        # =>This Inner Loop Header: Depth=1
	movq	832(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %r9
	vmovups	8(%r11,%r9,4), %xmm0
	vmovaps	%xmm0, 1328(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%r9,4), %xmm0
	vmovaps	%xmm0, 1424(%rsp)       # 16-byte Spill
	movq	112(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	%xmm8, %xmm11
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	vmovdqa	480(%rsp), %xmm2        # 16-byte Reload
	vpextrd	$1, %xmm2, %r14d
	movl	%r14d, 1176(%rsp)       # 4-byte Spill
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm2, %ebx
	movl	%ebx, 1172(%rsp)        # 4-byte Spill
	cltd
	idivl	%ebx
	movq	576(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %esi
	movl	%esi, 1184(%rsp)        # 4-byte Spill
	vmovd	%edx, %xmm1
	vpinsrd	$1, %ecx, %xmm1, %xmm1
	vpextrd	$2, %xmm0, %eax
	vpextrd	$2, %xmm2, %r8d
	movl	%r8d, 1160(%rsp)        # 4-byte Spill
	cltd
	idivl	%r8d
	vpinsrd	$2, %edx, %xmm1, %xmm1
	vpextrd	$3, %xmm0, %eax
	vpextrd	$3, %xmm2, %r15d
	movl	%r15d, 1136(%rsp)       # 4-byte Spill
	cltd
	idivl	%r15d
	vpinsrd	$3, %edx, %xmm1, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vmovdqa	1232(%rsp), %xmm14      # 16-byte Reload
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %xmm9
	vmovdqa	%xmm9, 1008(%rsp)       # 16-byte Spill
	vmovdqa	256(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpcmpeqd	%xmm5, %xmm5, %xmm5
	vmovdqa	448(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vmovdqa	1248(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm2
	vmovdqa	%xmm3, %xmm8
	vmovdqa	1216(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vmovdqa	1264(%rsp), %xmm6       # 16-byte Reload
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	1280(%rsp), %xmm13      # 16-byte Reload
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm6, %xmm0, %xmm0
	movq	160(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movl	%eax, 1504(%rsp)        # 4-byte Spill
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm6, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vmovdqa	1616(%rsp), %xmm1       # 16-byte Reload
	vpmulld	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm1, %xmm7
	vpsubd	464(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	movq	1696(%rsp), %rsi        # 8-byte Reload
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebp
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r15d
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	416(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm1, %xmm1
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	400(%rsp), %xmm2        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm2, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpcmpgtd	%xmm0, %xmm8, %xmm2
	vpsubd	%xmm0, %xmm4, %xmm3
	vblendvps	%xmm2, %xmm0, %xmm3, %xmm0
	vpaddd	%xmm6, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vpmaxsd	%xmm6, %xmm0, %xmm0
	movq	176(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm6, %xmm2, %xmm2
	vblendvps	%xmm1, %xmm0, %xmm2, %xmm0
	vpmulld	%xmm7, %xmm0, %xmm0
	vpaddd	1600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	sarq	$32, %rcx
	movslq	%eax, %rdi
	sarq	$32, %rax
	vmovss	(%rsi,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 1792(%rsp)       # 16-byte Spill
	movq	288(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1632(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 1104(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 1088(%rsp)        # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, 1072(%rsp)        # 4-byte Spill
	movq	136(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpaddd	%xmm11, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r14d
	movl	%edx, 1056(%rsp)        # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%ebx
	movl	%edx, 1040(%rsp)        # 4-byte Spill
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebp
	movq	144(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm2
	movq	496(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	movslq	%eax, %rcx
	vmovups	24608(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	movq	560(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 1440(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1488(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	movq	504(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 1456(%rsp)        # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1360(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1024(%rsp)       # 16-byte Spill
	movq	552(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 1472(%rsp)        # 8-byte Spill
	vmovups	8(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vmovups	24(%r11,%rax,4), %xmm0
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	movq	512(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	cltq
	movq	%rax, 1120(%rsp)        # 8-byte Spill
	vmovups	24608(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1344(%rsp)       # 16-byte Spill
	vpbroadcastd	%xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpextrd	$1, %xmm2, %eax
	cltd
	vmovups	16(%r11,%r9,4), %xmm0
	vmovaps	%xmm0, 1840(%rsp)       # 16-byte Spill
	vmovups	32(%r11,%r9,4), %xmm0
	vmovaps	%xmm0, 1312(%rsp)       # 16-byte Spill
	vmovups	24616(%r10,%rcx,4), %xmm12
	vmovaps	%xmm12, 1776(%rsp)      # 16-byte Spill
	vmovups	24632(%r10,%rcx,4), %xmm0
	vmovaps	%xmm0, 1296(%rsp)       # 16-byte Spill
	vmovups	(%r11,%r9,4), %xmm10
	vmovaps	%xmm10, 784(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rcx,4), %xmm15
	vmovaps	%xmm15, 800(%rsp)       # 16-byte Spill
	idivl	%r14d
	movl	%edx, %ecx
	vmovd	%xmm2, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	vmovd	1104(%rsp), %xmm7       # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vpinsrd	$1, 1632(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$2, 1088(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpinsrd	$3, 1072(%rsp), %xmm7, %xmm7 # 4-byte Folded Reload
	vpsrad	$31, %xmm7, %xmm5
	vpand	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	vmovdqa	%xmm8, %xmm0
	vpcmpgtd	%xmm5, %xmm0, %xmm7
	vpsubd	%xmm5, %xmm4, %xmm1
	vblendvps	%xmm7, %xmm5, %xmm1, %xmm1
	vmovdqa	352(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm5
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vpxor	%xmm8, %xmm5, %xmm5
	vmovdqa	240(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm6, %xmm1, %xmm1
	vpminsd	%xmm13, %xmm1, %xmm1
	vpmaxsd	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm11, %xmm9, %xmm7
	vpminsd	%xmm13, %xmm7, %xmm7
	vpmaxsd	%xmm6, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm1, %xmm7, %xmm1
	vpextrd	$3, %xmm2, %eax
	vmovd	1040(%rsp), %xmm2       # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 1056(%rsp), %xmm2, %xmm2 # 4-byte Folded Reload
	vpinsrd	$2, %r12d, %xmm2, %xmm2
	vpinsrd	$3, %ebp, %xmm2, %xmm2
	cltd
	idivl	%r15d
	vpsrad	$31, %xmm2, %xmm5
	vpand	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm2
	vpcmpgtd	%xmm2, %xmm0, %xmm5
	vpsubd	%xmm2, %xmm4, %xmm7
	vblendvps	%xmm5, %xmm2, %xmm7, %xmm2
	vmovdqa	336(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpcmpeqd	%xmm8, %xmm8, %xmm8
	vmovdqa	320(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm3, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm6, %xmm2, %xmm2
	vpminsd	%xmm13, %xmm2, %xmm2
	vpmaxsd	%xmm6, %xmm2, %xmm2
	movq	208(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm11, %xmm7, %xmm7
	vpminsd	%xmm13, %xmm7, %xmm7
	vpmaxsd	%xmm6, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm2, %xmm7, %xmm2
	vmovd	%edi, %xmm5
	vpinsrd	$1, %ecx, %xmm5, %xmm5
	vpinsrd	$2, %ebx, %xmm5, %xmm5
	vpinsrd	$3, %edx, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm7
	vpand	%xmm14, %xmm7, %xmm7
	vpaddd	%xmm5, %xmm7, %xmm5
	vpcmpgtd	%xmm5, %xmm0, %xmm7
	vpsubd	%xmm5, %xmm4, %xmm3
	vblendvps	%xmm7, %xmm5, %xmm3, %xmm3
	vmovdqa	304(%rsp), %xmm5        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vmovdqa	272(%rsp), %xmm7        # 16-byte Reload
	vpcmpgtd	%xmm9, %xmm7, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpaddd	%xmm6, %xmm3, %xmm3
	vpminsd	%xmm13, %xmm3, %xmm3
	vpmaxsd	%xmm6, %xmm3, %xmm3
	movq	224(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %xmm7
	vpaddd	%xmm11, %xmm7, %xmm7
	vpminsd	%xmm13, %xmm7, %xmm7
	vpmaxsd	%xmm6, %xmm7, %xmm7
	vblendvps	%xmm5, %xmm3, %xmm7, %xmm6
	vmovdqa	1616(%rsp), %xmm8       # 16-byte Reload
	vpmulld	%xmm8, %xmm1, %xmm1
	vmovdqa	1600(%rsp), %xmm4       # 16-byte Reload
	vpaddd	%xmm1, %xmm4, %xmm1
	vpextrq	$1, %xmm1, %r14
	vpmulld	%xmm8, %xmm2, %xmm2
	vmovq	%xmm1, %r11
	vpaddd	%xmm2, %xmm4, %xmm1
	vmovdqa	%xmm4, %xmm11
	vpextrq	$1, %xmm1, %r10
	vmovq	%xmm1, %rax
	vshufps	$221, %xmm12, %xmm15, %xmm1 # xmm1 = xmm15[1,3],xmm12[1,3]
	vmovaps	1824(%rsp), %xmm7       # 16-byte Reload
	vsubps	%xmm7, %xmm1, %xmm1
	vmovaps	1584(%rsp), %xmm4       # 16-byte Reload
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	816(%rsp), %xmm3        # 16-byte Reload
	vmulps	1792(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1808(%rsp), %xmm13      # 16-byte Reload
	vminps	%xmm13, %xmm1, %xmm1
	vpxor	%xmm14, %xmm14, %xmm14
	vmaxps	%xmm14, %xmm1, %xmm1
	vshufps	$221, 1840(%rsp), %xmm10, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm10[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm15
	vmovaps	1680(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1344(%rsp), %xmm0, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm0[1,3],mem[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmovaps	1760(%rsp), %xmm0       # 16-byte Reload
	vmulps	1552(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm14, %xmm1, %xmm1
	vmovaps	1664(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1648(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vsubps	%xmm2, %xmm1, %xmm12
	vmovaps	1408(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm3, %xmm0, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1360(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1024(%rsp), %xmm10      # 16-byte Reload
	vshufps	$221, %xmm10, %xmm2, %xmm2 # xmm2 = xmm2[1,3],xmm10[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	1568(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
	vmulps	%xmm2, %xmm5, %xmm2
	vminps	%xmm13, %xmm2, %xmm2
	vmaxps	%xmm14, %xmm2, %xmm2
	vmovaps	1488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm0, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm0[1,3],mem[1,3]
	vsubps	%xmm5, %xmm2, %xmm14
	vmovaps	1328(%rsp), %xmm9       # 16-byte Reload
	vpmulld	%xmm8, %xmm6, %xmm2
	vpxor	%xmm8, %xmm8, %xmm8
	vmovaps	1424(%rsp), %xmm5       # 16-byte Reload
	vpaddd	%xmm2, %xmm11, %xmm2
	vpextrq	$1, %xmm2, %rcx
	vmovq	%xmm2, %rdx
	movslq	%edx, %rdi
	sarq	$32, %rdx
	vmovss	(%rsi,%rdi,4), %xmm2    # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movslq	%ecx, %rdx
	vinsertps	$32, (%rsi,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	sarq	$32, %rcx
	vinsertps	$48, (%rsi,%rcx,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	movq	%rsi, %r9
	movq	%r11, %rbx
	sarq	$32, %rbx
	movq	%r14, %rdi
	sarq	$32, %rdi
	movq	%rax, %rbp
	sarq	$32, %rbp
	movq	%r10, %rdx
	sarq	$32, %rdx
	movl	1504(%rsp), %ecx        # 4-byte Reload
	andl	$1, %ecx
	testl	%ecx, %ecx
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vshufps	$221, %xmm5, %xmm9, %xmm2 # xmm2 = xmm9[1,3],xmm5[1,3]
	vsubps	%xmm2, %xmm1, %xmm6
	movq	544(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %r8d
	movq	520(%rsp), %rcx         # 8-byte Reload
	leal	(%rcx,%r13), %ecx
	movq	584(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r13), %r15d
	movq	528(%rsp), %rsi         # 8-byte Reload
	leal	(%rsi,%r13), %r12d
	jne	.LBB162_42
# BB#131:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm0, 592(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 688(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 704(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, 720(%rsp)       # 16-byte Spill
	vmovaps	%xmm2, 848(%rsp)        # 16-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vxorps	%xmm12, %xmm12, %xmm12
	vmovaps	1776(%rsp), %xmm11      # 16-byte Reload
	jmp	.LBB162_132
	.align	16, 0x90
.LBB162_42:                             #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm2, 848(%rsp)        # 16-byte Spill
	vmulps	%xmm0, %xmm3, %xmm1
	vmovaps	%xmm0, 592(%rsp)        # 16-byte Spill
	vmovaps	1776(%rsp), %xmm11      # 16-byte Reload
	vshufps	$221, 1296(%rsp), %xmm11, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm11[1,3],mem[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1840(%rsp), %xmm2       # 16-byte Reload
	vshufps	$221, 1312(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[1,3],mem[1,3]
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm8, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	%xmm14, %xmm6, %xmm2
	vmovaps	%xmm6, 672(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 688(%rsp)       # 16-byte Spill
	vaddps	%xmm12, %xmm2, %xmm2
	vmovaps	%xmm12, 704(%rsp)       # 16-byte Spill
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm15, %xmm1
	vmovaps	%xmm15, 720(%rsp)       # 16-byte Spill
	vmulps	96(%rsp), %xmm1, %xmm0  # 16-byte Folded Reload
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vxorps	%xmm12, %xmm12, %xmm12
.LBB162_132:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vmovaps	1392(%rsp), %xmm2       # 16-byte Reload
	vmovaps	1376(%rsp), %xmm8       # 16-byte Reload
	vmovaps	1360(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1344(%rsp), %xmm15      # 16-byte Reload
	movslq	%r11d, %rsi
	vmovss	(%r9,%rsi,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%r14d, %rsi
	vinsertps	$32, (%r9,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdi,4), %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm6, 992(%rsp)        # 16-byte Spill
	vshufps	$136, %xmm5, %xmm9, %xmm1 # xmm1 = xmm9[0,2],xmm5[0,2]
	vmovaps	%xmm1, 1424(%rsp)       # 16-byte Spill
	cltq
	vmovss	(%r9,%rax,4), %xmm1     # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movslq	%r10d, %rax
	vinsertps	$32, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%rdx,4), %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm5, 1328(%rsp)       # 16-byte Spill
	movq	%r9, %r14
	vshufps	$136, %xmm2, %xmm0, %xmm1 # xmm1 = xmm0[0,2],xmm2[0,2]
	vmulps	%xmm3, %xmm6, %xmm2
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm2, %xmm0
	vmovaps	%xmm0, 1408(%rsp)       # 16-byte Spill
	vmovaps	1488(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, %xmm8, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm8[0,2]
	vmovaps	%xmm0, 1104(%rsp)       # 16-byte Spill
	vmulps	1568(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
	vshufps	$136, %xmm10, %xmm14, %xmm0 # xmm0 = xmm14[0,2],xmm10[0,2]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm0, %xmm1, %xmm9
	vmovaps	1664(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	%xmm0, 1648(%rsp)       # 16-byte Spill
	vmulps	1552(%rsp), %xmm6, %xmm0 # 16-byte Folded Reload
	vmovaps	1680(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, %xmm15, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm15[0,2]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm8
	vmulps	%xmm3, %xmm5, %xmm0
	vshufps	$136, 1296(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[0,2],mem[0,2]
	vmovaps	%xmm11, 1776(%rsp)      # 16-byte Spill
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm0, %xmm11
	movslq	%ecx, %rdx
	movq	1752(%rsp), %r10        # 8-byte Reload
	vmovups	24608(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 976(%rsp)        # 16-byte Spill
	movslq	%r12d, %rax
	vmovups	24608(%r10,%rax,4), %xmm1
	vmovaps	%xmm1, 1360(%rsp)       # 16-byte Spill
	vmovups	24624(%r10,%rax,4), %xmm2
	vmovaps	%xmm2, 1344(%rsp)       # 16-byte Spill
	vmovaps	1520(%rsp), %xmm5       # 16-byte Reload
	vmovaps	1760(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm3
	vshufps	$221, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[1,3],xmm2[1,3]
	vsubps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm1, %xmm4, %xmm1
	vmulps	%xmm1, %xmm3, %xmm1
	vmovups	24624(%r10,%rdx,4), %xmm2
	vmovaps	%xmm2, 960(%rsp)        # 16-byte Spill
	vmovaps	1536(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm3, %xmm6, %xmm6
	vshufps	$221, %xmm2, %xmm0, %xmm2 # xmm2 = xmm0[1,3],xmm2[1,3]
	vsubps	%xmm7, %xmm2, %xmm2
	vmulps	%xmm2, %xmm4, %xmm2
	vmulps	%xmm2, %xmm6, %xmm14
	vmovups	24616(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1488(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rax,4), %xmm2
	vmovaps	%xmm2, 768(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm2, %xmm0 # xmm0 = xmm2[1,3],xmm0[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmovaps	1792(%rsp), %xmm6       # 16-byte Reload
	vmulps	%xmm5, %xmm6, %xmm5
	vmulps	%xmm0, %xmm5, %xmm2
	vmovups	24616(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 1664(%rsp)       # 16-byte Spill
	vmovups	24600(%r10,%rdx,4), %xmm5
	vmovaps	%xmm5, 736(%rsp)        # 16-byte Spill
	vshufps	$221, %xmm0, %xmm5, %xmm0 # xmm0 = xmm5[1,3],xmm0[1,3]
	vsubps	%xmm7, %xmm0, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vmulps	%xmm3, %xmm6, %xmm5
	vmulps	%xmm0, %xmm5, %xmm0
	vminps	%xmm13, %xmm9, %xmm5
	vmaxps	%xmm12, %xmm5, %xmm7
	vminps	%xmm13, %xmm8, %xmm3
	vmaxps	%xmm12, %xmm3, %xmm3
	vmovaps	%xmm3, 1680(%rsp)       # 16-byte Spill
	vmovaps	1840(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 1312(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmovaps	%xmm3, 1312(%rsp)       # 16-byte Spill
	vminps	%xmm13, %xmm11, %xmm4
	vmaxps	%xmm12, %xmm4, %xmm3
	vmovaps	%xmm3, 1296(%rsp)       # 16-byte Spill
	movslq	%r8d, %rcx
	movslq	%r15d, %rsi
	vminps	%xmm13, %xmm1, %xmm1
	vminps	%xmm13, %xmm14, %xmm4
	vminps	%xmm13, %xmm2, %xmm14
	vminps	%xmm13, %xmm0, %xmm0
	vmovaps	%xmm0, 1088(%rsp)       # 16-byte Spill
	vmovaps	1408(%rsp), %xmm0       # 16-byte Reload
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm12, %xmm0, %xmm13
	movl	568(%rsp), %r9d         # 4-byte Reload
	testl	%r9d, %r9d
	vmovups	24632(%r10,%rdx,4), %xmm0
	vmovaps	%xmm0, 944(%rsp)        # 16-byte Spill
	vmovups	24632(%r10,%rax,4), %xmm0
	vmovaps	%xmm0, 1392(%rsp)       # 16-byte Spill
	movq	1736(%rsp), %r11        # 8-byte Reload
	vmovups	8(%r11,%rsi,4), %xmm9
	vmovups	24(%r11,%rsi,4), %xmm15
	vmovups	16(%r11,%rsi,4), %xmm0
	vmovups	32(%r11,%rsi,4), %xmm3
	vmovaps	%xmm3, 1408(%rsp)       # 16-byte Spill
	vmovups	(%r11,%rsi,4), %xmm3
	vmovups	8(%r11,%rcx,4), %xmm6
	vmovups	24(%r11,%rcx,4), %xmm2
	vmovups	16(%r11,%rcx,4), %xmm8
	vmovups	32(%r11,%rcx,4), %xmm5
	vmovaps	%xmm5, 1376(%rsp)       # 16-byte Spill
	vmovups	(%r11,%rcx,4), %xmm10
	movq	1440(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r11,%rax,4), %xmm5
	vmovaps	%xmm5, 1024(%rsp)       # 16-byte Spill
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovups	24616(%r10,%rax,4), %xmm5
	vmovaps	%xmm5, 1040(%rsp)       # 16-byte Spill
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovups	16(%r11,%rax,4), %xmm5
	vmovaps	%xmm5, 1056(%rsp)       # 16-byte Spill
	movq	1120(%rsp), %r12        # 8-byte Reload
	vmovups	24616(%r10,%r12,4), %xmm11
	vmovaps	%xmm11, 1072(%rsp)      # 16-byte Spill
	je	.LBB162_134
# BB#133:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vxorps	%xmm5, %xmm5, %xmm5
	vmovaps	%xmm5, 1632(%rsp)       # 16-byte Spill
.LBB162_134:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vshufps	$221, %xmm15, %xmm9, %xmm5 # xmm5 = xmm9[1,3],xmm15[1,3]
	vmovaps	%xmm5, 896(%rsp)        # 16-byte Spill
	vmaxps	%xmm12, %xmm1, %xmm1
	vmovaps	%xmm1, 880(%rsp)        # 16-byte Spill
	vsubps	1104(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
	vshufps	$221, %xmm2, %xmm6, %xmm1 # xmm1 = xmm6[1,3],xmm2[1,3]
	vmovaps	%xmm1, 864(%rsp)        # 16-byte Spill
	vmovaps	%xmm2, 912(%rsp)        # 16-byte Spill
	vmovaps	%xmm6, 928(%rsp)        # 16-byte Spill
	vmaxps	%xmm12, %xmm4, %xmm4
	vmovaps	1680(%rsp), %xmm1       # 16-byte Reload
	vsubps	1648(%rsp), %xmm1, %xmm11 # 16-byte Folded Reload
	vshufps	$221, %xmm0, %xmm3, %xmm5 # xmm5 = xmm3[1,3],xmm0[1,3]
	vmovaps	%xmm3, 752(%rsp)        # 16-byte Spill
	vmovaps	%xmm0, 1760(%rsp)       # 16-byte Spill
	vmaxps	%xmm12, %xmm14, %xmm1
	vmovaps	1296(%rsp), %xmm0       # 16-byte Reload
	vsubps	1312(%rsp), %xmm0, %xmm14 # 16-byte Folded Reload
	vshufps	$221, %xmm8, %xmm10, %xmm2 # xmm2 = xmm10[1,3],xmm8[1,3]
	vmovaps	%xmm8, 1648(%rsp)       # 16-byte Spill
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	vmaxps	%xmm12, %xmm0, %xmm8
	vmovaps	1424(%rsp), %xmm0       # 16-byte Reload
	vsubps	%xmm0, %xmm13, %xmm6
	movl	1504(%rsp), %ecx        # 4-byte Reload
	movl	%ecx, %eax
	movq	1744(%rsp), %r8         # 8-byte Reload
	orl	%r8d, %eax
	testb	$1, %al
	movl	1184(%rsp), %r15d       # 4-byte Reload
	vmovaps	%xmm5, %xmm13
	vmovaps	%xmm4, %xmm12
	je	.LBB162_135
# BB#136:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm15, 1104(%rsp)      # 16-byte Spill
	vmovaps	%xmm9, 1296(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 640(%rsp)        # 16-byte Spill
	vmovaps	%xmm10, 656(%rsp)       # 16-byte Spill
	vmovaps	1584(%rsp), %xmm5       # 16-byte Reload
	jmp	.LBB162_137
	.align	16, 0x90
.LBB162_135:                            #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm15, 1104(%rsp)      # 16-byte Spill
	vmovaps	%xmm9, 1296(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 656(%rsp)       # 16-byte Spill
	movq	1440(%rsp), %rax        # 8-byte Reload
	vmovaps	1024(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 32(%r11,%rax,4), %xmm0, %xmm10 # xmm10 = xmm0[0,2],mem[0,2]
	vmovaps	1328(%rsp), %xmm0       # 16-byte Reload
	vmovaps	%xmm2, %xmm15
	vmulps	1568(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovaps	1040(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 24632(%r10,%rax,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vmovaps	1824(%rsp), %xmm3       # 16-byte Reload
	vsubps	%xmm3, %xmm4, %xmm4
	vmovaps	1584(%rsp), %xmm5       # 16-byte Reload
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	vmovaps	%xmm1, %xmm9
	vmovaps	1808(%rsp), %xmm1       # 16-byte Reload
	vminps	%xmm1, %xmm2, %xmm2
	vmaxps	.LCPI162_10(%rip), %xmm2, %xmm2
	vsubps	%xmm10, %xmm2, %xmm10
	vmulps	1552(%rsp), %xmm0, %xmm2 # 16-byte Folded Reload
	vmovaps	1072(%rsp), %xmm4       # 16-byte Reload
	vshufps	$136, 24632(%r10,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0,2],mem[0,2]
	vsubps	%xmm3, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm2, %xmm2
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovaps	1056(%rsp), %xmm3       # 16-byte Reload
	vshufps	$136, 32(%r11,%rax,4), %xmm3, %xmm4 # xmm4 = xmm3[0,2],mem[0,2]
	vminps	%xmm1, %xmm2, %xmm2
	vmovaps	%xmm9, %xmm1
	vmaxps	.LCPI162_10(%rip), %xmm2, %xmm2
	vsubps	%xmm4, %xmm2, %xmm2
	vaddps	%xmm11, %xmm6, %xmm4
	vaddps	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm7, 640(%rsp)        # 16-byte Spill
	vaddps	%xmm2, %xmm4, %xmm2
	vaddps	%xmm2, %xmm14, %xmm2
	vaddps	%xmm2, %xmm10, %xmm0
	vmovaps	%xmm15, %xmm2
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
.LBB162_137:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	896(%rsp), %xmm0        # 16-byte Reload
	vmovaps	880(%rsp), %xmm3        # 16-byte Reload
	vmovaps	864(%rsp), %xmm4        # 16-byte Reload
	vsubps	%xmm0, %xmm3, %xmm7
	vsubps	%xmm4, %xmm12, %xmm12
	vsubps	%xmm13, %xmm1, %xmm13
	vsubps	%xmm2, %xmm8, %xmm2
	vmovaps	%xmm2, 1312(%rsp)       # 16-byte Spill
	testl	%ecx, %r9d
	jne	.LBB162_138
# BB#139:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm6, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 864(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 896(%rsp)       # 16-byte Spill
	vmovaps	%xmm7, 1088(%rsp)       # 16-byte Spill
	vmovaps	1824(%rsp), %xmm14      # 16-byte Reload
	vmovaps	1536(%rsp), %xmm12      # 16-byte Reload
	vmovaps	1520(%rsp), %xmm7       # 16-byte Reload
	vmovaps	1808(%rsp), %xmm6       # 16-byte Reload
	vxorps	%xmm9, %xmm9, %xmm9
	vmovaps	1664(%rsp), %xmm10      # 16-byte Reload
	vmovaps	1760(%rsp), %xmm11      # 16-byte Reload
	vmovaps	1648(%rsp), %xmm8       # 16-byte Reload
	vmovaps	%xmm13, 880(%rsp)       # 16-byte Spill
	vmovaps	992(%rsp), %xmm4        # 16-byte Reload
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm2        # 16-byte Reload
	vmovaps	944(%rsp), %xmm15       # 16-byte Reload
	jmp	.LBB162_140
	.align	16, 0x90
.LBB162_138:                            #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm6, 608(%rsp)        # 16-byte Spill
	vmovaps	%xmm14, 624(%rsp)       # 16-byte Spill
	vmovaps	%xmm11, 864(%rsp)       # 16-byte Spill
	vmovaps	1520(%rsp), %xmm15      # 16-byte Reload
	vmovaps	%xmm12, 896(%rsp)       # 16-byte Spill
	vmovaps	592(%rsp), %xmm2        # 16-byte Reload
	vmulps	%xmm2, %xmm15, %xmm0
	vmovaps	1488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$221, 1392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[1,3],mem[1,3]
	vmovaps	1824(%rsp), %xmm14      # 16-byte Reload
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm0, %xmm1, %xmm0
	vmovaps	1760(%rsp), %xmm11      # 16-byte Reload
	vshufps	$221, 1408(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[1,3],mem[1,3]
	vmovaps	1808(%rsp), %xmm6       # 16-byte Reload
	vminps	%xmm6, %xmm0, %xmm0
	vxorps	%xmm9, %xmm9, %xmm9
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm0
	vmovaps	1536(%rsp), %xmm3       # 16-byte Reload
	vmulps	%xmm2, %xmm3, %xmm1
	vmovaps	1664(%rsp), %xmm10      # 16-byte Reload
	vmovaps	944(%rsp), %xmm4        # 16-byte Reload
	vshufps	$221, %xmm4, %xmm10, %xmm2 # xmm2 = xmm10[1,3],xmm4[1,3]
	vsubps	%xmm14, %xmm2, %xmm2
	vmulps	%xmm2, %xmm5, %xmm2
	vmulps	%xmm1, %xmm2, %xmm1
	vmovaps	1648(%rsp), %xmm8       # 16-byte Reload
	vshufps	$221, 1376(%rsp), %xmm8, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm8[1,3],mem[1,3]
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm2, %xmm1, %xmm1
	vaddps	1312(%rsp), %xmm12, %xmm2 # 16-byte Folded Reload
	vaddps	%xmm1, %xmm2, %xmm1
	vaddps	%xmm1, %xmm13, %xmm1
	vaddps	%xmm1, %xmm7, %xmm1
	vmovaps	%xmm7, 1088(%rsp)       # 16-byte Spill
	vmovaps	%xmm15, %xmm7
	vmovaps	%xmm3, %xmm12
	vaddps	%xmm1, %xmm0, %xmm0
	vmulps	1200(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmovaps	%xmm0, 1632(%rsp)       # 16-byte Spill
	vmovaps	%xmm13, 880(%rsp)       # 16-byte Spill
	vmovaps	%xmm4, %xmm15
	vmovaps	992(%rsp), %xmm4        # 16-byte Reload
	vmovaps	976(%rsp), %xmm1        # 16-byte Reload
	vmovaps	960(%rsp), %xmm2        # 16-byte Reload
.LBB162_140:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	928(%rsp), %xmm0        # 16-byte Reload
	vmovaps	912(%rsp), %xmm3        # 16-byte Reload
	vmovaps	%xmm11, 1760(%rsp)      # 16-byte Spill
	vshufps	$136, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0,2],xmm3[0,2]
	vshufps	$136, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,2],xmm2[0,2]
	vmulps	%xmm12, %xmm4, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm0
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	vmovaps	1328(%rsp), %xmm13      # 16-byte Reload
	vmulps	%xmm12, %xmm13, %xmm0
	vshufps	$136, %xmm15, %xmm10, %xmm1 # xmm1 = xmm10[0,2],xmm15[0,2]
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1376(%rsp), %xmm8, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm8[0,2],mem[0,2]
	vminps	%xmm6, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm3
	vmovaps	1296(%rsp), %xmm0       # 16-byte Reload
	vshufps	$136, 1104(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmovaps	1360(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1344(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vmulps	%xmm7, %xmm4, %xmm2
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm2, %xmm1
	vminps	%xmm6, %xmm1, %xmm1
	vmaxps	%xmm9, %xmm1, %xmm1
	vsubps	%xmm0, %xmm1, %xmm15
	vmulps	%xmm7, %xmm13, %xmm0
	vmovaps	%xmm6, %xmm13
	vmovaps	1488(%rsp), %xmm1       # 16-byte Reload
	vshufps	$136, 1392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,2],mem[0,2]
	vsubps	%xmm14, %xmm1, %xmm1
	vmulps	%xmm1, %xmm5, %xmm1
	vmulps	%xmm1, %xmm0, %xmm0
	vshufps	$136, 1408(%rsp), %xmm11, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm11[0,2],mem[0,2]
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm9, %xmm0, %xmm0
	vsubps	%xmm1, %xmm0, %xmm1
	andl	$1, %ecx
	je	.LBB162_141
# BB#142:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vmovaps	%xmm3, 1376(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1648(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1664(%rsp)      # 16-byte Spill
	vmovaps	1632(%rsp), %xmm11      # 16-byte Reload
	vmovaps	%xmm9, %xmm10
	vmovdqa	1008(%rsp), %xmm4       # 16-byte Reload
	vmovaps	1680(%rsp), %xmm0       # 16-byte Reload
	jmp	.LBB162_143
	.align	16, 0x90
.LBB162_141:                            #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm3, %xmm0
	vmovaps	%xmm0, 1376(%rsp)       # 16-byte Spill
	vmovaps	%xmm8, 1648(%rsp)       # 16-byte Spill
	vmovaps	%xmm10, 1664(%rsp)      # 16-byte Spill
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	%xmm1, 1504(%rsp)       # 16-byte Spill
	vaddps	%xmm0, %xmm15, %xmm0
	vmovaps	1680(%rsp), %xmm1       # 16-byte Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	80(%rsp), %xmm0, %xmm11 # 16-byte Folded Reload
	vmovaps	%xmm9, %xmm10
	vmovdqa	1008(%rsp), %xmm4       # 16-byte Reload
	vmovaps	%xmm1, %xmm0
.LBB162_143:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm0, 1680(%rsp)       # 16-byte Spill
	testl	%r9d, %r9d
	jne	.LBB162_145
# BB#144:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	1632(%rsp), %xmm11      # 16-byte Reload
.LBB162_145:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	movq	128(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vmovdqa	.LCPI162_2(%rip), %xmm8 # xmm8 = [0,2,4,6]
	vpaddd	%xmm8, %xmm0, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	1176(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ecx
	vmovd	%xmm0, %eax
	cltd
	idivl	1172(%rsp)              # 4-byte Folded Reload
	movl	%edx, %edi
	movl	%r15d, %ebx
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	1160(%rsp)              # 4-byte Folded Reload
	movl	%edx, %ebp
	andl	$1, %ebx
	vmovd	%edi, %xmm1
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	1136(%rsp)              # 4-byte Folded Reload
	vpinsrd	$1, %ecx, %xmm1, %xmm0
	vpinsrd	$2, %ebp, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm1
	vpand	1232(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	384(%rsp), %xmm1        # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm1, %xmm1
	vpxor	.LCPI162_9(%rip), %xmm1, %xmm1
	vmovdqa	368(%rsp), %xmm3        # 16-byte Reload
	vpcmpgtd	%xmm4, %xmm3, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vmovdqa	1248(%rsp), %xmm3       # 16-byte Reload
	vpcmpgtd	%xmm0, %xmm3, %xmm3
	vmovdqa	1216(%rsp), %xmm4       # 16-byte Reload
	vpsubd	%xmm0, %xmm4, %xmm4
	vblendvps	%xmm3, %xmm0, %xmm4, %xmm0
	vmovdqa	1264(%rsp), %xmm6       # 16-byte Reload
	vpaddd	%xmm6, %xmm0, %xmm0
	vmovdqa	1280(%rsp), %xmm4       # 16-byte Reload
	vpminsd	%xmm4, %xmm0, %xmm0
	vpmaxsd	%xmm6, %xmm0, %xmm0
	movq	192(%rsp), %rax         # 8-byte Reload
	leal	(%rax,%r13), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %xmm3
	vpaddd	%xmm8, %xmm3, %xmm3
	vpminsd	%xmm4, %xmm3, %xmm3
	vpmaxsd	%xmm6, %xmm3, %xmm3
	vblendvps	%xmm1, %xmm0, %xmm3, %xmm0
	vpmulld	1616(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	1600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpextrq	$1, %xmm0, %rax
	vmovq	%xmm0, %rcx
	movslq	%ecx, %rdx
	movslq	%eax, %rsi
	sarq	$32, %rcx
	sarq	$32, %rax
	vmovss	(%r14,%rdx,4), %xmm0    # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r14,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r14,%rax,4), %xmm0, %xmm1 # xmm1 = xmm0[0,1,2],mem[0]
	testl	%ebx, %ebx
	jne	.LBB162_146
# BB#147:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
	jmp	.LBB162_148
	.align	16, 0x90
.LBB162_146:                            #   in Loop: Header=BB162_41 Depth=1
	vmovaps	784(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1840(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vmulps	816(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
	vmovaps	800(%rsp), %xmm2        # 16-byte Reload
	vshufps	$136, 1776(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm2[0,2],mem[0,2]
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm3, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	vmovaps	608(%rsp), %xmm2        # 16-byte Reload
	vaddps	640(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
	vaddps	864(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	624(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm3, %xmm0
	vmulps	96(%rsp), %xmm0, %xmm3  # 16-byte Folded Reload
.LBB162_148:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	movq	1712(%rsp), %rdx        # 8-byte Reload
	testl	%r9d, %r9d
	je	.LBB162_150
# BB#149:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vxorps	%xmm3, %xmm3, %xmm3
.LBB162_150:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	movl	%r15d, %eax
	orl	%r8d, %eax
	testb	$1, %al
	jne	.LBB162_152
# BB#151:                               #   in Loop: Header=BB162_41 Depth=1
	movq	1440(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm0
	vshufps	$221, 1024(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[1,3],mem[1,3]
	movq	1456(%rsp), %rax        # 8-byte Reload
	vmovups	24600(%r10,%rax,4), %xmm3
	vshufps	$221, 1040(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovaps	1792(%rsp), %xmm2       # 16-byte Reload
	vmulps	1568(%rsp), %xmm2, %xmm4 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm4, %xmm3
	vminps	%xmm13, %xmm3, %xmm3
	vmaxps	%xmm10, %xmm3, %xmm3
	vsubps	%xmm0, %xmm3, %xmm0
	movq	1472(%rsp), %rax        # 8-byte Reload
	vmovups	(%r11,%rax,4), %xmm3
	vshufps	$221, 1056(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[1,3],mem[1,3]
	vmovups	24600(%r10,%r12,4), %xmm4
	vshufps	$221, 1072(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm4[1,3],mem[1,3]
	vmulps	1552(%rsp), %xmm2, %xmm6 # 16-byte Folded Reload
	vsubps	%xmm14, %xmm4, %xmm4
	vmulps	%xmm4, %xmm5, %xmm4
	vmulps	%xmm4, %xmm6, %xmm4
	vminps	%xmm13, %xmm4, %xmm4
	vmaxps	%xmm10, %xmm4, %xmm4
	vsubps	%xmm3, %xmm4, %xmm3
	vaddps	720(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
	vaddps	%xmm3, %xmm0, %xmm0
	vaddps	704(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	688(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
.LBB162_152:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	848(%rsp), %xmm2        # 16-byte Reload
	testl	%r15d, %r9d
	je	.LBB162_154
# BB#153:                               #   in Loop: Header=BB162_41 Depth=1
	vmovaps	736(%rsp), %xmm0        # 16-byte Reload
	vshufps	$136, 1664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,2],mem[0,2]
	vsubps	%xmm14, %xmm0, %xmm0
	vmovaps	768(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1488(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vmulps	%xmm12, %xmm1, %xmm4
	vmulps	%xmm0, %xmm5, %xmm0
	vmulps	%xmm0, %xmm4, %xmm0
	vsubps	%xmm14, %xmm3, %xmm3
	vmulps	%xmm7, %xmm1, %xmm1
	vmulps	%xmm3, %xmm5, %xmm3
	vmulps	%xmm3, %xmm1, %xmm1
	vmovaps	656(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1648(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm13, %xmm0, %xmm0
	vmaxps	%xmm10, %xmm0, %xmm0
	vsubps	%xmm3, %xmm0, %xmm0
	vmovaps	752(%rsp), %xmm3        # 16-byte Reload
	vshufps	$136, 1760(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,2],mem[0,2]
	vminps	%xmm13, %xmm1, %xmm1
	vmaxps	%xmm10, %xmm1, %xmm1
	vaddps	1680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vsubps	%xmm3, %xmm1, %xmm1
	vaddps	1376(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm0, %xmm15, %xmm0
	vaddps	1504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	1200(%rsp), %xmm0, %xmm3 # 16-byte Folded Reload
.LBB162_154:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	1424(%rsp), %xmm1       # 16-byte Reload
	vmovaps	1088(%rsp), %xmm0       # 16-byte Reload
	movl	%r15d, %eax
	andl	$1, %eax
	je	.LBB162_155
# BB#156:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm7, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1536(%rsp)      # 16-byte Spill
	vmovaps	%xmm3, %xmm0
	jmp	.LBB162_157
	.align	16, 0x90
.LBB162_155:                            #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm7, 1520(%rsp)       # 16-byte Spill
	vmovaps	%xmm12, 1536(%rsp)      # 16-byte Spill
	vaddps	896(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vaddps	1312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vmulps	80(%rsp), %xmm0, %xmm0  # 16-byte Folded Reload
.LBB162_157:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm13, 1808(%rsp)      # 16-byte Spill
	vmovaps	%xmm14, 1824(%rsp)      # 16-byte Spill
	vmovaps	%xmm5, 1584(%rsp)       # 16-byte Spill
	testl	%r9d, %r9d
	jne	.LBB162_159
# BB#158:                               # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vmovaps	%xmm3, %xmm0
.LBB162_159:                            # %for f8.s0.v10.v1016
                                        #   in Loop: Header=BB162_41 Depth=1
	vaddps	%xmm0, %xmm1, %xmm0
	vaddps	%xmm11, %xmm2, %xmm1
	vmovaps	.LCPI162_7(%rip), %ymm2 # ymm2 = <0,u,1,u,2,u,3,u>
	vpermps	%ymm0, %ymm2, %ymm0
	vmovaps	.LCPI162_8(%rip), %ymm2 # ymm2 = <u,0,u,1,u,2,u,3>
	vpermps	%ymm1, %ymm2, %ymm1
	vblendps	$170, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[0],ymm1[1],ymm0[2],ymm1[3],ymm0[4],ymm1[5],ymm0[6],ymm1[7]
	movslq	%r15d, %rax
	movq	536(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	1704(%rsp), %rcx        # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	addl	$8, %r13d
	addl	$-1, %edx
	movq	%rdx, 1712(%rsp)        # 8-byte Spill
	jne	.LBB162_41
.LBB162_160:                            # %destructor_block
	xorl	%eax, %eax
	addq	$1864, %rsp             # imm = 0x748
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end162:
	.size	par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178, .Lfunc_end162-par_for_par_for___sharpi_f0.s0.v11.v14_f8.s0.v11.178

	.section	.text.par_for___sharpi_transpose.s0.v12,"ax",@progbits
	.align	16, 0x90
	.type	par_for___sharpi_transpose.s0.v12,@function
par_for___sharpi_transpose.s0.v12:      # @par_for___sharpi_transpose.s0.v12
# BB#0:                                 # %entry
	subq	$88, %rsp
	vmovups	(%rdx), %ymm0
	movl	32(%rdx), %eax
	movl	36(%rdx), %r8d
	movl	40(%rdx), %r9d
	movl	44(%rdx), %r10d
	vmovups	48(%rdx), %xmm1
	vmovups	64(%rdx), %xmm2
	vextractf128	$1, %ymm0, %xmm3
	vmovd	%xmm3, %ecx
	leal	-1(%rcx), %edx
	sarl	$3, %edx
	addl	$1, %edx
	cmpl	$1, %ecx
	movl	$1, %ecx
	cmovgl	%edx, %ecx
	vmovups	%ymm0, (%rsp)
	movl	%eax, 32(%rsp)
	movl	%esi, 36(%rsp)
	movl	%r8d, 40(%rsp)
	movl	%r9d, 44(%rsp)
	movl	%r10d, 48(%rsp)
	vmovups	%xmm1, 56(%rsp)
	vmovups	%xmm2, 72(%rsp)
	leaq	par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146(%rip), %rsi
	leaq	(%rsp), %r8
	xorl	%edx, %edx
	vzeroupper
	callq	halide_do_par_for@PLT
	addq	$88, %rsp
	retq
.Lfunc_end163:
	.size	par_for___sharpi_transpose.s0.v12, .Lfunc_end163-par_for___sharpi_transpose.s0.v12

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI164_0:
	.long	3212836864              # float -1
.LCPI164_1:
	.long	3186723360              # float -0.117939234
.LCPI164_2:
	.long	3190598332              # float -0.16862005
.LCPI164_3:
	.long	3196053750              # float -0.249912113
.LCPI164_4:
	.long	3204448274              # float -0.500001073
.LCPI164_5:
	.long	1028743925              # float 0.0511197634
.LCPI164_6:
	.long	1041846319              # float 0.149719939
.LCPI164_7:
	.long	1045207583              # float 0.199806675
.LCPI164_8:
	.long	1051372237              # float 0.333334357
.LCPI164_9:
	.long	1065353216              # float 1
.LCPI164_10:
	.long	1060205080              # float 0.693147182
.LCPI164_12:
	.long	1048576000              # float 0.25
.LCPI164_13:
	.long	1069066811              # float 1.44269502
.LCPI164_14:
	.long	3207688704              # float -0.693145751
.LCPI164_15:
	.long	3049242254              # float -1.42860677E-6
.LCPI164_16:
	.long	983314022               # float 0.00119156833
.LCPI164_17:
	.long	1026188988              # float 0.0416018814
.LCPI164_18:
	.long	1056964574              # float 0.499998987
.LCPI164_19:
	.long	967284723               # float 3.19659332E-4
.LCPI164_20:
	.long	1007360298              # float 0.00848988629
.LCPI164_21:
	.long	1042984479              # float 0.166679844
.LCPI164_22:
	.long	2139095040              # float +Inf
	.section	.rodata.cst8,"aM",@progbits,8
	.align	4
.LCPI164_11:
	.long	4286578688              # float -inf
	.long	2143289344              # float nan
	.section	.text.par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146,@function
par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146: # @par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$280, %rsp              # imm = 0x118
	movslq	16(%rdx), %rcx
	movq	%rcx, 248(%rsp)         # 8-byte Spill
	shll	$3, %esi
	movl	%esi, 272(%rsp)         # 4-byte Spill
	leal	-8(%rcx), %eax
	cmpq	$1, %rcx
	movl	20(%rdx), %r14d
	movl	28(%rdx), %ecx
	movq	%rcx, 216(%rsp)         # 8-byte Spill
	movl	32(%rdx), %r15d
	movslq	48(%rdx), %r8
	movq	%r8, 256(%rsp)          # 8-byte Spill
	movl	$-7, %ebp
	cmovgl	%eax, %ebp
	cmpl	%esi, %ebp
	cmovgl	%esi, %ebp
	movl	%ebp, 152(%rsp)         # 4-byte Spill
	leal	(%r15,%rcx), %eax
	cmpl	%eax, %r14d
	movl	%eax, %ecx
	cmovgel	%r14d, %ecx
	movl	%r8d, %ebx
	sarl	$31, %ebx
	andl	%r8d, %ebx
	movq	%rbx, 200(%rsp)         # 8-byte Spill
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	addl	$-1, %eax
	addl	$-1, %ecx
	leal	-2(%r14), %esi
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	xorl	%esi, %esi
	testl	%ecx, %ecx
	cmovnsl	%ecx, %esi
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	$1, %eax
	subl	%ebx, %eax
	addl	%esi, %eax
	shlq	$5, %rax
	movabsq	$135291469824, %rcx     # imm = 0x1F80000000
	testq	%rcx, %rax
	jne	.LBB164_27
# BB#1:                                 # %assert succeeded
	movslq	(%rdx), %rcx
	movq	%rcx, 264(%rsp)         # 8-byte Spill
	movslq	4(%rdx), %rcx
	movq	%rcx, 240(%rsp)         # 8-byte Spill
	movslq	8(%rdx), %rbp
	movslq	12(%rdx), %r12
	vmovss	24(%rdx), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 160(%rsp)        # 4-byte Spill
	movslq	36(%rdx), %rbx
	movslq	40(%rdx), %rcx
	movq	%rcx, 224(%rsp)         # 8-byte Spill
	movslq	44(%rdx), %rcx
	movq	%rcx, 232(%rsp)         # 8-byte Spill
	movq	56(%rdx), %r13
	movq	72(%rdx), %rcx
	movq	%rcx, 192(%rsp)         # 8-byte Spill
	orq	$4, %rax
	movq	%rdi, (%rsp)            # 8-byte Spill
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r10
	testq	%r10, %r10
	je	.LBB164_28
# BB#2:                                 # %for blur.s1.v10.preheader
	movq	264(%rsp), %rdx         # 8-byte Reload
	imulq	%rbx, %rdx
	movq	%rdx, 264(%rsp)         # 8-byte Spill
	movq	%rbx, 208(%rsp)         # 8-byte Spill
	movq	240(%rsp), %rax         # 8-byte Reload
	leaq	1(%rax), %rax
	movq	%rax, 168(%rsp)         # 8-byte Spill
	movq	%r12, %rsi
	sarq	$63, %rsi
	andq	%r12, %rsi
	movq	%r12, 176(%rsp)         # 8-byte Spill
	movq	%rsi, %rcx
	imulq	%rax, %rcx
	movq	%rbp, %rdi
	sarq	$63, %rdi
	andq	%rbp, %rdi
	movq	%rbp, 184(%rsp)         # 8-byte Spill
	addq	%rdi, %rcx
	movq	%rdx, %rax
	subq	%rcx, %rax
	movq	256(%rsp), %r11         # 8-byte Reload
	movq	%r11, %r9
	sarq	$63, %r9
	andq	%r11, %r9
	leaq	(,%r9,8), %rcx
	negq	%rcx
	movslq	152(%rsp), %rdx         # 4-byte Folded Reload
	subq	%rdx, %rcx
	movl	$7, %ebp
	movq	248(%rsp), %rbx         # 8-byte Reload
	subl	%ebx, %ebp
	cmpl	$1, %ebx
	movl	272(%rsp), %r8d         # 4-byte Reload
	notl	%r8d
	movl	%r8d, 272(%rsp)         # 4-byte Spill
	movl	$6, %ebx
	cmovgl	%ebp, %ebx
	cmpl	%r8d, %ebx
	cmovll	%r8d, %ebx
	notl	%ebx
	movslq	%ebx, %rbp
	leaq	(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	1(%rbp,%rax), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	1(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	2(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	2(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	3(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	3(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	4(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	4(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	5(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	5(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	6(%rax,%rbp), %rbx
	movzbl	(%r13,%rbx), %ebx
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%ebx, %xmm0, %xmm0
	leaq	6(%rcx,%rbp), %rbx
	vmovss	%xmm0, (%r10,%rbx,4)
	leaq	7(%rax,%rbp), %rax
	movzbl	(%r13,%rax), %eax
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ssl	%eax, %xmm0, %xmm0
	leaq	7(%rcx,%rbp), %rax
	vmovss	%xmm0, (%r10,%rax,4)
	cmpl	$1, %r14d
	movq	200(%rsp), %r12         # 8-byte Reload
	jle	.LBB164_21
# BB#3:                                 # %for blur.s2.r83$x.preheader
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	160(%rsp), %xmm1        # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vucomiss	%xmm1, %xmm0
	vmovd	%xmm1, %ecx
	movl	$1065353216, %eax       # imm = 0x3F800000
	cmovbl	%ecx, %eax
	vucomiss	%xmm1, %xmm0
	seta	%cl
	vmovss	.LCPI164_9(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	movzbl	%cl, %ecx
	jae	.LBB164_4
# BB#5:                                 # %for blur.s2.r83$x.preheader
	movl	%eax, %ebp
	andl	$-2139095041, %ebp      # imm = 0xFFFFFFFF807FFFFF
	movl	%ebp, %ecx
	sarl	$22, %ecx
	movl	$127, %ebx
	subl	%ecx, %ebx
	sarl	$23, %eax
	subl	%ebx, %eax
	shll	$23, %ebx
	orl	%ebp, %ebx
	vmovd	%ebx, %xmm1
	vaddss	.LCPI164_0(%rip), %xmm1, %xmm1
	vmulss	%xmm1, %xmm1, %xmm2
	vcvtsi2ssl	%eax, %xmm0, %xmm3
	vmovss	.LCPI164_1(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI164_2(%rip), %xmm2, %xmm4
	vfmadd213ss	.LCPI164_3(%rip), %xmm2, %xmm4
	vfmadd213ss	.LCPI164_4(%rip), %xmm2, %xmm4
	vmovss	.LCPI164_5(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI164_6(%rip), %xmm2, %xmm5
	vfmadd213ss	.LCPI164_7(%rip), %xmm2, %xmm5
	vfmadd213ss	.LCPI164_8(%rip), %xmm2, %xmm5
	vfmadd213ss	%xmm0, %xmm2, %xmm5
	vmulss	%xmm5, %xmm1, %xmm1
	vfmadd213ss	%xmm1, %xmm2, %xmm4
	vmovss	.LCPI164_10(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm4, %xmm3, %xmm1
	jmp	.LBB164_6
.LBB164_4:
	leaq	.LCPI164_11(%rip), %rax
	vmovss	(%rax,%rcx,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
.LBB164_6:                              # %for blur.s2.r83$x.preheader
	movq	184(%rsp), %r8          # 8-byte Reload
	movq	176(%rsp), %rbx         # 8-byte Reload
	vmulss	.LCPI164_12(%rip), %xmm1, %xmm2
	vmulss	.LCPI164_13(%rip), %xmm2, %xmm1
	vroundss	$1, %xmm1, %xmm0, %xmm1
	vcvttss2si	%xmm1, %eax
	leal	127(%rax), %ecx
	cmpl	$255, %ecx
	jl	.LBB164_7
# BB#8:                                 # %for blur.s2.r83$x.preheader
	vmovss	.LCPI164_22(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB164_9
.LBB164_7:
	vmovss	.LCPI164_14(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm2, %xmm1, %xmm3
	vmovss	.LCPI164_15(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm3, %xmm1, %xmm2
	vmulss	%xmm2, %xmm2, %xmm1
	shll	$23, %ecx
	vmovss	.LCPI164_16(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI164_17(%rip), %xmm1, %xmm3
	vmovd	%ecx, %xmm4
	vfmadd213ss	.LCPI164_18(%rip), %xmm1, %xmm3
	vmovss	.LCPI164_19(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI164_20(%rip), %xmm1, %xmm5
	vfmadd213ss	.LCPI164_21(%rip), %xmm1, %xmm5
	vfmadd213ss	%xmm0, %xmm1, %xmm3
	vfmadd213ss	%xmm0, %xmm1, %xmm5
	vfmadd213ss	%xmm3, %xmm2, %xmm5
	vmulss	%xmm5, %xmm4, %xmm1
.LBB164_9:                              # %for blur.s2.r83$x.preheader
	cmpl	$-127, %eax
	jg	.LBB164_11
# BB#10:                                # %for blur.s2.r83$x.preheader
	vxorps	%xmm1, %xmm1, %xmm1
.LBB164_11:                             # %for blur.s2.r83$x.preheader
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %eax
	testb	$1, %r14b
	jne	.LBB164_13
# BB#12:                                # %for blur.s2.r83$x.prol
	subq	%rdi, %rdx
	addq	264(%rsp), %rdx         # 8-byte Folded Reload
	movl	$1, %eax
	movl	$1, %ecx
	subq	%rsi, %rcx
	imulq	168(%rsp), %rcx         # 8-byte Folded Reload
	addq	%rdx, %rcx
	vpmovzxbd	(%r13,%rcx), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	subq	%r9, %rax
	shlq	$5, %rax
	vmovaps	-32(%rax,%r10), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%rax,%r10)
	movl	$2, %eax
.LBB164_13:                             # %for blur.s2.r83$x.preheader.split
	movq	%r13, 184(%rsp)         # 8-byte Spill
	cmpl	$2, %r14d
	je	.LBB164_16
# BB#14:                                # %for blur.s2.r83$x.preheader.split.split
	movl	%r14d, %edx
	subl	%eax, %edx
	notq	%r8
	cmpq	$-2, %r8
	movq	$-1, %rsi
	cmovleq	%rsi, %r8
	notq	%rbx
	cmpq	$-2, %rbx
	cmovleq	%rsi, %rbx
	leaq	1(%rax,%rbx), %rcx
	movq	168(%rsp), %r11         # 8-byte Reload
	imulq	%r11, %rcx
	leaq	(%rcx,%r8), %rcx
	movq	264(%rsp), %r13         # 8-byte Reload
	addq	%r13, %rcx
	movq	248(%rsp), %rbp         # 8-byte Reload
	testl	%ebp, %ebp
	movl	$1, %edi
	cmovgl	%ebp, %edi
	movl	$7, %ebp
	subl	%edi, %ebp
	movl	272(%rsp), %edi         # 4-byte Reload
	cmpl	%ebp, %edi
	cmovgel	%edi, %ebp
	notl	%ebp
	movslq	%ebp, %rdi
	addq	%rdi, %rcx
	movq	256(%rsp), %rbp         # 8-byte Reload
	notq	%rbp
	cmpq	$-2, %rbp
	cmovleq	%rsi, %rbp
	leaq	(%rbp,%rax), %rsi
	leaq	2(%rax,%rbx), %rbp
	imulq	%r11, %rbp
	addq	%r8, %rbp
	addq	%r13, %rbp
	addq	%rdi, %rbp
	movq	184(%rsp), %r13         # 8-byte Reload
	leaq	1(%r13,%rcx), %rax
	movq	240(%rsp), %rcx         # 8-byte Reload
	leaq	2(%rcx,%rcx), %rcx
	shlq	$5, %rsi
	leaq	(%rsi,%r10), %rsi
	leaq	1(%r13,%rbp), %rdi
	xorl	%ebp, %ebp
	.align	16, 0x90
.LBB164_15:                             # %for blur.s2.r83$x
                                        # =>This Inner Loop Header: Depth=1
	vpmovzxbd	(%rax,%rbp), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vmovaps	(%rsi), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rsi)
	vpmovzxbd	(%rdi,%rbp), %ymm2 # ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
	vcvtdq2ps	%ymm2, %ymm2
	vmulps	%ymm0, %ymm2, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rsi)
	addq	$64, %rsi
	addq	%rcx, %rbp
	addl	$-2, %edx
	jne	.LBB164_15
.LBB164_16:                             # %for blur.s3.r83$x.preheader
	movslq	%r14d, %rcx
	movl	$1, %edx
	testb	$1, %r14b
	jne	.LBB164_18
# BB#17:                                # %for blur.s3.r83$x.prol
	movl	%r14d, %eax
	subl	%r12d, %eax
	movq	%rcx, %rdx
	subq	%r9, %rdx
	addl	$-1, %eax
	cltq
	shlq	$5, %rax
	vmovaps	-32(%rax,%r10), %ymm2
	vmulps	(%rax,%r10), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	shlq	$5, %rdx
	vmovaps	%ymm2, -64(%rdx,%r10)
	movl	$2, %edx
.LBB164_18:                             # %for blur.s3.r83$x.preheader.split
	cmpl	$2, %r14d
	movq	256(%rsp), %r11         # 8-byte Reload
	je	.LBB164_21
# BB#19:                                # %for blur.s3.r83$x.preheader.split.split
	negl	%r12d
	movl	%r12d, %eax
	subl	%edx, %eax
	movq	%r11, %rsi
	notq	%rsi
	cmpq	$-2, %rsi
	movq	$-1, %rdi
	cmovgq	%rsi, %rdi
	addq	%rcx, %rdi
	movq	%rdi, %rcx
	shlq	$5, %rcx
	subq	%rdx, %rdi
	movq	%rdx, %rsi
	shlq	$5, %rsi
	subq	%rsi, %rcx
	leaq	(%rcx,%r10), %rcx
	shlq	$5, %rdi
	leaq	-32(%rdi,%r10), %rsi
	leal	1(%rdx), %edi
	subl	%edi, %r12d
	.align	16, 0x90
.LBB164_20:                             # %for blur.s3.r83$x
                                        # =>This Inner Loop Header: Depth=1
	leal	(%rax,%r14), %edi
	movslq	%edi, %rdi
	shlq	$5, %rdi
	vmovaps	-32(%rdi,%r10), %ymm2
	vmulps	(%rdi,%r10), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rcx)
	leal	(%r12,%r14), %edi
	movslq	%edi, %rdi
	shlq	$5, %rdi
	vmovaps	-32(%rdi,%r10), %ymm2
	vmulps	(%rdi,%r10), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	addl	$-2, %r14d
	addq	$-64, %rcx
	addq	$-64, %rsi
	cmpl	%r14d, %edx
	jne	.LBB164_20
.LBB164_21:                             # %consume blur
	movq	%r10, 8(%rsp)           # 8-byte Spill
	movq	216(%rsp), %rax         # 8-byte Reload
	leal	7(%rax), %ecx
	sarl	$3, %ecx
	movl	%ecx, 184(%rsp)         # 4-byte Spill
	testl	%ecx, %ecx
	jle	.LBB164_26
# BB#22:                                # %for transpose.s0.v10.v144.preheader
	movq	%r11, %rcx
	notq	%rcx
	cmpq	$-2, %rcx
	movq	$-1, %rdx
	cmovleq	%rdx, %rcx
	movq	%rcx, 176(%rsp)         # 8-byte Spill
	movl	$7, %ecx
	movl	$7, %esi
	subl	%r15d, %esi
	notl	%r15d
	subl	%eax, %esi
	movl	%esi, 168(%rsp)         # 4-byte Spill
	movq	8(%rsp), %rdi           # 8-byte Reload
	leaq	260(%rdi), %rax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	xorl	%ebx, %ebx
	movq	248(%rsp), %rbp         # 8-byte Reload
	movq	%rbp, %rsi
	addq	$-1, %rsi
	cmovsq	%rbx, %rsi
	movl	$6, %eax
	subq	%rsi, %rax
	leaq	228(%rdi), %rsi
	movq	%rsi, 152(%rsp)         # 8-byte Spill
	cmpq	$-2, %rax
	cmovleq	%rdx, %rax
	testl	%ebp, %ebp
	movl	$1, %edx
	cmovgl	%ebp, %edx
	leaq	196(%rdi), %rsi
	movq	%rsi, 144(%rsp)         # 8-byte Spill
	subl	%edx, %ecx
	leaq	164(%rdi), %rdx
	movq	%rdx, 136(%rsp)         # 8-byte Spill
	movl	272(%rsp), %edx         # 4-byte Reload
	cmpl	%ecx, %edx
	cmovgel	%edx, %ecx
	leaq	132(%rdi), %rdx
	movq	%rdx, 128(%rsp)         # 8-byte Spill
	notl	%ecx
	movslq	%ecx, %rcx
	movq	224(%rsp), %rdx         # 8-byte Reload
	imulq	208(%rsp), %rdx         # 8-byte Folded Reload
	leaq	2(%rax,%rcx), %rsi
	leaq	1(%rax,%rcx), %rcx
	leaq	100(%rdi), %rax
	movq	%rax, 104(%rsp)         # 8-byte Spill
	movq	232(%rsp), %rax         # 8-byte Reload
	addq	$1, %rax
	subq	%r11, %rax
	imulq	%rax, %rsi
	addq	%rdx, %rsi
	imulq	%rax, %rcx
	addq	%rdx, %rcx
	leaq	68(%rdi), %rdx
	movq	%rdx, 96(%rsp)          # 8-byte Spill
	subq	%r11, %rsi
	movq	%rsi, 120(%rsp)         # 8-byte Spill
	subq	%r11, %rcx
	movq	%rcx, 112(%rsp)         # 8-byte Spill
	movq	%rdi, %rcx
	subq	$-128, %rcx
	movq	%rcx, 88(%rsp)          # 8-byte Spill
	leaq	(,%rax,8), %rax
	movq	%rax, 80(%rsp)          # 8-byte Spill
	leaq	36(%rdi), %rax
	movq	%rax, 72(%rsp)          # 8-byte Spill
	leaq	256(%rdi), %rax
	movq	%rax, 64(%rsp)          # 8-byte Spill
	leaq	224(%rdi), %rax
	movq	%rax, 56(%rsp)          # 8-byte Spill
	leaq	192(%rdi), %rax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	leaq	160(%rdi), %rax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	leaq	96(%rdi), %rax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	leaq	64(%rdi), %rax
	movq	%rax, 24(%rsp)          # 8-byte Spill
	leaq	32(%rdi), %rax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	movq	80(%rsp), %r12          # 8-byte Reload
	.align	16, 0x90
.LBB164_23:                             # %for transpose.s0.v10.v144
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB164_24 Depth 2
	movq	%rbx, 200(%rsp)         # 8-byte Spill
	movq	%r15, 208(%rsp)         # 8-byte Spill
	movl	168(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %r15d
	cmovgel	%r15d, %eax
	notl	%eax
	cltq
	movq	176(%rsp), %rcx         # 8-byte Reload
	leaq	(%rcx,%rax), %r10
	shlq	$5, %r10
	movq	120(%rsp), %rcx         # 8-byte Reload
	leaq	(%rcx,%rax), %r11
	movq	112(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 272(%rsp)         # 8-byte Spill
	movq	160(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 264(%rsp)         # 8-byte Spill
	movq	152(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 256(%rsp)         # 8-byte Spill
	movq	144(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 248(%rsp)         # 8-byte Spill
	movq	136(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 240(%rsp)         # 8-byte Spill
	movq	128(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 232(%rsp)         # 8-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 224(%rsp)         # 8-byte Spill
	movq	96(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r10), %rax
	movq	%rax, 216(%rsp)         # 8-byte Spill
	movq	72(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r10), %r15
	movq	64(%rsp), %rdx          # 8-byte Reload
	leaq	(%rdx,%r10), %rdx
	movq	56(%rsp), %rsi          # 8-byte Reload
	leaq	(%rsi,%r10), %rbx
	movq	48(%rsp), %rsi          # 8-byte Reload
	leaq	(%rsi,%r10), %rsi
	movq	40(%rsp), %rdi          # 8-byte Reload
	leaq	(%rdi,%r10), %rbp
	movq	88(%rsp), %rdi          # 8-byte Reload
	leaq	(%rdi,%r10), %r8
	movq	32(%rsp), %rdi          # 8-byte Reload
	leaq	(%rdi,%r10), %rdi
	movq	24(%rsp), %rcx          # 8-byte Reload
	leaq	(%rcx,%r10), %r9
	movq	16(%rsp), %rcx          # 8-byte Reload
	leaq	(%r10,%rcx), %r10
	movq	%r11, %rcx
	movq	192(%rsp), %r11         # 8-byte Reload
	xorl	%r14d, %r14d
	.align	16, 0x90
.LBB164_24:                             # %for transpose.s0.v11.v11
                                        #   Parent Loop BB164_23 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%r14), %xmm0      # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbx,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rdx,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r10,%r14), %xmm1      # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r8,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	272(%rsp), %r13         # 8-byte Reload
	vmovups	%ymm0, (%r11,%r13,4)
	movq	240(%rsp), %rax         # 8-byte Reload
	vmovss	(%rax,%r14), %xmm0      # xmm0 = mem[0],zero,zero,zero
	movq	248(%rsp), %rax         # 8-byte Reload
	vinsertps	$16, (%rax,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	256(%rsp), %rax         # 8-byte Reload
	vinsertps	$32, (%rax,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	264(%rsp), %rax         # 8-byte Reload
	vinsertps	$48, (%rax,%r14), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r15,%r14), %xmm1      # xmm1 = mem[0],zero,zero,zero
	movq	216(%rsp), %rax         # 8-byte Reload
	vinsertps	$16, (%rax,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	224(%rsp), %rax         # 8-byte Reload
	vinsertps	$32, (%rax,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	232(%rsp), %rax         # 8-byte Reload
	vinsertps	$48, (%rax,%r14), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovups	%ymm0, (%r11,%rcx,4)
	addq	$8, %r14
	addq	%r12, %r11
	cmpq	$32, %r14
	jne	.LBB164_24
# BB#25:                                # %end for transpose.s0.v11.v11
                                        #   in Loop: Header=BB164_23 Depth=1
	movq	200(%rsp), %rbx         # 8-byte Reload
	addl	$1, %ebx
	movq	208(%rsp), %r15         # 8-byte Reload
	addl	$-8, %r15d
	cmpl	184(%rsp), %ebx         # 4-byte Folded Reload
	jne	.LBB164_23
.LBB164_26:                             # %call_destructor.exit
	movq	(%rsp), %rdi            # 8-byte Reload
	movq	8(%rsp), %rsi           # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%eax, %eax
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB164_27:                             # %assert failed
	leaq	.Lstr.180(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rax, %rdx
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_buffer_allocation_too_large@PLT # TAILCALL
.LBB164_28:                             # %assert failed1
	movq	(%rsp), %rdi            # 8-byte Reload
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_out_of_memory@PLT # TAILCALL
.Lfunc_end164:
	.size	par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146, .Lfunc_end164-par_for_par_for___sharpi_transpose.s0.v12_transpose.s0.v11.v146

	.section	".text.par_for___sharpi_transpose$1.s0.v12","ax",@progbits
	.align	16, 0x90
	.type	par_for___sharpi_transpose$1.s0.v12,@function
par_for___sharpi_transpose$1.s0.v12:    # @"par_for___sharpi_transpose$1.s0.v12"
# BB#0:                                 # %entry
	subq	$88, %rsp
	vmovdqu	(%rdx), %xmm0
	movl	16(%rdx), %eax
	movl	20(%rdx), %r8d
	vmovups	24(%rdx), %xmm1
	movl	40(%rdx), %r9d
	movl	44(%rdx), %r10d
	vmovups	48(%rdx), %xmm2
	vmovups	64(%rdx), %xmm3
	vpextrd	$3, %xmm0, %ecx
	addl	$7, %ecx
	sarl	$3, %ecx
	vmovdqa	%xmm0, (%rsp)
	movl	%eax, 16(%rsp)
	movl	%r8d, 20(%rsp)
	movl	%esi, 24(%rsp)
	vmovups	%xmm1, 28(%rsp)
	movl	%r9d, 44(%rsp)
	movl	%r10d, 48(%rsp)
	vmovups	%xmm2, 56(%rsp)
	vmovups	%xmm3, 72(%rsp)
	leaq	par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187(%rip), %rsi
	leaq	(%rsp), %r8
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
	addq	$88, %rsp
	retq
.Lfunc_end165:
	.size	par_for___sharpi_transpose$1.s0.v12, .Lfunc_end165-par_for___sharpi_transpose$1.s0.v12

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI166_0:
	.long	3212836864              # float -1
.LCPI166_1:
	.long	3186723360              # float -0.117939234
.LCPI166_2:
	.long	3190598332              # float -0.16862005
.LCPI166_3:
	.long	3196053750              # float -0.249912113
.LCPI166_4:
	.long	3204448274              # float -0.500001073
.LCPI166_5:
	.long	1028743925              # float 0.0511197634
.LCPI166_6:
	.long	1041846319              # float 0.149719939
.LCPI166_7:
	.long	1045207583              # float 0.199806675
.LCPI166_8:
	.long	1051372237              # float 0.333334357
.LCPI166_9:
	.long	1065353216              # float 1
.LCPI166_10:
	.long	1060205080              # float 0.693147182
.LCPI166_12:
	.long	1048576000              # float 0.25
.LCPI166_13:
	.long	1069066811              # float 1.44269502
.LCPI166_14:
	.long	3207688704              # float -0.693145751
.LCPI166_15:
	.long	3049242254              # float -1.42860677E-6
.LCPI166_16:
	.long	983314022               # float 0.00119156833
.LCPI166_17:
	.long	1026188988              # float 0.0416018814
.LCPI166_18:
	.long	1056964574              # float 0.499998987
.LCPI166_19:
	.long	967284723               # float 3.19659332E-4
.LCPI166_20:
	.long	1007360298              # float 0.00848988629
.LCPI166_21:
	.long	1042984479              # float 0.166679844
.LCPI166_22:
	.long	2139095040              # float +Inf
	.section	.rodata.cst8,"aM",@progbits,8
	.align	4
.LCPI166_11:
	.long	4286578688              # float -inf
	.long	2143289344              # float nan
	.section	".text.par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187","ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187,@function
par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187: # @"par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187"
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$280, %rsp              # imm = 0x118
	movq	%rsi, 224(%rsp)         # 8-byte Spill
	movl	(%rdx), %r14d
	movslq	8(%rdx), %r8
	movl	12(%rdx), %ebx
	movq	%rbx, 256(%rsp)         # 8-byte Spill
	movl	16(%rdx), %r9d
	movl	20(%rdx), %ecx
	movq	%rcx, 264(%rsp)         # 8-byte Spill
	movslq	32(%rdx), %rbp
	movq	%rbp, 272(%rsp)         # 8-byte Spill
	leal	(%rcx,%rsi,8), %eax
	leal	-8(%rbx,%rcx), %ecx
	cmpl	%eax, %ecx
	cmovgl	%eax, %ecx
	movl	%ecx, 160(%rsp)         # 4-byte Spill
	leal	(%r9,%r8), %eax
	cmpl	%eax, %r14d
	movl	%eax, %ecx
	cmovgel	%r14d, %ecx
	movl	%ebp, %ebx
	sarl	$31, %ebx
	andl	%ebp, %ebx
	movq	%rbx, 176(%rsp)         # 8-byte Spill
	cmpl	%r14d, %eax
	cmovll	%r14d, %eax
	addl	$-1, %eax
	addl	$-1, %ecx
	leal	-2(%r14), %esi
	cmpl	%esi, %ecx
	cmovll	%esi, %ecx
	xorl	%esi, %esi
	testl	%ecx, %ecx
	cmovnsl	%ecx, %esi
	cmpl	%esi, %eax
	cmovgel	%eax, %esi
	movl	$1, %eax
	subl	%ebx, %eax
	addl	%esi, %eax
	shlq	$5, %rax
	movabsq	$135291469824, %rcx     # imm = 0x1F80000000
	testq	%rcx, %rax
	jne	.LBB166_27
# BB#1:                                 # %assert succeeded
	movq	%r8, 248(%rsp)          # 8-byte Spill
	movq	%r9, 208(%rsp)          # 8-byte Spill
	vmovss	4(%rdx), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 172(%rsp)        # 4-byte Spill
	movslq	24(%rdx), %rbx
	movslq	28(%rdx), %rcx
	movq	%rcx, 240(%rsp)         # 8-byte Spill
	movslq	36(%rdx), %rcx
	movq	%rcx, 232(%rsp)         # 8-byte Spill
	movslq	40(%rdx), %r13
	movslq	44(%rdx), %r15
	movslq	48(%rdx), %r12
	movq	56(%rdx), %rbp
	movq	72(%rdx), %rcx
	movq	%rcx, 192(%rsp)         # 8-byte Spill
	orq	$4, %rax
	movq	%rdi, (%rsp)            # 8-byte Spill
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, %r8
	testq	%r8, %r8
	je	.LBB166_28
# BB#2:                                 # %for blur$1.s1.v10.preheader
	movslq	%r14d, %r9
	movq	%r9, 216(%rsp)          # 8-byte Spill
	movq	224(%rsp), %rdx         # 8-byte Reload
	shll	$3, %edx
	imulq	%rbx, %r13
	movq	%rbx, 224(%rsp)         # 8-byte Spill
	movq	%r15, %r10
	movq	%r15, 200(%rsp)         # 8-byte Spill
	subq	%r12, %r10
	addq	$1, %r10
	xorl	%eax, %eax
	addq	$-1, %r9
	cmovnsq	%r9, %rax
	addq	$-7, %rax
	movq	%rax, %rdi
	sarq	$63, %rdi
	andq	%rax, %rdi
	movq	%r10, %rcx
	imulq	%rdi, %rcx
	movq	%r13, %rax
	subq	%r12, %rax
	movq	%r12, 184(%rsp)         # 8-byte Spill
	subq	%rcx, %rax
	movq	272(%rsp), %rcx         # 8-byte Reload
	movq	%rcx, %r11
	sarq	$63, %r11
	andq	%rcx, %r11
	leaq	(,%r11,8), %rsi
	negq	%rsi
	movslq	160(%rsp), %r12         # 4-byte Folded Reload
	subq	%r12, %rsi
	movq	264(%rsp), %rcx         # 8-byte Reload
	movl	%ecx, %r15d
	notl	%r15d
	subl	%edx, %r15d
	movl	$7, %ebx
	movq	256(%rsp), %rdx         # 8-byte Reload
	subl	%edx, %ebx
	subl	%ecx, %ebx
	cmpl	%ebx, %r15d
	cmovgel	%r15d, %ebx
	notl	%ebx
	movslq	%ebx, %rbx
	leaq	(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	1(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	1(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	2(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	2(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	3(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	3(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	4(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	4(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	5(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	5(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	6(%rax,%rbx), %rcx
	movl	(%rbp,%rcx,4), %ecx
	leaq	6(%rsi,%rbx), %rdx
	movl	%ecx, (%r8,%rdx,4)
	leaq	7(%rax,%rbx), %rax
	leaq	7(%rsi,%rbx), %rcx
	movl	(%rbp,%rax,4), %eax
	movl	%eax, (%r8,%rcx,4)
	cmpl	$1, %r14d
	jle	.LBB166_21
# BB#3:                                 # %for blur$1.s2.r110$x.preheader
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	172(%rsp), %xmm1        # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vucomiss	%xmm1, %xmm0
	vmovd	%xmm1, %ecx
	movl	$1065353216, %eax       # imm = 0x3F800000
	cmovbl	%ecx, %eax
	vucomiss	%xmm1, %xmm0
	seta	%cl
	vmovss	.LCPI166_9(%rip), %xmm0 # xmm0 = mem[0],zero,zero,zero
	movzbl	%cl, %esi
	jae	.LBB166_4
# BB#5:                                 # %for blur$1.s2.r110$x.preheader
	movl	%eax, %ecx
	andl	$-2139095041, %ecx      # imm = 0xFFFFFFFF807FFFFF
	movl	%ecx, %edx
	sarl	$22, %edx
	movl	$127, %esi
	subl	%edx, %esi
	sarl	$23, %eax
	subl	%esi, %eax
	shll	$23, %esi
	orl	%ecx, %esi
	vmovd	%esi, %xmm1
	vaddss	.LCPI166_0(%rip), %xmm1, %xmm1
	vmulss	%xmm1, %xmm1, %xmm2
	vcvtsi2ssl	%eax, %xmm0, %xmm3
	vmovss	.LCPI166_1(%rip), %xmm4 # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI166_2(%rip), %xmm2, %xmm4
	vfmadd213ss	.LCPI166_3(%rip), %xmm2, %xmm4
	vfmadd213ss	.LCPI166_4(%rip), %xmm2, %xmm4
	vmovss	.LCPI166_5(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI166_6(%rip), %xmm2, %xmm5
	vfmadd213ss	.LCPI166_7(%rip), %xmm2, %xmm5
	vfmadd213ss	.LCPI166_8(%rip), %xmm2, %xmm5
	vfmadd213ss	%xmm0, %xmm2, %xmm5
	vmulss	%xmm5, %xmm1, %xmm1
	vfmadd213ss	%xmm1, %xmm2, %xmm4
	vmovss	.LCPI166_10(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm4, %xmm3, %xmm1
	jmp	.LBB166_6
.LBB166_4:
	leaq	.LCPI166_11(%rip), %rax
	vmovss	(%rax,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
.LBB166_6:                              # %for blur$1.s2.r110$x.preheader
	vmulss	.LCPI166_12(%rip), %xmm1, %xmm2
	vmulss	.LCPI166_13(%rip), %xmm2, %xmm1
	vroundss	$1, %xmm1, %xmm0, %xmm1
	vcvttss2si	%xmm1, %eax
	leal	127(%rax), %esi
	cmpl	$255, %esi
	jl	.LBB166_7
# BB#8:                                 # %for blur$1.s2.r110$x.preheader
	vmovss	.LCPI166_22(%rip), %xmm1 # xmm1 = mem[0],zero,zero,zero
	jmp	.LBB166_9
.LBB166_7:
	vmovss	.LCPI166_14(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm2, %xmm1, %xmm3
	vmovss	.LCPI166_15(%rip), %xmm2 # xmm2 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm3, %xmm1, %xmm2
	vmulss	%xmm2, %xmm2, %xmm1
	shll	$23, %esi
	vmovss	.LCPI166_16(%rip), %xmm3 # xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI166_17(%rip), %xmm1, %xmm3
	vmovd	%esi, %xmm4
	vfmadd213ss	.LCPI166_18(%rip), %xmm1, %xmm3
	vmovss	.LCPI166_19(%rip), %xmm5 # xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	.LCPI166_20(%rip), %xmm1, %xmm5
	vfmadd213ss	.LCPI166_21(%rip), %xmm1, %xmm5
	vfmadd213ss	%xmm0, %xmm1, %xmm3
	vfmadd213ss	%xmm0, %xmm1, %xmm5
	vfmadd213ss	%xmm3, %xmm2, %xmm5
	vmulss	%xmm5, %xmm4, %xmm1
.LBB166_9:                              # %for blur$1.s2.r110$x.preheader
	cmpl	$-127, %eax
	jg	.LBB166_11
# BB#10:                                # %for blur$1.s2.r110$x.preheader
	vxorps	%xmm1, %xmm1, %xmm1
.LBB166_11:                             # %for blur$1.s2.r110$x.preheader
	vsubss	%xmm1, %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vbroadcastss	%xmm1, %ymm1
	movl	$1, %esi
	testb	$1, %r14b
	jne	.LBB166_13
# BB#12:                                # %for blur$1.s2.r110$x.prol
	leaq	(%r12,%r13), %rax
	subq	184(%rsp), %rax         # 8-byte Folded Reload
	movl	$1, %ecx
	movl	$1, %edx
	subq	%rdi, %rdx
	imulq	%r10, %rdx
	addq	%rax, %rdx
	vmulps	(%rbp,%rdx,4), %ymm0, %ymm2
	subq	%r11, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r8), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, (%rcx,%r8)
	movl	$2, %esi
.LBB166_13:                             # %for blur$1.s2.r110$x.preheader.split
	cmpl	$2, %r14d
	movq	200(%rsp), %rcx         # 8-byte Reload
	je	.LBB166_16
# BB#14:                                # %for blur$1.s2.r110$x.preheader.split.split
	movl	%r14d, %edx
	subl	%esi, %edx
	xorl	%eax, %eax
	testq	%r9, %r9
	cmovnsq	%r9, %rax
	movl	$6, %edi
	subq	%rax, %rdi
	cmpq	$-2, %rdi
	movq	$-1, %r9
	cmovleq	%r9, %rdi
	leaq	1(%rsi,%rdi), %rax
	addq	$1, %rcx
	movq	184(%rsp), %r12         # 8-byte Reload
	subq	%r12, %rcx
	imulq	%rcx, %rax
	addq	%r13, %rax
	movl	$7, %ebx
	movq	%rcx, %r10
	movq	256(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %ebx
	movq	264(%rsp), %rcx         # 8-byte Reload
	subl	%ecx, %ebx
	cmpl	%ebx, %r15d
	cmovgel	%r15d, %ebx
	notl	%ebx
	movslq	%ebx, %rbx
	addq	%rbx, %rax
	subq	%r12, %rax
	movq	272(%rsp), %rcx         # 8-byte Reload
	notq	%rcx
	cmpq	$-2, %rcx
	cmovleq	%r9, %rcx
	leaq	(%rcx,%rsi), %rcx
	leaq	2(%rsi,%rdi), %rsi
	imulq	%r10, %rsi
	addq	%r13, %rsi
	addq	%rbx, %rsi
	subq	%r12, %rsi
	shlq	$5, %rcx
	leaq	(%rcx,%r8), %rdi
	leaq	(,%r10,8), %rbx
	.align	16, 0x90
.LBB166_15:                             # %for blur$1.s2.r110$x
                                        # =>This Inner Loop Header: Depth=1
	vmulps	(%rbp,%rax,4), %ymm0, %ymm2
	vmovaps	(%rdi), %ymm3
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 32(%rdi)
	vmulps	(%rbp,%rsi,4), %ymm0, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm3
	vmovaps	%ymm3, 64(%rdi)
	addq	$64, %rdi
	addq	%rbx, %rbp
	addl	$-2, %edx
	jne	.LBB166_15
.LBB166_16:                             # %for blur$1.s3.r110$x.preheader
	movl	$1, %edx
	testb	$1, %r14b
	movq	176(%rsp), %rbp         # 8-byte Reload
	jne	.LBB166_18
# BB#17:                                # %for blur$1.s3.r110$x.prol
	movl	%r14d, %eax
	subl	%ebp, %eax
	movq	216(%rsp), %rcx         # 8-byte Reload
	subq	%r11, %rcx
	addl	$-1, %eax
	cltq
	shlq	$5, %rax
	vmovaps	-32(%rax,%r8), %ymm2
	vmulps	(%rax,%r8), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	shlq	$5, %rcx
	vmovaps	%ymm2, -64(%rcx,%r8)
	movl	$2, %edx
.LBB166_18:                             # %for blur$1.s3.r110$x.preheader.split
	cmpl	$2, %r14d
	je	.LBB166_21
# BB#19:                                # %for blur$1.s3.r110$x.preheader.split.split
	negl	%ebp
	movl	%ebp, %eax
	subl	%edx, %eax
	movq	272(%rsp), %rcx         # 8-byte Reload
	notq	%rcx
	cmpq	$-2, %rcx
	movq	$-1, %rdi
	cmovgq	%rcx, %rdi
	addq	216(%rsp), %rdi         # 8-byte Folded Reload
	movq	%rdi, %rcx
	shlq	$5, %rcx
	subq	%rdx, %rdi
	movq	%rdx, %rsi
	shlq	$5, %rsi
	subq	%rsi, %rcx
	leaq	(%rcx,%r8), %rsi
	shlq	$5, %rdi
	leaq	-32(%rdi,%r8), %rdi
	leal	1(%rdx), %ecx
	subl	%ecx, %ebp
	.align	16, 0x90
.LBB166_20:                             # %for blur$1.s3.r110$x
                                        # =>This Inner Loop Header: Depth=1
	leal	(%rax,%r14), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r8), %ymm2
	vmulps	(%rcx,%r8), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rsi)
	leal	(%rbp,%r14), %ecx
	movslq	%ecx, %rcx
	shlq	$5, %rcx
	vmovaps	-32(%rcx,%r8), %ymm2
	vmulps	(%rcx,%r8), %ymm1, %ymm3
	vfmadd213ps	%ymm3, %ymm0, %ymm2
	vmovaps	%ymm2, (%rdi)
	addl	$-2, %r14d
	addq	$-64, %rsi
	addq	$-64, %rdi
	cmpl	%r14d, %edx
	jne	.LBB166_20
.LBB166_21:                             # %consume blur$1
	movq	%r8, 8(%rsp)            # 8-byte Spill
	movq	248(%rsp), %rbp         # 8-byte Reload
	leal	7(%rbp), %eax
	sarl	$3, %eax
	movl	%eax, 184(%rsp)         # 4-byte Spill
	testl	%eax, %eax
	movq	208(%rsp), %rdx         # 8-byte Reload
	jle	.LBB166_26
# BB#22:                                # %for transpose$1.s0.v10.v185.preheader
	movq	272(%rsp), %r8          # 8-byte Reload
	movq	%r8, %rax
	notq	%rax
	cmpq	$-2, %rax
	movq	$-1, %rcx
	cmovgq	%rax, %rcx
	movq	%rcx, 176(%rsp)         # 8-byte Spill
	movl	$7, %ecx
	movl	$7, %eax
	subl	%edx, %eax
	notl	%edx
	subl	%ebp, %eax
	movl	%eax, 172(%rsp)         # 4-byte Spill
	movq	8(%rsp), %rsi           # 8-byte Reload
	leaq	260(%rsi), %rax
	movq	%rax, 160(%rsp)         # 8-byte Spill
	cmpq	$7, %rbp
	movl	$8, %eax
	cmovgq	%rbp, %rax
	movq	256(%rsp), %rbp         # 8-byte Reload
	subl	%ebp, %ecx
	movq	264(%rsp), %rbp         # 8-byte Reload
	subl	%ebp, %ecx
	cmpl	%ecx, %r15d
	cmovgel	%r15d, %ecx
	notl	%ecx
	movslq	%ecx, %rbx
	leaq	1(%rbx), %rdi
	movq	232(%rsp), %rbp         # 8-byte Reload
	subq	%rbp, %rdi
	imulq	%rax, %rdi
	movq	240(%rsp), %rcx         # 8-byte Reload
	imulq	224(%rsp), %rcx         # 8-byte Folded Reload
	addq	%rcx, %rdi
	subq	%r8, %rdi
	movq	%rdi, 144(%rsp)         # 8-byte Spill
	movq	%rsi, %rdi
	subq	$-128, %rdi
	movq	%rdi, 136(%rsp)         # 8-byte Spill
	leaq	228(%rsi), %rdi
	movq	%rdi, 128(%rsp)         # 8-byte Spill
	leaq	196(%rsi), %rdi
	movq	%rdi, 120(%rsp)         # 8-byte Spill
	leaq	164(%rsi), %rdi
	movq	%rdi, 112(%rsp)         # 8-byte Spill
	leaq	132(%rsi), %rdi
	movq	%rdi, 104(%rsp)         # 8-byte Spill
	subq	%rbp, %rbx
	leaq	100(%rsi), %rdi
	movq	%rdi, 96(%rsp)          # 8-byte Spill
	imulq	%rax, %rbx
	addq	%rcx, %rbx
	leaq	68(%rsi), %rcx
	movq	%rcx, 88(%rsp)          # 8-byte Spill
	subq	%r8, %rbx
	movq	%rbx, 152(%rsp)         # 8-byte Spill
	leaq	36(%rsi), %rcx
	movq	%rcx, 80(%rsp)          # 8-byte Spill
	leaq	(,%rax,8), %rax
	movq	%rax, 72(%rsp)          # 8-byte Spill
	leaq	256(%rsi), %rax
	movq	%rax, 64(%rsp)          # 8-byte Spill
	leaq	224(%rsi), %rax
	movq	%rax, 56(%rsp)          # 8-byte Spill
	leaq	192(%rsi), %rax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	leaq	160(%rsi), %rax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	leaq	96(%rsi), %rax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	leaq	64(%rsi), %rax
	movq	%rax, 24(%rsp)          # 8-byte Spill
	leaq	32(%rsi), %rax
	movq	%rax, 16(%rsp)          # 8-byte Spill
	xorl	%eax, %eax
	movq	72(%rsp), %r10          # 8-byte Reload
	.align	16, 0x90
.LBB166_23:                             # %for transpose$1.s0.v10.v185
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB166_24 Depth 2
	movq	%rax, 200(%rsp)         # 8-byte Spill
	movq	%rdx, 208(%rsp)         # 8-byte Spill
	movl	172(%rsp), %eax         # 4-byte Reload
	cmpl	%eax, %edx
	cmovgel	%edx, %eax
	notl	%eax
	cltq
	movq	176(%rsp), %rcx         # 8-byte Reload
	leaq	(%rcx,%rax), %r8
	shlq	$5, %r8
	movq	144(%rsp), %rcx         # 8-byte Reload
	leaq	(%rcx,%rax), %r9
	movq	152(%rsp), %rcx         # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 272(%rsp)         # 8-byte Spill
	movq	160(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 264(%rsp)         # 8-byte Spill
	movq	128(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 256(%rsp)         # 8-byte Spill
	movq	120(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 248(%rsp)         # 8-byte Spill
	movq	112(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 240(%rsp)         # 8-byte Spill
	movq	104(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 232(%rsp)         # 8-byte Spill
	movq	96(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 224(%rsp)         # 8-byte Spill
	movq	88(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	%rax, 216(%rsp)         # 8-byte Spill
	movq	80(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %r13
	movq	64(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %r11
	movq	56(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %r15
	movq	48(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %rdi
	movq	40(%rsp), %rax          # 8-byte Reload
	leaq	(%rax,%r8), %rdx
	movq	136(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r8), %rax
	movq	32(%rsp), %rcx          # 8-byte Reload
	leaq	(%rcx,%r8), %rbp
	movq	24(%rsp), %rcx          # 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	16(%rsp), %rsi          # 8-byte Reload
	leaq	(%r8,%rsi), %r12
	movq	%r9, %rsi
	movq	192(%rsp), %r8          # 8-byte Reload
	xorl	%r9d, %r9d
	.align	16, 0x90
.LBB166_24:                             # %for transpose$1.s0.v11.v11
                                        #   Parent Loop BB166_23 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rdx,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r11,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r12,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbp,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rax,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	272(%rsp), %r14         # 8-byte Reload
	vmovups	%ymm0, (%r8,%r14,4)
	movq	240(%rsp), %rbx         # 8-byte Reload
	vmovss	(%rbx,%r9), %xmm0       # xmm0 = mem[0],zero,zero,zero
	movq	248(%rsp), %rbx         # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	256(%rsp), %rbx         # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	264(%rsp), %rbx         # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r13,%r9), %xmm1       # xmm1 = mem[0],zero,zero,zero
	movq	216(%rsp), %rbx         # 8-byte Reload
	vinsertps	$16, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	224(%rsp), %rbx         # 8-byte Reload
	vinsertps	$32, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	232(%rsp), %rbx         # 8-byte Reload
	vinsertps	$48, (%rbx,%r9), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovups	%ymm0, (%r8,%rsi,4)
	addq	$8, %r9
	addq	%r10, %r8
	cmpq	$32, %r9
	jne	.LBB166_24
# BB#25:                                # %end for transpose$1.s0.v11.v11
                                        #   in Loop: Header=BB166_23 Depth=1
	movq	200(%rsp), %rax         # 8-byte Reload
	addl	$1, %eax
	movq	208(%rsp), %rdx         # 8-byte Reload
	addl	$-8, %edx
	cmpl	184(%rsp), %eax         # 4-byte Folded Reload
	jne	.LBB166_23
.LBB166_26:                             # %call_destructor.exit
	movq	(%rsp), %rdi            # 8-byte Reload
	movq	8(%rsp), %rsi           # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%eax, %eax
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.LBB166_27:                             # %assert failed
	leaq	.Lstr.182(%rip), %rsi
	movl	$2147483647, %ecx       # imm = 0x7FFFFFFF
	movq	%rax, %rdx
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_buffer_allocation_too_large@PLT # TAILCALL
.LBB166_28:                             # %assert failed1
	movq	(%rsp), %rdi            # 8-byte Reload
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_out_of_memory@PLT # TAILCALL
.Lfunc_end166:
	.size	par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187, .Lfunc_end166-par_for_par_for___sharpi_transpose$1.s0.v12_transpose$1.s0.v11.v187

	.section	.text.par_for___sharpi_sharpi.s0.v12,"ax",@progbits
	.align	16, 0x90
	.type	par_for___sharpi_sharpi.s0.v12,@function
par_for___sharpi_sharpi.s0.v12:         # @par_for___sharpi_sharpi.s0.v12
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	subq	$120, %rsp
	vmovups	(%rdx), %xmm0
	movl	16(%rdx), %r8d
	movb	20(%rdx), %al
	andb	$1, %al
	vmovups	24(%rdx), %xmm1
	movl	40(%rdx), %r9d
	movl	44(%rdx), %r10d
	movl	48(%rdx), %r11d
	movl	52(%rdx), %ebx
	movl	56(%rdx), %ebp
	movl	60(%rdx), %r14d
	movl	64(%rdx), %r15d
	vmovups	72(%rdx), %ymm2
	vmovups	104(%rdx), %xmm3
	leal	31(%r9), %ecx
	sarl	$5, %ecx
	vmovaps	%xmm0, (%rsp)
	movl	%r8d, 16(%rsp)
	movb	%al, 20(%rsp)
	vmovups	%xmm1, 24(%rsp)
	movl	%r9d, 40(%rsp)
	movl	%r10d, 44(%rsp)
	movl	%r11d, 48(%rsp)
	movl	%esi, 52(%rsp)
	movl	%ebx, 56(%rsp)
	movl	%ebp, 60(%rsp)
	movl	%r14d, 64(%rsp)
	movl	%r15d, 68(%rsp)
	vmovups	%ymm2, 72(%rsp)
	vmovups	%xmm3, 104(%rsp)
	leaq	par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14(%rip), %rsi
	leaq	(%rsp), %r8
	xorl	%edx, %edx
	vzeroupper
	callq	halide_do_par_for@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end167:
	.size	par_for___sharpi_sharpi.s0.v12, .Lfunc_end167-par_for___sharpi_sharpi.s0.v12

	.section	.rodata.cst4,"aM",@progbits,4
	.align	4
.LCPI168_0:
	.long	1065353216              # float 1
.LCPI168_1:
	.long	255                     # 0xff
.LCPI168_2:
	.long	1069066811              # float 1.44269502
.LCPI168_3:
	.long	901758606               # float 1.42860677E-6
.LCPI168_4:
	.long	1060205056              # float 0.693145751
.LCPI168_5:
	.long	127                     # 0x7f
.LCPI168_6:
	.long	983314022               # float 0.00119156833
.LCPI168_7:
	.long	1026188988              # float 0.0416018814
.LCPI168_8:
	.long	1056964574              # float 0.499998987
.LCPI168_9:
	.long	967284723               # float 3.19659332E-4
.LCPI168_10:
	.long	1007360298              # float 0.00848988629
.LCPI168_11:
	.long	1042984479              # float 0.166679844
.LCPI168_12:
	.long	4286578688              # float -Inf
.LCPI168_13:
	.long	1132396544              # float 255
	.section	.rodata,"a",@progbits
	.align	32
.LCPI168_14:
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	0                       # 0x0
	.byte	1                       # 0x1
	.byte	4                       # 0x4
	.byte	5                       # 0x5
	.byte	8                       # 0x8
	.byte	9                       # 0x9
	.byte	12                      # 0xc
	.byte	13                      # 0xd
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.byte	128                     # 0x80
	.section	.text.par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14,"ax",@progbits
	.align	16, 0x90
	.type	par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14,@function
par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14: # @par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14
# BB#0:                                 # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movl	8(%rdx), %r8d
	movl	12(%rdx), %ebp
	movb	20(%rdx), %bl
	andb	$1, %bl
	leaq	24(%rdx), %r14
	leaq	32(%rdx), %rcx
	movl	36(%rdx), %eax
	movl	%eax, -92(%rsp)         # 4-byte Spill
	movslq	52(%rdx), %r9
	movl	$2, %edi
	subl	%r9d, %edi
	testb	%bl, %bl
	cmovel	%r9d, %edi
	movl	%ebp, %ebx
	sarl	$31, %ebx
	andl	%ebp, %ebx
	movl	%r8d, %r15d
	sarl	$31, %r15d
	andl	%r8d, %r15d
	cmpl	$7, %eax
	movl	$8, %r8d
	cmovgl	%eax, %r8d
	movslq	60(%rdx), %rax
	movslq	(%rdx), %rbp
	movq	%r9, %r10
	imulq	%rbp, %r10
	movslq	%edi, %r12
	movq	%r12, %r13
	imulq	%rax, %r13
	imulq	%r9, %rax
	movq	%rax, 80(%rsp)          # 8-byte Spill
	cmpl	$1, %r12d
	cmoveq	%r14, %rcx
	leaq	28(%rdx), %rdi
	testl	%r12d, %r12d
	cmoveq	%rdi, %rcx
	movslq	4(%rdx), %rdi
	movq	%rdi, 88(%rsp)          # 8-byte Spill
	vmovss	(%rcx), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movslq	48(%rdx), %r14
	imulq	%rbp, %r12
	movl	%r14d, %ebp
	notl	%ebp
	shll	$5, %esi
	subl	%esi, %ebp
	movl	$31, %eax
	subl	40(%rdx), %eax
	subl	%r14d, %eax
	cmpl	%eax, %ebp
	cmovgel	%ebp, %eax
	notl	%ebx
	subl	%eax, %ebx
	leal	1(%rdi), %ecx
	imull	%ebx, %ecx
	movl	68(%rdx), %ebx
	movslq	%r8d, %rsi
	notl	%ebx
	subl	%eax, %ebx
	imull	%esi, %ebx
	movl	44(%rdx), %ebp
	leal	(%rbx,%rbp), %r11d
	addl	%ebp, %ecx
	subl	%r15d, %ecx
	movq	72(%rdx), %r8
	notl	%eax
	movslq	%eax, %rbx
	subq	%r14, %rbx
	movslq	56(%rdx), %rbp
	imulq	%rbp, %rbx
	addq	%r9, %rbx
	movq	104(%rdx), %r15
	vbroadcastss	16(%rdx), %ymm7
	subl	64(%rdx), %r11d
	addq	88(%rdx), %rbx
	leaq	1(%rdi), %rdx
	vaddss	.LCPI168_0(%rip), %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm6
	vpbroadcastd	.LCPI168_1(%rip), %ymm2
	vbroadcastss	.LCPI168_2(%rip), %ymm12
	vbroadcastss	.LCPI168_3(%rip), %ymm9
	vbroadcastss	.LCPI168_4(%rip), %ymm10
	vbroadcastss	.LCPI168_5(%rip), %ymm0
	vmovups	%ymm0, 32(%rsp)         # 32-byte Spill
	vbroadcastss	.LCPI168_6(%rip), %ymm8
	vbroadcastss	.LCPI168_7(%rip), %ymm14
	vbroadcastss	.LCPI168_8(%rip), %ymm5
	vbroadcastss	.LCPI168_0(%rip), %ymm11
	vbroadcastss	.LCPI168_9(%rip), %ymm0
	vmovups	%ymm0, (%rsp)           # 32-byte Spill
	vbroadcastss	.LCPI168_10(%rip), %ymm0
	vmovups	%ymm0, -32(%rsp)        # 32-byte Spill
	vpbroadcastd	.LCPI168_11(%rip), %ymm0
	vmovdqu	%ymm0, -64(%rsp)        # 32-byte Spill
	vmovdqa	.LCPI168_14(%rip), %ymm13 # ymm13 = [0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128,0,1,4,5,8,9,12,13,128,128,128,128,128,128,128,128]
	leal	(,%rsi,8), %eax
	movl	%eax, -96(%rsp)         # 4-byte Spill
	leal	8(,%rdi,8), %eax
	movl	%eax, -100(%rsp)        # 4-byte Spill
	leaq	(,%rbp,8), %rax
	movq	%rax, -112(%rsp)        # 8-byte Spill
	xorl	%eax, %eax
	.align	16, 0x90
.LBB168_1:                              # %for sharpi.s0.v11.v13.v13
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB168_2 Depth 2
	movq	%rax, -88(%rsp)         # 8-byte Spill
	movq	%rbx, -80(%rsp)         # 8-byte Spill
	movl	%r11d, -72(%rsp)        # 4-byte Spill
	movl	%ecx, -68(%rsp)         # 4-byte Spill
	movl	-92(%rsp), %eax         # 4-byte Reload
	movl	%eax, %edi
	movl	%ecx, %r14d
	testl	%eax, %eax
	vxorps	%ymm3, %ymm3, %ymm3
	jle	.LBB168_3
	.align	16, 0x90
.LBB168_2:                              # %for sharpi.s0.v10
                                        #   Parent Loop BB168_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%edi, 100(%rsp)         # 4-byte Spill
	movslq	%r11d, %r11
	movq	%r13, %rax
	movq	80(%rsp), %rcx          # 8-byte Reload
	leaq	(%r11,%rcx), %r13
	leaq	(%r15,%r13,4), %r9
	movq	%r8, %rdi
	movq	%r12, %r8
	movq	%r10, %r12
	leaq	(%r9,%rsi,4), %r10
	vmovss	(%r15,%r13,4), %xmm15   # xmm15 = mem[0],zero,zero,zero
	movq	%rax, %r13
	leaq	(%r10,%rsi,4), %rax
	vinsertps	$16, (%r9,%rsi,4), %xmm15, %xmm0 # xmm0 = xmm15[0],mem[0],xmm15[2,3]
	movq	%rbx, %r9
	leaq	(%rax,%rsi,4), %rbx
	vmovss	(%rbx,%rsi,4), %xmm1    # xmm1 = mem[0],zero,zero,zero
	leaq	(%rbx,%rsi,4), %rbx
	vinsertps	$32, (%r10,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r12, %r10
	movq	%r8, %r12
	movq	%rdi, %r8
	movq	88(%rsp), %rdi          # 8-byte Reload
	leaq	(%rbx,%rsi,4), %rcx
	vinsertps	$48, (%rax,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %r14
	leaq	(%r14,%r10), %rax
	vinsertps	$16, (%rbx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	leaq	(%r8,%rax), %rbx
	vpinsrb	$0, (%r8,%rax), %xmm0, %xmm4
	vpinsrb	$2, 1(%rdi,%rbx), %xmm4, %xmm4
	leaq	(%rcx,%rsi,4), %rax
	leaq	(%rbx,%rdx), %rbx
	vpinsrb	$4, 1(%rdi,%rbx), %xmm4, %xmm4
	leaq	(%rbx,%rdx), %rbx
	vinsertps	$32, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	leaq	(%rbx,%rdx), %rcx
	vpinsrb	$6, 1(%rdi,%rbx), %xmm4, %xmm4
	leaq	(%rcx,%rdx), %rbx
	vinsertps	$48, (%rax,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	leaq	(%rbx,%rdx), %rax
	vpinsrb	$8, 1(%rdi,%rcx), %xmm4, %xmm4
	vpinsrb	$10, 1(%rdi,%rbx), %xmm4, %xmm4
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vpinsrb	$12, 1(%rdi,%rax), %xmm4, %xmm1
	addq	%rdx, %rax
	vpinsrb	$14, 1(%rdi,%rax), %xmm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpand	%ymm2, %ymm1, %ymm1
	vcvtdq2ps	%ymm1, %ymm1
	vsubps	%ymm0, %ymm1, %ymm0
	vmulps	%ymm0, %ymm0, %ymm0
	vfnmadd213ps	%ymm3, %ymm7, %ymm0
	vmulps	%ymm12, %ymm0, %ymm1
	vroundps	$1, %ymm1, %ymm1
	vcvttps2dq	%ymm1, %ymm4
	vmovaps	%ymm10, %ymm15
	vfnmadd213ps	%ymm0, %ymm1, %ymm15
	vfnmadd213ps	%ymm15, %ymm9, %ymm1
	vmulps	%ymm1, %ymm1, %ymm0
	vmovaps	%ymm8, %ymm15
	vfmadd213ps	%ymm14, %ymm0, %ymm15
	vfmadd213ps	%ymm5, %ymm0, %ymm15
	vmovaps	%ymm5, %ymm3
	vmovaps	%ymm14, %ymm5
	vmovaps	%ymm8, %ymm14
	vmovaps	%ymm10, %ymm8
	vmovaps	%ymm9, %ymm10
	vmovaps	%ymm12, %ymm9
	vmovups	(%rsp), %ymm12          # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm0, %ymm15
	vfmadd213ps	-32(%rsp), %ymm0, %ymm12 # 32-byte Folded Reload
	vfmadd213ps	-64(%rsp), %ymm0, %ymm12 # 32-byte Folded Reload
	vfmadd213ps	%ymm11, %ymm0, %ymm12
	leaq	(%r11,%r13), %rax
	leaq	(%r15,%rax,4), %rcx
	leaq	(%rcx,%rsi,4), %rbx
	vpaddd	32(%rsp), %ymm4, %ymm0  # 32-byte Folded Reload
	vfmadd213ps	%ymm15, %ymm1, %ymm12
	vpslld	$23, %ymm0, %ymm1
	vfnmadd213ps	%ymm11, %ymm1, %ymm12
	vbroadcastss	.LCPI168_12(%rip), %ymm1
	vpcmpgtd	%ymm0, %ymm2, %ymm4
	vblendvps	%ymm4, %ymm12, %ymm1, %ymm1
	vmovss	(%r15,%rax,4), %xmm4    # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	leaq	(%rbx,%rsi,4), %rax
	leaq	(%rax,%rsi,4), %rcx
	vmovaps	%ymm7, %ymm12
	vmovss	(%rcx,%rsi,4), %xmm7    # xmm7 = mem[0],zero,zero,zero
	leaq	(%rcx,%rsi,4), %rcx
	vinsertps	$16, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	leaq	(%rcx,%rsi,4), %rcx
	vinsertps	$32, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	leaq	(%rcx,%rsi,4), %rcx
	vinsertps	$48, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r9, %rbx
	vinsertps	$48, (%rax,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm4, %ymm4
	vmovaps	%ymm12, %ymm7
	vmovaps	%ymm9, %ymm12
	vmovaps	%ymm10, %ymm9
	vmovaps	%ymm8, %ymm10
	vmovaps	%ymm14, %ymm8
	vmovaps	%ymm5, %ymm14
	vmovaps	%ymm3, %ymm5
	vxorps	%ymm3, %ymm3, %ymm3
	vpcmpgtd	%ymm3, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm11, %ymm0
	leaq	(%r14,%r12), %rax
	vpinsrb	$0, (%r8,%rax), %xmm0, %xmm1
	leaq	(%r8,%rax), %rax
	vpinsrb	$2, 1(%rdi,%rax), %xmm1, %xmm1
	leaq	(%rax,%rdx), %rax
	vpinsrb	$4, 1(%rdi,%rax), %xmm1, %xmm1
	leaq	(%rax,%rdx), %rax
	vpinsrb	$6, 1(%rdi,%rax), %xmm1, %xmm1
	leaq	(%rax,%rdx), %rax
	vpinsrb	$8, 1(%rdi,%rax), %xmm1, %xmm1
	leaq	(%rax,%rdx), %rax
	vpinsrb	$10, 1(%rdi,%rax), %xmm1, %xmm1
	leaq	(%rax,%rdx), %rax
	vpinsrb	$12, 1(%rdi,%rax), %xmm1, %xmm1
	addq	%rdx, %rax
	vpinsrb	$14, 1(%rdi,%rax), %xmm1, %xmm1
	movl	100(%rsp), %edi         # 4-byte Reload
	vpmovzxwd	%xmm1, %ymm1    # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpand	%ymm2, %ymm1, %ymm1
	vcvtdq2ps	%ymm1, %ymm1
	vsubps	%ymm4, %ymm1, %ymm1
	vmulps	%ymm0, %ymm1, %ymm0
	vfmadd213ps	%ymm4, %ymm6, %ymm0
	vbroadcastss	.LCPI168_13(%rip), %ymm1
	vminps	%ymm1, %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vcvttps2dq	%ymm0, %ymm0
	vpshufb	%ymm13, %ymm0, %ymm0
	vpermq	$232, %ymm0, %ymm0      # ymm0 = ymm0[0,2,2,3]
	leaq	(%rbx,%rbp), %rax
	vpextrb	$0, %xmm0, (%rbx)
	vpextrb	$2, %xmm0, (%rbx,%rbp)
	vpextrb	$4, %xmm0, (%rbp,%rax)
	addq	%rbp, %rax
	vpextrb	$6, %xmm0, (%rbp,%rax)
	addq	%rbp, %rax
	vpextrb	$8, %xmm0, (%rbp,%rax)
	addq	%rbp, %rax
	vpextrb	$10, %xmm0, (%rbp,%rax)
	addq	%rbp, %rax
	vpextrb	$12, %xmm0, (%rbp,%rax)
	addq	%rbp, %rax
	vpextrb	$14, %xmm0, (%rbp,%rax)
	addl	$1, %r11d
	addl	$1, %r14d
	addq	$3, %rbx
	addl	$-1, %edi
	jne	.LBB168_2
.LBB168_3:                              # %end for sharpi.s0.v10
                                        #   in Loop: Header=BB168_1 Depth=1
	movq	-88(%rsp), %rax         # 8-byte Reload
	addq	$1, %rax
	movl	-72(%rsp), %r11d        # 4-byte Reload
	addl	-96(%rsp), %r11d        # 4-byte Folded Reload
	movl	-68(%rsp), %ecx         # 4-byte Reload
	addl	-100(%rsp), %ecx        # 4-byte Folded Reload
	movq	-80(%rsp), %rbx         # 8-byte Reload
	addq	-112(%rsp), %rbx        # 8-byte Folded Reload
	cmpq	$4, %rax
	jne	.LBB168_1
# BB#4:                                 # %destructor_block
	xorl	%eax, %eax
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end168:
	.size	par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14, .Lfunc_end168-par_for_par_for___sharpi_sharpi.s0.v12_sharpi.s0.v11.v14

	.section	.text.sharpi,"ax",@progbits
	.globl	sharpi
	.align	16, 0x90
	.type	sharpi,@function
sharpi:                                 # @sharpi
# BB#0:                                 # %entry
	testq	%rdi, %rdi
	je	.LBB169_7
# BB#1:                                 # %assert succeeded
	testq	%rcx, %rcx
	je	.LBB169_8
# BB#2:                                 # %assert succeeded11
	testq	%r8, %r8
	je	.LBB169_9
# BB#3:                                 # %assert succeeded30
	testq	%r9, %r9
	je	.LBB169_10
# BB#4:                                 # %assert succeeded49
	movq	80(%rsp), %rax
	testq	%rax, %rax
	je	.LBB169_11
# BB#5:                                 # %assert succeeded68
	movq	96(%rsp), %rax
	testq	%rax, %rax
	je	.LBB169_12
# BB#6:                                 # %assert succeeded87
	jmp	__sharpi@PLT            # TAILCALL
.LBB169_7:                              # %assert failed
	leaq	.Lstr(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB169_8:                              # %assert failed10
	leaq	.Lstr.137(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB169_9:                              # %assert failed29
	leaq	.Lstr.138(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB169_10:                             # %assert failed48
	leaq	.Lstr.139(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB169_11:                             # %assert failed67
	leaq	.Lstr.140(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.LBB169_12:                             # %assert failed86
	leaq	.Lstr.141(%rip), %rsi
	xorl	%edi, %edi
	jmp	halide_error_buffer_argument_is_null@PLT # TAILCALL
.Lfunc_end169:
	.size	sharpi, .Lfunc_end169-sharpi

	.section	.text.sharpi_argv,"ax",@progbits
	.globl	sharpi_argv
	.align	16, 0x90
	.type	sharpi_argv,@function
sharpi_argv:                            # @sharpi_argv
# BB#0:                                 # %entry
	pushq	%r14
	pushq	%rbx
	subq	$104, %rsp
	movq	%rdi, %r10
	movq	(%r10), %rdi
	movq	8(%r10), %rcx
	movl	(%rcx), %esi
	movq	16(%r10), %rcx
	movl	(%rcx), %edx
	movq	24(%r10), %rcx
	movq	32(%r10), %r8
	movq	40(%r10), %rax
	vmovss	(%rax), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 100(%rsp)        # 4-byte Spill
	movq	48(%r10), %rax
	vmovss	(%rax), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	56(%r10), %rax
	vmovss	(%rax), %xmm2           # xmm2 = mem[0],zero,zero,zero
	movq	64(%r10), %rax
	vmovss	(%rax), %xmm3           # xmm3 = mem[0],zero,zero,zero
	movq	72(%r10), %rax
	vmovss	(%rax), %xmm4           # xmm4 = mem[0],zero,zero,zero
	movq	80(%r10), %rax
	vmovss	(%rax), %xmm5           # xmm5 = mem[0],zero,zero,zero
	movq	88(%r10), %rax
	vmovss	(%rax), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	96(%r10), %rax
	vmovss	(%rax), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	104(%r10), %rax
	vmovss	(%rax), %xmm8           # xmm8 = mem[0],zero,zero,zero
	movq	112(%r10), %rax
	vmovss	(%rax), %xmm9           # xmm9 = mem[0],zero,zero,zero
	movq	120(%r10), %rax
	vmovss	(%rax), %xmm10          # xmm10 = mem[0],zero,zero,zero
	movq	128(%r10), %rax
	vmovss	(%rax), %xmm11          # xmm11 = mem[0],zero,zero,zero
	movq	136(%r10), %rax
	vmovss	(%rax), %xmm12          # xmm12 = mem[0],zero,zero,zero
	movq	144(%r10), %rax
	vmovss	(%rax), %xmm13          # xmm13 = mem[0],zero,zero,zero
	movq	152(%r10), %rax
	vmovss	(%rax), %xmm14          # xmm14 = mem[0],zero,zero,zero
	movq	160(%r10), %rax
	vmovss	(%rax), %xmm15          # xmm15 = mem[0],zero,zero,zero
	movq	168(%r10), %r14
	movq	176(%r10), %r9
	movq	184(%r10), %r11
	movq	192(%r10), %rbx
	movq	200(%r10), %rax
	movzbl	(%rbx), %ebx
	vmovss	(%r14), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	%rax, 88(%rsp)
	movl	%ebx, 80(%rsp)
	movq	%r11, 72(%rsp)
	vmovss	%xmm0, 64(%rsp)
	vmovss	%xmm15, 56(%rsp)
	vmovss	%xmm14, 48(%rsp)
	vmovss	%xmm13, 40(%rsp)
	vmovss	%xmm12, 32(%rsp)
	vmovss	%xmm11, 24(%rsp)
	vmovss	%xmm10, 16(%rsp)
	vmovss	%xmm9, 8(%rsp)
	vmovss	%xmm8, (%rsp)
	vmovss	100(%rsp), %xmm0        # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	callq	sharpi@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r14
	retq
.Lfunc_end170:
	.size	sharpi_argv, .Lfunc_end170-sharpi_argv

	.section	.text.sharpi_metadata,"ax",@progbits
	.globl	sharpi_metadata
	.align	16, 0x90
	.type	sharpi_metadata,@function
sharpi_metadata:                        # @sharpi_metadata
# BB#0:                                 # %entry
	leaq	.Lsharpi_metadata_storage(%rip), %rax
	retq
.Lfunc_end171:
	.size	sharpi_metadata, .Lfunc_end171-sharpi_metadata

	.type	_ZN6Halide7Runtime8Internal13custom_mallocE,@object # @_ZN6Halide7Runtime8Internal13custom_mallocE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal13custom_mallocE
	.align	8
_ZN6Halide7Runtime8Internal13custom_mallocE:
	.quad	_ZN6Halide7Runtime8Internal14default_mallocEPvm
	.size	_ZN6Halide7Runtime8Internal13custom_mallocE, 8

	.type	_ZN6Halide7Runtime8Internal11custom_freeE,@object # @_ZN6Halide7Runtime8Internal11custom_freeE
	.weak	_ZN6Halide7Runtime8Internal11custom_freeE
	.align	8
_ZN6Halide7Runtime8Internal11custom_freeE:
	.quad	_ZN6Halide7Runtime8Internal12default_freeEPvS2_
	.size	_ZN6Halide7Runtime8Internal11custom_freeE, 8

	.type	_ZN6Halide7Runtime8Internal13error_handlerE,@object # @_ZN6Halide7Runtime8Internal13error_handlerE
	.weak	_ZN6Halide7Runtime8Internal13error_handlerE
	.align	8
_ZN6Halide7Runtime8Internal13error_handlerE:
	.quad	_ZN6Halide7Runtime8Internal21default_error_handlerEPvPKc
	.size	_ZN6Halide7Runtime8Internal13error_handlerE, 8

	.type	.L.str,@object          # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"Error: "
	.size	.L.str, 8

	.type	_ZN6Halide7Runtime8Internal12custom_printE,@object # @_ZN6Halide7Runtime8Internal12custom_printE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal12custom_printE
	.align	8
_ZN6Halide7Runtime8Internal12custom_printE:
	.quad	_ZN6Halide7Runtime8Internal17halide_print_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal12custom_printE, 8

	.type	halide_reference_clock_inited,@object # @halide_reference_clock_inited
	.bss
	.weak	halide_reference_clock_inited
halide_reference_clock_inited:
	.byte	0                       # 0x0
	.size	halide_reference_clock_inited, 1

	.type	halide_reference_clock,@object # @halide_reference_clock
	.weak	halide_reference_clock
	.align	8
halide_reference_clock:
	.zero	16
	.size	halide_reference_clock, 16

	.type	.L.str.7,@object        # @.str.7
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.7:
	.asciz	"/tmp/"
	.size	.L.str.7, 6

	.type	.L.str.1,@object        # @.str.1
.L.str.1:
	.asciz	"XXXXXX"
	.size	.L.str.1, 7

	.type	_ZN6Halide7Runtime8Internal10work_queueE,@object # @_ZN6Halide7Runtime8Internal10work_queueE
	.bss
	.weak	_ZN6Halide7Runtime8Internal10work_queueE
	.align	8
_ZN6Halide7Runtime8Internal10work_queueE:
	.zero	800
	.size	_ZN6Halide7Runtime8Internal10work_queueE, 800

	.type	custom_do_task,@object  # @custom_do_task
	.section	.data.rel,"aw",@progbits
	.weak	custom_do_task
	.align	8
custom_do_task:
	.quad	_ZN6Halide7Runtime8Internal15default_do_taskEPvPFiS2_iPhEiS3_
	.size	custom_do_task, 8

	.type	custom_do_par_for,@object # @custom_do_par_for
	.weak	custom_do_par_for
	.align	8
custom_do_par_for:
	.quad	_ZN6Halide7Runtime8Internal18default_do_par_forEPvPFiS2_iPhEiiS3_
	.size	custom_do_par_for, 8

	.section	.dtors,"aw",@progbits
	.align	8
	.quad	halide_thread_pool_cleanup
	.quad	halide_trace_cleanup
	.quad	halide_cache_cleanup
	.quad	halide_profiler_shutdown
	.type	.L.str.8,@object        # @.str.8
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.8:
	.asciz	"HL_NUM_THREADS"
	.size	.L.str.8, 15

	.type	.L.str.1.9,@object      # @.str.1.9
.L.str.1.9:
	.asciz	"HL_NUMTHREADS"
	.size	.L.str.1.9, 14

	.type	.L.str.2,@object        # @.str.2
.L.str.2:
	.asciz	"halide_set_num_threads: must be >= 0."
	.size	.L.str.2, 38

	.type	_ZN6Halide7Runtime8Internal17custom_get_symbolE,@object # @_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal17custom_get_symbolE
	.align	8
_ZN6Halide7Runtime8Internal17custom_get_symbolE:
	.quad	_ZN6Halide7Runtime8Internal22halide_get_symbol_implEPKc
	.size	_ZN6Halide7Runtime8Internal17custom_get_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal19custom_load_libraryE,@object # @_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.weak	_ZN6Halide7Runtime8Internal19custom_load_libraryE
	.align	8
_ZN6Halide7Runtime8Internal19custom_load_libraryE:
	.quad	_ZN6Halide7Runtime8Internal24halide_load_library_implEPKc
	.size	_ZN6Halide7Runtime8Internal19custom_load_libraryE, 8

	.type	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE,@object # @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.weak	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE
	.align	8
_ZN6Halide7Runtime8Internal25custom_get_library_symbolE:
	.quad	_ZN6Halide7Runtime8Internal30halide_get_library_symbol_implEPvPKc
	.size	_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, 8

	.type	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE,@object # @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE
	.align	4
_ZN6Halide7Runtime8Internal17halide_gpu_deviceE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE
_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, 1

	.type	.L.str.10,@object       # @.str.10
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.10:
	.asciz	"HL_GPU_DEVICE"
	.size	.L.str.10, 14

	.type	_ZN6Halide7Runtime8Internal17halide_trace_fileE,@object # @_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17halide_trace_fileE
	.align	4
_ZN6Halide7Runtime8Internal17halide_trace_fileE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal17halide_trace_fileE, 4

	.type	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE,@object # @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.weak	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE
	.align	4
_ZN6Halide7Runtime8Internal22halide_trace_file_lockE:
	.long	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, 4

	.type	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE,@object # @_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
	.weak	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE
_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE, 1

	.type	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE,@object # @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
	.weak	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE
_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE:
	.byte	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, 1

	.type	_ZN6Halide7Runtime8Internal19halide_custom_traceE,@object # @_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal19halide_custom_traceE
	.align	8
_ZN6Halide7Runtime8Internal19halide_custom_traceE:
	.quad	_ZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_t
	.size	_ZN6Halide7Runtime8Internal19halide_custom_traceE, 8

	.type	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids
	.data
	.align	4
_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids:
	.long	1                       # 0x1
	.size	_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE3ids, 4

	.type	.L.str.14,@object       # @.str.14
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.14:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:59 Assert failed: written == total_size && \"Can't write to trace file\"\n"
	.size	.L.str.14, 112

	.type	.L.str.1.15,@object     # @.str.1.15
.L.str.1.15:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:68 Assert failed: print_bits <= 64 && \"Tracing bad type\"\n"
	.size	.L.str.1.15, 98

	.type	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types,@object # @_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types
	.section	.data.rel.ro.local,"aw",@progbits
	.align	8
.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types:
	.quad	.L.str.2.17
	.quad	.L.str.3
	.quad	.L.str.4
	.quad	.L.str.5
	.quad	.L.str.6
	.quad	.L.str.7.18
	.quad	.L.str.8.19
	.quad	.L.str.9
	.quad	.L.str.10.20
	.quad	.L.str.11
	.size	.L_ZZN6Halide7Runtime8Internal13default_traceEPvPK20halide_trace_event_tE11event_types, 80

	.type	.L.str.15,@object       # @.str.15
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.15:
	.asciz	"<"
	.size	.L.str.15, 2

	.type	.L.str.16,@object       # @.str.16
.L.str.16:
	.asciz	">, <"
	.size	.L.str.16, 5

	.type	.L.str.17,@object       # @.str.17
.L.str.17:
	.asciz	", "
	.size	.L.str.17, 3

	.type	.L.str.18,@object       # @.str.18
.L.str.18:
	.asciz	">)"
	.size	.L.str.18, 3

	.type	.L.str.20,@object       # @.str.20
.L.str.20:
	.asciz	" = <"
	.size	.L.str.20, 5

	.type	.L.str.21,@object       # @.str.21
.L.str.21:
	.asciz	" = "
	.size	.L.str.21, 4

	.type	.L.str.22,@object       # @.str.22
.L.str.22:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:136 Assert failed: print_bits >= 16 && \"Tracing a bad type\"\n"
	.size	.L.str.22, 101

	.type	.L.str.23,@object       # @.str.23
.L.str.23:
	.asciz	">"
	.size	.L.str.23, 2

	.type	.L.str.25,@object       # @.str.25
.L.str.25:
	.asciz	"HL_TRACE_FILE"
	.size	.L.str.25, 14

	.type	.L.str.26,@object       # @.str.26
.L.str.26:
	.asciz	"/home/fb/Halide/src/runtime/tracing.cpp:194 Assert failed: (fd > 0) && \"Failed to open trace file\\n\"\n"
	.size	.L.str.26, 102

	.type	.L.str.2.17,@object     # @.str.2.17
.L.str.2.17:
	.asciz	"Load"
	.size	.L.str.2.17, 5

	.type	.L.str.3,@object        # @.str.3
.L.str.3:
	.asciz	"Store"
	.size	.L.str.3, 6

	.type	.L.str.4,@object        # @.str.4
.L.str.4:
	.asciz	"Begin realization"
	.size	.L.str.4, 18

	.type	.L.str.5,@object        # @.str.5
.L.str.5:
	.asciz	"End realization"
	.size	.L.str.5, 16

	.type	.L.str.6,@object        # @.str.6
.L.str.6:
	.asciz	"Produce"
	.size	.L.str.6, 8

	.type	.L.str.7.18,@object     # @.str.7.18
.L.str.7.18:
	.asciz	"End produce"
	.size	.L.str.7.18, 12

	.type	.L.str.8.19,@object     # @.str.8.19
.L.str.8.19:
	.asciz	"Consume"
	.size	.L.str.8.19, 8

	.type	.L.str.9,@object        # @.str.9
.L.str.9:
	.asciz	"End consume"
	.size	.L.str.9, 12

	.type	.L.str.10.20,@object    # @.str.10.20
.L.str.10.20:
	.asciz	"Begin pipeline"
	.size	.L.str.10.20, 15

	.type	.L.str.11,@object       # @.str.11
.L.str.11:
	.asciz	"End pipeline"
	.size	.L.str.11, 13

	.type	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE,@object # @_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.data
	.weak	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE
	.align	2
_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE:
	.short	3                       # 0x3
	.short	3                       # 0x3
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.short	1                       # 0x1
	.short	2                       # 0x2
	.size	_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE, 20

	.type	.L.str.27,@object       # @.str.27
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.27:
	.asciz	"wb"
	.size	.L.str.27, 3

	.type	_ZN6Halide7Runtime8Internal16memoization_lockE,@object # @_ZN6Halide7Runtime8Internal16memoization_lockE
	.bss
	.weak	_ZN6Halide7Runtime8Internal16memoization_lockE
	.align	8
_ZN6Halide7Runtime8Internal16memoization_lockE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal16memoization_lockE, 64

	.type	_ZN6Halide7Runtime8Internal13cache_entriesE,@object # @_ZN6Halide7Runtime8Internal13cache_entriesE
	.weak	_ZN6Halide7Runtime8Internal13cache_entriesE
	.align	8
_ZN6Halide7Runtime8Internal13cache_entriesE:
	.zero	2048
	.size	_ZN6Halide7Runtime8Internal13cache_entriesE, 2048

	.type	_ZN6Halide7Runtime8Internal18most_recently_usedE,@object # @_ZN6Halide7Runtime8Internal18most_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal18most_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal18most_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal18most_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal19least_recently_usedE,@object # @_ZN6Halide7Runtime8Internal19least_recently_usedE
	.weak	_ZN6Halide7Runtime8Internal19least_recently_usedE
	.align	8
_ZN6Halide7Runtime8Internal19least_recently_usedE:
	.quad	0
	.size	_ZN6Halide7Runtime8Internal19least_recently_usedE, 8

	.type	_ZN6Halide7Runtime8Internal14max_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.data
	.weak	_ZN6Halide7Runtime8Internal14max_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal14max_cache_sizeE:
	.quad	1048576                 # 0x100000
	.size	_ZN6Halide7Runtime8Internal14max_cache_sizeE, 8

	.type	_ZN6Halide7Runtime8Internal18current_cache_sizeE,@object # @_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.bss
	.weak	_ZN6Halide7Runtime8Internal18current_cache_sizeE
	.align	8
_ZN6Halide7Runtime8Internal18current_cache_sizeE:
	.quad	0                       # 0x0
	.size	_ZN6Halide7Runtime8Internal18current_cache_sizeE, 8

	.type	.L.str.3.29,@object     # @.str.3.29
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.3.29:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:245 Assert failed: prev_hash_entry != NULL\n"
	.size	.L.str.3.29, 82

	.type	.L.str.4.30,@object     # @.str.4.30
.L.str.4.30:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:335 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.4.30, 85

	.type	.L.str.5.31,@object     # @.str.5.31
.L.str.5.31:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:339 Assert failed: least_recently_used == entry\n"
	.size	.L.str.5.31, 87

	.type	.L.str.6.32,@object     # @.str.6.32
.L.str.6.32:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:342 Assert failed: entry->more_recent != NULL\n"
	.size	.L.str.6.32, 85

	.type	.L.str.8.33,@object     # @.str.8.33
.L.str.8.33:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:433 Assert failed: no_host_pointers_equal\n"
	.size	.L.str.8.33, 81

	.type	.L.str.11.34,@object    # @.str.11.34
.L.str.11.34:
	.asciz	"/home/fb/Halide/src/runtime/cache.cpp:518 Assert failed: entry->in_use_count > 0\n"
	.size	.L.str.11.34, 82

	.type	.L.str.45,@object       # @.str.45
.L.str.45:
	.asciz	"-nan"
	.size	.L.str.45, 5

	.type	.L.str.1.46,@object     # @.str.1.46
.L.str.1.46:
	.asciz	"nan"
	.size	.L.str.1.46, 4

	.type	.L.str.2.47,@object     # @.str.2.47
.L.str.2.47:
	.asciz	"-inf"
	.size	.L.str.2.47, 5

	.type	.L.str.3.48,@object     # @.str.3.48
.L.str.3.48:
	.asciz	"inf"
	.size	.L.str.3.48, 4

	.type	.L.str.4.49,@object     # @.str.4.49
.L.str.4.49:
	.asciz	"-0.000000e+00"
	.size	.L.str.4.49, 14

	.type	.L.str.5.50,@object     # @.str.5.50
.L.str.5.50:
	.asciz	"0.000000e+00"
	.size	.L.str.5.50, 13

	.type	.L.str.6.51,@object     # @.str.6.51
.L.str.6.51:
	.asciz	"-0.000000"
	.size	.L.str.6.51, 10

	.type	.L.str.7.52,@object     # @.str.7.52
.L.str.7.52:
	.asciz	"0.000000"
	.size	.L.str.7.52, 9

	.type	.L.str.8.53,@object     # @.str.8.53
.L.str.8.53:
	.asciz	"-"
	.size	.L.str.8.53, 2

	.type	.L.str.10.55,@object    # @.str.10.55
.L.str.10.55:
	.asciz	"e+"
	.size	.L.str.10.55, 3

	.type	.L.str.11.56,@object    # @.str.11.56
.L.str.11.56:
	.asciz	"e-"
	.size	.L.str.11.56, 3

	.type	.L.str.12.57,@object    # @.str.12.57
.L.str.12.57:
	.asciz	"0123456789abcdef"
	.size	.L.str.12.57, 17

	.type	_ZN6Halide7Runtime8Internal17device_copy_mutexE,@object # @_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.bss
	.weak	_ZN6Halide7Runtime8Internal17device_copy_mutexE
	.align	8
_ZN6Halide7Runtime8Internal17device_copy_mutexE:
	.zero	64
	.size	_ZN6Halide7Runtime8Internal17device_copy_mutexE, 64

	.type	.L.str.25.64,@object    # @.str.25.64
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.25.64:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:138 Assert failed: !buf->host_dirty\n"
	.size	.L.str.25.64, 86

	.type	.L.str.40,@object       # @.str.40
.L.str.40:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:248 Assert failed: buf->dev == 0\n"
	.size	.L.str.40, 83

	.type	.L.str.37,@object       # @.str.37
.L.str.37:
	.asciz	"halide_device_malloc doesn't support switching interfaces\n"
	.size	.L.str.37, 59

	.type	.L.str.42,@object       # @.str.42
.L.str.42:
	.asciz	"halide_device_and_host_malloc doesn't support switching interfaces\n"
	.size	.L.str.42, 68

	.type	.L.str.44,@object       # @.str.44
.L.str.44:
	.asciz	"/home/fb/Halide/src/runtime/device_interface.cpp:321 Assert failed: buf->dev == 0\n"
	.size	.L.str.44, 83

	.type	.L.str.68,@object       # @.str.68
.L.str.68:
	.asciz	"Bounds inference call to external stage "
	.size	.L.str.68, 41

	.type	.L.str.1.69,@object     # @.str.1.69
.L.str.1.69:
	.asciz	" returned non-zero value: "
	.size	.L.str.1.69, 27

	.type	.L.str.53,@object       # @.str.53
.L.str.53:
	.asciz	"Printer buffer allocation failed.\n"
	.size	.L.str.53, 35

	.type	.L.str.2.70,@object     # @.str.2.70
.L.str.2.70:
	.asciz	"Call to external stage "
	.size	.L.str.2.70, 24

	.type	.L.str.3.71,@object     # @.str.3.71
.L.str.3.71:
	.asciz	"Bounds given for "
	.size	.L.str.3.71, 18

	.type	.L.str.4.72,@object     # @.str.4.72
.L.str.4.72:
	.asciz	" in "
	.size	.L.str.4.72, 5

	.type	.L.str.5.73,@object     # @.str.5.73
.L.str.5.73:
	.asciz	" (from "
	.size	.L.str.5.73, 8

	.type	.L.str.6.74,@object     # @.str.6.74
.L.str.6.74:
	.asciz	" to "
	.size	.L.str.6.74, 5

	.type	.L.str.7.75,@object     # @.str.7.75
.L.str.7.75:
	.asciz	") do not cover required region (from "
	.size	.L.str.7.75, 38

	.type	.L.str.8.76,@object     # @.str.8.76
.L.str.8.76:
	.asciz	")"
	.size	.L.str.8.76, 2

	.type	.L.str.9.77,@object     # @.str.9.77
.L.str.9.77:
	.asciz	" has type "
	.size	.L.str.9.77, 11

	.type	.L.str.10.78,@object    # @.str.10.78
.L.str.10.78:
	.asciz	" but elem_size of the buffer passed in is "
	.size	.L.str.10.78, 43

	.type	.L.str.11.79,@object    # @.str.11.79
.L.str.11.79:
	.asciz	" instead of "
	.size	.L.str.11.79, 13

	.type	.L.str.12.80,@object    # @.str.12.80
.L.str.12.80:
	.asciz	" is accessed at "
	.size	.L.str.12.80, 17

	.type	.L.str.13.81,@object    # @.str.13.81
.L.str.13.81:
	.asciz	", which is before the min ("
	.size	.L.str.13.81, 28

	.type	.L.str.14.82,@object    # @.str.14.82
.L.str.14.82:
	.asciz	") in dimension "
	.size	.L.str.14.82, 16

	.type	.L.str.15.83,@object    # @.str.15.83
.L.str.15.83:
	.asciz	", which is beyond the max ("
	.size	.L.str.15.83, 28

	.type	.L.str.16.84,@object    # @.str.16.84
.L.str.16.84:
	.asciz	"Total allocation for buffer "
	.size	.L.str.16.84, 29

	.type	.L.str.17.85,@object    # @.str.17.85
.L.str.17.85:
	.asciz	" is "
	.size	.L.str.17.85, 5

	.type	.L.str.18.86,@object    # @.str.18.86
.L.str.18.86:
	.asciz	", which exceeds the maximum size of "
	.size	.L.str.18.86, 37

	.type	.L.str.19.87,@object    # @.str.19.87
.L.str.19.87:
	.asciz	"The extents for buffer "
	.size	.L.str.19.87, 24

	.type	.L.str.20.88,@object    # @.str.20.88
.L.str.20.88:
	.asciz	" dimension "
	.size	.L.str.20.88, 12

	.type	.L.str.21.89,@object    # @.str.21.89
.L.str.21.89:
	.asciz	" is negative ("
	.size	.L.str.21.89, 15

	.type	.L.str.22.90,@object    # @.str.22.90
.L.str.22.90:
	.asciz	"Product of extents for buffer "
	.size	.L.str.22.90, 31

	.type	.L.str.23.91,@object    # @.str.23.91
.L.str.23.91:
	.asciz	"Applying the constraints on "
	.size	.L.str.23.91, 29

	.type	.L.str.24.92,@object    # @.str.24.92
.L.str.24.92:
	.asciz	" to the required region made it smaller. "
	.size	.L.str.24.92, 42

	.type	.L.str.25.93,@object    # @.str.25.93
.L.str.25.93:
	.asciz	"Required size: "
	.size	.L.str.25.93, 16

	.type	.L.str.26.94,@object    # @.str.26.94
.L.str.26.94:
	.asciz	". "
	.size	.L.str.26.94, 3

	.type	.L.str.27.95,@object    # @.str.27.95
.L.str.27.95:
	.asciz	"Constrained size: "
	.size	.L.str.27.95, 19

	.type	.L.str.28,@object       # @.str.28
.L.str.28:
	.asciz	"."
	.size	.L.str.28, 2

	.type	.L.str.29,@object       # @.str.29
.L.str.29:
	.asciz	"Constraint violated: "
	.size	.L.str.29, 22

	.type	.L.str.30,@object       # @.str.30
.L.str.30:
	.asciz	" ("
	.size	.L.str.30, 3

	.type	.L.str.31,@object       # @.str.31
.L.str.31:
	.asciz	") == "
	.size	.L.str.31, 6

	.type	.L.str.32,@object       # @.str.32
.L.str.32:
	.asciz	"Parameter "
	.size	.L.str.32, 11

	.type	.L.str.33,@object       # @.str.33
.L.str.33:
	.asciz	" but must be at least "
	.size	.L.str.33, 23

	.type	.L.str.34,@object       # @.str.34
.L.str.34:
	.asciz	" but must be at most "
	.size	.L.str.34, 22

	.type	.L.str.35,@object       # @.str.35
.L.str.35:
	.asciz	"Out of memory (halide_malloc returned NULL)"
	.size	.L.str.35, 44

	.type	.L.str.36,@object       # @.str.36
.L.str.36:
	.asciz	"Buffer argument "
	.size	.L.str.36, 17

	.type	.L.str.37.96,@object    # @.str.37.96
.L.str.37.96:
	.asciz	" is NULL"
	.size	.L.str.37.96, 9

	.type	.L.str.38,@object       # @.str.38
.L.str.38:
	.asciz	"Failed to dump function "
	.size	.L.str.38, 25

	.type	.L.str.39,@object       # @.str.39
.L.str.39:
	.asciz	" to file "
	.size	.L.str.39, 10

	.type	.L.str.40.97,@object    # @.str.40.97
.L.str.40.97:
	.asciz	" with error "
	.size	.L.str.40.97, 13

	.type	.L.str.41,@object       # @.str.41
.L.str.41:
	.asciz	"The host pointer of "
	.size	.L.str.41, 21

	.type	.L.str.42.98,@object    # @.str.42.98
.L.str.42.98:
	.asciz	" is not aligned to a "
	.size	.L.str.42.98, 22

	.type	.L.str.43,@object       # @.str.43
.L.str.43:
	.asciz	" bytes boundary."
	.size	.L.str.43, 17

	.type	.L.str.44.99,@object    # @.str.44.99
.L.str.44.99:
	.asciz	"The folded storage dimension "
	.size	.L.str.44.99, 30

	.type	.L.str.45.100,@object   # @.str.45.100
.L.str.45.100:
	.asciz	" of "
	.size	.L.str.45.100, 5

	.type	.L.str.46,@object       # @.str.46
.L.str.46:
	.asciz	" was accessed out of order by loop "
	.size	.L.str.46, 36

	.type	.L.str.47,@object       # @.str.47
.L.str.47:
	.asciz	"The fold factor ("
	.size	.L.str.47, 18

	.type	.L.str.48,@object       # @.str.48
.L.str.48:
	.asciz	") of dimension "
	.size	.L.str.48, 16

	.type	.L.str.49,@object       # @.str.49
.L.str.49:
	.asciz	" is too small to store the required region accessed by loop "
	.size	.L.str.49, 61

	.type	.L.str.50,@object       # @.str.50
.L.str.50:
	.asciz	")."
	.size	.L.str.50, 3

	.type	.L.str.51,@object       # @.str.51
.L.str.51:
	.asciz	"Requirement Failed: ("
	.size	.L.str.51, 22

	.type	.L.str.52,@object       # @.str.52
.L.str.52:
	.asciz	") "
	.size	.L.str.52, 3

	.type	_ZZ25halide_profiler_get_stateE1s,@object # @_ZZ25halide_profiler_get_stateE1s
	.data
	.align	8
_ZZ25halide_profiler_get_stateE1s:
	.zero	64
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	0                       # 0x0
	.long	0                       # 0x0
	.quad	0
	.quad	0
	.byte	0                       # 0x0
	.zero	7
	.size	_ZZ25halide_profiler_get_stateE1s, 104

	.type	.L.str.102,@object      # @.str.102
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.102:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:204 Assert failed: p_stats != NULL\n"
	.size	.L.str.102, 77

	.type	.L.str.1.103,@object    # @.str.1.103
.L.str.1.103:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:231 Assert failed: p_stats != NULL\n"
	.size	.L.str.1.103, 77

	.type	.L.str.2.104,@object    # @.str.2.104
.L.str.2.104:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:232 Assert failed: func_id >= 0\n"
	.size	.L.str.2.104, 74

	.type	.L.str.3.105,@object    # @.str.3.105
.L.str.3.105:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:233 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.3.105, 90

	.type	.L.str.4.106,@object    # @.str.4.106
.L.str.4.106:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:267 Assert failed: p_stats != NULL\n"
	.size	.L.str.4.106, 77

	.type	.L.str.5.107,@object    # @.str.5.107
.L.str.5.107:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:268 Assert failed: func_id >= 0\n"
	.size	.L.str.5.107, 74

	.type	.L.str.6.108,@object    # @.str.6.108
.L.str.6.108:
	.asciz	"/home/fb/Halide/src/runtime/profiler.cpp:269 Assert failed: func_id < p_stats->num_funcs\n"
	.size	.L.str.6.108, 90

	.type	.L.str.7.109,@object    # @.str.7.109
.L.str.7.109:
	.asciz	"\n"
	.size	.L.str.7.109, 2

	.type	.L.str.8.110,@object    # @.str.8.110
.L.str.8.110:
	.asciz	" total time: "
	.size	.L.str.8.110, 14

	.type	.L.str.9.111,@object    # @.str.9.111
.L.str.9.111:
	.asciz	" ms"
	.size	.L.str.9.111, 4

	.type	.L.str.10.112,@object   # @.str.10.112
.L.str.10.112:
	.asciz	"  samples: "
	.size	.L.str.10.112, 12

	.type	.L.str.11.113,@object   # @.str.11.113
.L.str.11.113:
	.asciz	"  runs: "
	.size	.L.str.11.113, 9

	.type	.L.str.12.114,@object   # @.str.12.114
.L.str.12.114:
	.asciz	"  time/run: "
	.size	.L.str.12.114, 13

	.type	.L.str.13.115,@object   # @.str.13.115
.L.str.13.115:
	.asciz	" ms\n"
	.size	.L.str.13.115, 5

	.type	.L.str.14.116,@object   # @.str.14.116
.L.str.14.116:
	.asciz	" average threads used: "
	.size	.L.str.14.116, 24

	.type	.L.str.15.117,@object   # @.str.15.117
.L.str.15.117:
	.asciz	" heap allocations: "
	.size	.L.str.15.117, 20

	.type	.L.str.16.118,@object   # @.str.16.118
.L.str.16.118:
	.asciz	"  peak heap usage: "
	.size	.L.str.16.118, 20

	.type	.L.str.17.119,@object   # @.str.17.119
.L.str.17.119:
	.asciz	" bytes\n"
	.size	.L.str.17.119, 8

	.type	.L.str.18.120,@object   # @.str.18.120
.L.str.18.120:
	.asciz	"  "
	.size	.L.str.18.120, 3

	.type	.L.str.19.121,@object   # @.str.19.121
.L.str.19.121:
	.asciz	": "
	.size	.L.str.19.121, 3

	.type	.L.str.20.122,@object   # @.str.20.122
.L.str.20.122:
	.asciz	" "
	.size	.L.str.20.122, 2

	.type	.L.str.21.123,@object   # @.str.21.123
.L.str.21.123:
	.asciz	"ms"
	.size	.L.str.21.123, 3

	.type	.L.str.22.124,@object   # @.str.22.124
.L.str.22.124:
	.asciz	"("
	.size	.L.str.22.124, 2

	.type	.L.str.23.125,@object   # @.str.23.125
.L.str.23.125:
	.asciz	"%)"
	.size	.L.str.23.125, 3

	.type	.L.str.24.126,@object   # @.str.24.126
.L.str.24.126:
	.asciz	"threads: "
	.size	.L.str.24.126, 10

	.type	.L.str.25.127,@object   # @.str.25.127
.L.str.25.127:
	.asciz	" peak: "
	.size	.L.str.25.127, 8

	.type	.L.str.26.128,@object   # @.str.26.128
.L.str.26.128:
	.asciz	" num: "
	.size	.L.str.26.128, 7

	.type	.L.str.27.129,@object   # @.str.27.129
.L.str.27.129:
	.asciz	" avg: "
	.size	.L.str.27.129, 7

	.type	.L.str.28.130,@object   # @.str.28.130
.L.str.28.130:
	.asciz	" stack: "
	.size	.L.str.28.130, 9

	.type	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE,@object # @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.section	.data.rel,"aw",@progbits
	.weak	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE
	.align	8
_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE:
	.quad	halide_default_can_use_target_features
	.size	_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, 8

	.type	_ZZ38halide_default_can_use_target_featuresE11initialized,@object # @_ZZ38halide_default_can_use_target_featuresE11initialized
	.local	_ZZ38halide_default_can_use_target_featuresE11initialized
	.comm	_ZZ38halide_default_can_use_target_featuresE11initialized,1,1
	.type	_ZZ38halide_default_can_use_target_featuresE12cpu_features,@object # @_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.local	_ZZ38halide_default_can_use_target_featuresE12cpu_features
	.comm	_ZZ38halide_default_can_use_target_featuresE12cpu_features,16,8
	.type	.Lstr,@object           # @str
	.section	.rodata,"a",@progbits
	.align	32
.Lstr:
	.asciz	"p0"
	.size	.Lstr, 3

	.type	.Lstr.137,@object       # @str.137
	.align	32
.Lstr.137:
	.asciz	"vignetteH"
	.size	.Lstr.137, 10

	.type	.Lstr.138,@object       # @str.138
	.align	32
.Lstr.138:
	.asciz	"vignetteV"
	.size	.Lstr.138, 10

	.type	.Lstr.139,@object       # @str.139
	.align	32
.Lstr.139:
	.asciz	"ccm"
	.size	.Lstr.139, 4

	.type	.Lstr.140,@object       # @str.140
	.align	32
.Lstr.140:
	.asciz	"toneTable"
	.size	.Lstr.140, 10

	.type	.Lstr.141,@object       # @str.141
	.align	32
.Lstr.141:
	.asciz	"sharpi"
	.size	.Lstr.141, 7

	.type	.Lstr.142,@object       # @str.142
	.align	32
.Lstr.142:
	.asciz	"Input buffer ccm"
	.size	.Lstr.142, 17

	.type	.Lstr.143,@object       # @str.143
	.align	32
.Lstr.143:
	.asciz	"float32"
	.size	.Lstr.143, 8

	.type	.Lstr.144,@object       # @str.144
	.align	32
.Lstr.144:
	.asciz	"Input buffer p0"
	.size	.Lstr.144, 16

	.type	.Lstr.145,@object       # @str.145
	.align	32
.Lstr.145:
	.asciz	"uint16"
	.size	.Lstr.145, 7

	.type	.Lstr.146,@object       # @str.146
	.align	32
.Lstr.146:
	.asciz	"Output buffer sharpi"
	.size	.Lstr.146, 21

	.type	.Lstr.147,@object       # @str.147
	.align	32
.Lstr.147:
	.asciz	"uint8"
	.size	.Lstr.147, 6

	.type	.Lstr.148,@object       # @str.148
	.align	32
.Lstr.148:
	.asciz	"Input buffer toneTable"
	.size	.Lstr.148, 23

	.type	.Lstr.149,@object       # @str.149
	.align	32
.Lstr.149:
	.asciz	"Input buffer vignetteH"
	.size	.Lstr.149, 23

	.type	.Lstr.150,@object       # @str.150
	.align	32
.Lstr.150:
	.asciz	"Input buffer vignetteV"
	.size	.Lstr.150, 23

	.type	.Lstr.151,@object       # @str.151
	.align	32
.Lstr.151:
	.asciz	"ccm.stride.0"
	.size	.Lstr.151, 13

	.type	.Lstr.152,@object       # @str.152
	.align	32
.Lstr.152:
	.asciz	"1"
	.size	.Lstr.152, 2

	.type	.Lstr.153,@object       # @str.153
	.align	32
.Lstr.153:
	.asciz	"p0.stride.0"
	.size	.Lstr.153, 12

	.type	.Lstr.154,@object       # @str.154
	.align	32
.Lstr.154:
	.asciz	"sharpi.stride.0"
	.size	.Lstr.154, 16

	.type	.Lstr.155,@object       # @str.155
	.align	32
.Lstr.155:
	.asciz	"3"
	.size	.Lstr.155, 2

	.type	.Lstr.156,@object       # @str.156
	.align	32
.Lstr.156:
	.asciz	"sharpi.stride.2"
	.size	.Lstr.156, 16

	.type	.Lstr.157,@object       # @str.157
	.align	32
.Lstr.157:
	.asciz	"sharpi.min.2"
	.size	.Lstr.157, 13

	.type	.Lstr.158,@object       # @str.158
	.align	32
.Lstr.158:
	.asciz	"0"
	.size	.Lstr.158, 2

	.type	.Lstr.159,@object       # @str.159
	.align	32
.Lstr.159:
	.asciz	"sharpi.extent.2"
	.size	.Lstr.159, 16

	.type	.Lstr.160,@object       # @str.160
	.align	32
.Lstr.160:
	.asciz	"toneTable.stride.0"
	.size	.Lstr.160, 19

	.type	.Lstr.161,@object       # @str.161
	.align	32
.Lstr.161:
	.asciz	"vignetteH.stride.0"
	.size	.Lstr.161, 19

	.type	.Lstr.162,@object       # @str.162
	.align	32
.Lstr.162:
	.asciz	"vignetteV.stride.0"
	.size	.Lstr.162, 19

	.type	.Lstr.163,@object       # @str.163
	.align	32
.Lstr.163:
	.asciz	"f0"
	.size	.Lstr.163, 3

	.type	.Lstr.164,@object       # @str.164
	.align	32
.Lstr.164:
	.asciz	"deinterleaved$1"
	.size	.Lstr.164, 16

	.type	.Lstr.165,@object       # @str.165
	.align	32
.Lstr.165:
	.asciz	"gH"
	.size	.Lstr.165, 3

	.type	.Lstr.167,@object       # @str.167
	.align	32
.Lstr.167:
	.asciz	"dV"
	.size	.Lstr.167, 3

	.type	.Lstr.169,@object       # @str.169
	.align	32
.Lstr.169:
	.asciz	"f4"
	.size	.Lstr.169, 3

	.type	.Lstr.170,@object       # @str.170
	.align	32
.Lstr.170:
	.asciz	"f7"
	.size	.Lstr.170, 3

	.type	.Lstr.179,@object       # @str.179
	.align	32
.Lstr.179:
	.asciz	"transpose"
	.size	.Lstr.179, 10

	.type	.Lstr.180,@object       # @str.180
	.align	32
.Lstr.180:
	.asciz	"blur"
	.size	.Lstr.180, 5

	.type	.Lstr.181,@object       # @str.181
	.align	32
.Lstr.181:
	.asciz	"transpose$1"
	.size	.Lstr.181, 12

	.type	.Lstr.182,@object       # @str.182
	.align	32
.Lstr.182:
	.asciz	"blur$1"
	.size	.Lstr.182, 7

	.type	.Lstr.183,@object       # @str.183
	.align	32
.Lstr.183:
	.asciz	"p1"
	.size	.Lstr.183, 3

	.type	.L__unnamed_1,@object   # @0
	.align	4
.L__unnamed_1:
	.long	0                       # 0x0
	.size	.L__unnamed_1, 4

	.type	.Lstr.184,@object       # @str.184
	.align	32
.Lstr.184:
	.asciz	"p2"
	.size	.Lstr.184, 3

	.type	.L__unnamed_2,@object   # @1
	.align	4
.L__unnamed_2:
	.long	0                       # 0x0
	.size	.L__unnamed_2, 4

	.type	.Lstr.185,@object       # @str.185
	.align	32
.Lstr.185:
	.asciz	"blackLevelR"
	.size	.Lstr.185, 12

	.type	.L__unnamed_3,@object   # @2
	.align	4
.L__unnamed_3:
	.long	0                       # float 0
	.size	.L__unnamed_3, 4

	.type	.Lstr.186,@object       # @str.186
	.align	32
.Lstr.186:
	.asciz	"blackLevelG"
	.size	.Lstr.186, 12

	.type	.L__unnamed_4,@object   # @3
	.align	4
.L__unnamed_4:
	.long	0                       # float 0
	.size	.L__unnamed_4, 4

	.type	.Lstr.187,@object       # @str.187
	.align	32
.Lstr.187:
	.asciz	"blackLevelB"
	.size	.Lstr.187, 12

	.type	.L__unnamed_5,@object   # @4
	.align	4
.L__unnamed_5:
	.long	0                       # float 0
	.size	.L__unnamed_5, 4

	.type	.Lstr.188,@object       # @str.188
	.align	32
.Lstr.188:
	.asciz	"whiteBalanceGainR"
	.size	.Lstr.188, 18

	.type	.L__unnamed_6,@object   # @5
	.align	4
.L__unnamed_6:
	.long	0                       # float 0
	.size	.L__unnamed_6, 4

	.type	.Lstr.189,@object       # @str.189
	.align	32
.Lstr.189:
	.asciz	"whiteBalanceGainG"
	.size	.Lstr.189, 18

	.type	.L__unnamed_7,@object   # @6
	.align	4
.L__unnamed_7:
	.long	0                       # float 0
	.size	.L__unnamed_7, 4

	.type	.Lstr.190,@object       # @str.190
	.align	32
.Lstr.190:
	.asciz	"whiteBalanceGainB"
	.size	.Lstr.190, 18

	.type	.L__unnamed_8,@object   # @7
	.align	4
.L__unnamed_8:
	.long	0                       # float 0
	.size	.L__unnamed_8, 4

	.type	.Lstr.191,@object       # @str.191
	.align	32
.Lstr.191:
	.asciz	"clampMinR"
	.size	.Lstr.191, 10

	.type	.L__unnamed_9,@object   # @8
	.align	4
.L__unnamed_9:
	.long	0                       # float 0
	.size	.L__unnamed_9, 4

	.type	.Lstr.192,@object       # @str.192
	.align	32
.Lstr.192:
	.asciz	"clampMinG"
	.size	.Lstr.192, 10

	.type	.L__unnamed_10,@object  # @9
	.align	4
.L__unnamed_10:
	.long	0                       # float 0
	.size	.L__unnamed_10, 4

	.type	.Lstr.193,@object       # @str.193
	.align	32
.Lstr.193:
	.asciz	"clampMinB"
	.size	.Lstr.193, 10

	.type	.L__unnamed_11,@object  # @10
	.align	4
.L__unnamed_11:
	.long	0                       # float 0
	.size	.L__unnamed_11, 4

	.type	.Lstr.194,@object       # @str.194
	.align	32
.Lstr.194:
	.asciz	"clampMaxR"
	.size	.Lstr.194, 10

	.type	.L__unnamed_12,@object  # @11
	.align	4
.L__unnamed_12:
	.long	0                       # float 0
	.size	.L__unnamed_12, 4

	.type	.Lstr.195,@object       # @str.195
	.align	32
.Lstr.195:
	.asciz	"clampMaxG"
	.size	.Lstr.195, 10

	.type	.L__unnamed_13,@object  # @12
	.align	4
.L__unnamed_13:
	.long	0                       # float 0
	.size	.L__unnamed_13, 4

	.type	.Lstr.196,@object       # @str.196
	.align	32
.Lstr.196:
	.asciz	"clampMaxB"
	.size	.Lstr.196, 10

	.type	.L__unnamed_14,@object  # @13
	.align	4
.L__unnamed_14:
	.long	0                       # float 0
	.size	.L__unnamed_14, 4

	.type	.Lstr.197,@object       # @str.197
	.align	32
.Lstr.197:
	.asciz	"sharpenningR"
	.size	.Lstr.197, 13

	.type	.L__unnamed_15,@object  # @14
	.align	4
.L__unnamed_15:
	.long	0                       # float 0
	.size	.L__unnamed_15, 4

	.type	.Lstr.198,@object       # @str.198
	.align	32
.Lstr.198:
	.asciz	"sharpenningG"
	.size	.Lstr.198, 13

	.type	.L__unnamed_16,@object  # @15
	.align	4
.L__unnamed_16:
	.long	0                       # float 0
	.size	.L__unnamed_16, 4

	.type	.Lstr.199,@object       # @str.199
	.align	32
.Lstr.199:
	.asciz	"sharpenninngB"
	.size	.Lstr.199, 14

	.type	.L__unnamed_17,@object  # @16
	.align	4
.L__unnamed_17:
	.long	0                       # float 0
	.size	.L__unnamed_17, 4

	.type	.Lstr.200,@object       # @str.200
	.align	32
.Lstr.200:
	.asciz	"sharpeningSupport"
	.size	.Lstr.200, 18

	.type	.L__unnamed_18,@object  # @17
	.align	4
.L__unnamed_18:
	.long	0                       # float 0
	.size	.L__unnamed_18, 4

	.type	.Lstr.201,@object       # @str.201
	.align	32
.Lstr.201:
	.asciz	"noiseCore"
	.size	.Lstr.201, 10

	.type	.L__unnamed_19,@object  # @18
	.align	4
.L__unnamed_19:
	.long	0                       # float 0
	.size	.L__unnamed_19, 4

	.type	.Lstr.202,@object       # @str.202
	.align	32
.Lstr.202:
	.asciz	"p3"
	.size	.Lstr.202, 3

	.type	.L__unnamed_20,@object  # @19
.L__unnamed_20:
	.byte	0                       # 0x0
	.size	.L__unnamed_20, 1

	.type	.L__unnamed_21,@object  # @20
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.L__unnamed_21:
	.quad	.Lstr
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	16                      # 0x10
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.183
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_1
	.quad	0
	.quad	0
	.quad	.Lstr.184
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	0                       # 0x0
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_2
	.quad	0
	.quad	0
	.quad	.Lstr.137
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.138
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.185
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_3
	.quad	0
	.quad	0
	.quad	.Lstr.186
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_4
	.quad	0
	.quad	0
	.quad	.Lstr.187
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_5
	.quad	0
	.quad	0
	.quad	.Lstr.188
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_6
	.quad	0
	.quad	0
	.quad	.Lstr.189
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_7
	.quad	0
	.quad	0
	.quad	.Lstr.190
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_8
	.quad	0
	.quad	0
	.quad	.Lstr.191
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_9
	.quad	0
	.quad	0
	.quad	.Lstr.192
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_10
	.quad	0
	.quad	0
	.quad	.Lstr.193
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_11
	.quad	0
	.quad	0
	.quad	.Lstr.194
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_12
	.quad	0
	.quad	0
	.quad	.Lstr.195
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_13
	.quad	0
	.quad	0
	.quad	.Lstr.196
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_14
	.quad	0
	.quad	0
	.quad	.Lstr.197
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_15
	.quad	0
	.quad	0
	.quad	.Lstr.198
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_16
	.quad	0
	.quad	0
	.quad	.Lstr.199
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_17
	.quad	0
	.quad	0
	.quad	.Lstr.200
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_18
	.quad	0
	.quad	0
	.quad	.Lstr.201
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_19
	.quad	0
	.quad	0
	.quad	.Lstr.139
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	2                       # 0x2
	.byte	32                      # 0x20
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.140
	.long	1                       # 0x1
	.long	2                       # 0x2
	.byte	1                       # 0x1
	.byte	8                       # 0x8
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.202
	.long	0                       # 0x0
	.long	0                       # 0x0
	.byte	1                       # 0x1
	.byte	1                       # 0x1
	.short	1                       # 0x1
	.zero	4
	.quad	.L__unnamed_20
	.quad	0
	.quad	0
	.quad	.Lstr.141
	.long	2                       # 0x2
	.long	3                       # 0x3
	.byte	1                       # 0x1
	.byte	8                       # 0x8
	.short	1                       # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.size	.L__unnamed_21, 1248

	.type	.Lstr.203,@object       # @str.203
	.section	.rodata,"a",@progbits
	.align	32
.Lstr.203:
	.asciz	"x86-64-linux-avx-avx2-f16c-fma-sse41"
	.size	.Lstr.203, 37

	.type	.Lsharpi_metadata_storage,@object # @sharpi_metadata_storage
	.section	.data.rel.ro.local,"aw",@progbits
	.align	16
.Lsharpi_metadata_storage:
	.long	0                       # 0x0
	.long	26                      # 0x1a
	.quad	.L__unnamed_21
	.quad	.Lstr.203
	.quad	.Lstr.141
	.size	.Lsharpi_metadata_storage, 32


	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.ident	"clang version 3.7.1 (branches/release_37 294077)"
	.section	".note.GNU-stack","",@progbits
